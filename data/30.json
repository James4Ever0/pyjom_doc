{
    "3000": {
        "file_id": 341,
        "content": "/tests/bilibili_practices/bilibili_tarot/functional_voice_with_pictures.py",
        "type": "filepath"
    },
    "3001": {
        "file_id": 341,
        "content": "This code generates typography for videos using TTS, external tools, and FFMPEG, performing directory operations, merging audio/video, exporting, calculating tempo, and applying it to the audio track.",
        "type": "summary"
    },
    "3002": {
        "file_id": 341,
        "content": "import os\nfrom test_common import *\nimport shutil\ndef split_sentences(sent):\n    spliters = \"\\n，。、？： \"\n    cursent = \"\"\n    results = []\n    for elem in sent:\n        cursent += elem\n        if elem in spliters:\n            results.append(cursent)\n            cursent = \"\"\n    if len(cursent) > 0:\n        results.append(cursent)\n    return results\ndef get_speech(sent,output):\n    assert output.endswith(\".wav\")\n    os.system(\"bash kill_pdspc.sh\")\n    with open(\"temp.txt\", \"w+\",encoding=\"utf-8\") as f:\n        f.write(sent.replace(\"\\n\",\"\")) # important.\n    os.system(\"cat temp.txt | paddlespeech tts --output {}\".format(output))\nfrom pydub import AudioSegment\nfrom functional_gen_typo_video_seq import gen_video\n# import matplotlib # doing this before importing moviepy editor. or we will fail.\n# matplotlib.use(\"TkAgg\")\n# from moviepy.editor import VideoFileClip\n# cannot mix moviepy with vidpy or we get fucked.\nfrom MediaInfo import MediaInfo\ndef merge_audio(asegs):\n    audio_3 = AudioSegment.empty() #shit\n    for seg in asegs:",
        "type": "code",
        "location": "/tests/bilibili_practices/bilibili_tarot/functional_voice_with_pictures.py:1-35"
    },
    "3003": {
        "file_id": 341,
        "content": "The code imports necessary libraries and defines functions for handling sentences, obtaining speech output, and merging audio segments. It also sets up a function that generates a video using the functional_gen_typo_video_seq module. The code uses bash scripts and external tools like PaddleSpeech and MediaInfo to manipulate text-to-speech and audio/video files.",
        "type": "comment"
    },
    "3004": {
        "file_id": 341,
        "content": "        try:\n            audio_3 = audio_3.append(seg,crossfade=100) # also shit.\n        except:\n            audio_3 = audio_3.append(seg,crossfade=0) # also shit.\n    return audio_3\n    # audio_3.export(\"audio_3.wav\", format=\"wav\")\ndef gen_typography_part2(intro_text, bgm_path,target_video):\n    # intro_text = \"\"\"塔罗牌，由“TAROT”一词音译而来，被称为“大自然的奥秘库”。抽取一张塔罗牌，今天的你会是怎样的呢？\"\"\"\n    os.system(\"bash kill_pdspc.sh\")\n    sents = split_sentences(intro_text)\n    # breakpoint()\n    voice_dir = \"voice\"\n    video_dir = \"video\"\n    os.system(\"rm -rf {}\".format(voice_dir))\n    os.system(\"rm -rf {}\".format(video_dir))\n    os.mkdir(\"{}\".format(voice_dir))\n    os.mkdir(\"{}\".format(video_dir))\n    index = 0\n    voice_clips = []\n    video_names = []\n    for i,sent in enumerate(sents):\n        print(\"READING:\",sent)\n        aname = \"{}/{}.wav\".format(voice_dir,i)\n        get_speech(sent,aname)\n        lsent = len(sent)\n        # if no audio then just skip.\n        if not os.path.exists(aname):\n            index += lsent\n            continue",
        "type": "code",
        "location": "/tests/bilibili_practices/bilibili_tarot/functional_voice_with_pictures.py:36-66"
    },
    "3005": {
        "file_id": 341,
        "content": "This code is attempting to generate typography for a video using voice and pictures. It first clears the existing voice and video directories, then creates new ones. It splits the input text into sentences, and for each sentence, it attempts to get speech audio from that sentence and create a corresponding picture. If no audio is found, it skips that sentence. Finally, it returns the generated audio.",
        "type": "comment"
    },
    "3006": {
        "file_id": 341,
        "content": "        seg = AudioSegment.from_wav(aname)\n        duration = seg.duration_seconds\n        voice_clips.append(seg)\n        # get the duration you fuck.\n        # breakpoint()\n        current_indexs = list(range(index,index+lsent))\n        # you can generate video for it.\n        index += lsent\n        vname = \"{}/{}.mp4\".format(video_dir,i)\n        gen_video(vname,current_indexs,duration) # where from?\n        video_names.append(vname)\n    # and finally?\n    final_video = \"{}/final_video.mp4\".format(video_dir)\n    final_audio = \"{}/final_audio.wav\".format(voice_dir)\n    audio_merged = merge_audio(voice_clips)\n    # bgm_path = \"/root/Desktop/works/bilibili_tarot/some_bgm.mp3\"\n    bgm = AudioSegment.from_mp3(bgm_path)\n    # duration2 = audio_merged.duration_seconds\n    # bgm = bgm[:duration2*1000] # really?\n    # breakpoint()\n    # audio_merged = audio_merged.overlay(audio_merged,bgm,loop=True)  #wtf?\n    audio_merged = audio_merged.overlay(bgm,loop=True)\n    # audio_merged = audio_merged.normalize()\n    # is it needed?",
        "type": "code",
        "location": "/tests/bilibili_practices/bilibili_tarot/functional_voice_with_pictures.py:67-91"
    },
    "3007": {
        "file_id": 341,
        "content": "This code generates videos for each segment of audio and appends the video names to a list. It then combines all audio clips into one merged audio file, overlays background music, and saves the final audio and video files. The code also includes debugging tools like breakpoint() to help with troubleshooting.",
        "type": "comment"
    },
    "3008": {
        "file_id": 341,
        "content": "    # shit.\n    audio_merged.export(final_audio, format=\"wav\")\n    final_video2 = \"{}/final_video2.mp4\".format(video_dir)\n    with open(\"mylist.txt\",\"w+\") as f:\n        for n in video_names:\n            f.write(\"file \"+n+\"\\n\")\n    os.system(\"ffmpeg -f concat -safe 0 -i mylist.txt -c copy {}\".format(final_video))\n    # output_length = VideoFileClip(final_video).duration\n    output_length = MediaInfo(filename=final_video).getInfo()[\"videoDuration\"]\n    output_length = float(output_length)\n    input_length = AudioSegment.from_wav(final_audio).duration_seconds\n    tempo = input_length/output_length\n    t_a,t_b = tempo.as_integer_ratio()\n    os.system('ffmpeg -i {} -i {} -c:v copy -c:a aac -filter:a \"atempo={}/{}\" -map 0:v:0 -map 1:a:0 {}'.format(final_video,final_audio,t_a,t_b,final_video2))\n    shutil.move(final_video2,target_video)\ndef gen_typography_part3(intro_text, target_video): #slient\n    # intro_text = \"\"\"塔罗牌，由“TAROT”一词音译而来，被称为“大自然的奥秘库”。抽取一张塔罗牌，今天的你会是怎样的呢？\"\"\"\n    os.system(\"bash kill_pdspc.sh\")\n    sents = split_sentences(intro_text)",
        "type": "code",
        "location": "/tests/bilibili_practices/bilibili_tarot/functional_voice_with_pictures.py:92-114"
    },
    "3009": {
        "file_id": 341,
        "content": "This code performs video and audio processing, using ffmpeg commands to merge and manipulate the files. It exports an audio file in wav format, creates a mylist.txt file with video names, concatenates videos using ffmpeg, calculates the tempo between audio and video duration, applies the tempo to the final audio track, and finally moves the final video to the target location. This function also includes a shell command to kill pdspc process when finished.",
        "type": "comment"
    },
    "3010": {
        "file_id": 341,
        "content": "    # breakpoint()\n    voice_dir = \"voice\"\n    video_dir = \"video\"\n    os.system(\"rm -rf {}\".format(voice_dir))\n    os.system(\"rm -rf {}\".format(video_dir))\n    os.mkdir(\"{}\".format(voice_dir))\n    os.mkdir(\"{}\".format(video_dir))\n    index = 0\n    voice_clips = []\n    video_names = []\n    for i,sent in enumerate(sents):\n        print(\"READING:\",sent)\n        aname = \"{}/{}.wav\".format(voice_dir,i)\n        get_speech(sent,aname)\n        lsent = len(sent)\n        # if no audio then just skip.\n        if not os.path.exists(aname):\n            index += lsent\n            continue\n        seg = AudioSegment.from_wav(aname)\n        duration = seg.duration_seconds\n        voice_clips.append(seg)\n        # get the duration you fuck.\n        # breakpoint()\n        current_indexs = list(range(index,index+lsent))\n        # you can generate video for it.\n        index += lsent\n        vname = \"{}/{}.mp4\".format(video_dir,i)\n        gen_video(vname,current_indexs,duration) # where from?\n        video_names.append(vname)\n    # and finally?",
        "type": "code",
        "location": "/tests/bilibili_practices/bilibili_tarot/functional_voice_with_pictures.py:115-147"
    },
    "3011": {
        "file_id": 341,
        "content": "This code removes existing voice and video directories, creates new ones, reads sentences, saves corresponding audio files for each sentence, checks if audio files are generated correctly, generates videos based on the sentences and their respective positions in the text, and stores the names of generated videos.",
        "type": "comment"
    },
    "3012": {
        "file_id": 341,
        "content": "    final_video = \"{}/final_video.mp4\".format(video_dir)\n    final_audio = \"{}/final_audio.wav\".format(voice_dir)\n    audio_merged = merge_audio(voice_clips)\n    # bgm_path = \"/root/Desktop/works/bilibili_tarot/some_bgm.mp3\"\n    # bgm = AudioSegment.from_mp3(bgm_path)\n    # duration2 = audio_merged.duration_seconds\n    # bgm = bgm[:duration2*1000] # really?\n    # breakpoint()\n    # audio_merged = audio_merged.overlay(audio_merged,bgm,loop=True)  #wtf?\n    # audio_merged = audio_merged.overlay(bgm,loop=True)\n    # audio_merged = audio_merged.normalize()\n    # is it needed?\n    # shit.\n    audio_merged.export(final_audio, format=\"wav\")\n    final_video2 = \"{}/final_video2.mp4\".format(video_dir)\n    with open(\"mylist.txt\",\"w+\") as f:\n        for n in video_names:\n            f.write(\"file \"+n+\"\\n\")\n    os.system(\"ffmpeg -f concat -safe 0 -i mylist.txt -c copy {}\".format(final_video))\n    # output_length = VideoFileClip(final_video).duration\n    output_length = MediaInfo(filename=final_video).getInfo()[\"videoDuration\"]",
        "type": "code",
        "location": "/tests/bilibili_practices/bilibili_tarot/functional_voice_with_pictures.py:148-170"
    },
    "3013": {
        "file_id": 341,
        "content": "This code is performing audio and video merging, exporting the final audio file, creating a mylist.txt file for ffmpeg concatenation, and determining the output length of the final video. The code seems to have undergone revisions as there are comments stating \"wtf?\", \"shit.\", and \"is it needed?\" suggesting possible confusion or uncertainty about certain parts of the code.",
        "type": "comment"
    },
    "3014": {
        "file_id": 341,
        "content": "    output_length = float(output_length)\n    input_length = AudioSegment.from_wav(final_audio).duration_seconds\n    tempo = input_length/output_length\n    t_a,t_b = tempo.as_integer_ratio()\n    os.system('ffmpeg -i {} -i {} -c:v copy -c:a aac -filter:a \"atempo={}/{}\" -map 0:v:0 -map 1:a:0 {}'.format(final_video,final_audio,t_a,t_b,final_video2))\n    shutil.move(final_video2,target_video)",
        "type": "code",
        "location": "/tests/bilibili_practices/bilibili_tarot/functional_voice_with_pictures.py:171-176"
    },
    "3015": {
        "file_id": 341,
        "content": "This code calculates the tempo of an audio file and then applies it to another audio-video file using FFMPEG. It then moves the resulting file to a specified target location.",
        "type": "comment"
    },
    "3016": {
        "file_id": 342,
        "content": "/tests/bilibili_practices/bilibili_tarot/functional_generate_demo_tarot.py",
        "type": "filepath"
    },
    "3017": {
        "file_id": 342,
        "content": "This code generates a tarot image sequence, applies various filters and effects, resizes videos, pads them if necessary, compresses audio, and merges videos using FFmpeg commands.",
        "type": "summary"
    },
    "3018": {
        "file_id": 342,
        "content": "import os\nfrom vidpy import Clip, Composition  #many shitty things...\n# tarot_target = \"/root/Desktop/works/bilibili_tarot/tarot_pictures/0_THE_FOOL.jpg\"\nimport random\ndef gen_tarot(tarot_target,bgm_path,final_output):\n    os.system(\"rm tarot_demo.mp4\")\n    fps =60\n    # myprofile = {'width': 1320, 'height': 2644} # wtf?\n    # just create profile from it. are you sure?\n    clip = Clip(tarot_target, output_fps=fps,start=0, end=16,override=True)\n    # clip.edgeglow()\n    # clip.crop\n    # 1320x2645 # unbelievable.\n    # clip.fx(\"\",{})\n    # clip.resize(w=1920, h=1080, distort=True)\n    # distort=False\n    c_w = clip.width\n    c_h = clip.height\n    # comp = Composition([clip])\n    clip.dither(amount=0.07) # the greater the better.\n    clip.fadein(0.5)      # fade the clip in over 1 second\n    # clip.fadeout(3.5)   # fade the clip over 0.5 seconds\n    # clip.glow(3.5)         # add a glow effect\n    clip.spin(4, axis=\"z\")\n    clip.vignette()\n    clip.dust()\n    clip.hue(shift = 1-random.random()*0.5)\n    clip.pixelize(width = 0.005,height=0.01)",
        "type": "code",
        "location": "/tests/bilibili_practices/bilibili_tarot/functional_generate_demo_tarot.py:1-30"
    },
    "3019": {
        "file_id": 342,
        "content": "Code snippet imports necessary libraries, defines a function for generating a tarot image sequence, and applies various filters and effects to the input image. The function generates a random hue shift, pixelizes the image, adds a vignette effect, spins the image on the z-axis, and fades the clip in and out. The code uses overridden parameters for output resolution and aspect ratio, potentially causing inconsistencies or errors.",
        "type": "comment"
    },
    "3020": {
        "file_id": 342,
        "content": "    # clip.invert()\n    # clip.luminance\n    # clip.charcoal()\n    # clip.crop(right=c_w,bottom=c_h)\n    clip.save(\"tarot_demo.mp4\", fps=60,duration = 3,width=c_w,height=c_h) # good.\n    # print(c_w,c_h)\n    # 720 576\n    r1 = c_w/c_h\n    target_w, target_h = 1920, 1080\n    r2 = target_w/ target_h\n    if r1 < r2:\n        os.system('ffmpeg -y -i tarot_demo.mp4  -vf \"scale=-1:{},pad={}:ih:(ow-iw)/2\"  tarot_demo2.mp4'.format(target_h,target_w))\n    else:\n        os.system('ffmpeg -y -i tarot_demo.mp4  -vf \"scale={}:-1,pad=iw:{}:0:(oh-ih)/2\"  tarot_demo2.mp4'.format(target_w,target_h))\n    os.system(\"ffmpeg -y -i {} -i {} -c:v copy -c:a aac -map 0:v:0 -map 1:a:0 -shortest {}\".format(\"tarot_demo2.mp4\",bgm_path,final_output))\n    os.system(\"rm -rf tarot_demo2.mp4\")\n    os.system(\"rm -rf tarot_demo.mp4\")",
        "type": "code",
        "location": "/tests/bilibili_practices/bilibili_tarot/functional_generate_demo_tarot.py:31-49"
    },
    "3021": {
        "file_id": 342,
        "content": "The code resizes and pads a video, applies audio compression, then deletes intermediate files. It uses FFmpeg commands to scale the video, pad it if necessary, compress audio, and merge videos.",
        "type": "comment"
    },
    "3022": {
        "file_id": 343,
        "content": "/tests/bilibili_practices/bilibili_tarot/functional_gen_typo_video_seq.py",
        "type": "filepath"
    },
    "3023": {
        "file_id": 343,
        "content": "This function generates a video sequence with input \"seq\" and duration, applies filters to each clip, uses vidpy library for handling video composition, and saves the final composition as a video file.",
        "type": "summary"
    },
    "3024": {
        "file_id": 343,
        "content": "# seq = [0,1,2,3,4,5,6] # 7\n# duration = 4\nfrom vidpy import Composition, Clip\ndef gen_video(vname, seq, duration):\n    mduration = duration / len(seq)\n    clips = []\n    width,height =1920,1080\n    fps=60\n    orig_fps = 24\n    shift = fps/orig_fps\n    for i,s in enumerate(seq):\n        codec = str(s)\n        codec = \"0\"*(4-len(codec)) + codec\n        path = \"/root/Desktop/works/bilibili_tarot/demo_typography/screenshot{}.png\".format(codec)\n        start = i*mduration\n        end = start + mduration\n        print(start,end)\n        clip = Clip(path,output_fps=fps,start=0,end=mduration*shift,offset = start*shift,profile_override = {\"fps\":60,\"width\": width, \"height\": height})\n        clip.vignette()\n        clip.dust()\n        # clip.charcoal()\n        clip.dither(amount=0.10)\n        # clip.\n        # clip.pixelize()\n        clip.pixelize(width = 0.002,height=0.002)\n        clips.append(clip)\n    # breakpoint()\n    # # maybe some other bgm.\n    # bgm = Clip(bgm_path,start=0)\n    # clips.append(bgm)\n    # breakpoint()",
        "type": "code",
        "location": "/tests/bilibili_practices/bilibili_tarot/functional_gen_typo_video_seq.py:1-39"
    },
    "3025": {
        "file_id": 343,
        "content": "This function generates a video sequence based on input \"seq\" and duration, with each clip corresponding to a number in the sequence. It reads image files from \"/root/Desktop/works/bilibili_tarot/demo_typography/\" and applies various filters (vignette, dust, dithering, pixelize) to each clip before adding it to the list of clips. The function uses vidpy library for handling video composition and Clip class for each image frame.",
        "type": "comment"
    },
    "3026": {
        "file_id": 343,
        "content": "    comp = Composition(clips,duration=duration,fps=fps,width=width,height=height)\n    comp.save(vname,fps=60,duration = duration,width=width,height=height)",
        "type": "code",
        "location": "/tests/bilibili_practices/bilibili_tarot/functional_gen_typo_video_seq.py:40-42"
    },
    "3027": {
        "file_id": 343,
        "content": "The code above creates a Composition object with the specified clips, duration, fps, width, and height. Then, it saves this composition as a video file under the given 'vname' while maintaining the same settings.",
        "type": "comment"
    },
    "3028": {
        "file_id": 344,
        "content": "/tests/bilibili_practices/bilibili_tarot/flipcards.py",
        "type": "filepath"
    },
    "3029": {
        "file_id": 344,
        "content": "This code generates flipcards for Major and Minor Arcana in Tarot. It first clears directories, creates them, and then iterates over dictionaries to generate flipcard videos with specified background music, storing them accordingly.",
        "type": "summary"
    },
    "3030": {
        "file_id": 344,
        "content": "# generate all flipcards.\nfrom tarot_correspondences import *\nfrom functional_generate_demo_tarot import gen_tarot\n# mtarget_0, mtarget_1\ndir_0 = \"major\"\ndir_1 = \"minor\"\nos.system(\"rm -rf {}\".format(dir_0))\nos.system(\"rm -rf {}\".format(dir_1))\nos.mkdir(dir_0)\nos.mkdir(dir_1)\nbgm_path = \"/root/Desktop/works/bilibili_tarot/some_bgm.mp3\"\nfor k in mtarget_0.keys():\n    value = mtarget_0[k]\n    videoPath = \"/\".join([dir_0,\"{}.mp4\".format(k)])\n    picture_path = value\n    gen_tarot(picture_path,bgm_path,videoPath)\nfor k in mtarget_1.keys():\n    value = mtarget_1[k]\n    videoPath = \"/\".join([dir_1,\"{}.mp4\".format(k)])\n    picture_path = value\n    gen_tarot(picture_path,bgm_path,videoPath)",
        "type": "code",
        "location": "/tests/bilibili_practices/bilibili_tarot/flipcards.py:1-27"
    },
    "3031": {
        "file_id": 344,
        "content": "This code generates flipcards for Major and Minor Arcana in Tarot. It first clears directories, creates them, and then iterates over dictionaries to generate flipcard videos with specified background music, storing them accordingly.",
        "type": "comment"
    },
    "3032": {
        "file_id": 345,
        "content": "/tests/bilibili_practices/bilibili_tarot/all_typography_underline_subtitle.py",
        "type": "filepath"
    },
    "3033": {
        "file_id": 345,
        "content": "The code removes files, uses external scripts and programs to generate typography for a bilibili video, and is part of a larger video editing or manipulation process.",
        "type": "summary"
    },
    "3034": {
        "file_id": 345,
        "content": "from tarot_descriptions import *\n# mdict, smdict2\nimport os\ndef gen_typography_part1(content):\n    with open(\"demo_text.log\",\"w+\",encoding=\"utf8\") as f:\n        f.write(content)\n    os.system(\"xvfb-run -s '-screen 0 1920x1080x24' python3 scriptable_generate_typography_with_voice_underline_subtitle.py\")\ndef kill_script():\n    os.system(\"bash kill_xb.sh\")\ntyp_0 = \"typo_0\"\ntyp_1 = \"typo_1\"\n# os.system(\"rm -rf {}\".format(typ_0))\n# os.system(\"rm -rf {}\".format(typ_1))\n# os.mkdir(typ_0)\n# os.mkdir(typ_1)\nfrom functional_voice_with_pictures import gen_typography_part3\n# intro_text = \"\"\"塔罗牌，是一种针对人、事、物进行分析、预测和提供建议的工具，被称为“大自然的奥秘库”。\n# 抽取一张塔罗牌，今天的你会是怎样的呢？\"\"\"\nintro_text = \"\"\"奥拓是只猫～\"\"\"\n# bgm_path = \"/root/Desktop/works/bilibili_tarot/tarot_random_shuffle.mp3\"\ntarget_video = \"cat_intro_video.mp4\"\nos.system(\"rm {}\".format(target_video))\nkill_script()\n# v = mdict[k]\nv = intro_text\ngen_typography_part1(v)\n# target_video = \"/\".join([typ_0,\"{}.mp4\".format(k)])\ngen_typography_part3(v,target_video)\nkill_script()\nintro_text = \"\"\"喜欢本期视频的话 点个关注再走吧～\"\"\"",
        "type": "code",
        "location": "/tests/bilibili_practices/bilibili_tarot/all_typography_underline_subtitle.py:1-43"
    },
    "3035": {
        "file_id": 345,
        "content": "This code generates typography for a bilibili video, involves creating directories and removing files, uses os.system to run external scripts and programs, and has an introductory message followed by an ending message for the video.",
        "type": "comment"
    },
    "3036": {
        "file_id": 345,
        "content": "# bgm_path = \"/root/Desktop/works/bilibili_tarot/tarot_random_shuffle.mp3\"\ntarget_video = \"cat_outro_video.mp4\"\nos.system(\"rm {}\".format(target_video))\nkill_script()\n# v = mdict[k]\nv = intro_text\ngen_typography_part1(v)\n# target_video = \"/\".join([typ_0,\"{}.mp4\".format(k)])\ngen_typography_part3(v,target_video)\nkill_script()",
        "type": "code",
        "location": "/tests/bilibili_practices/bilibili_tarot/all_typography_underline_subtitle.py:45-56"
    },
    "3037": {
        "file_id": 345,
        "content": "This code removes the target video file, terminates a script, generates typography for a specific text using functions gen_typography_part1 and gen_typography_part3, and then terminates another script. It seems to be part of a process involving video editing or manipulation.",
        "type": "comment"
    },
    "3038": {
        "file_id": 346,
        "content": "/tests/bilibili_practices/bilibili_tarot/all_typography.py",
        "type": "filepath"
    },
    "3039": {
        "file_id": 346,
        "content": "This code generates typography for a video and stores it in a specific format, using functions `gen_typography_part1`, `gen_typography_part2`, and `kill_script()`. It imports modules for creating files, executing scripts, and generating intermediate videos.",
        "type": "summary"
    },
    "3040": {
        "file_id": 346,
        "content": "from tarot_descriptions import *\n# mdict, smdict2\nimport os\ndef gen_typography_part1(content):\n    with open(\"demo_text.log\",\"w+\",encoding=\"utf8\") as f:\n        f.write(content)\n    os.system(\"xvfb-run -s '-screen 0 1920x1080x24' python3 scriptable_generate_typography_with_voice.py\")\ndef kill_script():\n    os.system(\"bash kill_xb.sh\")\ntyp_0 = \"typo_0\"\ntyp_1 = \"typo_1\"\n# os.system(\"rm -rf {}\".format(typ_0))\n# os.system(\"rm -rf {}\".format(typ_1))\n# os.mkdir(typ_0)\n# os.mkdir(typ_1)\nfrom functional_voice_with_pictures import gen_typography_part2\ninter_text = \"\"\"再抽取一张牌吧~\"\"\"\nbgm_path = \"/root/Desktop/works/bilibili_tarot/tarot_random_shuffle.mp3\"\ntarget_video = \"intermediate_video.mp4\"\nos.system(\"rm {}\".format(target_video))\nkill_script()\n# v = mdict[k]\nv = inter_text\ngen_typography_part1(v)\n# target_video = \"/\".join([typ_0,\"{}.mp4\".format(k)])\ngen_typography_part2(v,bgm_path,target_video)\nkill_script()\nintro_text = \"\"\"塔罗牌，是一种针对人、事、物进行分析、预测和提供建议的工具，被称为“大自然的奥秘库”。\n抽取一张塔罗牌，今天的你会是怎样的呢？\"\"\"\n# intro_text =\n# bgm_path = \"/root/Desktop/works/bilibili_tarot/tarot_random_shuffle.mp3\"",
        "type": "code",
        "location": "/tests/bilibili_practices/bilibili_tarot/all_typography.py:1-46"
    },
    "3041": {
        "file_id": 346,
        "content": "The code imports modules and defines functions for generating typography with voice and video. It creates files, executes scripts, and generates intermediate videos for a tarot reading process. It also generates a final video after killing the script.",
        "type": "comment"
    },
    "3042": {
        "file_id": 346,
        "content": "# target_video = \"intro_video.mp4\"\n# os.system(\"rm {}\".format(target_video))\n# kill_script()\n# # v = mdict[k]\n# v = intro_text\n# gen_typography_part1(v)\n# # target_video = \"/\".join([typ_0,\"{}.mp4\".format(k)])\n# gen_typography_part2(v,bgm_path,target_video)\n# kill_script()\nbgms = [\"you_got_me_acc.wav\", \"tarot_desc_acc.wav\"]\n# outro_text = \"\"\"今天的你运气不错哦～\n# 喜欢的话请分享点赞，一键三联哦～\"\"\"\n# bgm_path = bgms[0]\n# target_video = \"outro_video.mp4\"\n# os.system(\"rm {}\".format(target_video))\n# kill_script()\n# # v = mdict[k]\n# v = outro_text\n# gen_typography_part1(v)\n# # target_video = \"/\".join([typ_0,\"{}.mp4\".format(k)])\n# gen_typography_part2(v,bgm_path,target_video)\n# kill_script()\nimport random\n# for k in mdict.keys():\n#     if k !=16:\n#         continue\n#     kill_script()\n#     v = mdict[k]\n#     gen_typography_part1(v)\n#     target_video = \"/\".join([typ_0,\"{}.mp4\".format(k)])\n#     gen_typography_part2(v,random.choice(bgms),target_video)\n#     kill_script()\n# for k in smdict.keys():\n#     v = smdict[k]\n#     # kill_script()\n#     # v = mdict[k]",
        "type": "code",
        "location": "/tests/bilibili_practices/bilibili_tarot/all_typography.py:48-93"
    },
    "3043": {
        "file_id": 346,
        "content": "The code removes the target video, generates typography for intro and outro text using different background music, and randomly selects a background music from the given list for each card in mdict and smdict.",
        "type": "comment"
    },
    "3044": {
        "file_id": 346,
        "content": "#     gen_typography_part1(v)\n#     target_video = \"/\".join([typ_1,\"{}.mp4\".format(k)])\n#     gen_typography_part2(v,random.choice(bgms),target_video)\n#     kill_script()",
        "type": "code",
        "location": "/tests/bilibili_practices/bilibili_tarot/all_typography.py:94-97"
    },
    "3045": {
        "file_id": 346,
        "content": "This code section generates typography for a video and stores it in a specific format. It first calls a function `gen_typography_part1` passing some parameter v, then combines the typography name with the video number as the file name. The next step is to call another function `gen_typography_part2`, which takes two parameters: v and a randomly chosen bgm (background music) from some list of choices. It also passes the target video file as an argument. Lastly, it calls the `kill_script()` function to terminate the script execution.",
        "type": "comment"
    },
    "3046": {
        "file_id": 347,
        "content": "/tests/hyper_param_optimization/test.py",
        "type": "filepath"
    },
    "3047": {
        "file_id": 347,
        "content": "This code uses Hyperopt library for parameter optimization, chooses hyperparameters from different cases via choice function, and samples 10 times for each search space.",
        "type": "summary"
    },
    "3048": {
        "file_id": 347,
        "content": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\nfrom hyperopt import hp\n# usually this hyper parameter optimization is done regularlly, and the optimized parameters will be used for a while till next update.\n# but can we optimize these parameters offline?\n# if not offline then we can only use traditional machine learning instead...\n# or this trial and error process is actually a kind of offline machine learning, like random search and graph inference...\n# better use hyperopt with a discriminator ML algorithm.\n# space = hp.choice(\n#     \"a\",\n#     [(\"case 1\", 1 + hp.lognormal(\"c1\", 0, 1)), (\"case 2\", hp.uniform(\"c2\", -10, 10))],\n# )\nimport hyperopt.pyll.stochastic as stochastic\nspace = hp.choice(\"lambda\",[lambda :1, lambda:2]) # if it is lambda, function will not resolve. however, after passing this thing into the main criterion function, it will utilize the lambda function.\nfor _ in range(10):\n    sample = stochastic.sample(space)\n    print(\"SAMPLE:\", sample) # this will return the tuple. can we put some custom functions here?",
        "type": "code",
        "location": "/tests/hyper_param_optimization/test.py:1-24"
    },
    "3049": {
        "file_id": 347,
        "content": "This code uses the hyperopt library for parameter optimization. The hyperparameters are chosen from different cases using a choice function, including lambda functions. The space is sampled 10 times using stochastic sampling, and each sample is printed to the console.",
        "type": "comment"
    },
    "3050": {
        "file_id": 347,
        "content": "    # there must be some integrations with custom functions. for example: scikit-learn\nprint(\"_______________________________\") # splited.\nfrom hyperopt.pyll import scope\n@scope.define # this is how we sample the \"LAMBDA\".\ndef my_func(a,b=1):\n    print(\"running function my_func\", a,b)\n    return a*b\nspace_0 = scope.my_func(hp.choice(\"myChoice\",[1,2]))\nspace_1 = scope.my_func(hp.choice(\"myChoice\",[1,2]), hp.choice(\"myChoice2\",[2,3,4]))\nfor _ in range(10):\n    print(stochastic.sample(space_0), stochastic.sample(space_1))",
        "type": "code",
        "location": "/tests/hyper_param_optimization/test.py:25-40"
    },
    "3051": {
        "file_id": 347,
        "content": "This code defines and samples two hyperparameter search spaces using the Hyperopt library's Pyll module. The \"my_func\" function is defined within a scope, allowing for easy integration with custom functions like Scikit-Learn. It then prints and samples from these search spaces 10 times.",
        "type": "comment"
    },
    "3052": {
        "file_id": 348,
        "content": "/tests/hyper_param_optimization/README.md",
        "type": "filepath"
    },
    "3053": {
        "file_id": 348,
        "content": "This code provides a reference to the tutorials section and official documentation of the hyperopt library, found on its GitHub repository.",
        "type": "summary"
    },
    "3054": {
        "file_id": 348,
        "content": "[tutorials](https://github.com/hyperopt/hyperopt/wiki/FMin) found from [official documentation](http://hyperopt.github.io/hyperopt/) of [hyperopt](https://github.com/hyperopt/hyperopt).",
        "type": "code",
        "location": "/tests/hyper_param_optimization/README.md:1-1"
    },
    "3055": {
        "file_id": 348,
        "content": "This code provides a reference to the tutorials section and official documentation of the hyperopt library, found on its GitHub repository.",
        "type": "comment"
    },
    "3056": {
        "file_id": 349,
        "content": "/tests/hyper_param_optimization/optimize_suggest.py",
        "type": "filepath"
    },
    "3057": {
        "file_id": 349,
        "content": "Code defines a function and uses Hyperopt's Tree-structured Parzen Estimators (TPE) algorithm to optimize the given function. It sets the hyperparameter space using hp.uniform and runs 100 trials, printing the result of the best trial.",
        "type": "summary"
    },
    "3058": {
        "file_id": 349,
        "content": "from hyperopt import tpe, fmin, hp, STATUS_OK, STATUS_FAIL\nimport requests\ndef function(x):\n    print(\"trying timeout:\",x)\n    # result = x**2\n    status = STATUS_FAIL\n    try:\n        r = requests.get('https://www.baidu.com/', timeout=x)\n        if r.status_code == 200:\n            status = STATUS_OK\n    except:\n        print(\"FAILED WITH TIMEOUT:\", x) # this will rule out the unwanted ones.\n    return {\"loss\":x, \"status\":status}\nspace = hp.uniform(\"param\",0,2)\nresult = fmin(fn=function, space=space, algo=tpe.suggest, max_evals=100)\nprint(result)\n# {'param': 0.10165862536290635}\n# really working? 100ms could be so damn short...\n# by using `Trials` we could inspect results of every trial.",
        "type": "code",
        "location": "/tests/hyper_param_optimization/optimize_suggest.py:1-21"
    },
    "3059": {
        "file_id": 349,
        "content": "Code defines a function and uses Hyperopt's Tree-structured Parzen Estimators (TPE) algorithm to optimize the given function. It sets the hyperparameter space using hp.uniform and runs 100 trials, printing the result of the best trial.",
        "type": "comment"
    },
    "3060": {
        "file_id": 350,
        "content": "/tests/keepalive_service/test.sh",
        "type": "filepath"
    },
    "3061": {
        "file_id": 350,
        "content": "This line of code is running the \"keepalive\" executable with the argument \"echo abc\". The purpose seems to be testing and logging that the process is still alive.",
        "type": "summary"
    },
    "3062": {
        "file_id": 350,
        "content": "./keepalive echo abc",
        "type": "code",
        "location": "/tests/keepalive_service/test.sh:1-1"
    },
    "3063": {
        "file_id": 350,
        "content": "This line of code is running the \"keepalive\" executable with the argument \"echo abc\". The purpose seems to be testing and logging that the process is still alive.",
        "type": "comment"
    },
    "3064": {
        "file_id": 351,
        "content": "/tests/keepalive_service/install.sh",
        "type": "filepath"
    },
    "3065": {
        "file_id": 351,
        "content": "This line of code copies the 'keepalive' file to '/usr/local/bin/' directory, allowing it to be accessed and executed system-wide.",
        "type": "summary"
    },
    "3066": {
        "file_id": 351,
        "content": "cp keepalive /usr/local/bin/",
        "type": "code",
        "location": "/tests/keepalive_service/install.sh:1-1"
    },
    "3067": {
        "file_id": 351,
        "content": "This line of code copies the 'keepalive' file to '/usr/local/bin/' directory, allowing it to be accessed and executed system-wide.",
        "type": "comment"
    },
    "3068": {
        "file_id": 352,
        "content": "/tests/hmm_test_speech_recognization_time_series/test.py",
        "type": "filepath"
    },
    "3069": {
        "file_id": 352,
        "content": "The code utilizes numpy and hmmlearn libraries for unsupervised learning. It creates a GaussianHMM model with 3 components, generates random dataset X for training, fits the model, predicts states Z, and calculates score, where lower score implies better performance.",
        "type": "summary"
    },
    "3070": {
        "file_id": 352,
        "content": "import numpy as np\nfrom hmmlearn import hmm\n# np.random.seed(42)\n# hmmlearn is simply unsupervised learning.\n# for supervised sequence learning use seqlearn instead\n# pomegranate also supports labeled sequence learning.\n# you may feed the sequence into unsupervised learning, output with supervised learning.\n# wtf?\n# we can use the 'score' to identify 'trained' sequences and 'alien' sequences, thus get the 'supervised' effect.\n# https://github.com/wblgers/hmm_speech_recognition_demo/blob/master/demo.py\nmodel = hmm.GaussianHMM(n_components=3, covariance_type=\"full\")\n# model.startprob_ = np.array([0.6, 0.3, 0.1])\n# model.transmat_ = np.array([[0.7, 0.2, 0.1],\n#                             [0.3, 0.5, 0.2],\n#                             [0.3, 0.3, 0.4]])\n# model.means_ = np.array([[0.0, 0.0], [3.0, -3.0], [5.0, 10.0]])\n# model.covars_ = np.tile(np.identity(2), (3, 1, 1))\n# not fitteed since we do not manually specify all the parameters.\nX = np.random.random((100,8)) # it can be anything. the Z contains three labels.",
        "type": "code",
        "location": "/tests/hmm_test_speech_recognization_time_series/test.py:1-24"
    },
    "3071": {
        "file_id": 352,
        "content": "Code is importing numpy and hmmlearn libraries for unsupervised learning. It then creates a GaussianHMM model with 3 components, but leaves its parameters unspecified as it will be fitted later. A random dataset X of size (100,8) is generated for training.",
        "type": "comment"
    },
    "3072": {
        "file_id": 352,
        "content": "# X, Z = model.sample(100)\n# print(X) # the observations.\nmodel.fit(X)\n# # (100, 2)\nZ_predicted = model.predict(X)\n# print(Z) # the states.\nprint(X.shape, Z_predicted.shape)\n# # (100,)\nscore = model.score(X)\nprint('score:', score)\n# score: -32.50027336204506\n# it must mean something? man?\n# simply use another model and fit it again, get the best score!\nbreakpoint()",
        "type": "code",
        "location": "/tests/hmm_test_speech_recognization_time_series/test.py:25-38"
    },
    "3073": {
        "file_id": 352,
        "content": "This code fits a model to some observations (X) and predicts states (Z) using the fitted model. It then calculates a score (score) for the model's performance on the observations. The code suggests that the lower the score, the better the model's performance, but further analysis might be needed.",
        "type": "comment"
    },
    "3074": {
        "file_id": 353,
        "content": "/tests/hmm_test_speech_recognization_time_series/seqlearn_test.py",
        "type": "filepath"
    },
    "3075": {
        "file_id": 353,
        "content": "This code is training a Structured Perceptron on one-hot encoded features with varying sequence lengths. The classifier is then evaluated using whole sequence accuracy.",
        "type": "summary"
    },
    "3076": {
        "file_id": 353,
        "content": "from seqlearn.perceptron import StructuredPerceptron  # it's like mini neural network.\n# the lengths_train marked each individual sequence's length as an array.\nimport numpy as np\nX_train = np.random.random((5, 4))  # one-hot encoded? not? features=4\ny_train = np.random.randint(0, 5, (5,))  # the freaking label.\nlengths_train = [1, 1, 2, 1]  # may i apologize. sum=5\nclassifier = StructuredPerceptron()\nclassifier.fit(X_train, y_train, lengths_train)\n# from seqlearn.evaluation import bio_f_score\nfrom seqlearn.evaluation import whole_sequence_accuracy\ny_pred = classifier.predict(X_train, lengths_train)\nprint(\"TRAINED ACCURACY: {:.2f} %\".format(100*whole_sequence_accuracy(y_train, y_pred, lengths_train)))\n# breakpoint()",
        "type": "code",
        "location": "/tests/hmm_test_speech_recognization_time_series/seqlearn_test.py:1-19"
    },
    "3077": {
        "file_id": 353,
        "content": "This code is training a Structured Perceptron on one-hot encoded features with varying sequence lengths. The classifier is then evaluated using whole sequence accuracy.",
        "type": "comment"
    },
    "3078": {
        "file_id": 354,
        "content": "/tests/hmm_test_speech_recognization_time_series/pomegranate_test.py",
        "type": "filepath"
    },
    "3079": {
        "file_id": 354,
        "content": "This code snippet introduces the Pomegranate library, which offers advanced features for Hidden Markov Model (HMM) training and prediction with a variety of models available.",
        "type": "summary"
    },
    "3080": {
        "file_id": 354,
        "content": "# this library goes way advanced than hmmlearn/seqlearn\n# it provides convenient methods for training and prediction.\n# also lots of different models\n# https://pomegranate.readthedocs.io/en/latest/HiddenMarkovModel.html",
        "type": "code",
        "location": "/tests/hmm_test_speech_recognization_time_series/pomegranate_test.py:1-5"
    },
    "3081": {
        "file_id": 354,
        "content": "This code snippet introduces the Pomegranate library, which offers advanced features for Hidden Markov Model (HMM) training and prediction with a variety of models available.",
        "type": "comment"
    },
    "3082": {
        "file_id": 355,
        "content": "/tests/generator_yield_from_python_extract_element_one_by_one/test.py",
        "type": "filepath"
    },
    "3083": {
        "file_id": 355,
        "content": "This code uses generators to iterate through numbers, cleaning up temporary files after use. It demonstrates using lambda functions for simplified iteration and exception handling for resource management. The code initializes generator2, calls generator3 with generator2 and a tempfile, checks if the file exists, closes the generator, and again checks if the file exists.",
        "type": "summary"
    },
    "3084": {
        "file_id": 355,
        "content": "from lazero.filesystem.temp import tmpfile\nimport pathlib\nimport os\ndef checkFileExists(filePath, debug=False):\n    result = os.path.exists(filePath)\n    if debug:\n        print('exists?', result)\ndef generator(tempfile):\n    # for index in range(12): # 0 to 11 means 12\n    for index in range(11): # what if it is 11? -> StopIteration and shit get cleaned.\n        with tmpfile(tempfile):\n            pathlib.Path(tempfile).touch()\n            yield index\ndef generator2(tempfile):\n    yield from generator(tempfile)  # this is to simplifying the process of iteration.\ndef iterator(lambdaFunction, tempfile):\n    for _ in range(4):\n        result = lambdaFunction()\n        print(result) # cleaned after next FAILED iteration, which is what we need the most.\n        checkFileExists(tempfile, debug=True)\n        # cleaning after 'close' or next iteration.\ndef generator3(myGenerator, tempfile):\n    getNextNumber = lambda: myGenerator.__next__()\n    for _ in range(3):\n        iterator(getNextNumber, tempfile)\n        print(\"_\" * 30)",
        "type": "code",
        "location": "/tests/generator_yield_from_python_extract_element_one_by_one/test.py:1-35"
    },
    "3085": {
        "file_id": 355,
        "content": "This code defines a series of functions that utilize generators to generate and iterate through numbers, while also checking if the temporary file exists and cleaning it up after each iteration. The code demonstrates how generators can be used with lambda functions for simplified iteration, and how exception handling can be employed to clean up resources after use.",
        "type": "comment"
    },
    "3086": {
        "file_id": 355,
        "content": "if __name__ == \"__main__\":\n    tempfile = \"tmp_test\"\n    if os.path.exists(tempfile):\n        os.remove(tempfile)\n    myGenerator = generator2(tempfile)\n    print(type(myGenerator))\n    breakpoint()\n    generator3(myGenerator, tempfile)  # good.\n    # not over yet.\n    checkFileExists(tempfile, debug=True)\n    myGenerator.close() # choose to close this so you would get this result.\n    checkFileExists(tempfile, debug=True)\n    # another test on generator, about tempfiles during iteration.",
        "type": "code",
        "location": "/tests/generator_yield_from_python_extract_element_one_by_one/test.py:38-50"
    },
    "3087": {
        "file_id": 355,
        "content": "Code initializes generator2 with a temporary file name and prints its type. Then, it calls generator3 passing the generator2 and tempfile as arguments. After that, it checks if the temporary file exists using checkFileExists function in debug mode. Finally, it closes the generator and again checks if the temporary file exists.",
        "type": "comment"
    },
    "3088": {
        "file_id": 356,
        "content": "/tests/dapp_ethereum_python_crypto/test.py",
        "type": "filepath"
    },
    "3089": {
        "file_id": 356,
        "content": "The code uses Web3 to connect to a local Ethereum node, imports necessary libraries, checks connection status and account balance, unlocks accounts, sends transactions, and verifies received funds.",
        "type": "summary"
    },
    "3090": {
        "file_id": 356,
        "content": "from web3 import Web3\n# testnet, bitcoind, regtest\n# https://bitcoin.stackexchange.com/questions/42026/is-it-possible-to-use-bitcoind-as-a-private-blockchain\n# mine only when pending transaction happens:\n# https://ethereum.stackexchange.com/questions/3151/how-to-make-miner-to-mine-only-when-there-are-pending-transactions\n# maybe you want money even if without transaction, or low in cash.\n# https://hackernoon.com/hands-on-creating-your-own-local-private-geth-node-beginner-friendly-3d45902cc612\nlink = \"/root/.ethereum/geth.ipc\"\nweb3 = Web3(Web3.IPCProvider(link))\nprint(web3.isConnected())\n# account_genesis = \"0xde478bde26d711414fae26133e759d8a82a202ab\"  # aka: eth.coinbase\n# account_genesis = \"0x6fe20a7157fdb705278fffda4ea0ebf4694f31ea\"\naccount_genesis = \"0xd6e79c8d5b7d41cc1a3b98373c98618ea267852f\"\naccount_genesis = Web3.toChecksumAddress(account_genesis)\npassword_genesis = \"abcdefg\"\n# let's see!\n# target_account = \"0x033799af9b29e1d7dbf3c8dd64647df345f67bf1\"\ntarget_account = \"0x463f061d2add7987e2a7d14920e18194107ea991\"",
        "type": "code",
        "location": "/tests/dapp_ethereum_python_crypto/test.py:1-26"
    },
    "3091": {
        "file_id": 356,
        "content": "The code imports Web3, sets the IPC link to connect to a local Ethereum node, checks the connection status, assigns an account address and password, and specifies a target account.",
        "type": "comment"
    },
    "3092": {
        "file_id": 356,
        "content": "target_account = Web3.toChecksumAddress(target_account)\n# you was connected ethereum to mainnet! not good.\n# anyway, we need money!\nb = web3.eth.get_balance(web3.eth.coinbase)\nprint(b)\n# proof of authority, puppeth\n## need password!\nweb3.geth.personal.unlock_account(web3.eth.coinbase, password_genesis)\nweb3.eth.send_transaction(\n    {\n        \"to\": target_account,\n        \"from\": web3.eth.coinbase,\n        \"value\": 1,\n    }\n)\nweb3.geth.personal.lock_account(web3.eth.coinbase)\n# you can choose to use 'with' statement.\nb = web3.eth.get_balance(target_account)\nprint(b)\n# still no money! fuck.",
        "type": "code",
        "location": "/tests/dapp_ethereum_python_crypto/test.py:27-52"
    },
    "3093": {
        "file_id": 356,
        "content": "Code connects to Ethereum mainnet, checks balance of the coinbase account, unlocks account using a password, sends transaction to target_account, and verifies if funds have been received.",
        "type": "comment"
    },
    "3094": {
        "file_id": 357,
        "content": "/tests/dapp_ethereum_python_crypto/README.md",
        "type": "filepath"
    },
    "3095": {
        "file_id": 357,
        "content": "The code is expressing the difficulty in validating a 'hacker' program within AGI and the need to create dummy crypto elements for testing purposes.",
        "type": "summary"
    },
    "3096": {
        "file_id": 357,
        "content": "not sure how to validate my 'hacker' program in AGI. just create some dummy crypto things.",
        "type": "code",
        "location": "/tests/dapp_ethereum_python_crypto/README.md:1-1"
    },
    "3097": {
        "file_id": 357,
        "content": "The code is expressing the difficulty in validating a 'hacker' program within AGI and the need to create dummy crypto elements for testing purposes.",
        "type": "comment"
    },
    "3098": {
        "file_id": 358,
        "content": "/tests/english_chinese_mixing_spliter/test_tts.py",
        "type": "filepath"
    },
    "3099": {
        "file_id": 358,
        "content": "This code imports TTS module and generates audio from a given text, iterating through analyzed data for each language. English tool is needed as no English option available currently.",
        "type": "summary"
    }
}