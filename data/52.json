{
    "5200": {
        "file_id": 682,
        "content": "/tests/vapoursynth_linux_test/scene_change_detection.sh",
        "type": "filepath"
    },
    "5201": {
        "file_id": 682,
        "content": "This code is using FFmpeg to process a video file, extracting scenes by selecting frames where the scene change is greater than 0.1 and displaying information about each scene change. The output is redirected to null.",
        "type": "summary"
    },
    "5202": {
        "file_id": 682,
        "content": "ffmpeg -hide_banner -i \"/root/Desktop/works/pyjom/samples/video/LiEIfnsvn.mp4\" -an \\\n-filter:v \"select='gt(scene,0.1)',showinfo\" \\\n-f null \\\n- 2>&1",
        "type": "code",
        "location": "/tests/vapoursynth_linux_test/scene_change_detection.sh:3-6"
    },
    "5203": {
        "file_id": 682,
        "content": "This code is using FFmpeg to process a video file, extracting scenes by selecting frames where the scene change is greater than 0.1 and displaying information about each scene change. The output is redirected to null.",
        "type": "comment"
    },
    "5204": {
        "file_id": 683,
        "content": "/tests/vapoursynth_linux_test/test.sh",
        "type": "filepath"
    },
    "5205": {
        "file_id": 683,
        "content": "This code is running vspipe, a command-line tool for processing video files with VapourSynth script. It takes a .vpy script file as input and uses the -c flag for y4m format output.",
        "type": "summary"
    },
    "5206": {
        "file_id": 683,
        "content": "vspipe -c y4m script.vpy -",
        "type": "code",
        "location": "/tests/vapoursynth_linux_test/test.sh:1-1"
    },
    "5207": {
        "file_id": 683,
        "content": "This code is running vspipe, a command-line tool for processing video files with VapourSynth script. It takes a .vpy script file as input and uses the -c flag for y4m format output.",
        "type": "comment"
    },
    "5208": {
        "file_id": 684,
        "content": "/tests/vapoursynth_linux_test/test_ffmpeg_docker.sh",
        "type": "filepath"
    },
    "5209": {
        "file_id": 684,
        "content": "Code snippet attempts to download a video file 'flower_cif.y4m' using wget, and then applies various filters with ffmpeg-tensorflow Docker container to upscale the video resolution by 2x and save it as 'flower_cif_2x.mp4'. The code also provides an alias for easier execution of ffmpeg-tensorflow command and specifies video filter complexities within the ffmpeg command.",
        "type": "summary"
    },
    "5210": {
        "file_id": 684,
        "content": "wget https://media.xiph.org/video/derf/y4m/flower_cif.y4m\n# no good for using docker gpu containers.\n# alias ffmpeg-tensorflow='docker run --rm --gpus all -u $(id -u):$(id -g) -v \"$PWD\":/data -w /data -i miratmu/ffmpeg-tensorflow'\n# ffmpeg-tensorflow -i flower_cif.y4m -filter_complex '[0:v] format=pix_fmts=yuv420p, extractplanes=y+u+v [y][u][v]; [y] sr=dnn_backend=tensorflow:scale_factor=2:model=/models/espcn.pb [y_scaled]; [u] scale=iw*2:ih*2 [u_scaled]; [v] scale=iw*2:ih*2 [v_scaled]; [y_scaled][u_scaled][v_scaled] mergeplanes=0x001020:yuv420p [merged]' -map [merged] -sws_flags lanczos -c:v libx264 -crf 17 -c:a copy -y flower_cif_2x.mp4",
        "type": "code",
        "location": "/tests/vapoursynth_linux_test/test_ffmpeg_docker.sh:1-6"
    },
    "5211": {
        "file_id": 684,
        "content": "Code snippet attempts to download a video file 'flower_cif.y4m' using wget, and then applies various filters with ffmpeg-tensorflow Docker container to upscale the video resolution by 2x and save it as 'flower_cif_2x.mp4'. The code also provides an alias for easier execution of ffmpeg-tensorflow command and specifies video filter complexities within the ffmpeg command.",
        "type": "comment"
    },
    "5212": {
        "file_id": 685,
        "content": "/tests/vapoursynth_linux_test/view_test.py",
        "type": "filepath"
    },
    "5213": {
        "file_id": 685,
        "content": "The code imports VapourSynth library functions, creates a Video object with FFMS2 source and option to transpose, but generating previews isn't working as vspipe uses existing APIs and can only generate raw frame data. OpenCV might help; example at https://github.com/UniversalAl/view.",
        "type": "summary"
    },
    "5214": {
        "file_id": 685,
        "content": "videoPath = \"/root/Desktop/works/pyjom/samples/video/dog_with_text.mp4\"\n# videoPath = \"/Users/jamesbrown/desktop/works/pyjom_remote/samples/video/dog_with_text.mp4\"\n# reference: http://www.vapoursynth.com/doc/pythonreference.html\n# The VideoFrame and AudioFrame classes contains one picture/audio chunk and all the metadata associated with it. It is possible to access the raw data using either get_read_ptr(plane) or get_write_ptr(plane) and get_stride(plane) with ctypes.\n# A more Python friendly wrapping is also available where each plane/channel can be accessed as a Python array using frame[plane/channel].\n# To get a frame simply call get_frame(n) on a clip. Should you desire to get all frames in a clip, use this code:\n# for frame in clip.frames():\n#     # Do stuff with your frame\n#     pass\nfrom vapoursynth import core\nvideo = core.ffms2.Source(source=videoPath)\n# video = core.std.Transpose(video)\n# video.set_output()\n# from viewKali import Preview\n# clip = vs.core.lsmas.LibavSMASHSource('source.mp4')",
        "type": "code",
        "location": "/tests/vapoursynth_linux_test/view_test.py:1-23"
    },
    "5215": {
        "file_id": 685,
        "content": "The code imports the necessary functions from the VapourSynth library and defines a video path. It then creates a Video object using the FFMS2 source and provides an option to transpose the video if needed, but it's currently commented out. Additionally, there is a reference to another file called \"viewKali\" where a Preview function may be used, but it's not implemented yet.",
        "type": "comment"
    },
    "5216": {
        "file_id": 685,
        "content": "# seems not working\n# Preview(video)\n# vspipe is a wrapper around existing apis. vapoursynth can only generate raw frame data so we cannot encode video here alone. maybe we need opencv for this?\n# opencv preview https://github.com/UniversalAl/view",
        "type": "code",
        "location": "/tests/vapoursynth_linux_test/view_test.py:24-28"
    },
    "5217": {
        "file_id": 685,
        "content": "This code appears to attempt creating a preview using VapourSynth, but the functionality isn't working. It suggests that vspipe is a wrapper around existing APIs and VapourSynth can only generate raw frame data, so encoding a video alone might not be possible. OpenCV might help in generating previews, and there's an example at this GitHub link: https://github.com/UniversalAl/view.",
        "type": "comment"
    },
    "5218": {
        "file_id": 686,
        "content": "/tests/video_detector_tests/cocoNames.py",
        "type": "filepath"
    },
    "5219": {
        "file_id": 686,
        "content": "The code defines two dictionaries, \"cocoName\" and \"cocoRealName\", used for image classification tasks based on the MS COCO dataset. It maps labels to object names and indexes respectively, correcting 0-indexing in dataset labels.",
        "type": "summary"
    },
    "5220": {
        "file_id": 686,
        "content": "cocoName = {0: '__background__',\n\t 1: 'person',\n\t 2: 'bicycle',\n\t 3: 'car',\n\t 4: 'motorcycle',\n\t 5: 'airplane',\n\t 6: 'bus',\n\t 7: 'train',\n\t 8: 'truck',\n\t 9: 'boat',\n\t 10: 'traffic light',\n\t 11: 'fire hydrant',\n\t 12: 'stop sign',\n\t 13: 'parking meter',\n\t 14: 'bench',\n\t 15: 'bird',\n\t 16: 'cat',\n\t 17: 'dog',\n\t 18: 'horse',\n\t 19: 'sheep',\n\t 20: 'cow',\n\t 21: 'elephant',\n\t 22: 'bear',\n\t 23: 'zebra',\n\t 24: 'giraffe',\n\t 25: 'backpack',\n\t 26: 'umbrella',\n\t 27: 'handbag',\n\t 28: 'tie',\n\t 29: 'suitcase',\n\t 30: 'frisbee',\n\t 31: 'skis',\n\t 32: 'snowboard',\n\t 33: 'sports ball',\n\t 34: 'kite',\n\t 35: 'baseball bat',\n\t 36: 'baseball glove',\n\t 37: 'skateboard',\n\t 38: 'surfboard',\n\t 39: 'tennis racket',\n\t 40: 'bottle',\n\t 41: 'wine glass',\n\t 42: 'cup',\n\t 43: 'fork',\n\t 44: 'knife',\n\t 45: 'spoon',\n\t 46: 'bowl',\n\t 47: 'banana',\n\t 48: 'apple',\n\t 49: 'sandwich',\n\t 50: 'orange',\n\t 51: 'broccoli',\n\t 52: 'carrot',\n\t 53: 'hot dog',\n\t 54: 'pizza',\n\t 55: 'donut',\n\t 56: 'cake',\n\t 57: 'chair',\n\t 58: 'couch',\n\t 59: 'potted plant',\n\t 60: 'bed',\n\t 61: 'dining table',\n\t 62: 'toilet',\n\t 63: 'tv',",
        "type": "code",
        "location": "/tests/video_detector_tests/cocoNames.py:1-64"
    },
    "5221": {
        "file_id": 686,
        "content": "This code defines a dictionary named \"cocoName\" that maps integer labels to object names, used for image classification tasks based on the MS COCO dataset.",
        "type": "comment"
    },
    "5222": {
        "file_id": 686,
        "content": "\t 64: 'laptop',\n\t 65: 'mouse',\n\t 66: 'remote',\n\t 67: 'keyboard',\n\t 68: 'cell phone',\n\t 69: 'microwave',\n\t 70: 'oven',\n\t 71: 'toaster',\n\t 72: 'sink',\n\t 73: 'refrigerator',\n\t 74: 'book',\n\t 75: 'clock',\n\t 76: 'vase',\n\t 77: 'scissors',\n\t 78: 'teddy bear',\n\t 79: 'hair drier',\n\t 80: 'toothbrush'}\ncocoRealName = {k-1:cocoName[k] for k in cocoName.keys()}",
        "type": "code",
        "location": "/tests/video_detector_tests/cocoNames.py:65-83"
    },
    "5223": {
        "file_id": 686,
        "content": "The code defines a dictionary named \"cocoRealName\" that maps object names to corresponding COCO indexes. It uses a dictionary comprehension to subtract 1 from each key in the original \"cocoName\" dictionary, which assumes an offset of 0-indexing in the dataset labels.",
        "type": "comment"
    },
    "5224": {
        "file_id": 687,
        "content": "/tests/video_detector_tests/detectron2_model_zoo_url.py",
        "type": "filepath"
    },
    "5225": {
        "file_id": 687,
        "content": "This code maps Detectron2 COCO model names to checkpoint files, defining configurations for trained parameters and providing pretrained model URLs and checkpoints.",
        "type": "summary"
    },
    "5226": {
        "file_id": 687,
        "content": "from typing import Optional\nclass _ModelZooUrls(object):\n    \"\"\"\n    Mapping from names to officially released Detectron2 pre-trained models.\n    \"\"\"\n    S3_PREFIX = \"https://dl.fbaipublicfiles.com/detectron2/\"\n    # format: {config_path.yaml} -> model_id/model_final_{commit}.pkl\n    CONFIG_PATH_TO_URL_SUFFIX = {\n        # COCO Detection with Faster R-CNN\n        \"COCO-Detection/faster_rcnn_R_50_C4_1x\": \"137257644/model_final_721ade.pkl\",\n        \"COCO-Detection/faster_rcnn_R_50_DC5_1x\": \"137847829/model_final_51d356.pkl\",\n        \"COCO-Detection/faster_rcnn_R_50_FPN_1x\": \"137257794/model_final_b275ba.pkl\",\n        \"COCO-Detection/faster_rcnn_R_50_C4_3x\": \"137849393/model_final_f97cb7.pkl\",\n        \"COCO-Detection/faster_rcnn_R_50_DC5_3x\": \"137849425/model_final_68d202.pkl\",\n        \"COCO-Detection/faster_rcnn_R_50_FPN_3x\": \"137849458/model_final_280758.pkl\",\n        \"COCO-Detection/faster_rcnn_R_101_C4_3x\": \"138204752/model_final_298dad.pkl\",\n        \"COCO-Detection/faster_rcnn_R_101_DC5_3x\": \"138204841/model_final_3e0943.pkl\",",
        "type": "code",
        "location": "/tests/video_detector_tests/detectron2_model_zoo_url.py:2-20"
    },
    "5227": {
        "file_id": 687,
        "content": "The code defines a class \"_ModelZooUrls\" that provides a mapping between Detectron2 pre-trained model names and their respective URLs. It uses the \"S3_PREFIX\" to specify the base URL for downloading models and stores each model's path in the \"CONFIG_PATH_TO_URL_SUFFIX\" dictionary. The code includes various pre-trained COCO Detection models, such as Faster R-CNN with different architectures and scales.",
        "type": "comment"
    },
    "5228": {
        "file_id": 687,
        "content": "        \"COCO-Detection/faster_rcnn_R_101_FPN_3x\": \"137851257/model_final_f6e8b1.pkl\",\n        \"COCO-Detection/faster_rcnn_X_101_32x8d_FPN_3x\": \"139173657/model_final_68b088.pkl\",\n        # COCO Detection with RetinaNet\n        \"COCO-Detection/retinanet_R_50_FPN_1x\": \"190397773/model_final_bfca0b.pkl\",\n        \"COCO-Detection/retinanet_R_50_FPN_3x\": \"190397829/model_final_5bd44e.pkl\",\n        \"COCO-Detection/retinanet_R_101_FPN_3x\": \"190397697/model_final_971ab9.pkl\",\n        # COCO Detection with RPN and Fast R-CNN\n        \"COCO-Detection/rpn_R_50_C4_1x\": \"137258005/model_final_450694.pkl\",\n        \"COCO-Detection/rpn_R_50_FPN_1x\": \"137258492/model_final_02ce48.pkl\",\n        \"COCO-Detection/fast_rcnn_R_50_FPN_1x\": \"137635226/model_final_e5f7ce.pkl\",\n        # COCO Instance Segmentation Baselines with Mask R-CNN\n        \"COCO-InstanceSegmentation/mask_rcnn_R_50_C4_1x\": \"137259246/model_final_9243eb.pkl\",\n        \"COCO-InstanceSegmentation/mask_rcnn_R_50_DC5_1x\": \"137260150/model_final_4f86c3.pkl\",\n        \"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_1x\": \"137260431/model_final_a54504.pkl\",",
        "type": "code",
        "location": "/tests/video_detector_tests/detectron2_model_zoo_url.py:21-34"
    },
    "5229": {
        "file_id": 687,
        "content": "This code maps various Detectron2 models (e.g., faster_rcnn, retinanet, mask_rcnn) to their corresponding pre-trained model files stored in specific URLs or locations. These models are used for tasks like instance segmentation and detection on the COCO dataset.",
        "type": "comment"
    },
    "5230": {
        "file_id": 687,
        "content": "        \"COCO-InstanceSegmentation/mask_rcnn_R_50_C4_3x\": \"137849525/model_final_4ce675.pkl\",\n        \"COCO-InstanceSegmentation/mask_rcnn_R_50_DC5_3x\": \"137849551/model_final_84107b.pkl\",\n        \"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x\": \"137849600/model_final_f10217.pkl\",\n        \"COCO-InstanceSegmentation/mask_rcnn_R_101_C4_3x\": \"138363239/model_final_a2914c.pkl\",\n        \"COCO-InstanceSegmentation/mask_rcnn_R_101_DC5_3x\": \"138363294/model_final_0464b7.pkl\",\n        \"COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x\": \"138205316/model_final_a3ec72.pkl\",\n        \"COCO-InstanceSegmentation/mask_rcnn_X_101_32x8d_FPN_3x\": \"139653917/model_final_2d9806.pkl\",  # noqa\n        # New baselines using Large-Scale Jitter and Longer Training Schedule\n        \"new_baselines/mask_rcnn_R_50_FPN_100ep_LSJ\": \"42047764/model_final_bb69de.pkl\",\n        \"new_baselines/mask_rcnn_R_50_FPN_200ep_LSJ\": \"42047638/model_final_89a8d3.pkl\",\n        \"new_baselines/mask_rcnn_R_50_FPN_400ep_LSJ\": \"42019571/model_final_14d201.pkl\",",
        "type": "code",
        "location": "/tests/video_detector_tests/detectron2_model_zoo_url.py:35-45"
    },
    "5231": {
        "file_id": 687,
        "content": "This code maps different model names to their corresponding checkpoint files in the Detectron2 Model Zoo. It includes COCO instance segmentation models and new baselines with Large-Scale Jitter and longer training schedules.",
        "type": "comment"
    },
    "5232": {
        "file_id": 687,
        "content": "        \"new_baselines/mask_rcnn_R_101_FPN_100ep_LSJ\": \"42025812/model_final_4f7b58.pkl\",\n        \"new_baselines/mask_rcnn_R_101_FPN_200ep_LSJ\": \"42131867/model_final_0bb7ae.pkl\",\n        \"new_baselines/mask_rcnn_R_101_FPN_400ep_LSJ\": \"42073830/model_final_f96b26.pkl\",\n        \"new_baselines/mask_rcnn_regnetx_4gf_dds_FPN_100ep_LSJ\": \"42047771/model_final_b7fbab.pkl\",  # noqa\n        \"new_baselines/mask_rcnn_regnetx_4gf_dds_FPN_200ep_LSJ\": \"42132721/model_final_5d87c1.pkl\",  # noqa\n        \"new_baselines/mask_rcnn_regnetx_4gf_dds_FPN_400ep_LSJ\": \"42025447/model_final_f1362d.pkl\",  # noqa\n        \"new_baselines/mask_rcnn_regnety_4gf_dds_FPN_100ep_LSJ\": \"42047784/model_final_6ba57e.pkl\",  # noqa\n        \"new_baselines/mask_rcnn_regnety_4gf_dds_FPN_200ep_LSJ\": \"42047642/model_final_27b9c1.pkl\",  # noqa\n        \"new_baselines/mask_rcnn_regnety_4gf_dds_FPN_400ep_LSJ\": \"42045954/model_final_ef3a80.pkl\",  # noqa\n        # COCO Person Keypoint Detection Baselines with Keypoint R-CNN\n        \"COCO-Keypoints/keypoint_rcnn_R_50_FPN_1x\": \"137261548/model_final_04e291.pkl\",",
        "type": "code",
        "location": "/tests/video_detector_tests/detectron2_model_zoo_url.py:46-56"
    },
    "5233": {
        "file_id": 687,
        "content": "This code defines a mapping of model names to their corresponding final pickle files. The models are Detectron2 COCO Person Keypoint Detection Baselines and include variations of Mask R-CNN, Mask R-CNN with RegNetX/Y, and the COCO-Keypoints Keypoint R-CNN.",
        "type": "comment"
    },
    "5234": {
        "file_id": 687,
        "content": "        \"COCO-Keypoints/keypoint_rcnn_R_50_FPN_3x\": \"137849621/model_final_a6e10b.pkl\",\n        \"COCO-Keypoints/keypoint_rcnn_R_101_FPN_3x\": \"138363331/model_final_997cc7.pkl\",\n        \"COCO-Keypoints/keypoint_rcnn_X_101_32x8d_FPN_3x\": \"139686956/model_final_5ad38f.pkl\",\n        # COCO Panoptic Segmentation Baselines with Panoptic FPN\n        \"COCO-PanopticSegmentation/panoptic_fpn_R_50_1x\": \"139514544/model_final_dbfeb4.pkl\",\n        \"COCO-PanopticSegmentation/panoptic_fpn_R_50_3x\": \"139514569/model_final_c10459.pkl\",\n        \"COCO-PanopticSegmentation/panoptic_fpn_R_101_3x\": \"139514519/model_final_cafdb1.pkl\",\n        # LVIS Instance Segmentation Baselines with Mask R-CNN\n        \"LVISv0.5-InstanceSegmentation/mask_rcnn_R_50_FPN_1x\": \"144219072/model_final_571f7c.pkl\",  # noqa\n        \"LVISv0.5-InstanceSegmentation/mask_rcnn_R_101_FPN_1x\": \"144219035/model_final_824ab5.pkl\",  # noqa\n        \"LVISv0.5-InstanceSegmentation/mask_rcnn_X_101_32x8d_FPN_1x\": \"144219108/model_final_5e3439.pkl\",  # noqa",
        "type": "code",
        "location": "/tests/video_detector_tests/detectron2_model_zoo_url.py:57-67"
    },
    "5235": {
        "file_id": 687,
        "content": "This code is a dictionary mapping model names to their corresponding checkpoint files. These models are for Detectron2's object detection, keypoint estimation, and segmentation tasks on COCO and LVIS datasets. The checkpoint files store the trained model parameters for each specific configuration.",
        "type": "comment"
    },
    "5236": {
        "file_id": 687,
        "content": "        # Cityscapes & Pascal VOC Baselines\n        \"Cityscapes/mask_rcnn_R_50_FPN\": \"142423278/model_final_af9cf5.pkl\",\n        \"PascalVOC-Detection/faster_rcnn_R_50_C4\": \"142202221/model_final_b1acc2.pkl\",\n        # Other Settings\n        \"Misc/mask_rcnn_R_50_FPN_1x_dconv_c3-c5\": \"138602867/model_final_65c703.pkl\",\n        \"Misc/mask_rcnn_R_50_FPN_3x_dconv_c3-c5\": \"144998336/model_final_821d0b.pkl\",\n        \"Misc/cascade_mask_rcnn_R_50_FPN_1x\": \"138602847/model_final_e9d89b.pkl\",\n        \"Misc/cascade_mask_rcnn_R_50_FPN_3x\": \"144998488/model_final_480dd8.pkl\",\n        \"Misc/mask_rcnn_R_50_FPN_3x_syncbn\": \"169527823/model_final_3b3c51.pkl\",\n        \"Misc/mask_rcnn_R_50_FPN_3x_gn\": \"138602888/model_final_dc5d9e.pkl\",\n        \"Misc/scratch_mask_rcnn_R_50_FPN_3x_gn\": \"138602908/model_final_01ca85.pkl\",\n        \"Misc/scratch_mask_rcnn_R_50_FPN_9x_gn\": \"183808979/model_final_da7b4c.pkl\",\n        \"Misc/scratch_mask_rcnn_R_50_FPN_9x_syncbn\": \"184226666/model_final_5ce33e.pkl\",\n        \"Misc/panoptic_fpn_R_101_dconv_cascade_gn_3x\": \"139797668/model_final_be35db.pkl\",",
        "type": "code",
        "location": "/tests/video_detector_tests/detectron2_model_zoo_url.py:68-81"
    },
    "5237": {
        "file_id": 687,
        "content": "This code defines model configurations and their corresponding checkpoint file paths for various tasks like Cityscapes, Pascal VOC detection, and panoptic segmentation. These configurations include different architectures and training strategies such as syncBN and gn. The checkpoint files store the trained model parameters which can be loaded to replicate the results.",
        "type": "comment"
    },
    "5238": {
        "file_id": 687,
        "content": "        \"Misc/cascade_mask_rcnn_X_152_32x8d_FPN_IN5k_gn_dconv\": \"18131413/model_0039999_e76410.pkl\",  # noqa\n        # D1 Comparisons\n        \"Detectron1-Comparisons/faster_rcnn_R_50_FPN_noaug_1x\": \"137781054/model_final_7ab50c.pkl\",  # noqa\n        \"Detectron1-Comparisons/mask_rcnn_R_50_FPN_noaug_1x\": \"137781281/model_final_62ca52.pkl\",  # noqa\n        \"Detectron1-Comparisons/keypoint_rcnn_R_50_FPN_1x\": \"137781195/model_final_cce136.pkl\",\n    }\n    @staticmethod\n    def query(config_path: str) -> Optional[str]:\n        \"\"\"\n        Args:\n            config_path: relative config filename\n        \"\"\"\n        name = config_path.replace(\".yaml\", \"\").replace(\".py\", \"\")\n        if name in _ModelZooUrls.CONFIG_PATH_TO_URL_SUFFIX:\n            suffix = _ModelZooUrls.CONFIG_PATH_TO_URL_SUFFIX[name]\n            return _ModelZooUrls.S3_PREFIX + name + \"/\" + suffix\n        return None\ndef get_checkpoint_url(config_path):\n    \"\"\"\n    Returns the URL to the model trained using the given config\n    Args:\n        config_path (str): config file name relative to detectron2's \"configs/\"",
        "type": "code",
        "location": "/tests/video_detector_tests/detectron2_model_zoo_url.py:82-107"
    },
    "5239": {
        "file_id": 687,
        "content": "This code provides a function to query the model URL and checkpoint from a given configuration path. It maps specific configurations to their respective URL suffixes and uses them to generate the model's URL, including a prefix. The function returns the URL if a valid mapping is found; otherwise, it returns None.",
        "type": "comment"
    },
    "5240": {
        "file_id": 687,
        "content": "            directory, e.g., \"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_1x.yaml\"\n    Returns:\n        str: a URL to the model\n    \"\"\"\n    url = _ModelZooUrls.query(config_path)\n    if url is None:\n        raise RuntimeError(\"Pretrained model for {} is not available!\".format(config_path))\n    return url\nif __name__ == \"__main__\":\n    test_config = \"Detectron1-Comparisons/mask_rcnn_R_50_FPN_noaug_1x\"\n    url = get_checkpoint_url(test_config)\n    print(\"model name:\",test_config)\n    print(\"model url:\",url)",
        "type": "code",
        "location": "/tests/video_detector_tests/detectron2_model_zoo_url.py:108-122"
    },
    "5241": {
        "file_id": 687,
        "content": "This code defines a function `get_checkpoint_url` that returns the URL of a pretrained model based on its configuration path. It also checks if a valid URL exists for the given config, and raises an error if not. The provided example demonstrates how to use this function with a specific config, printing the model name and URL.",
        "type": "comment"
    },
    "5242": {
        "file_id": 688,
        "content": "/tests/video_detector_tests/detectron2_norfair.py",
        "type": "filepath"
    },
    "5243": {
        "file_id": 688,
        "content": "The code uses Detectron2 for object detection, tracks \"person\" or \"dog\", updates tracked_objects, and displays bounding boxes. It utilizes OpenCV for video display and waits for 'q' to terminate, closing windows upon exiting.",
        "type": "summary"
    },
    "5244": {
        "file_id": 688,
        "content": "import cv2\nimport numpy as np\nfrom detectron2.config import get_cfg\nfrom detectron2.engine import DefaultPredictor\nfrom cocoNames import cocoRealName\nfrom norfair import Detection, Tracker, Video, draw_tracked_objects\ndef euclidean_distance(detection, tracked_object):\n    return np.linalg.norm(detection.points - tracked_object.estimate)\n# Set up Detectron2 object detector\ncfg = get_cfg()\ncfg.merge_from_file(\"norfair/demos/faster_rcnn_R_50_FPN_3x.yaml\")\ncfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5 # looks like it does not recognize dog.\n# cfg.MODEL.WEIGHTS = \"detectron2://COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl\"\ncfg.MODEL.WEIGHTS = \"/root/Desktop/works/pyjom/tests/video_detector_tests/detectron2_models/model_final_f10217.pkl\"\n# it is stored in s3\n# https://dl.fbaipublicfiles.com/detectron2/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl\n# download cache: /root/.torch/iopath_cache/detectron2/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl",
        "type": "code",
        "location": "/tests/video_detector_tests/detectron2_norfair.py:1-21"
    },
    "5245": {
        "file_id": 688,
        "content": "The code imports necessary libraries and sets up a Detectron2 object detector using pre-trained weights for instance segmentation. It also defines a function to calculate Euclidean distance between detection and tracked objects. The configuration file specifies the model architecture, which is R_50_FPN with three stages and the specific weights (model_final_f10217.pkl) to be used for detection. The weights can either be downloaded from a public S3 storage or retrieved from the local cache if already downloaded.",
        "type": "comment"
    },
    "5246": {
        "file_id": 688,
        "content": "detector = DefaultPredictor(cfg)\n# what are the classes output by the model?\n# Norfair\nvideo_path = \"/root/Desktop/works/pyjom/samples/video/dog_with_text.mp4\"\n# video_path = \"/root/Desktop/works/pyjom/samples/video/LlfeL29BP.mp4\"\nvideo = Video(input_path=video_path)\ntracker = Tracker(distance_function=euclidean_distance, distance_threshold=400,hit_inertia_min=2,hit_inertia_max=20,initialization_delay=1) # what the heck?\ntracked_objects = None\ndisplay=True\nfor index, frame in enumerate(video): # we need to speed up.\n    if index%10 == 0:\n        detections = detector(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n        print(\"original detections:\",detections)\n        instances = detections[\"instances\"]\n        print(\"instances:\",instances)\n        # breakpoint()\n        pred_classes = instances.pred_classes\n        if len(pred_classes) == 0:\n            continue\n        detections2=[]\n        for index,class_ in enumerate(pred_classes):\n            print(\"index:\",index)\n            class_ = int(class_.cpu().numpy().tolist())",
        "type": "code",
        "location": "/tests/video_detector_tests/detectron2_norfair.py:23-48"
    },
    "5247": {
        "file_id": 688,
        "content": "The code initializes a video detector using Detector class and then processes each frame of the video. It predicts instances in each frame, prints detected classes and instances, and continues only if there are predictions. The tracker is used to track objects over frames, but its parameters might need clarification. Speedup is mentioned as needed, which implies potential optimizations.",
        "type": "comment"
    },
    "5248": {
        "file_id": 688,
        "content": "            print(\"class:\",class_)\n            box = instances.pred_boxes.tensor[index].cpu().numpy().tolist()\n            box = [int(x) for x in box]\n            score = float(instances.scores[index].cpu().numpy().tolist())\n            print('box:',box)\n            print('score:',score)\n            className = cocoRealName[class_]\n            # we filter our targets.\n            if className not in [\"person\",\"dog\"]:\n                continue\n            mdata = {\"box\":box,\"class\":{\"id\":class_,\"name\":className}}\n            det = Detection(instances.pred_boxes.get_centers()[index].cpu().numpy(),scores=np.array([score]),data=mdata)\n            detections2.append(det)\n            # breakpoint()\n        # detections = [Detection(p) for p in instances.pred_boxes.get_centers().cpu().numpy()] # what is this instance anyway?\n        # you would lost data you dick!\n        print(\"detections2\",detections2)\n        tracked_objects = tracker.update(detections=detections2)\n        # print(detections)\n        print(\"tracked objects:\",tracked_objects) # you don't track shit?",
        "type": "code",
        "location": "/tests/video_detector_tests/detectron2_norfair.py:49-68"
    },
    "5249": {
        "file_id": 688,
        "content": "This code is filtering and creating detection objects for \"person\" or \"dog\" instances from a given dataset. It prints the box coordinates, score, and class name before adding it to the detections2 list. The code then updates tracked_objects using the tracker function with the detections2 list.",
        "type": "comment"
    },
    "5250": {
        "file_id": 688,
        "content": "    if tracked_objects is not None:\n        if tracked_objects!=[]:\n            # there is no bounding box avaliable?\n            for obj in tracked_objects:\n                point = obj.estimate[0]\n                position = tuple(point.astype(int))\n                color = (255,0,0)\n                # breakpoint()\n                name = obj.last_detection.data[\"class\"][\"name\"]\n                cv2.circle(\n                        frame,\n                        position,\n                        radius=100,\n                        color=color,\n                        thickness=2,\n                    )\n                cv2.putText(\n                    frame,\n                    \"[{}][{}]\".format(str(obj.id),name),\n                    (position[0]-100,position[1]),\n                    cv2.FONT_HERSHEY_SIMPLEX,\n                    2,\n                    (0,255,0),\n                    3,\n                    cv2.LINE_AA,\n                )\n            # breakpoint()\n        # i want to draw you in a different way.\n        # draw_tracked_objects(frame, tracked_objects,color=(255,0,0))",
        "type": "code",
        "location": "/tests/video_detector_tests/detectron2_norfair.py:69-97"
    },
    "5251": {
        "file_id": 688,
        "content": "The code checks if there are any tracked objects and then proceeds to draw bounding boxes around them, add labels for the objects' class names, and display their IDs on the frame using OpenCV functions. Additionally, it offers an alternative way to draw the objects in a different color.",
        "type": "comment"
    },
    "5252": {
        "file_id": 688,
        "content": "    if display:\n        cv2.imshow(\"window\",frame)\n        key  =  cv2.waitKey(1) & 0xff\n        if key == ord('q'):\n            break\n        # maybe we shall print this shit somehow.\n    # video.write(frame) # you write what?\nif display:\n    cv2.destroyAllWindows()",
        "type": "code",
        "location": "/tests/video_detector_tests/detectron2_norfair.py:98-106"
    },
    "5253": {
        "file_id": 688,
        "content": "The code displays a frame from a video using OpenCV's imshow function and waits for a user input (key) to terminate. The key input is checked if it matches the character 'q', which signals a break in the loop. OpenCV's destroyAllWindows() is called to close the window when display is enabled.",
        "type": "comment"
    },
    "5254": {
        "file_id": 689,
        "content": "/tests/video_detector_tests/frameDifference.py",
        "type": "filepath"
    },
    "5255": {
        "file_id": 689,
        "content": "This function detects motion in a video by comparing frames, calculating differences, applying thresholding and morphology operations, and identifying contours. The code displays two consecutive frames side-by-side using OpenCV and stops when 'Esc' is pressed.",
        "type": "summary"
    },
    "5256": {
        "file_id": 689,
        "content": "import cv2 as cv\nimport numpy as np\nfrom matplotlib import pyplot as plt\ndef motionDetection(videoPath):\n    cap = cv.VideoCapture(videoPath)\n    ret, frame1 = cap.read()\n    ret, frame2 = cap.read()\n    while cap.isOpened():\n        if frame1 is not None and frame2 is not None:\n            pass\n        else:\n            break\n        diff = cv.absdiff(frame1, frame2)\n        diff_gray = cv.cvtColor(diff, cv.COLOR_BGR2GRAY)\n        blur = cv.GaussianBlur(diff_gray, (5, 5), 0)\n        _, thresh = cv.threshold(blur, 20, 255, cv.THRESH_BINARY)\n        dilated = cv.dilate(thresh, None, iterations=3)\n        contours, _ = cv.findContours(\n            dilated, cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE)\n        for contour in contours:\n            (x, y, w, h) = cv.boundingRect(contour)\n            if cv.contourArea(contour) < 900:\n                continue\n            cv.rectangle(frame1, (x, y), (x+w, y+h), (0, 255, 0), 2)\n            cv.putText(frame1, \"Status: {}\".format('Movement'), (10, 20), cv.FONT_HERSHEY_SIMPLEX,\n                        1, (255, 0, 0), 3)",
        "type": "code",
        "location": "/tests/video_detector_tests/frameDifference.py:1-30"
    },
    "5257": {
        "file_id": 689,
        "content": "This function performs motion detection by comparing successive frames in a video, calculates the difference, applies thresholding and morphology operations, and finally detects contours to identify areas with significant changes.",
        "type": "comment"
    },
    "5258": {
        "file_id": 689,
        "content": "        # cv.drawContours(frame1, contours, -1, (0, 255, 0), 2)\n        cv.imshow(\"Video\", frame1)\n        frame1 = frame2\n        ret, frame2 = cap.read()\n        if cv.waitKey(50) == 27:\n            break\n    cap.release()\n    cv.destroyAllWindows()\nif __name__ == \"__main__\":\n    motionDetection(\"../../samples/video/LiEIfnsvn.mp4\")",
        "type": "code",
        "location": "/tests/video_detector_tests/frameDifference.py:32-46"
    },
    "5259": {
        "file_id": 689,
        "content": "This code displays two consecutive frames of a video side by side, highlighting the difference between them using OpenCV. It reads frames from a video file and continuously checks for user input to break the loop when key 'Esc' is pressed.",
        "type": "comment"
    },
    "5260": {
        "file_id": 690,
        "content": "/tests/video_detector_tests/mathlib.py",
        "type": "filepath"
    },
    "5261": {
        "file_id": 690,
        "content": "The code uses Sympy to merge overlapping intervals in a list of tuples, creates unified boundaries, and returns final results as merged continual mappings.",
        "type": "summary"
    },
    "5262": {
        "file_id": 690,
        "content": "# not overriding math.\n# do some ranged stuff here...\ndef getContinualNonSympyMergeResult(inputMSetCandidates):\n    # basically the same example.\n    # assume no overlapping here.\n    import sympy\n    def unionToTupleList(myUnion):\n        unionBoundaries = list(myUnion.boundary)\n        unionBoundaries.sort()\n        leftBoundaries = unionBoundaries[::2]\n        rightBoundaries = unionBoundaries[1::2]\n        return list(zip(leftBoundaries, rightBoundaries))\n    def tupleSetToUncertain(mSet):\n        mUncertain = None\n        for start, end in mSet:\n            if mUncertain is None:\n                mUncertain = sympy.Interval(start, end)\n            else:\n                mUncertain += sympy.Interval(start, end)\n        typeUncertain = type(mUncertain)\n        return mUncertain, typeUncertain\n    def mergeOverlappedInIntervalTupleList(intervalTupleList):\n        mUncertain, _ = tupleSetToUncertain(intervalTupleList)\n        mUncertainBoundaryList = list(mUncertain.boundary)\n        mUncertainBoundaryList.sort()",
        "type": "code",
        "location": "/tests/video_detector_tests/mathlib.py:1-28"
    },
    "5263": {
        "file_id": 690,
        "content": "This code defines three functions for set operations involving intervals. The functions are getContinualNonSympyMergeResult, unionToTupleList, and mergeOverlappedInIntervalTupleList. The code uses Sympy library to handle mathematical operations on intervals and merges non-overlapping intervals into a single uncertain variable. It sorts and converts intervals into tuples for further processing.",
        "type": "comment"
    },
    "5264": {
        "file_id": 690,
        "content": "        mergedIntervalTupleList = list(\n            zip(mUncertainBoundaryList[::2], mUncertainBoundaryList[1::2])\n        )\n        return mergedIntervalTupleList\n    # mSet = mergeOverlappedInIntervalTupleList([(0, 1), (2, 3)])\n    # mSet2 = mergeOverlappedInIntervalTupleList([(0.5, 1.5), (1.6, 2.5)])\n    # print(\"MSET\", mSet)\n    # print(\"MSET2\", mSet2)\n    mSetCandidates = [\n        mergeOverlappedInIntervalTupleList(x) for x in inputMSetCandidates\n    ]\n    mSetUnified = [x for y in mSetCandidates for x in y]\n    leftBoundaryList = set([x[0] for x in mSetUnified])\n    rightBoundaryList = set([x[1] for x in mSetUnified])\n    # they may freaking overlap.\n    # if want nearby-merge strategy, simply just expand all intervals, merge them with union and shrink the individual intervals inside union respectively.\n    markers = {\n        \"enter\": {k: [] for k in leftBoundaryList},\n        \"exit\": {k: [] for k in rightBoundaryList},\n    }\n    for index, mSetCandidate in enumerate(mSetCandidates):\n        leftBoundaryListOfCandidate = [x[0] for x in mSetCandidate]",
        "type": "code",
        "location": "/tests/video_detector_tests/mathlib.py:29-55"
    },
    "5265": {
        "file_id": 690,
        "content": "This code defines a function `mergeOverlappedInIntervalTupleList` which takes a list of interval tuples, merges overlapping intervals, and returns the merged list. The main purpose is to unify all the data in the same scope with potential overlap. It first creates a set of left and right boundaries from the unified data, then initializes a `markers` dictionary with \"enter\" and \"exit\" markers for each left boundary. Finally, it iterates over each candidate set, extracting the left boundaries and using them to update the marker dictionary. The final merged interval tuples are not explicitly calculated or returned here, but can be derived from the information in `markers`.",
        "type": "comment"
    },
    "5266": {
        "file_id": 690,
        "content": "        rightBoundaryListOfCandidate = [x[1] for x in mSetCandidate]\n        for leftBoundaryOfCandidate in leftBoundaryListOfCandidate:\n            markers[\"enter\"][leftBoundaryOfCandidate].append(index)  # remap this thing!\n        for rightBoundaryOfCandidate in rightBoundaryListOfCandidate:\n            markers[\"exit\"][rightBoundaryOfCandidate].append(index)  # remap this thing!\n    # now, iterate through the boundaries of mSetUnified.\n    unifiedBoundaryList = leftBoundaryList.union(\n        rightBoundaryList\n    )  # call me a set instead of a list please? now we must sort this thing\n    unifiedBoundaryList = list(unifiedBoundaryList)\n    unifiedBoundaryList.sort()\n    unifiedBoundaryMarks = {}\n    finalMappings = {}\n    # print(\"MARKERS\", markers)\n    # breakpoint()\n    for index, boundary in enumerate(unifiedBoundaryList):\n        previousMark = unifiedBoundaryMarks.get(index - 1, [])\n        enterList = markers[\"enter\"].get(boundary, [])\n        exitList = markers[\"exit\"].get(boundary, [])\n        currentMark = set(previousMark + enterList).difference(set(exitList))",
        "type": "code",
        "location": "/tests/video_detector_tests/mathlib.py:56-77"
    },
    "5267": {
        "file_id": 690,
        "content": "This code creates a set of unified boundaries and maps the markers accordingly. It first gathers the \"enter\" and \"exit\" markers for each boundary, then forms the final mappings by taking the difference between the \"enter\" and \"exit\" lists. The code also sorts the boundaries and retrieves previous marks to form the current mark set.",
        "type": "comment"
    },
    "5268": {
        "file_id": 690,
        "content": "        currentMark = list(currentMark)\n        unifiedBoundaryMarks.update({index: currentMark})\n        # now, handle the change? or not?\n        # let's just deal those empty ones, shall we?\n        if previousMark == []:  # inside it is empty range.\n            # elif currentMark == []:\n            if index == 0:\n                continue  # just the start, no need to note this down.\n            else:\n                finalMappings.update(\n                    {\n                        \"empty\": finalMappings.get(\"empty\", [])\n                        + [(unifiedBoundaryList[index - 1], boundary)]\n                    }\n                )\n            # the end of previous mark! this interval belongs to previousMark\n        else:\n            key = previousMark.copy()\n            key.sort()\n            key = tuple(key)\n            finalMappings.update(\n                {\n                    key: finalMappings.get(key, [])\n                    + [(unifiedBoundaryList[index - 1], boundary)]\n                }\n            )",
        "type": "code",
        "location": "/tests/video_detector_tests/mathlib.py:78-103"
    },
    "5269": {
        "file_id": 690,
        "content": "This code checks if the current mark is empty and updates the finalMappings accordingly. If previousMark is empty, it skips noting down just the start of a range. Otherwise, it sorts and makes a unique key using previousMark, then adds the interval to finalMappings for that key.",
        "type": "comment"
    },
    "5270": {
        "file_id": 690,
        "content": "            # also the end of previous mark! belongs to previousMark.\n    ### NOW THE FINAL OUTPUT ###\n    finalCats = {}\n    for key, value in finalMappings.items():\n        # value is an array containing subInterval tuples.\n        value = mergeOverlappedInIntervalTupleList(value)\n        finalCats.update({key: value})\n    # print(\"______________FINAL CATS______________\")\n    # print(finalCats)\n    return finalCats\ndef getContinualMappedNonSympyMergeResult(mRangesDict, concatSymbol=\"|\", noEmpty=True):\n    mKeyMaps = list(mRangesDict.keys())\n    mSetCandidates = [mRangesDict[key] for key in mKeyMaps]\n    # the next step will automatically merge all overlapped candidates.\n    finalCats = getContinualNonSympyMergeResult(mSetCandidates)\n    finalCatsMapped = {\n        concatSymbol.join([mKeyMaps[k] for k in mTuple]): finalCats[mTuple]\n        for mTuple in finalCats.keys()\n        if type(mTuple) == tuple\n    }\n    if not noEmpty:\n        finalCatsMapped.update(\n            {k: finalCats[k] for k in finalCats.keys() if type(k) != tuple}",
        "type": "code",
        "location": "/tests/video_detector_tests/mathlib.py:104-130"
    },
    "5271": {
        "file_id": 690,
        "content": "This code calculates merged, continual results for a dictionary of sets. It maps the results to a format using a specified concatenation symbol, and allows for empty sets if requested. It uses functions like getContinualNonSympyMergeResult and mergeOverlappedInIntervalTupleList to merge overlapping intervals. The final result is returned as a dictionary of merged continual mappings.",
        "type": "comment"
    },
    "5272": {
        "file_id": 690,
        "content": "        )\n    return finalCatsMapped\n    # default not to output empty set?\ndef getContinualMappedNonSympyMergeResultWithRangedEmpty(\n    mRangesDict, start, end, concatSymbol=\"|\"\n):\n    import uuid\n    emptySetName = str(uuid.uuid4())\n    newRangesDict = mRangesDict.copy()\n    newRangesDict.update({emptySetName: [(start, end)]})\n    newRangesDict = getContinualMappedNonSympyMergeResult(\n        newRangesDict, concatSymbol=\"|\", noEmpty=True\n    )\n    newRangesDict = {\n        key: [\n            (mStart, mEnd)\n            for mStart, mEnd in newRangesDict[key]\n            if mStart >= start and mEnd <= end\n        ]\n        for key in newRangesDict.keys()\n    }\n    newRangesDict = {\n        key: newRangesDict[key]\n        for key in newRangesDict.keys()\n        if newRangesDict[key] != []\n    }\n    finalNewRangesDict = {}\n    for key in newRangesDict.keys():\n        mergedEmptySetName = \"{}{}\".format(concatSymbol, emptySetName)\n        if mergedEmptySetName in key:\n            newKey = key.replace(mergedEmptySetName,\"\")",
        "type": "code",
        "location": "/tests/video_detector_tests/mathlib.py:131-164"
    },
    "5273": {
        "file_id": 690,
        "content": "Function to get a continual mapped non-Sympy merge result with range based on input parameters. It creates a new dictionary with an empty set named UUID, then updates the existing dictionary with this new one. Filters out any ranges that do not fall within the given start and end values. Removes any keys in the newRangesDict dictionary if their corresponding value is an empty list. Finally, iterates over each key in newRangesDict and checks if mergedEmptySetName exists; if it does, it replaces the key with an empty string.",
        "type": "comment"
    },
    "5274": {
        "file_id": 690,
        "content": "            finalNewRangesDict.update({newKey:newRangesDict[key]})\n        elif key == emptySetName:\n            finalNewRangesDict.update({'empty':newRangesDict[key]})\n        else:\n            finalNewRangesDict.update({key:newRangesDict[key]})\n    return finalNewRangesDict\ndef mergedRangesToSequential(renderDict):\n    renderList = []\n    for renderCommandString in renderDict.keys():\n        commandTimeSpans = renderDict[renderCommandString].copy()\n        # commandTimeSpan.sort(key=lambda x: x[0])\n        for commandTimeSpan in commandTimeSpans:\n            renderList.append([renderCommandString, commandTimeSpan].copy())\n    renderList.sort(key=lambda x: x[1][0])\n    return renderList\n    # for renderCommandString, commandTimeSpan in renderList:\n    #     print(renderCommandString, commandTimeSpan)\ndef sequentialToMergedRanges(sequence):\n    mergedRanges = {}\n    for commandString, commandTimeSpan in sequence:\n        mergedRanges.update({commandString: mergedRanges.get(commandString,[])+[commandTimeSpan]})",
        "type": "code",
        "location": "/tests/video_detector_tests/mathlib.py:165-188"
    },
    "5275": {
        "file_id": 690,
        "content": "Function `mergedRangesToSequential` takes a dictionary where keys are commands and values are time spans, sorts them by start time in ascending order, and returns the sorted list of commands with their respective time spans.\n\nFunction `sequentialToMergedRanges` takes a list of command strings and their corresponding start times, groups them by command string, and produces a dictionary with commands as keys and lists of time spans as values.",
        "type": "comment"
    },
    "5276": {
        "file_id": 690,
        "content": "    mergedRanges = getContinualMappedNonSympyMergeResult(mergedRanges)\n    return mergedRanges",
        "type": "code",
        "location": "/tests/video_detector_tests/mathlib.py:189-190"
    },
    "5277": {
        "file_id": 690,
        "content": "This code block retrieves the merged ranges of continuous numbers using getContinualMappedNonSympyMergeResult function and assigns it to variable 'mergedRanges'. Finally, it returns the mergedRanges.",
        "type": "comment"
    },
    "5278": {
        "file_id": 691,
        "content": "/tests/video_detector_tests/motion_github.py",
        "type": "filepath"
    },
    "5279": {
        "file_id": 691,
        "content": "This code imports libraries, initializes a motion detector algorithm and sets up video capture. It continuously reads frames from the source, applies an algorithm to create output images, displays them in separate windows, and runs until a frame is not ready or Esc key pressed.",
        "type": "summary"
    },
    "5280": {
        "file_id": 691,
        "content": "# motion detectors are used to track objects. though you may want to separate objects with it.\nimport numpy as np\nimport cv2\nimport pybgs as bgs\n# suspect by static image analysis, and then create bounding box over the thing.\n# check image quality.\nalgorithm = bgs.FrameDifference() # track object we need that.\n# algorithm = bgs.SuBSENSE()\n# video_file = \"../../samples/video/highway_car.avi\"\n# video_file = \"../../samples/video/dog_with_text.mp4\"\nvideo_file = \"../../samples/video/LiEIfnsvn.mp4\" # this one with cropped boundaries. \n# video_file = \"../../samples/video/LlfeL29BP.mp4\"\n# maybe we should consider something else to crop the thing? or not?\n# accumulate the delta over time to see the result?\n# use static detection method.\ncapture = cv2.VideoCapture(video_file)\nwhile not capture.isOpened():\n  capture = cv2.VideoCapture(video_file)\n  cv2.waitKey(1000)\n  print(\"Wait for the header\")\n#pos_frame = capture.get(cv2.cv.CV_CAP_PROP_POS_FRAMES)\n#pos_frame = capture.get(cv2.CV_CAP_PROP_POS_FRAMES)\npos_frame = capture.get(1)",
        "type": "code",
        "location": "/tests/video_detector_tests/motion_github.py:1-29"
    },
    "5281": {
        "file_id": 691,
        "content": "This code imports necessary libraries and initializes a motion detector algorithm (FrameDifference) to track objects in a video. It also defines the video file path and sets up a VideoCapture object. The code waits for the video header, retrieves the current frame position, and is ready to process frames using the motion detection algorithm.",
        "type": "comment"
    },
    "5282": {
        "file_id": 691,
        "content": "while True:\n  flag, frame = capture.read()\n  if flag:\n    cv2.imshow('video', frame)\n    #pos_frame = capture.get(cv2.cv.CV_CAP_PROP_POS_FRAMES)\n    #pos_frame = capture.get(cv2.CV_CAP_PROP_POS_FRAMES)\n    pos_frame = capture.get(1)\n    #print str(pos_frame)+\" frames\"\n    img_output = algorithm.apply(frame)\n    img_bgmodel = algorithm.getBackgroundModel()\n    cv2.imshow('img_output', img_output)\n    cv2.imshow('img_bgmodel', img_bgmodel)\n  else:\n    #capture.set(cv2.cv.CV_CAP_PROP_POS_FRAMES, pos_frame-1)\n    #capture.set(cv2.CV_CAP_PROP_POS_FRAMES, pos_frame-1)\n    #capture.set(1, pos_frame-1)\n    #print \"Frame is not ready\"\n    cv2.waitKey(1000)\n    break\n  if 0xFF & cv2.waitKey(10) == 27:\n    break\n  #if capture.get(cv2.cv.CV_CAP_PROP_POS_FRAMES) == capture.get(cv2.cv.CV_CAP_PROP_FRAME_COUNT):\n  #if capture.get(cv2.CV_CAP_PROP_POS_FRAMES) == capture.get(cv2.CV_CAP_PROP_FRAME_COUNT):\n  #if capture.get(1) == capture.get(cv2.CV_CAP_PROP_FRAME_COUNT):\n    #break\ncv2.destroyAllWindows()",
        "type": "code",
        "location": "/tests/video_detector_tests/motion_github.py:30-62"
    },
    "5283": {
        "file_id": 691,
        "content": "The code continuously reads frames from a video source and displays them. It captures the current frame position, applies an algorithm to create output images, and shows the output and background model images in separate windows. It keeps running until a frame is not ready or the user presses Esc key, closing all windows at the end.",
        "type": "comment"
    },
    "5284": {
        "file_id": 692,
        "content": "/tests/video_detector_tests/motion_gpl.sh",
        "type": "filepath"
    },
    "5285": {
        "file_id": 692,
        "content": "Kills motion process and attempts to capture video from webcam using ffmpeg, then starts motion with the mconfig.conf configuration file. Not suitable for media file processing.",
        "type": "summary"
    },
    "5286": {
        "file_id": 692,
        "content": "killall -s KILL motion\n# ffmpeg -re -i ../../samples/video/LlfeL29BP.mp4 -f v4l2 /dev/video0 &\nmotion -c mconfig.conf\n# to conclude, this is only useful for webcams, not for media file processing.\n# are you sure if you want to capture shits over webcams by this?",
        "type": "code",
        "location": "/tests/video_detector_tests/motion_gpl.sh:1-6"
    },
    "5287": {
        "file_id": 692,
        "content": "Kills motion process and attempts to capture video from webcam using ffmpeg, then starts motion with the mconfig.conf configuration file. Not suitable for media file processing.",
        "type": "comment"
    },
    "5288": {
        "file_id": 693,
        "content": "/tests/video_detector_tests/pip_meanVariance_stablize.py",
        "type": "filepath"
    },
    "5289": {
        "file_id": 693,
        "content": "The function uses calculations, filters, and analyses to process data and perform tasks such as linear regression and image analysis. The code creates a rectangular plot using matplotlib with randomly chosen colors and displays it using plt.show().",
        "type": "summary"
    },
    "5290": {
        "file_id": 693,
        "content": "from mathlib import *\n# from ...pyjom.mathlib import sequentialToMergedRanges\n# you can use yolo to train network to detect these sharp corners, total four sharp corners.\n# but it might fail to do so.\n# but what about other stuff?\n# whatever let's just use this.\ndef sampledStablePipRegionExporter(data, defaultWidth, defaultHeight):\n    defaultWidth, defaultHeight = int(defaultWidth), int(defaultHeight)\n    import numpy as np\n    data = np.array(data)\n    def getAlikeValueMerged(mArray, threshold=35):\n        for index, elem in enumerate(mArray[:-1]):\n            nextElem = mArray[index + 1]\n            if abs(nextElem - elem) < threshold:\n                mArray[index + 1] = elem\n        return mArray\n    def listToRangedDictWithLabel(mList, label):\n        resultDict = {}\n        for index, elem in enumerate(mList):\n            mKey = \"{}:{}\".format(label, int(elem))\n            resultDict.update({mKey: resultDict.get(mKey, []) + [(index, index + 1)]})\n        return resultDict\n    def get1DArrayEMA(mArray,N=5):",
        "type": "code",
        "location": "/tests/video_detector_tests/pip_meanVariance_stablize.py:1-33"
    },
    "5291": {
        "file_id": 693,
        "content": "The function sampledStablePipRegionExporter takes data, defaultWidth, and defaultHeight as inputs. It converts the data into a numpy array, then provides three helper functions: getAlikeValueMerged, listToRangedDictWithLabel, and get1DArrayEMA. The purpose of these helper functions is to manipulate and process the data into desired ranges for further analysis or processing.",
        "type": "comment"
    },
    "5292": {
        "file_id": 693,
        "content": "        weights=np.exp(np.linspace(0,1,N))\n        weights =weights/np.sum(weights)\n        ema = np.convolve(weights, mArray, mode='valid')\n        return ema\n    def pointsToRangedDictWithLabel(mArray, label, threshold=35):\n        mArray = get1DArrayEMA(mArray)\n        mArray = getAlikeValueMerged(mArray, threshold=threshold)\n        return listToRangedDictWithLabel(mArray, label)\n    threshold = int(max(defaultWidth, defaultHeight)*0.02734375)\n    xLeftPoints = pointsToRangedDictWithLabel(data[:, 0, 0], \"xleft\", threshold = threshold)\n    yLeftPoints = pointsToRangedDictWithLabel(data[:, 0, 1], \"yleft\", threshold = threshold)\n    xRightPoints = pointsToRangedDictWithLabel(data[:, 1, 0], \"xright\", threshold = threshold)\n    yRightPoints = pointsToRangedDictWithLabel(data[:, 1, 1], \"yright\", threshold = threshold)\n    commandDict = {}\n    for mDict in [xLeftPoints, yLeftPoints, xRightPoints, yRightPoints]:\n        commandDict.update(mDict)\n    commandDict = getContinualMappedNonSympyMergeResult(commandDict)",
        "type": "code",
        "location": "/tests/video_detector_tests/pip_meanVariance_stablize.py:34-52"
    },
    "5293": {
        "file_id": 693,
        "content": "The code calculates the exponential moving average (EMA) of a 1D array and converts points into a ranged dictionary with labels. It then updates a command dictionary using the labeled points and applies non-sympy merge to get the final result. The threshold value is determined based on the maximum width or height, and there are four types of points processed: xLeft, yLeft, xRight, and yRight.",
        "type": "comment"
    },
    "5294": {
        "file_id": 693,
        "content": "    commandDictSequential = mergedRangesToSequential(commandDict)\n    def getSpanDuration(span):\n        start, end = span\n        return end - start\n    itemDurationThreshold = 10\n    # framerate?\n    while True:\n        # print(\"LOOP COUNT:\", loopCount)\n        # loopCount+=1\n        # noAlter = True\n        beforeChange = [item[0] for item in commandDictSequential].copy()\n        for i in range(len(commandDictSequential) - 1):\n            currentItem = commandDictSequential[i]\n            nextItem = commandDictSequential[i + 1]\n            currentItemCommand = currentItem[0]\n            currentItemDuration = getSpanDuration(currentItem[1])\n            nextItemCommand = nextItem[0]\n            nextItemDuration = getSpanDuration(nextItem[1])\n            if currentItemDuration < itemDurationThreshold:\n                if nextItemCommand != currentItemCommand and nextItemDuration >= itemDurationThreshold:\n                    # print(\"HERE0\",i, currentItemCommand, nextItemCommand)\n                    commandDictSequential[i][0] = nextItemCommand",
        "type": "code",
        "location": "/tests/video_detector_tests/pip_meanVariance_stablize.py:53-77"
    },
    "5295": {
        "file_id": 693,
        "content": "This code loops through a sequence of command pairs, adjusting commands with durations below the threshold by taking the next command if it has a duration above the threshold. This ensures that there is no gap between consecutive commands and maintains smooth video detection.",
        "type": "comment"
    },
    "5296": {
        "file_id": 693,
        "content": "                    # noAlter=False\n            if nextItemDuration < itemDurationThreshold:\n                if nextItemCommand != currentItemCommand :\n                    # print(\"HERE1\",i, currentItemCommand, nextItemCommand)\n                    commandDictSequential[i + 1][0] = currentItemCommand\n                    # noAlter=False\n        afterChange = [item[0] for item in commandDictSequential].copy()\n        noAlter = beforeChange == afterChange\n        if noAlter:\n            break\n    preFinalCommandDict = sequentialToMergedRanges(commandDictSequential)\n    finalCommandDict = {}\n    for key, elem in preFinalCommandDict.items():\n        # print(key,elem)\n        varNames = [\"xleft\", \"yleft\", \"xright\", \"yright\"]\n        defaultValues = [0, 0, defaultWidth, defaultHeight]\n        for varName, defaultValue in zip(varNames, defaultValues):\n            key = key.replace(\n                \"{}:empty\".format(varName), \"{}:{}\".format(varName, defaultValue)\n            )\n        # print(key,elem)\n        # breakpoint()",
        "type": "code",
        "location": "/tests/video_detector_tests/pip_meanVariance_stablize.py:78-99"
    },
    "5297": {
        "file_id": 693,
        "content": "The code checks if there is a change in item commands and updates the command dictionary accordingly. If no changes occur, it breaks the loop. It then converts the sequential command dictionary to merged ranges and stores default values for variables.",
        "type": "comment"
    },
    "5298": {
        "file_id": 693,
        "content": "        import parse\n        formatString = (\n            \"xleft:{xleft:d}|yleft:{yleft:d}|xright:{xright:d}|yright:{yright:d}\"\n        )\n        commandArguments = parse.parse(formatString, key)\n        x, y, w, h = (\n            commandArguments[\"xleft\"],\n            commandArguments[\"yleft\"],\n            commandArguments[\"xright\"] - commandArguments[\"xleft\"],\n            commandArguments[\"yright\"] - commandArguments[\"yleft\"],\n        )\n        if w <= 0 or h <= 0:\n            continue\n        cropCommand = \"crop_{}_{}_{}_{}\".format(x, y, w, h)\n        # print(cropCommand)\n        finalCommandDict.update({cropCommand: elem})\n        # print(elem)\n        # the parser shall be in x,y,w,h with keywords.\n        # we might want to parse the command string and reengineer this shit.\n    return finalCommandDict\ndef kalmanStablePipRegionExporter(data, defaultWidth, defaultHeight):\n    defaultWidth, defaultHeight = int(defaultWidth), int(defaultHeight)\n    import numpy as np\n    data = np.array(data)\n    from pykalman import KalmanFilter",
        "type": "code",
        "location": "/tests/video_detector_tests/pip_meanVariance_stablize.py:100-129"
    },
    "5299": {
        "file_id": 693,
        "content": "The code imports the \"parse\" library, defines a format string for command arguments, uses parse to extract x, y, w, and h values, checks if these values are valid, creates a crop command based on these values, updates finalCommandDict with this command and corresponding element, and finally imports numpy and pykalman for further processing.",
        "type": "comment"
    }
}