{
    "5200": {
        "file_id": 673,
        "content": "/tests/search_engine_suggestion_based_qa_bot/search_for_picture_embedding.py",
        "type": "filepath"
    },
    "5201": {
        "file_id": 673,
        "content": "This code utilizes BaiDu image search API to find similar images and prints details, implements time delays for safety. It currently uses textrank model for improvements. The code is facing issues with `getBaiduImageSearchAjaxInfoParsed` function from `parse_baidu_search_ajax` module. It handles exceptions, provides URL structure info, and offers debugging support.",
        "type": "summary"
    },
    "5202": {
        "file_id": 673,
        "content": "# actually the clip model does well for this.\n# though you want to use bm25 based textrank\nimage = \"prettyGirl.jpeg\" # girl image\nfrom PicImageSearch.sync import BaiDu\nbaidu = BaiDu()\nresult = baidu.search(file=image)\n# print(result)\n# better not to query 'ajax' unless you want to get banned.\n# breakpoint()\n# you want to use phash, width, height for this.\nimport requests\nSLEEP= 1\nfor elem in result.raw:\n    elem = elem.__dict__\n    # print(elem)\n    # breakpoint()\n    thumbnail = elem.get('thumbnail')\n    simi = elem.get('similarity')\n    title = elem.get('title')\n    # url is not necessary since we almost can't get the picture.\n    ajaxUrl = elem['origin'].get('ajaxUrl')\n    import time\n    print(thumbnail, simi, title)\n    # print(thumbnail, simi, title, ajaxUrl)\n    time.sleep(SLEEP) # wait too long?\n    r = requests.get(ajaxUrl)\n    myJson = r.json()\n    # from lazero.filesystem.io import writeJsonObjectToFile\n    # writeJsonObjectToFile('jq_image_2.json',myJson)\n    # breakpoint()\n    # maybe no need to parse this thing.",
        "type": "code",
        "location": "/tests/search_engine_suggestion_based_qa_bot/search_for_picture_embedding.py:1-34"
    },
    "5203": {
        "file_id": 673,
        "content": "This code uses the BaiDu image search API to find similar images and their details. It prints thumbnail, similarity, title, and AJAX URL for each result. The code also includes time delays to avoid being banned. The clip model is mentioned for potential use in future improvements, but currently, bm25 based textrank is recommended. The code avoids querying 'ajax' to prevent potential bans.",
        "type": "comment"
    },
    "5204": {
        "file_id": 673,
        "content": "    # try: # TODO: skipping this parsing since multiple errors.\n    #     from parse_baidu_search_ajax import getBaiduImageSearchAjaxInfoParsed\n    #     title_some, url_meta_some= getBaiduImageSearchAjaxInfoParsed(myJson, debug=True)\n    #     # changed again?\n    # except:\n    #     import traceback\n    #     traceback.print_exc()\n    #     print(ajaxUrl)\n    #     print('error!')\n    #     breakpoint()\n    # breakpoint()\n# ['origin', 'raw', 'url']\n# result.raw[0].url is the original url. however you won't get the picture.\n# result.raw[0].thumbnail\n# 'origin', 'similarity', 'thumbnail', 'title', 'url'\n# result.raw[0].origin['ajaxUrl'] -> get more similar images of this one",
        "type": "code",
        "location": "/tests/search_engine_suggestion_based_qa_bot/search_for_picture_embedding.py:36-52"
    },
    "5205": {
        "file_id": 673,
        "content": "This code is trying to import the function `getBaiduImageSearchAjaxInfoParsed` from the module `parse_baidu_search_ajax`, but due to some errors, it's skipping this parsing process. It then handles any exceptions that may occur and prints the error message along with the URL. If an exception happens, it also calls a breakpoint to pause the code execution for debugging purposes. The code also provides information about the URL structure and how to access different parts of the URL, such as the original URL, thumbnail, and ajaxUrl to get more similar images.",
        "type": "comment"
    },
    "5206": {
        "file_id": 674,
        "content": "/tests/search_engine_suggestion_based_qa_bot/parse_baidu_title_abstract.py",
        "type": "filepath"
    },
    "5207": {
        "file_id": 674,
        "content": "This code reads a JSON file, cleans text, processes abstracts to generate phrases meeting minimum and maximum length requirements. It parses Baidu search result titles and abstracts for potential question-answering content using \"result_baidu.json\". Text preprocessing is performed, and the top 20 ranked candidate phrases are printed based on BM25 similarity and Chinese character portion in the query.",
        "type": "summary"
    },
    "5208": {
        "file_id": 674,
        "content": "from lazero.filesystem.io import readJsonObjectFromFile\nfrom lazero.utils.mathlib import checkMinMaxDict\ndata = readJsonObjectFromFile(\"result_baidu.json\")\nimport string\nfrom zhon import hanzi\npunctuations = set(list(string.punctuation + hanzi.punctuation))\npermitted = [\" \"]\nfor perm in permitted:\n    if perm in punctuations:\n        punctuations.remove(perm)\ndef removeTimeInfo(phrase):\n    import re\n    timeinfos = re.findall(r\"\\d+年\\d+月\\d+日\", phrase)\n    for timeinfo in timeinfos:\n        phrase = phrase.replace(timeinfo, \"\")\n    return phrase\ndef processQueryResult(abstract, minMaxDict={\"min\": 8, \"max\": 24}):\n    for punc in punctuations:\n        abstract = abstract.replace(punc, \"\\n\")\n    abstract = abstract.split(\"\\n\")\n    for phrase in abstract:\n        phrase = removeTimeInfo(phrase)\n        phrase = phrase.strip()\n        if not checkMinMaxDict(len(phrase), minMaxDict):\n            continue\n        else:\n            yield phrase\ncandidates = []\nquery = \"python有个问题想请教一下 为什么我这个函数跑不通\"\n# use another model please?",
        "type": "code",
        "location": "/tests/search_engine_suggestion_based_qa_bot/parse_baidu_title_abstract.py:1-41"
    },
    "5209": {
        "file_id": 674,
        "content": "This code reads a JSON file, removes time and punctuation information from text, and processes the abstract to yield phrases meeting minimum and maximum length requirements. The purpose is to parse Baidu search result titles and abstracts for potential question-answering content, using the \"result_baidu.json\" file as input. The code also includes a function to remove time information from text and ensures each phrase meets specific length criteria before yielding it. The query variable contains a sample input for testing or using with another model.",
        "type": "comment"
    },
    "5210": {
        "file_id": 674,
        "content": "# haystack?\nfor elem in data:\n    title = elem.get(\"title\")\n    print(\"title: %s\" % title)\n    spliters = [\" - \", \"-\", \"_\", \"－\"]\n    for spliter in spliters:\n        title = title.replace(spliter, \"_\")\n    potentialWebsiteNames = title.split(\"_\")\n    title = potentialWebsiteNames[0].strip()\n    realWebsiteNames = []\n    if len(potentialWebsiteNames) > 1:\n        websiteNames = potentialWebsiteNames[1:]\n        for name in websiteNames:\n            name = name.strip()\n            if len(name) > 0:\n                realWebsiteNames.append(name)\n    abstract = elem.get(\"abstract\")\n    # print(abstract)\n    # breakpoint()\n    for name in realWebsiteNames:\n        abstract = abstract.replace(name, \"\")  # remove website names\n    for phrase in processQueryResult(abstract):\n        if phrase not in candidates and not phrase.endswith(\"\"):  # magic char.\n            candidates.append(phrase)  # what is your query?\nimport jieba\ndef getCuttedWords(phrase):\n    candidates = jieba.lcut(phrase.lower())\n    wordList = []\n    for word in candidates:",
        "type": "code",
        "location": "/tests/search_engine_suggestion_based_qa_bot/parse_baidu_title_abstract.py:42-73"
    },
    "5211": {
        "file_id": 674,
        "content": "This code is iterating over a list of data items, extracting titles and abstracts. It cleans the titles by removing splitting characters like \"-\", \"_\", and \"－\" and then splits them into potential website names. It checks if there are additional website names in the title and removes them from the abstract. Then it cuts the abstract using Jieba's lcut function to generate candidates for further processing.",
        "type": "comment"
    },
    "5212": {
        "file_id": 674,
        "content": "        word = word.strip()\n        if len(word) > 0:\n            wordList.append(word)\n    return wordList\ndef countCommonWords(phrase_1, phrase_2, wordCount=False):\n    words_1 = getCuttedWords(phrase_1)\n    words_2 = getCuttedWords(phrase_2)\n    # count for longest total length?\n    result = list(set(words_1) & set(words_2))\n    if wordCount:\n        return len(result)\n    else:\n        return len(\"\".join(result))\n# candidates = list(set(candidates))\n# https://pypi.org/project/rank-bm25/\n# candidates.sort(key=lambda phrase: -countCommonWords(phrase,query))\n# use bm25?\n# this sorting is wrong.\nfrom rank_bm25 import BM25Okapi\ntokenized_corpus = [getCuttedWords(phrase) for phrase in candidates]\ntokenized_query = getCuttedWords(query)\nbm25 = BM25Okapi(tokenized_corpus)\n# doc_scores = bm25.get_scores(tokenized_query)\ntop_k = 20\nprint(\"TOP\", top_k)\ntopKCandidates = bm25.get_top_n(tokenized_query, candidates, n=top_k)\n# count chinese chars.\n# count for english/chinese portion. (strange hack.)\nimport numpy as np\ndef calculateChinesePortion(phrase):",
        "type": "code",
        "location": "/tests/search_engine_suggestion_based_qa_bot/parse_baidu_title_abstract.py:74-111"
    },
    "5213": {
        "file_id": 674,
        "content": "The code is performing text preprocessing, calculating the similarity between phrases, and ranking candidates using BM25 algorithm. It first tokenizes and cuts the words from the candidate phrases and the query. Then it calculates the common words between two phrases and uses this information to sort and rank the candidates. Finally, it applies the BM25Okapi algorithm to get the scores of each candidate based on their relevance to the query and selects the top 20 ranked candidates. The code also includes a function to calculate the Chinese portion in a phrase.",
        "type": "comment"
    },
    "5214": {
        "file_id": 674,
        "content": "    length = len(phrase)\n    mdata = []\n    isalpha, isascii, isdigit, ischinese = 0, 0, 0, 0\n    for char in phrase:\n        isalpha += int(char.isalpha())\n        isascii += int(char.isascii())\n        isdigit += int(char.isdigit())\n        ischinese += int(not (isalpha or isascii or isdigit))\n    mdata = np.array([isalpha, isascii, isdigit, ischinese]) / length\n    return mdata\nqueryChinesePortion = calculateChinesePortion(query)\nfrom scipy.spatial.distance import cosine\ntopKCandidates.sort(\n    key=lambda phrase: cosine(calculateChinesePortion(phrase), queryChinesePortion)\n)\n# topKCandidates.sort(key=lambda phrase: -len(phrase))\nfor elem in topKCandidates:\n    print(elem.__repr__())",
        "type": "code",
        "location": "/tests/search_engine_suggestion_based_qa_bot/parse_baidu_title_abstract.py:112-132"
    },
    "5215": {
        "file_id": 674,
        "content": "The code calculates the proportion of Chinese characters in a query and uses it to sort a list of candidate phrases. It then prints each candidate phrase, sorted by their similarity to the query based on the Chinese character portion.",
        "type": "comment"
    },
    "5216": {
        "file_id": 675,
        "content": "/tests/search_engine_suggestion_based_qa_bot/parse_baidu_search_ajax.py",
        "type": "filepath"
    },
    "5217": {
        "file_id": 675,
        "content": "This code parses Baidu Image Search results using two functions, extracting title snippets and image similarity, with potential img_sim issue. It retrieves image dimensions and appends to a dataframe before returning two dataframes in debug mode.",
        "type": "summary"
    },
    "5218": {
        "file_id": 675,
        "content": "import pyjq\ndef getBaiduImageSearchAjaxInfoParsed(obj, debug=False):\n    commonFilter = \"select(.extData) | .extData.showInfo | select(. != null) | {titles, snippets, imgs_src, simi} | select (.titles !=null)\"\n    def standardJsonParser(obj):\n        command = \".data.cardData[] | {}\".format(commonFilter)\n        processed_obj = pyjq.first(command, obj)\n        return processed_obj\n    def hiddenJsParser(obj):\n        processed_obj = obj\n        for index in range(3):\n            data = pyjq.first(\".data.commonData.js[{}]\".format(index), obj2)\n            if not ('titles' in data and 'titles_url' in data):\n                continue\n            lines = data.split(\"\\n\")\n            for line in lines:\n                line = line.strip()\n                hint = \"var cardData = \"\n                # print(line)\n                if line.startswith(hint):\n                    import javascript\n                    cardData = javascript.eval_js(line.replace(hint,\"\")).valueOf()\n                    real_data = pyjq.first(commonFilter,cardData)",
        "type": "code",
        "location": "/tests/search_engine_suggestion_based_qa_bot/parse_baidu_search_ajax.py:1-23"
    },
    "5219": {
        "file_id": 675,
        "content": "This code defines two functions: `standardJsonParser` and `hiddenJsParser`. The first function, `standardJsonParser`, processes the data using a common filter and returns the filtered results. The second function, `hiddenJsParser`, extracts data from hidden JavaScript strings and applies the same common filter to return the processed data. This code appears to be parsing Baidu Image Search AJAX information in different formats (standard JSON or hidden JavaScript).",
        "type": "comment"
    },
    "5220": {
        "file_id": 675,
        "content": "                    # import pprint\n                    return real_data\n                    # pprint.pprint(real_data)\n    import pandas as pd\n    processed_obj = None\n    methods = [standardJsonParser,hiddenJsParser]\n    for method in methods:\n        try:\n            processed_obj = method(obj)\n            if processed_obj is not None:\n                break\n        except:\n            ...\n    if processed_obj is None:\n        if debug:\n            print('cannot parse info from obj')\n    # print(processed_obj)\n    # breakpoint()\n    # from pprint import pprint\n    # pprint(processed_obj)\n    title_snippets = pyjq.first(\"{titles, snippets}\", processed_obj)\n    img_sim = pyjq.first(\"(.simi[]|=tonumber )|{imgs_src, simi}\", processed_obj) # TODO: error! what is going on?\n    # img_sim[\"simi\"] = img_sim[\"simi\"] # what is this?\n    # [('titles', 15), ('snippets', 15), ('imgs_src', 43), ('simi', 43)]\n    # 15, 15, 43, 43\n    df_title_snippets = pd.DataFrame(title_snippets)\n    df_img_sim = pd.DataFrame(img_sim)\n    elem = df_img_sim[\"simi\"][0]",
        "type": "code",
        "location": "/tests/search_engine_suggestion_based_qa_bot/parse_baidu_search_ajax.py:24-51"
    },
    "5221": {
        "file_id": 675,
        "content": "This code attempts to parse an object and extract title snippets and image similarity information. It uses various parsing methods, data frames for organization, and the pyjq library for data manipulation. The code also includes error handling and debugging options. However, there is a potential error in the img_sim variable parsing.",
        "type": "comment"
    },
    "5222": {
        "file_id": 675,
        "content": "    if debug:\n        print(df_title_snippets.head())\n        print(df_img_sim.head())\n        print(type(elem), elem)  # str?\n    # breakpoint()\n    from urllib.parse import parse_qs\n    def getWidthHeight(url):\n        qs = url.split(\"?\")[-1]\n        mdict = parse_qs(qs)\n        # print(mdict)\n        # breakpoint()\n        width = int(mdict[\"w\"][0])\n        height = int(mdict[\"h\"][0])\n        area = width * height\n        return width, height, area\n    # pre_qs = df_img_sim['imgs_src'].split(\"?\")\n    width_height = df_img_sim[\"imgs_src\"].apply(\n        lambda v: pd.Series(getWidthHeight(v), index=[\"width\", \"height\", \"area\"])\n    )\n    df_img_sim_width_height = pd.concat([df_img_sim, width_height], axis=1, join=\"inner\")\n    # qs = parse_qs(pre_qs)\n    # print(qs)\n    if debug:\n        print(df_img_sim_width_height.head())\n    return df_title_snippets, df_img_sim_width_height\n# the \"js\" response may contain video info which may help with our reverse video search.\n# but the keyword also helps!\nif __name__ == \"__main__\":",
        "type": "code",
        "location": "/tests/search_engine_suggestion_based_qa_bot/parse_baidu_search_ajax.py:53-85"
    },
    "5223": {
        "file_id": 675,
        "content": "This code snippet is parsing the Baidu search results and retrieving image width, height, and area information. It then appends these values to the dataframe df_img_sim_width_height and returns two dataframes: df_title_snippets and df_img_sim_width_height. The debug mode allows printing of important intermediate data for testing and validation.",
        "type": "comment"
    },
    "5224": {
        "file_id": 675,
        "content": "    from lazero.filesystem.io import readJsonObjectFromFile\n    # obj = readJsonObjectFromFile(\"ajax_baidu.json\")\n    obj2 = readJsonObjectFromFile(\"jq_image_2.json\")\n    getBaiduImageSearchAjaxInfoParsed(obj2, debug=True)",
        "type": "code",
        "location": "/tests/search_engine_suggestion_based_qa_bot/parse_baidu_search_ajax.py:86-89"
    },
    "5225": {
        "file_id": 675,
        "content": "This code imports the readJsonObjectFromFile function and reads two JSON files, \"ajax_baidu.json\" and \"jq_image_2.json\". The function getBaiduImageSearchAjaxInfoParsed is then called with the second file's content (obj2) and debug mode enabled.",
        "type": "comment"
    },
    "5226": {
        "file_id": 676,
        "content": "/tests/render_and_recognize_long_text_to_filter_unwanted_characters/test_render.py",
        "type": "filepath"
    },
    "5227": {
        "file_id": 676,
        "content": "The code utilizes pygame and specific libraries to generate text, render it, set up a game display window, and save the updated display as output_name.",
        "type": "summary"
    },
    "5228": {
        "file_id": 676,
        "content": "import os\n# https://github.com/ntasfi/PyGame-Learning-Environment/issues/26\nos.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"\nimport pygame\npygame.init()\nblack, white = pygame.Color('black'), pygame.Color('white')\n# pillow can also do that\n# https://plainenglish.io/blog/generating-text-on-image-with-python-eefe4430fe77\ntextContent = \"\".join([\"中\",\"ぁ\"]+[f\"[{index+1}]\" for index in range(100)]) # will see [100] at the end of text if successful.\n# pygame.font.get_fonts()\n# install your font to system please? but why all lower case font names?\n# fontName = \"notosans\"\n# this font is bad.\nfontSize = 40\n# font = pygame.font.SysFont(fontName,fontSize)\n# fontPath = \"/usr/share/fonts/truetype/noto/NotoSans-Regular.ttf\" # shit this fails.\nfontPath = \"./get_and_merge_fonts/GoNotoCurrent.ttf\"\n# use some kind of super large merged notofont.\nfont = pygame.font.Font(fontPath, fontSize)\noutput_name = \"test_render.png\"\nword_surface = font.render(textContent, False, black)\nword_width, word_height = word_surface.get_size()\nmargin=20\nSIZE=(word_width+margin*2, word_height+margin*2)",
        "type": "code",
        "location": "/tests/render_and_recognize_long_text_to_filter_unwanted_characters/test_render.py:1-33"
    },
    "5229": {
        "file_id": 676,
        "content": "The code imports necessary libraries, sets the video driver, initializes pygame, defines colors, generates text content with 100 placeholders, selects a font (GoNotoCurrent.ttf), renders the text, and determines the size of the rendered image.",
        "type": "comment"
    },
    "5230": {
        "file_id": 676,
        "content": "image = pygame.display.set_mode(SIZE, pygame.RESIZABLE)\nimage.fill(white)\nimage.blit(word_surface,(margin,margin))\npygame.display.update()\npygame.image.save(image,output_name)",
        "type": "code",
        "location": "/tests/render_and_recognize_long_text_to_filter_unwanted_characters/test_render.py:34-38"
    },
    "5231": {
        "file_id": 676,
        "content": "Initializes game display window with specified size, fills it with white color, blits word image onto the surface, updates pygame display and saves the updated display to output_name.",
        "type": "comment"
    },
    "5232": {
        "file_id": 677,
        "content": "/tests/render_and_recognize_long_text_to_filter_unwanted_characters/test_pytesseract.py",
        "type": "filepath"
    },
    "5233": {
        "file_id": 677,
        "content": "This code uses the pytesseract library to extract text from an image. It specifies a list of supported languages (English, Chinese Simplified, Chinese Traditional, Japanese), combines them into a single language code, and applies it to the \"test_render.png\" image file. The resulting extracted text is then printed out. However, there may be many incorrect results due to the complexity of character recognition in different languages.",
        "type": "summary"
    },
    "5234": {
        "file_id": 677,
        "content": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\nimport pytesseract\n# pytesseract.get_languages(config=\"\")\nlangs =['eng','chi_sim','chi_tra','jpn']\nlangCode = \"+\".join(langs)\npicPath = \"test_render.png\"\noutput = pytesseract.image_to_string(picPath, lang=langCode)\nprint(\"OUTPUT?\")\nprint(output)\n# many incorrect results?",
        "type": "code",
        "location": "/tests/render_and_recognize_long_text_to_filter_unwanted_characters/test_pytesseract.py:1-15"
    },
    "5235": {
        "file_id": 677,
        "content": "This code uses the pytesseract library to extract text from an image. It specifies a list of supported languages (English, Chinese Simplified, Chinese Traditional, Japanese), combines them into a single language code, and applies it to the \"test_render.png\" image file. The resulting extracted text is then printed out. However, there may be many incorrect results due to the complexity of character recognition in different languages.",
        "type": "comment"
    },
    "5236": {
        "file_id": 678,
        "content": "/tests/skin_clean/process_image.py",
        "type": "filepath"
    },
    "5237": {
        "file_id": 678,
        "content": "The code includes two image processing functions, beauty_face and beauty_face2, which enhance facial features using Gaussian blur, bilateral filtering, and custom processing. The results are saved as 'result1.png' and 'result2.png'. The source image file is set to \"IMG_20220515_2220565.jpg\" and the init function is called with this source parameter for potential further manipulations or analysis.",
        "type": "summary"
    },
    "5238": {
        "file_id": 678,
        "content": "import numpy as np\nimport cv2\ndef beauty_face(img):\n    '''\n    Dest =(Src * (100 - Opacity) + (Src + 2 * GuassBlur(EPFFilter(Src) - Src + 128) - 256) * Opacity) /100 ;\n    https://my.oschina.net/wujux/blog/1563461\n    '''\n    dst = np.zeros_like(img)\n    #int value1 = 3, value2 = 1; 磨皮程度与细节程度的确定\n    v1 = 3\n    v2 = 1\n    dx = v1 * 5 # 双边滤波参数之一 \n    fc = v1 * 12.5 # 双边滤波参数之一 \n    p = 0.1\n    temp4 = np.zeros_like(img)\n    temp1 = cv2.bilateralFilter(img,dx,fc,fc)\n    temp2 = cv2.subtract(temp1,img)\n    temp2 = cv2.add(temp2,(10,10,10,128))\n    temp3 = cv2.GaussianBlur(temp2,(2*v2 - 1,2*v2-1),0)\n    temp4 = cv2.add(img,temp3)\n    dst = cv2.addWeighted(img,p,temp4,1-p,0.0)\n    dst = cv2.add(dst,(10, 10, 10,255))\n    return dst\ndef beauty_face2(src):\n    '''\n    Dest =(Src * (100 - Opacity) + (Src + 2 * GuassBlur(EPFFilter(Src) - Src + 128) - 256) * Opacity) /100 ;\n    '''\n    dst = np.zeros_like(src)\n    #int value1 = 3, value2 = 1; 磨皮程度与细节程度的确定\n    v1 = 3\n    v2 = 1\n    dx = v1 * 5 # 双边滤波参数之一 \n    fc = v1 * 12.5 # 双边滤波参数之一 ",
        "type": "code",
        "location": "/tests/skin_clean/process_image.py:3-41"
    },
    "5239": {
        "file_id": 678,
        "content": "The code defines two functions, beauty_face and beauty_face2. The beauty_face function applies a series of image processing operations to create a smoothed and enhanced version of the input image. This includes bilateral filtering, subtraction, Gaussian blurring, addition, and weighted addition. The beauty_face2 function is similar but processes a different source image.",
        "type": "comment"
    },
    "5240": {
        "file_id": 678,
        "content": "    p = 0.1\n    temp4 = np.zeros_like(src)\n    temp1 = cv2.bilateralFilter(src,dx,fc,fc)\n    temp2 = cv2.subtract(temp1,src)\n    temp2 = cv2.add(temp2, (10,10,10,128))\n    temp3 = cv2.GaussianBlur(temp2,(2*v2 - 1,2*v2-1),0)\n    temp4 = cv2.subtract(cv2.add(cv2.add(temp3, temp3), src), (10, 10, 10, 255))\n    dst = cv2.addWeighted(src,p,temp4,1-p,0.0)\n    dst = cv2.add(dst, (10, 10, 10,255))\n    return dst\ndef init(source):\n    img = cv2.imread(source)\n    # blur1 = cv2.GaussianBlur(img, (5,5),0)\n    # blur2 = cv2.bilateralFilter(img, 9 , 75, 75)\n    blur3 = beauty_face(img)\n    blur4 = beauty_face2(img)\n    # cv2.imshow('image0', img)\n    # # cv2.imshow('image1', blur1)\n    # # cv2.imshow('image2', blur2)\n    # cv2.imshow('image3', blur3)\n    # cv2.imshow('image4', blur4)\n    # cv2.namedWindow('image', cv2.WINDOW_NORMAL)\n    # cv2.resizeWindow('image', 1000, 1000) #定义frame的大小\n    # cv2.waitKey(0)\n    cv2.imwrite('result1.png', blur3)\n    cv2.imwrite('result2.png', blur4)\n    # cv2.destroyAllWindows()\nif __name__ == \"__main__\":",
        "type": "code",
        "location": "/tests/skin_clean/process_image.py:42-79"
    },
    "5241": {
        "file_id": 678,
        "content": "Code applies image processing techniques to enhance facial features. It uses Gaussian blur, bilateral filtering, and custom beauty_face/beauty_face2 functions. The processed images are displayed in separate windows and saved as 'result1.png' and 'result2.png'.",
        "type": "comment"
    },
    "5242": {
        "file_id": 678,
        "content": "    source = \"IMG_20220515_2220565.jpg\"\n    init(source)",
        "type": "code",
        "location": "/tests/skin_clean/process_image.py:80-81"
    },
    "5243": {
        "file_id": 678,
        "content": "This code snippet sets the source image file name as \"IMG_20220515_2220565.jpg\" and calls the init function with this source parameter. The purpose of this step might be to initialize the image processing or loading process for further manipulations or analysis.",
        "type": "comment"
    },
    "5244": {
        "file_id": 679,
        "content": "/tests/setu_server_mail_collector_ad_poster_personalization_java/README.md",
        "type": "filepath"
    },
    "5245": {
        "file_id": 679,
        "content": "This Java code sets up a server for Bilibli UIDs and collects personal interests by analyzing images. It requests an email address after some time if not initially provided, then shares tracker links in sent emails as ads.",
        "type": "summary"
    },
    "5246": {
        "file_id": 679,
        "content": "a simple setu server written in java.\nwill ask for bilibili uid\nwill ask for email address after a while (if not provided)\nwill collect personal interest on pictures (planned)\nwill post tracker links on ads in email",
        "type": "code",
        "location": "/tests/setu_server_mail_collector_ad_poster_personalization_java/README.md:1-9"
    },
    "5247": {
        "file_id": 679,
        "content": "This Java code sets up a server for Bilibli UIDs and collects personal interests by analyzing images. It requests an email address after some time if not initially provided, then shares tracker links in sent emails as ads.",
        "type": "comment"
    },
    "5248": {
        "file_id": 680,
        "content": "/tests/vapoursynth_linux_test/view_test.py",
        "type": "filepath"
    },
    "5249": {
        "file_id": 680,
        "content": "The code imports VapourSynth library functions, creates a Video object with FFMS2 source and option to transpose, but generating previews isn't working as vspipe uses existing APIs and can only generate raw frame data. OpenCV might help; example at https://github.com/UniversalAl/view.",
        "type": "summary"
    },
    "5250": {
        "file_id": 680,
        "content": "videoPath = \"/root/Desktop/works/pyjom/samples/video/dog_with_text.mp4\"\n# videoPath = \"/Users/jamesbrown/desktop/works/pyjom_remote/samples/video/dog_with_text.mp4\"\n# reference: http://www.vapoursynth.com/doc/pythonreference.html\n# The VideoFrame and AudioFrame classes contains one picture/audio chunk and all the metadata associated with it. It is possible to access the raw data using either get_read_ptr(plane) or get_write_ptr(plane) and get_stride(plane) with ctypes.\n# A more Python friendly wrapping is also available where each plane/channel can be accessed as a Python array using frame[plane/channel].\n# To get a frame simply call get_frame(n) on a clip. Should you desire to get all frames in a clip, use this code:\n# for frame in clip.frames():\n#     # Do stuff with your frame\n#     pass\nfrom vapoursynth import core\nvideo = core.ffms2.Source(source=videoPath)\n# video = core.std.Transpose(video)\n# video.set_output()\n# from viewKali import Preview\n# clip = vs.core.lsmas.LibavSMASHSource('source.mp4')",
        "type": "code",
        "location": "/tests/vapoursynth_linux_test/view_test.py:1-23"
    },
    "5251": {
        "file_id": 680,
        "content": "The code imports the necessary functions from the VapourSynth library and defines a video path. It then creates a Video object using the FFMS2 source and provides an option to transpose the video if needed, but it's currently commented out. Additionally, there is a reference to another file called \"viewKali\" where a Preview function may be used, but it's not implemented yet.",
        "type": "comment"
    },
    "5252": {
        "file_id": 680,
        "content": "# seems not working\n# Preview(video)\n# vspipe is a wrapper around existing apis. vapoursynth can only generate raw frame data so we cannot encode video here alone. maybe we need opencv for this?\n# opencv preview https://github.com/UniversalAl/view",
        "type": "code",
        "location": "/tests/vapoursynth_linux_test/view_test.py:24-28"
    },
    "5253": {
        "file_id": 680,
        "content": "This code appears to attempt creating a preview using VapourSynth, but the functionality isn't working. It suggests that vspipe is a wrapper around existing APIs and VapourSynth can only generate raw frame data, so encoding a video alone might not be possible. OpenCV might help in generating previews, and there's an example at this GitHub link: https://github.com/UniversalAl/view.",
        "type": "comment"
    },
    "5254": {
        "file_id": 681,
        "content": "/tests/vapoursynth_linux_test/test_ffmpeg_docker.sh",
        "type": "filepath"
    },
    "5255": {
        "file_id": 681,
        "content": "Code snippet attempts to download a video file 'flower_cif.y4m' using wget, and then applies various filters with ffmpeg-tensorflow Docker container to upscale the video resolution by 2x and save it as 'flower_cif_2x.mp4'. The code also provides an alias for easier execution of ffmpeg-tensorflow command and specifies video filter complexities within the ffmpeg command.",
        "type": "summary"
    },
    "5256": {
        "file_id": 681,
        "content": "wget https://media.xiph.org/video/derf/y4m/flower_cif.y4m\n# no good for using docker gpu containers.\n# alias ffmpeg-tensorflow='docker run --rm --gpus all -u $(id -u):$(id -g) -v \"$PWD\":/data -w /data -i miratmu/ffmpeg-tensorflow'\n# ffmpeg-tensorflow -i flower_cif.y4m -filter_complex '[0:v] format=pix_fmts=yuv420p, extractplanes=y+u+v [y][u][v]; [y] sr=dnn_backend=tensorflow:scale_factor=2:model=/models/espcn.pb [y_scaled]; [u] scale=iw*2:ih*2 [u_scaled]; [v] scale=iw*2:ih*2 [v_scaled]; [y_scaled][u_scaled][v_scaled] mergeplanes=0x001020:yuv420p [merged]' -map [merged] -sws_flags lanczos -c:v libx264 -crf 17 -c:a copy -y flower_cif_2x.mp4",
        "type": "code",
        "location": "/tests/vapoursynth_linux_test/test_ffmpeg_docker.sh:1-6"
    },
    "5257": {
        "file_id": 681,
        "content": "Code snippet attempts to download a video file 'flower_cif.y4m' using wget, and then applies various filters with ffmpeg-tensorflow Docker container to upscale the video resolution by 2x and save it as 'flower_cif_2x.mp4'. The code also provides an alias for easier execution of ffmpeg-tensorflow command and specifies video filter complexities within the ffmpeg command.",
        "type": "comment"
    },
    "5258": {
        "file_id": 682,
        "content": "/tests/vapoursynth_linux_test/test.sh",
        "type": "filepath"
    },
    "5259": {
        "file_id": 682,
        "content": "This code is running vspipe, a command-line tool for processing video files with VapourSynth script. It takes a .vpy script file as input and uses the -c flag for y4m format output.",
        "type": "summary"
    },
    "5260": {
        "file_id": 682,
        "content": "vspipe -c y4m script.vpy -",
        "type": "code",
        "location": "/tests/vapoursynth_linux_test/test.sh:1-1"
    },
    "5261": {
        "file_id": 682,
        "content": "This code is running vspipe, a command-line tool for processing video files with VapourSynth script. It takes a .vpy script file as input and uses the -c flag for y4m format output.",
        "type": "comment"
    },
    "5262": {
        "file_id": 683,
        "content": "/tests/vapoursynth_linux_test/scene_change_detection.sh",
        "type": "filepath"
    },
    "5263": {
        "file_id": 683,
        "content": "This code is using FFmpeg to process a video file, extracting scenes by selecting frames where the scene change is greater than 0.1 and displaying information about each scene change. The output is redirected to null.",
        "type": "summary"
    },
    "5264": {
        "file_id": 683,
        "content": "ffmpeg -hide_banner -i \"/root/Desktop/works/pyjom/samples/video/LiEIfnsvn.mp4\" -an \\\n-filter:v \"select='gt(scene,0.1)',showinfo\" \\\n-f null \\\n- 2>&1",
        "type": "code",
        "location": "/tests/vapoursynth_linux_test/scene_change_detection.sh:3-6"
    },
    "5265": {
        "file_id": 683,
        "content": "This code is using FFmpeg to process a video file, extracting scenes by selecting frames where the scene change is greater than 0.1 and displaying information about each scene change. The output is redirected to null.",
        "type": "comment"
    },
    "5266": {
        "file_id": 684,
        "content": "/tests/vapoursynth_linux_test/pure_ffmpeg_interpolate_resolution_denoise.sh",
        "type": "filepath"
    },
    "5267": {
        "file_id": 684,
        "content": "This script uses FFmpeg and TensorFlow to process video files, applying Super Resolution and Edge Preserving Blur filters for improved quality. It utilizes Anaconda libraries for CUDA toolkit and CuDNN in a non-real-time processing manner.",
        "type": "summary"
    },
    "5268": {
        "file_id": 684,
        "content": "# ffmpeg -y -i \"/root/Desktop/works/pyjom/tests/random_giphy_gifs/samoyed.gif\" -vf \"minterpolate,scale=w=iw*2:h=ih*2:flags=lanczos,hqdn3d\" -r 60 ffmpeg_samoyed.mp4\n# SRCNN=espcn.pb\n# 5fps or something\n# env LD_LIBRARY_PATH=/root/anaconda3/pkgs/cudatoolkit-10.0.130-0/lib/:/root/anaconda3/pkgs/cudnn-7.6.5-cuda10.0_0/lib/:$LD_LIBRARY_PATH ffmpeg -i \"/root/Desktop/works/pyjom/tests/random_giphy_gifs/samoyed.gif\" -y -vf \"sr=dnn_backend=tensorflow:model=./sr_models/dnn_models/espcn.pb\"  ffmpeg_samoyed_espcn.mp4\n# 9fps or something\n# ffmpeg -i \"/root/Desktop/works/pyjom/tests/random_giphy_gifs/samoyed.gif\" -y -vf \"yaepblur\"  ffmpeg_samoyed_srcnn.mp4\n# strange shit.\n# env LD_LIBRARY_PATH=/root/anaconda3/pkgs/cudatoolkit-10.0.130-0/lib/:/root/anaconda3/pkgs/cudnn-7.6.5-cuda10.0_0/lib/:$LD_LIBRARY_PATH ffmpeg -i \"/root/Desktop/works/pyjom/tests/random_giphy_gifs/samoyed.gif\" -y -vf \"sr=dnn_backend=tensorflow:model=./sr/espcn.pb,yaepblur,hqdn3d\"  ffmpeg_samoyed_srcnn.mp4\n# env LD_LIBRARY_PATH=/root/anaco",
        "type": "code",
        "location": "/tests/vapoursynth_linux_test/pure_ffmpeg_interpolate_resolution_denoise.sh:1-13"
    },
    "5269": {
        "file_id": 684,
        "content": "The script contains FFmpeg commands to resize, denoise and apply different filters on a GIF file. It uses the TensorFlow model \"espcn.pb\" for super-resolution and the \"yaepblur\" filter. The environment variable LD_LIBRARY_PATH is used to specify paths for CUDA toolkit and CUDNN libraries. The final output is saved as \".mp4\" files with different names.",
        "type": "comment"
    },
    "5270": {
        "file_id": 684,
        "content": "nda3/pkgs/cudatoolkit-10.0.130-0/lib/:/root/anaconda3/pkgs/cudnn-7.6.5-cuda10.0_0/lib/:$LD_LIBRARY_PATH ffmpeg -i \"/root/Desktop/works/pyjom/tests/random_giphy_gifs/samoyed.gif\" -y -vf \"sr=dnn_backend=tensorflow:model=./sr/espcn.pb,yaepblur\"  ffmpeg_samoyed_dctdnoiz.mp4\nenv LD_LIBRARY_PATH=/root/anaconda3/pkgs/cudatoolkit-10.0.130-0/lib/:/root/anaconda3/pkgs/cudnn-7.6.5-cuda10.0_0/lib/:$LD_LIBRARY_PATH ffmpeg -i \"/root/Desktop/works/pyjom/samples/video/LiEIfnsvn.mp4\" -y -vf \"sr=dnn_backend=tensorflow:model=./sr/espcn.pb,yaepblur\"  supertest.mp4\n# dctdnoiz is not for real time processing. it is slow.\n# but somehow it makes the picture great. is it?\n#  TSC hqdn3d            V->V       Apply a High Quality 3D Denoiser.\n# check out all filters by `ffmpeg -filters`\n# yaepblur\n# yet another edge preserving blur filter\n# ffmpeg -y -i \"/root/Desktop/works/pyjom/tests/random_giphy_gifs/samoyed.gif\" -filter \"minterpolate=mi_mode=2\" -r 60 ffmpeg_samoyed.mp4\n# use deep learning models:\n# https://video.stackexchange.com/questions/29337/how-do-the-super-resolution-filters-in-ffmpeg-work",
        "type": "code",
        "location": "/tests/vapoursynth_linux_test/pure_ffmpeg_interpolate_resolution_denoise.sh:13-27"
    },
    "5271": {
        "file_id": 684,
        "content": "This code uses FFmpeg to process video files, applying filters like Super Resolution (SR) using deep learning models and Edge Preserving Blur. It utilizes TensorFlow as the dnn_backend for SR filter and Anaconda libraries for CUDA toolkit and CuDNN. The processing is not real-time but improves picture quality.",
        "type": "comment"
    },
    "5272": {
        "file_id": 685,
        "content": "/tests/vapoursynth_linux_test/previewTestVideo.sh",
        "type": "filepath"
    },
    "5273": {
        "file_id": 685,
        "content": "Executing VapourSynth script on Linux, piping the output to FFplay for visualization. No frame-by-frame shift slider available.",
        "type": "summary"
    },
    "5274": {
        "file_id": 685,
        "content": "vspipe -c y4m basic_test.py - | ffplay -i pipe: \n# working! but no frame by frame shift slider avaliable",
        "type": "code",
        "location": "/tests/vapoursynth_linux_test/previewTestVideo.sh:1-2"
    },
    "5275": {
        "file_id": 685,
        "content": "Executing VapourSynth script on Linux, piping the output to FFplay for visualization. No frame-by-frame shift slider available.",
        "type": "comment"
    },
    "5276": {
        "file_id": 686,
        "content": "/tests/vapoursynth_linux_test/pip_examine.sh",
        "type": "filepath"
    },
    "5277": {
        "file_id": 686,
        "content": "The code captures, crops, and saves FFmpeg-generated screenshots from a video at specific timestamps in the \"pip_examine\" directory.",
        "type": "summary"
    },
    "5278": {
        "file_id": 686,
        "content": "ffmpeg -y -ss 0.400000 -i /root/Desktop/works/pyjom/samples/video/LiEIfnsvn.mp4 -vf crop=464:640:68:186 -vframes 1 pip_examine/screenshot_0.jpg\nffmpeg -y -ss 0.600000 -i /root/Desktop/works/pyjom/samples/video/LiEIfnsvn.mp4 -vf crop=464:640:68:186 -vframes 1 pip_examine/screenshot_1.jpg\nffmpeg -y -ss 0.800000 -i /root/Desktop/works/pyjom/samples/video/LiEIfnsvn.mp4 -vf crop=464:640:68:186 -vframes 1 pip_examine/screenshot_2.jpg\nffmpeg -y -ss 1.000000 -i /root/Desktop/works/pyjom/samples/video/LiEIfnsvn.mp4 -vf crop=464:640:68:186 -vframes 1 pip_examine/screenshot_3.jpg\nffmpeg -y -ss 1.200000 -i /root/Desktop/works/pyjom/samples/video/LiEIfnsvn.mp4 -vf crop=464:640:68:186 -vframes 1 pip_examine/screenshot_4.jpg\nffmpeg -y -ss 1.400000 -i /root/Desktop/works/pyjom/samples/video/LiEIfnsvn.mp4 -vf crop=464:640:68:186 -vframes 1 pip_examine/screenshot_5.jpg\nffmpeg -y -ss 1.600000 -i /root/Desktop/works/pyjom/samples/video/LiEIfnsvn.mp4 -vf crop=464:640:68:186 -vframes 1 pip_examine/screenshot_6.jpg",
        "type": "code",
        "location": "/tests/vapoursynth_linux_test/pip_examine.sh:1-7"
    },
    "5279": {
        "file_id": 686,
        "content": "This code uses FFmpeg to capture screenshots at specific time intervals from a video file. It crops the screenshots with specific dimensions and saves them in the \"pip_examine\" folder.",
        "type": "comment"
    },
    "5280": {
        "file_id": 686,
        "content": "ffmpeg -y -ss 1.800000 -i /root/Desktop/works/pyjom/samples/video/LiEIfnsvn.mp4 -vf crop=464:640:68:186 -vframes 1 pip_examine/screenshot_7.jpg\nffmpeg -y -ss 2.000000 -i /root/Desktop/works/pyjom/samples/video/LiEIfnsvn.mp4 -vf crop=464:640:68:186 -vframes 1 pip_examine/screenshot_8.jpg\nffmpeg -y -ss 2.200000 -i /root/Desktop/works/pyjom/samples/video/LiEIfnsvn.mp4 -vf crop=464:640:68:200 -vframes 1 pip_examine/screenshot_9.jpg\nffmpeg -y -ss 2.400000 -i /root/Desktop/works/pyjom/samples/video/LiEIfnsvn.mp4 -vf crop=464:640:68:200 -vframes 1 pip_examine/screenshot_10.jpg\nffmpeg -y -ss 2.600000 -i /root/Desktop/works/pyjom/samples/video/LiEIfnsvn.mp4 -vf crop=464:640:68:200 -vframes 1 pip_examine/screenshot_11.jpg\nffmpeg -y -ss 2.800000 -i /root/Desktop/works/pyjom/samples/video/LiEIfnsvn.mp4 -vf crop=464:624:68:200 -vframes 1 pip_examine/screenshot_12.jpg\nffmpeg -y -ss 3.000000 -i /root/Desktop/works/pyjom/samples/video/LiEIfnsvn.mp4 -vf crop=464:608:68:212 -vframes 1 pip_examine/screenshot_13.jpg",
        "type": "code",
        "location": "/tests/vapoursynth_linux_test/pip_examine.sh:8-14"
    },
    "5281": {
        "file_id": 686,
        "content": "These ffmpeg commands capture screenshots at specific timestamps from a video file and save them with corresponding filenames in the pip_examine directory. The `-ss`, `-i`, `-vf`, and `-vframes` options specify the start time, input file, video filter, and number of frames to capture for each screenshot respectively.",
        "type": "comment"
    },
    "5282": {
        "file_id": 686,
        "content": "ffmpeg -y -ss 3.200000 -i /root/Desktop/works/pyjom/samples/video/LiEIfnsvn.mp4 -vf crop=464:608:68:212 -vframes 1 pip_examine/screenshot_14.jpg\nffmpeg -y -ss 3.400000 -i /root/Desktop/works/pyjom/samples/video/LiEIfnsvn.mp4 -vf crop=464:608:68:212 -vframes 1 pip_examine/screenshot_15.jpg\nffmpeg -y -ss 3.600000 -i /root/Desktop/works/pyjom/samples/video/LiEIfnsvn.mp4 -vf crop=464:608:68:216 -vframes 1 pip_examine/screenshot_16.jpg\nffmpeg -y -ss 3.800000 -i /root/Desktop/works/pyjom/samples/video/LiEIfnsvn.mp4 -vf crop=464:608:68:216 -vframes 1 pip_examine/screenshot_17.jpg\nffmpeg -y -ss 4.000000 -i /root/Desktop/works/pyjom/samples/video/LiEIfnsvn.mp4 -vf crop=464:608:68:218 -vframes 1 pip_examine/screenshot_18.jpg\nffmpeg -y -ss 4.200000 -i /root/Desktop/works/pyjom/samples/video/LiEIfnsvn.mp4 -vf crop=464:608:68:218 -vframes 1 pip_examine/screenshot_19.jpg\nffmpeg -y -ss 4.400000 -i /root/Desktop/works/pyjom/samples/video/LiEIfnsvn.mp4 -vf crop=464:608:68:218 -vframes 1 pip_examine/screenshot_20.jpg",
        "type": "code",
        "location": "/tests/vapoursynth_linux_test/pip_examine.sh:15-21"
    },
    "5283": {
        "file_id": 686,
        "content": "This code uses FFmpeg to capture screenshots at specific timestamps from a video file. It crops the images to a particular size and saves them with sequential filenames in the \"pip_examine\" directory.",
        "type": "comment"
    },
    "5284": {
        "file_id": 686,
        "content": "ffmpeg -y -ss 4.600000 -i /root/Desktop/works/pyjom/samples/video/LiEIfnsvn.mp4 -vf crop=464:624:68:218 -vframes 1 pip_examine/screenshot_21.jpg\nffmpeg -y -ss 4.800000 -i /root/Desktop/works/pyjom/samples/video/LiEIfnsvn.mp4 -vf crop=464:624:68:212 -vframes 1 pip_examine/screenshot_22.jpg\nffmpeg -y -ss 5.000000 -i /root/Desktop/works/pyjom/samples/video/LiEIfnsvn.mp4 -vf crop=464:640:68:200 -vframes 1 pip_examine/screenshot_23.jpg\nffmpeg -y -ss 5.200000 -i /root/Desktop/works/pyjom/samples/video/LiEIfnsvn.mp4 -vf crop=464:640:68:200 -vframes 1 pip_examine/screenshot_24.jpg\nffmpeg -y -ss 5.400000 -i /root/Desktop/works/pyjom/samples/video/LiEIfnsvn.mp4 -vf crop=464:640:68:200 -vframes 1 pip_examine/screenshot_25.jpg\nffmpeg -y -ss 5.600000 -i /root/Desktop/works/pyjom/samples/video/LiEIfnsvn.mp4 -vf crop=464:656:68:186 -vframes 1 pip_examine/screenshot_26.jpg\nffmpeg -y -ss 5.800000 -i /root/Desktop/works/pyjom/samples/video/LiEIfnsvn.mp4 -vf crop=464:656:68:186 -vframes 1 pip_examine/screenshot_27.jpg",
        "type": "code",
        "location": "/tests/vapoursynth_linux_test/pip_examine.sh:22-28"
    },
    "5285": {
        "file_id": 686,
        "content": "This code uses FFmpeg to capture a series of screenshots from a video at specific timestamps, cropping the images with specific dimensions and saving them in the \"pip_examine\" folder.",
        "type": "comment"
    },
    "5286": {
        "file_id": 686,
        "content": "ffmpeg -y -ss 6.000000 -i /root/Desktop/works/pyjom/samples/video/LiEIfnsvn.mp4 -vf crop=464:656:68:186 -vframes 1 pip_examine/screenshot_28.jpg\nffmpeg -y -ss 6.200000 -i /root/Desktop/works/pyjom/samples/video/LiEIfnsvn.mp4 -vf crop=464:656:68:186 -vframes 1 pip_examine/screenshot_29.jpg\nffmpeg -y -ss 6.400000 -i /root/Desktop/works/pyjom/samples/video/LiEIfnsvn.mp4 -vf crop=464:672:68:186 -vframes 1 pip_examine/screenshot_30.jpg\nffmpeg -y -ss 6.600000 -i /root/Desktop/works/pyjom/samples/video/LiEIfnsvn.mp4 -vf crop=464:672:68:186 -vframes 1 pip_examine/screenshot_31.jpg\nffmpeg -y -ss 6.800000 -i /root/Desktop/works/pyjom/samples/video/LiEIfnsvn.mp4 -vf crop=464:672:68:186 -vframes 1 pip_examine/screenshot_32.jpg\nffmpeg -y -ss 7.000000 -i /root/Desktop/works/pyjom/samples/video/LiEIfnsvn.mp4 -vf crop=464:672:68:186 -vframes 1 pip_examine/screenshot_33.jpg\nffmpeg -y -ss 7.200000 -i /root/Desktop/works/pyjom/samples/video/LiEIfnsvn.mp4 -vf crop=464:672:68:186 -vframes 1 pip_examine/screenshot_34.jpg",
        "type": "code",
        "location": "/tests/vapoursynth_linux_test/pip_examine.sh:29-35"
    },
    "5287": {
        "file_id": 686,
        "content": "This code is using FFmpeg to capture screenshots from a video at specific time intervals. It crops the images and saves them with corresponding filenames in the \"pip_examine\" directory.",
        "type": "comment"
    },
    "5288": {
        "file_id": 686,
        "content": "ffmpeg -y -ss 7.400000 -i /root/Desktop/works/pyjom/samples/video/LiEIfnsvn.mp4 -vf crop=464:672:68:186 -vframes 1 pip_examine/screenshot_35.jpg\nffmpeg -y -ss 7.600000 -i /root/Desktop/works/pyjom/samples/video/LiEIfnsvn.mp4 -vf crop=464:672:68:186 -vframes 1 pip_examine/screenshot_36.jpg\nffmpeg -y -ss 7.800000 -i /root/Desktop/works/pyjom/samples/video/LiEIfnsvn.mp4 -vf crop=464:672:68:186 -vframes 1 pip_examine/screenshot_37.jpg\nffmpeg -y -ss 8.000000 -i /root/Desktop/works/pyjom/samples/video/LiEIfnsvn.mp4 -vf crop=464:672:68:186 -vframes 1 pip_examine/screenshot_38.jpg\nffmpeg -y -ss 8.200000 -i /root/Desktop/works/pyjom/samples/video/LiEIfnsvn.mp4 -vf crop=464:672:68:186 -vframes 1 pip_examine/screenshot_39.jpg\nffmpeg -y -ss 8.400000 -i /root/Desktop/works/pyjom/samples/video/LiEIfnsvn.mp4 -vf crop=464:672:68:186 -vframes 1 pip_examine/screenshot_40.jpg\nffmpeg -y -ss 8.600000 -i /root/Desktop/works/pyjom/samples/video/LiEIfnsvn.mp4 -vf crop=464:672:68:186 -vframes 1 pip_examine/screenshot_41.jpg",
        "type": "code",
        "location": "/tests/vapoursynth_linux_test/pip_examine.sh:36-42"
    },
    "5289": {
        "file_id": 686,
        "content": "The code is using FFmpeg to capture screenshots at specific timestamps of a video file, crop the images, and save them with corresponding filenames. It runs for 10 different timestamps.",
        "type": "comment"
    },
    "5290": {
        "file_id": 686,
        "content": "ffmpeg -y -ss 8.800000 -i /root/Desktop/works/pyjom/samples/video/LiEIfnsvn.mp4 -vf crop=464:672:68:186 -vframes 1 pip_examine/screenshot_42.jpg\nffmpeg -y -ss 9.000000 -i /root/Desktop/works/pyjom/samples/video/LiEIfnsvn.mp4 -vf crop=464:672:68:186 -vframes 1 pip_examine/screenshot_43.jpg\nffmpeg -y -ss 9.200000 -i /root/Desktop/works/pyjom/samples/video/LiEIfnsvn.mp4 -vf crop=464:672:68:186 -vframes 1 pip_examine/screenshot_44.jpg\nffmpeg -y -ss 9.400000 -i /root/Desktop/works/pyjom/samples/video/LiEIfnsvn.mp4 -vf crop=464:672:68:186 -vframes 1 pip_examine/screenshot_45.jpg\nffmpeg -y -ss 9.600000 -i /root/Desktop/works/pyjom/samples/video/LiEIfnsvn.mp4 -vf crop=464:672:68:186 -vframes 1 pip_examine/screenshot_46.jpg\nffmpeg -y -ss 9.800000 -i /root/Desktop/works/pyjom/samples/video/LiEIfnsvn.mp4 -vf crop=464:672:68:186 -vframes 1 pip_examine/screenshot_47.jpg\nffmpeg -y -ss 10.000000 -i /root/Desktop/works/pyjom/samples/video/LiEIfnsvn.mp4 -vf crop=464:672:68:186 -vframes 1 pip_examine/screenshot_48.jpg",
        "type": "code",
        "location": "/tests/vapoursynth_linux_test/pip_examine.sh:43-49"
    },
    "5291": {
        "file_id": 686,
        "content": "This code is using ffmpeg to capture screenshots at specific time intervals from a video file, saving each as an individual .jpg image in the pip_examine directory. The crop filters are applied to ensure the correct portion of the frame is captured for each shot.",
        "type": "comment"
    },
    "5292": {
        "file_id": 686,
        "content": "ffmpeg -y -ss 10.200000 -i /root/Desktop/works/pyjom/samples/video/LiEIfnsvn.mp4 -vf crop=464:656:68:200 -vframes 1 pip_examine/screenshot_49.jpg\nffmpeg -y -ss 10.400000 -i /root/Desktop/works/pyjom/samples/video/LiEIfnsvn.mp4 -vf crop=464:656:68:202 -vframes 1 pip_examine/screenshot_50.jpg\nffmpeg -y -ss 10.600000 -i /root/Desktop/works/pyjom/samples/video/LiEIfnsvn.mp4 -vf crop=464:480:68:378 -vframes 1 pip_examine/screenshot_51.jpg\nffmpeg -y -ss 10.800000 -i /root/Desktop/works/pyjom/samples/video/LiEIfnsvn.mp4 -vf crop=464:480:68:378 -vframes 1 pip_examine/screenshot_52.jpg\nffmpeg -y -ss 11.000000 -i /root/Desktop/works/pyjom/samples/video/LiEIfnsvn.mp4 -vf crop=464:480:68:378 -vframes 1 pip_examine/screenshot_53.jpg\nffmpeg -y -ss 11.200000 -i /root/Desktop/works/pyjom/samples/video/LiEIfnsvn.mp4 -vf crop=464:480:68:378 -vframes 1 pip_examine/screenshot_54.jpg\nffmpeg -y -ss 11.400000 -i /root/Desktop/works/pyjom/samples/video/LiEIfnsvn.mp4 -vf crop=464:448:68:378 -vframes 1 pip_examine/screenshot_55.jpg",
        "type": "code",
        "location": "/tests/vapoursynth_linux_test/pip_examine.sh:50-56"
    },
    "5293": {
        "file_id": 686,
        "content": "These lines are using FFmpeg to capture screenshots at specific timestamps from a video file, with varying crop dimensions and saving them as JPEGs in the pip_examine directory.",
        "type": "comment"
    },
    "5294": {
        "file_id": 686,
        "content": "ffmpeg -y -ss 11.600000 -i /root/Desktop/works/pyjom/samples/video/LiEIfnsvn.mp4 -vf crop=464:320:68:382 -vframes 1 pip_examine/screenshot_56.jpg\nffmpeg -y -ss 11.800000 -i /root/Desktop/works/pyjom/samples/video/LiEIfnsvn.mp4 -vf crop=464:320:68:382 -vframes 1 pip_examine/screenshot_57.jpg\nffmpeg -y -ss 12.000000 -i /root/Desktop/works/pyjom/samples/video/LiEIfnsvn.mp4 -vf crop=464:320:68:382 -vframes 1 pip_examine/screenshot_58.jpg\nffmpeg -y -ss 12.200000 -i /root/Desktop/works/pyjom/samples/video/LiEIfnsvn.mp4 -vf crop=464:320:68:382 -vframes 1 pip_examine/screenshot_59.jpg\nffmpeg -y -ss 12.400000 -i /root/Desktop/works/pyjom/samples/video/LiEIfnsvn.mp4 -vf crop=464:320:68:382 -vframes 1 pip_examine/screenshot_60.jpg\nffmpeg -y -ss 12.600000 -i /root/Desktop/works/pyjom/samples/video/LiEIfnsvn.mp4 -vf crop=464:320:68:382 -vframes 1 pip_examine/screenshot_61.jpg\nffmpeg -y -ss 12.800000 -i /root/Desktop/works/pyjom/samples/video/LiEIfnsvn.mp4 -vf crop=464:320:68:382 -vframes 1 pip_examine/screenshot_62.jpg",
        "type": "code",
        "location": "/tests/vapoursynth_linux_test/pip_examine.sh:57-63"
    },
    "5295": {
        "file_id": 686,
        "content": "This code is using FFmpeg to capture screenshots at specific timestamps from a video file. The `-ss` flag specifies the start time, `-vf crop` defines the cropping area, and `-vframes 1` captures one frame. Each resulting image is saved in the 'pip_examine' directory with the corresponding filename.",
        "type": "comment"
    },
    "5296": {
        "file_id": 686,
        "content": "ffmpeg -y -ss 13.000000 -i /root/Desktop/works/pyjom/samples/video/LiEIfnsvn.mp4 -vf crop=464:320:68:382 -vframes 1 pip_examine/screenshot_63.jpg\nffmpeg -y -ss 13.200000 -i /root/Desktop/works/pyjom/samples/video/LiEIfnsvn.mp4 -vf crop=464:320:68:382 -vframes 1 pip_examine/screenshot_64.jpg\nffmpeg -y -ss 13.400000 -i /root/Desktop/works/pyjom/samples/video/LiEIfnsvn.mp4 -vf crop=464:448:68:378 -vframes 1 pip_examine/screenshot_65.jpg\nffmpeg -y -ss 13.600000 -i /root/Desktop/works/pyjom/samples/video/LiEIfnsvn.mp4 -vf crop=464:448:68:378 -vframes 1 pip_examine/screenshot_66.jpg\nffmpeg -y -ss 13.800000 -i /root/Desktop/works/pyjom/samples/video/LiEIfnsvn.mp4 -vf crop=464:320:68:382 -vframes 1 pip_examine/screenshot_67.jpg\nffmpeg -y -ss 14.000000 -i /root/Desktop/works/pyjom/samples/video/LiEIfnsvn.mp4 -vf crop=464:320:68:382 -vframes 1 pip_examine/screenshot_68.jpg\nffmpeg -y -ss 14.200000 -i /root/Desktop/works/pyjom/samples/video/LiEIfnsvn.mp4 -vf crop=464:320:68:382 -vframes 1 pip_examine/screenshot_69.jpg",
        "type": "code",
        "location": "/tests/vapoursynth_linux_test/pip_examine.sh:64-70"
    },
    "5297": {
        "file_id": 686,
        "content": "This code is using FFmpeg to generate a series of screenshots at specific time intervals from a video file. The `-ss` flag sets the start time for each screenshot, and the `-vf crop` option specifies the cropping parameters for each image. Each screenshot is saved in the \"pip_examine\" directory with a corresponding filename.",
        "type": "comment"
    },
    "5298": {
        "file_id": 686,
        "content": "ffmpeg -y -ss 14.400000 -i /root/Desktop/works/pyjom/samples/video/LiEIfnsvn.mp4 -vf crop=464:320:68:382 -vframes 1 pip_examine/screenshot_70.jpg\nffmpeg -y -ss 14.600000 -i /root/Desktop/works/pyjom/samples/video/LiEIfnsvn.mp4 -vf crop=464:320:68:382 -vframes 1 pip_examine/screenshot_71.jpg\nffmpeg -y -ss 14.800000 -i /root/Desktop/works/pyjom/samples/video/LiEIfnsvn.mp4 -vf crop=464:320:68:382 -vframes 1 pip_examine/screenshot_72.jpg\nffmpeg -y -ss 15.000000 -i /root/Desktop/works/pyjom/samples/video/LiEIfnsvn.mp4 -vf crop=464:320:68:382 -vframes 1 pip_examine/screenshot_73.jpg\nffmpeg -y -ss 15.200000 -i /root/Desktop/works/pyjom/samples/video/LiEIfnsvn.mp4 -vf crop=464:320:68:382 -vframes 1 pip_examine/screenshot_74.jpg\nffmpeg -y -ss 15.400000 -i /root/Desktop/works/pyjom/samples/video/LiEIfnsvn.mp4 -vf crop=464:320:68:382 -vframes 1 pip_examine/screenshot_75.jpg\nffmpeg -y -ss 15.600000 -i /root/Desktop/works/pyjom/samples/video/LiEIfnsvn.mp4 -vf crop=464:320:68:382 -vframes 1 pip_examine/screenshot_76.jpg",
        "type": "code",
        "location": "/tests/vapoursynth_linux_test/pip_examine.sh:71-77"
    },
    "5299": {
        "file_id": 686,
        "content": "This code uses FFmpeg to capture screenshots from a video file at specific time intervals. It crops the images to a specific size and saves them as separate files in the \"pip_examine\" directory. Each line represents one screenshot command executed consecutively, starting at 14.4 seconds and incrementing by 0.2 seconds until reaching 15.6 seconds.",
        "type": "comment"
    }
}