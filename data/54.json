{
    "5400": {
        "file_id": 706,
        "content": "checked as having_alike_rect then cut its life. otherwise extend its life, though not extend above max_rect_life.\n    (old_x1,old_y1), (old_x2, old_y2) = oldRect\n    (new_x1,new_y1), (new_x2, new_y2) = newRect\n    old_w = old_x2-old_x1\n    old_h = old_y2-old_y1\n    det_x1 = abs(new_x1 - old_x1)/ old_w\n    det_x2 = abs(new_x2 - old_x2)/ old_w\n    det_y1 = abs(new_y1 - old_y1)/ old_h\n    det_y2 = abs(new_y2 - old_y2)/ old_h\n    # print(\"deltas:\",det_x1, det_x2, det_y1, det_y2)\n    having_alike_rect =  (det_x1 < delta_thresh) and (det_y1 < delta_thresh) and (det_x2 < delta_thresh ) and (det_y2 < delta_thresh)\n    myRect = newRect\n    if having_alike_rect:\n        myRect = oldRect\n    return myRect, having_alike_rect\ndef rectSurge(oldRectList, newRectList,delta_thresh = 0.1, min_rect_life = 0, max_rect_life = 6):\n    newToOldDictList = []\n    oldRectDictList = [{\"rect\":x[\"rect\"], \"alike\":False, \"life\":x[\"life\"],\"uuid\":x[\"uuid\"]} for x in oldRectList] # actually they are all dict lists. you can pass an empty list as oldRectList anyway.",
        "type": "code",
        "location": "/tests/video_detector_tests/rect_detect.py:20-43"
    },
    "5401": {
        "file_id": 706,
        "content": "This code calculates the delta between two rectangles and checks if they are alike within a certain threshold. It updates the rectangle list based on this comparison, ensuring rectangles with little change remain unchanged while extending or cutting their life depending on the condition.",
        "type": "comment"
    },
    "5402": {
        "file_id": 706,
        "content": "    # print(\"OLDRECTDICTLIST:\",oldRectDictList)\n    for newRect in newRectList:\n        needAppend = True\n        for index, oldRectDict in enumerate(oldRectDictList):\n            # print(\"ENUMERATING OLD INDEX:\",index)\n            oldRect = oldRectDict[\"rect\"]\n            _, having_alike_rect = rectMerge(oldRect,newRect,delta_thresh=delta_thresh)\n            if having_alike_rect:\n                needAppend = False\n                if not oldRectDict[\"alike\"]:\n                    # print(\"SET ALIKE:\",index,oldRect)\n                    oldRectDictList[index][\"alike\"] = True\n                # ignore myRect.\n        if needAppend:\n            newToOldDictList.append({\"rect\":newRect,\"life\":1,\"uuid\":str(uuid.uuid4())}) # make sure it is not duplicated?\n            # if appended we shall break this loop. but when shall we append?\n    oldToOldDictList = []\n    for oldRectDict in oldRectDictList:\n        alike = oldRectDict[\"alike\"]\n        life = oldRectDict[\"life\"]\n        oldRect = oldRectDict[\"rect\"]\n        myUUID = oldRectDict[\"uuid\"]",
        "type": "code",
        "location": "/tests/video_detector_tests/rect_detect.py:44-65"
    },
    "5403": {
        "file_id": 706,
        "content": "The code is iterating through a list of new rectangles and an existing list of old rectangles. For each new rectangle, it checks if any old rectangle has a similar one using the rectMerge function. If a similar rectangle is found, it updates its \"alike\" flag in the oldRectDictList. If no similar rectangle is found, it appends the new rectangle to the newToOldDictList. Finally, it creates a new list, oldToOldDictList, by iterating through oldRectDictList and excluding any rectangles with \"alike\" set to True.",
        "type": "comment"
    },
    "5404": {
        "file_id": 706,
        "content": "        if not alike:\n            life -=1\n        else:\n            life +=1\n            life = min(max_rect_life, life)\n        if life <= min_rect_life:\n            continue\n        oldToOldDictList.append({\"rect\":oldRect,\"life\":life,\"uuid\":myUUID})\n    return oldToOldDictList + newToOldDictList # a combination.\ndef updateTotalRects(oldTotalRectDict,rectList,currentFrameIndex,diffFrame):\n    for elem in rectList:\n        uuid = elem[\"uuid\"]\n        rect = elem[\"rect\"]\n        if uuid not in oldTotalRectDict.keys():\n            oldTotalRectDict.update({uuid:{\"rect\":rect,\"startFrame\":currentFrameIndex,\"endFrame\":None,\"meanDifference\":None}}) # finally,remove those without endFrame.\n        else:\n            duration = currentFrameIndex - oldTotalRectDict[uuid][\"startFrame\"]\n            (x0,y0),(x1,y1) = rect\n            diff = diffFrame[y0:y1,x0:x1] # this is shit. we need to crop this shit.\n            # grayscale.\n            # std = np.abs(std)\n            # get the total delta over time?\n            # std = np.mean(std,axis=2)",
        "type": "code",
        "location": "/tests/video_detector_tests/rect_detect.py:66-89"
    },
    "5405": {
        "file_id": 706,
        "content": "This function updates a dictionary of rectangles by iterating over a list of rectangles and their properties. It adds new rectangles to the dictionary or updates existing ones with their duration, position, and difference from previous frames. It ignores rectangles that do not change and removes those without an end frame.",
        "type": "comment"
    },
    "5406": {
        "file_id": 706,
        "content": "            diff_x = np.mean(diff.flatten())\n            # std_x = np.std(std,axis=2)\n            # std_x = np.std(std_x,axis=1)\n            # std_x = np.std(std_x,axis=0)\n            std_total = diff_x # later we need to convert this float64.\n            # breakpoint()\n            if duration == 1:\n                oldTotalRectDict[uuid][\"meanDifference\"] = std_total\n            else:\n                dur2 = duration - 1\n                prev_std = oldTotalRectDict[uuid][\"meanDifference\"]\n                new_std = (dur2*prev_std + std_total)/duration # may freaking exceed limit.\n                oldTotalRectDict[uuid][\"meanDifference\"] = new_std\n            oldTotalRectDict[uuid][\"endFrame\"] = currentFrameIndex\n    return oldTotalRectDict\ntotal_rect_dict ={}\nrect_dict_main_list = []\nmin_rect_life_display_thresh = 3 # a filter.\nmode = 1\nline_thresh =  150\nincludeBoundaryLines = True # applied to those cornered crops.\n# this will slow down the process. or maybe?\nframeIndex = -1\nprevFrame = None\nif mode == 1:\n    import pybgs as bgs",
        "type": "code",
        "location": "/tests/video_detector_tests/rect_detect.py:90-117"
    },
    "5407": {
        "file_id": 706,
        "content": "This code calculates the mean difference of rectangles over a certain duration and updates an existing dictionary with this information. It then returns the updated dictionary. The dictionary contains rectangle information such as mean difference, start frame index, end frame index, and duration for each unique ID (uuid). If the duration is 1, it simply stores the current mean difference in the dictionary. Otherwise, it calculates a new mean difference by taking a weighted average of the previous mean difference and the new mean difference. The code also initializes variables and imports a module called pybgs as bgs.",
        "type": "comment"
    },
    "5408": {
        "file_id": 706,
        "content": "    algorithm = (\n    bgs.FrameDifference()\n)  # this\nwhile True:\n    ret, img = video.read()\n    if img is None:\n        if mode == 1:\n            popKeys = []\n            for key in total_rect_dict.keys():\n                elem = total_rect_dict[key]\n                if elem[\"endFrame\"] is None:\n                    popKeys.append(key)\n            for key in popKeys:\n                total_rect_dict.pop(key) # remove premature rectangles.\n        break\n    else: frameIndex+=1\n    if mode == 1:\n        diff_img_output = algorithm.apply(img)\n    # what about the freaking still image?\n    # Convert the img to grayscale\n    # gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n    # no need to use gray image.\n    # Apply edge detection method on the image\n    blurred = cv2.GaussianBlur(img, (5, 5), 0)\n    edges = cv2.Canny(blurred,20,210,apertureSize = 3) # great.\n    # why not applying edges directly to rectangles?\n    # This returns an array of r and theta values\n    # line_thresh =  200\n    # maintain a rectangle list. merge the alikes?",
        "type": "code",
        "location": "/tests/video_detector_tests/rect_detect.py:118-152"
    },
    "5409": {
        "file_id": 706,
        "content": "This code reads frames from a video and applies different algorithms to detect objects. It checks for premature rectangles and removes them if necessary, and applies edge detection methods on the image. The code uses GaussianBlur and Canny functions for edge detection. It maintains a rectangle list and considers merging similar rectangles.",
        "type": "comment"
    },
    "5410": {
        "file_id": 706,
        "content": "    if mode == 1:\n        lines = cv2.HoughLines(edges,1,np.pi/180,line_thresh)\n        angle_error = 0.00003   # this can only detect square things, absolute square.\n        # we need to know horizontal and vertical lines, when they cross we get points.\n        frameHeight, frameWidth = img.shape[:2]\n        # print(\"height: \", frameHeight)\n        # print(\"width: \", frameWidth)\n        mlines = {\"horizontal\":[], \"vertical\":[]}\n        if includeBoundaryLines:\n            originPoint = (0,0)\n            cornerPoint = (frameWidth,frameHeight)\n            mlines[\"horizontal\"].append(originPoint)\n            mlines[\"horizontal\"].append(cornerPoint)\n            mlines[\"vertical\"].append(originPoint)\n            mlines[\"vertical\"].append(cornerPoint)\n        for line in lines:\n            for r_theta in line:\n                # breakpoint()\n                r,theta = r_theta.tolist()\n                # Stores the value of cos(theta) in a\n                # filter detected lines?\n                # theta filter:\n                if not abs(theta % (np.pi/2) )< angle_error:",
        "type": "code",
        "location": "/tests/video_detector_tests/rect_detect.py:154-176"
    },
    "5411": {
        "file_id": 706,
        "content": "The code segment is filtering HoughLines-detected lines from an image, ensuring they are nearly horizontal or vertical. It stores the cosine of line angles in a variable called 'a', and applies an angle error threshold to filter out lines not close to 0 (horizontal) or Ï€/2 (vertical). If includeBoundaryLines is True, it adds four boundary points for both horizontal and vertical lines.",
        "type": "comment"
    },
    "5412": {
        "file_id": 706,
        "content": "                    continue # this is filtering.\n                # print(\"line parameter:\",r,theta)\n                a = np.cos(theta)\n                # Stores the value of sin(theta) in b\n                b = np.sin(theta)\n                # x0 stores the value rcos(theta)\n                x0 = a*r\n                # y0 stores the value rsin(theta)\n                y0 = b*r\n                # x1 stores the rounded off value of (rcos(theta)-1000sin(theta))\n                x1 = int(x0 + 1000*(-b))\n                # y1 stores the rounded off value of (rsin(theta)+1000cos(theta))\n                y1 = int(y0 + 1000*(a))\n                # x2 stores the rounded off value of (rcos(theta)+1000sin(theta))\n                x2 = int(x0 - 1000*(-b))\n                # y2 stores the rounded off value of (rsin(theta)-1000cos(theta))\n                y2 = int(y0 - 1000*(a))\n                # cv2.line draws a line in img from the point(x1,y1) to (x2,y2).\n                # (0,0,255) denotes the colour of the line to be\n                #drawn. In this case, it is red.",
        "type": "code",
        "location": "/tests/video_detector_tests/rect_detect.py:177-204"
    },
    "5413": {
        "file_id": 706,
        "content": "This code calculates line parameters using trigonometry and then draws a red line on the image. The loop filters out certain conditions, and the calculations are based on radius (r), polar angle (theta). Lines are drawn between different points calculated from these variables to create lines in the image.",
        "type": "comment"
    },
    "5414": {
        "file_id": 706,
        "content": "                df_x = abs(x1-x2)\n                df_y = abs(y1-y2)\n                lineType = \"vertical\"\n                if df_x > df_y:\n                    lineType = \"horizontal\"\n                # we just need one single point and lineType.\n                linePoint = (x1,y1)\n                mlines[lineType].append(linePoint)\n                # cv2.line(img,(x1,y1), (x2,y2), (0,0,255),2)\n                # would not draw lines this time. draw found rects instead.\n        # get rectangle points. or just all possible rectangles?\n        # enumerate all possible lines.\n        if len(mlines[\"horizontal\"]) < 2 or len(mlines[\"vertical\"]) < 2:\n            print(\"unable to form rectangles.\")\n            continue\n        else:\n            rects =[] # list of rectangles\n            for line_h1, line_h2 in itertools.combinations(mlines[\"horizontal\"],2):\n                ymin, ymax = list(sorted([line_h1[1],line_h2[1]]))\n                for line_v1, line_v2 in itertools.combinations(mlines[\"vertical\"], 2):\n                    xmin, xmax = list(sorted([line_v1[0],line_v2[0]]))",
        "type": "code",
        "location": "/tests/video_detector_tests/rect_detect.py:205-226"
    },
    "5415": {
        "file_id": 706,
        "content": "Calculating differences in x and y coordinates to determine line type. Appends line points based on line type to a dictionary. If there are less than 2 horizontal or vertical lines, the code cannot form rectangles and skips to the next iteration. Iterates through combinations of horizontal lines to find the upper and lower bounds, then does the same with vertical lines for left and right bounds.",
        "type": "comment"
    },
    "5416": {
        "file_id": 706,
        "content": "                    rect = ((xmin,ymin),(xmax,ymax))\n                    rects.append(rect)\n            rect_dict_main_list = rectSurge(rect_dict_main_list,rects)\n            # print(\"RECT DICT MAIN LIST:\")\n            # print(rect_dict_main_list) # maybe i want this shit?\n            total_rect_dict = updateTotalRects(total_rect_dict,rect_dict_main_list,frameIndex,diff_img_output)\n            mdisplayed_rect_count = 0\n            for rect_dict in rect_dict_main_list:\n                life = rect_dict[\"life\"]\n                if life < min_rect_life_display_thresh:\n                    continue # this is needed.\n                # draw shit now.\n                mdisplayed_rect_count +=1\n                (xmin,ymin),(xmax,ymax) = rect_dict[\"rect\"]\n                cv2.rectangle(img,(xmin,ymin),(xmax,ymax) , (255,0,0), 2)\n            #     (xmin,ymin),(xmax,ymax) = rect\n            #     rect_area = (xmax-xmin) * (ymax-ymin)\n            #     print(\"rect found:\",rect,rect_area)\n            prevFrame = img.copy()",
        "type": "code",
        "location": "/tests/video_detector_tests/rect_detect.py:227-245"
    },
    "5417": {
        "file_id": 706,
        "content": "The code is updating a list of rectangles and iterating over it to draw them on an image. It checks if the rectangle \"life\" is above a certain threshold before drawing, and maintains a count of displayed rectangles. The code also stores the previous frame for comparison in a later step.",
        "type": "comment"
    },
    "5418": {
        "file_id": 706,
        "content": "            # print(\"total rects:\",mdisplayed_rect_count)\n    elif mode == 2:\n        lines = cv2.HoughLinesP(edges,1,np.pi/180,line_thresh,minLineLength=2,maxLineGap=100) # these are not angle filtering.\n        for points in lines:\n      # Extracted points nested in the list\n            x1,y1,x2,y2=points[0]\n            # filter out angle errors?\n            # Draw the lines joing the points\n            # On the original image\n            cv2.line(img,(x1,y1),(x2,y2),(0,255,0),2)\n            # Maintain a simples lookup list for points\n            # lines_list.append([(x1,y1),(x2,y2)])\n    elif mode == 3:\n        # edges = cv2.GaussianBlur(edges, (5, 5), 0)\n        horizontal_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (10,1))\n        detect_horizontal = cv2.morphologyEx(edges, cv2.MORPH_OPEN, horizontal_kernel, iterations=3)\n        vertical_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (1,10))\n        detect_vertical = cv2.morphologyEx(edges, cv2.MORPH_OPEN, vertical_kernel, iterations=3)",
        "type": "code",
        "location": "/tests/video_detector_tests/rect_detect.py:246-264"
    },
    "5419": {
        "file_id": 706,
        "content": "The code applies different modes to detect lines and edges in an image. Mode 2 uses HoughLinesP to detect lines without angle filtering, drawing them on the original image. Mode 3 applies morphological operations using structuring elements for horizontal and vertical lines detection.",
        "type": "comment"
    },
    "5420": {
        "file_id": 706,
        "content": "        cnts_horizontal = cv2.findContours(detect_horizontal, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n        cnts_horizontal = cnts_horizontal[0] if len(cnts_horizontal) == 2 else cnts_horizontal[1]\n        cnts_vertical = cv2.findContours(detect_vertical, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n        cnts_vertical = cnts_vertical[0] if len(cnts_vertical) == 2 else cnts_vertical[1]\n        for c in cnts_horizontal:\n            cv2.drawContours(img, [c], -1, (255,0,0), 3)\n        for c in cnts_vertical:\n            cv2.drawContours(img, [c], -1, (255,0,0), 3)\n    # what the heck?\n    # The below for loop runs till r and theta values\n    # are in the range of the 2d array\n    # why you have middle lines?\n            # how to get the intersections? lines?\n    cv2.imshow('linesDetected.jpg', img)\n    # cv2.imshow(\"edges.jpg\",edges) # not for fun.\n    if cv2.waitKey(20) == ord(\"q\"):\n        print(\"QUIT INTERFACE.\")\n        break\n# All the changes made in the input image are finally\n# written on a new image houghlines.jpg",
        "type": "code",
        "location": "/tests/video_detector_tests/rect_detect.py:266-290"
    },
    "5421": {
        "file_id": 706,
        "content": "This code uses OpenCV to detect horizontal and vertical lines in an image. It finds contours for both line types and draws them on the original image. Then, it displays the image with the detected lines using cv2.imshow(). The code also checks for a 'q' key press to exit the interface and saves the edited image as houghlines.jpg.",
        "type": "comment"
    },
    "5422": {
        "file_id": 706,
        "content": "if mode == 1:\n    print(\"FINAL RESULT:\")\n    for key in total_rect_dict.keys():\n        elem = total_rect_dict[key]\n        print(\"RECT UUID\",key)\n        print(\"RECT CONTENT\",elem)",
        "type": "code",
        "location": "/tests/video_detector_tests/rect_detect.py:291-298"
    },
    "5423": {
        "file_id": 706,
        "content": "This code block checks if mode is 1, then prints the final results. It iterates through each key-value pair in total_rect_dict and outputs the rect UUID and content.",
        "type": "comment"
    },
    "5424": {
        "file_id": 707,
        "content": "/tests/video_detector_tests/rectangle_framedifference.py",
        "type": "filepath"
    },
    "5425": {
        "file_id": 707,
        "content": "The code utilizes a motion detector to continuously capture frames, detecting changes for object detection and tracking. It calculates merged bounding boxes, applies thresholds, updates coordinates based on average values, and displays frames using OpenCV.",
        "type": "summary"
    },
    "5426": {
        "file_id": 707,
        "content": "# motion detectors are used to track objects. though you may want to separate objects with it.\nimport numpy as np\nimport cv2\nimport pybgs as bgs\nimport talib  # wait till all points are stablized. find a way to stream this.\n# suspect by static image analysis, and then create bounding box over the thing.\n# check image quality.\n# this can generate frame borders.\nalgorithm = (\n    bgs.FrameDifference()\n)  # this is not stable since we have more boundaries. shall we group things?\nvideo_file = (\n    \"../../samples/video/dog_with_text.mp4\"  # this is doggy video without borders.\n)\n# video_file = \"../../samples/video/LiEIfnsvn.mp4\" # this one with cropped boundaries.\ncapture = cv2.VideoCapture(video_file)\nwhile not capture.isOpened():\n    capture = cv2.VideoCapture(video_file)\n    # cv2.waitKey(1000)\n    # print(\"Wait for the header\")\npos_frame = capture.get(1)\ndef getAppendArray(mx1, min_x, past_frames=19):\n    return np.append(mx1[-past_frames:], min_x)\ndef getFrameAppend(frameArray, pointArray, past_frames=19):\n    mx1, mx2, my1, my2 = [",
        "type": "code",
        "location": "/tests/video_detector_tests/rectangle_framedifference.py:1-33"
    },
    "5427": {
        "file_id": 707,
        "content": "The code initializes a motion detector using frame difference algorithm to track objects in a video. It reads the video file and prepares two functions: `getAppendArray` for appending array elements, and `getFrameAppend` for processing frame data. These functions are used with specified past frames to analyze the video and possibly group objects. The code also handles potential video file opening issues by retrying if necessary.",
        "type": "comment"
    },
    "5428": {
        "file_id": 707,
        "content": "        getAppendArray(a, b, past_frames=past_frames)\n        for a, b in zip(frameArray, pointArray)\n    ]\n    return mx1, mx2, my1, my2\ndef getStreamAvg(a, timeperiod=10):  # to maintain stability.\n    return talib.stream.EMA(a, timeperiod=timeperiod)\ndef checkChange(frame_x1, val_x1, h, change_threshold=0.2):\n    return (abs(frame_x1 - val_x1) / h) > change_threshold  # really changed.\nmx1, mx2, my1, my2 = [np.array([]) for _ in range(4)]\npast_frames = 19\nperc = 0.03\nframe_num = 0\n# what is the time to update the frame?\nframe_x1, frame_y1, frame_x2, frame_y2 = [None for _ in range(4)]\nreputation = 0\nmax_reputation = 3\nminVariance = 10\nframeDict = {}  # include index, start, end, coords.\nframeIndex = 0\nwhile True:\n    flag, frame = capture.read()\n    frameIndex += 1\n    if flag:\n        pos_frame = capture.get(1)  # this is getting previous frame without read again.\n        img_output = algorithm.apply(frame)\n        img_bgmodel = algorithm.getBackgroundModel()\n        _, contours = cv2.findContours(\n            img_output, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE",
        "type": "code",
        "location": "/tests/video_detector_tests/rectangle_framedifference.py:34-70"
    },
    "5429": {
        "file_id": 707,
        "content": "The code initializes variables for frame coordinates, past frames count, and other parameters. It then enters a loop to continuously capture frames from the camera, apply an algorithm to detect changes, and update relevant variables accordingly. The purpose is likely object detection and tracking using background subtraction or similar techniques.",
        "type": "comment"
    },
    "5430": {
        "file_id": 707,
        "content": "        )\n        # maybe you should merge all active areas.\n        if contours is not None:\n            # continue\n            counted = False\n            for contour in contours:\n                [x, y, w, h] = cv2.boundingRect(img_output)\n                if not counted:\n                    min_x, min_y = x, y\n                    max_x, max_y = x + w, y + h\n                    counted = True\n                else:\n                    min_x = min(min_x, x)\n                    min_y = min(min_y, y)\n                    max_x = max(max_x, x + w)\n                    max_y = max(max_y, y + h)\n                    # only create one single bounding box.\n            # print(\"points:\",min_x, min_y, max_x,max_y)\n            this_w = max_x - min_x\n            this_h = max_y - min_y\n            thresh_x = max(minVariance, int(perc * (this_w)))\n            thresh_y = max(minVariance, int(perc * (this_h)))\n            mx1, mx2, my1, my2 = getFrameAppend(\n                (mx1, mx2, my1, my2), (min_x, max_x, min_y, max_y)\n            )",
        "type": "code",
        "location": "/tests/video_detector_tests/rectangle_framedifference.py:71-95"
    },
    "5431": {
        "file_id": 707,
        "content": "The code finds the bounding box of all detected contours and merges them into a single one. It then calculates the width and height of this merged bounding box, applies thresholds based on its size and percentage, and updates existing frame append values with new min and max coordinates.",
        "type": "comment"
    },
    "5432": {
        "file_id": 707,
        "content": "            val_x1, val_x2, val_y1, val_y2 = [\n                getStreamAvg(a) for a in (mx1, mx2, my1, my2)\n            ]\n            # not a number. float\n            # will return False on any comparison, including equality.\n            if (\n                abs(val_x1 - min_x) < thresh_x\n                and abs(val_x2 - max_x) < thresh_x\n                and abs(val_y1 - min_y) < thresh_y\n                and abs(val_y2 - max_y) < thresh_y\n            ):\n                needChange = False\n                # this will create bounding rect.\n                # this cannot handle multiple active rects.\n                reputation = max_reputation\n                if frame_x1 == None:\n                    needChange = True\n                elif (\n                    checkChange(frame_x1, val_x1, this_w)\n                    or checkChange(frame_x2, val_x2, this_w)\n                    or checkChange(frame_y1, val_y1, this_h)\n                    or checkChange(frame_y2, val_y2, this_h)\n                ):\n                    needChange = True",
        "type": "code",
        "location": "/tests/video_detector_tests/rectangle_framedifference.py:96-119"
    },
    "5433": {
        "file_id": 707,
        "content": "This code calculates the average values of x1, x2, y1, and y2 using getStreamAvg function for variables mx1 to my2. If these averages are within a certain threshold from min_x, max_x, min_y, and max_y, it sets needChange to False and reputation to max_reputation. If frame_x1 is None or there's a change in any of the variables, needChange is set to True.",
        "type": "comment"
    },
    "5434": {
        "file_id": 707,
        "content": "                    # the #2 must be of this reason.\n                if needChange:\n                    frame_x1, frame_y1, frame_x2, frame_y2 = [\n                        int(a) for a in (min_x, min_y, max_x, max_y)\n                    ]\n                    print()\n                    print(\"########FRAME CHANGED########\")\n                    frame_num += 1\n                    frame_area = (frame_x2 - frame_x1) * (frame_y2 - frame_y1)\n                    # update the shit.\n                    coords = ((frame_x1, frame_y1), (frame_x2, frame_y2))\n                    frameDict[frame_num] = {\n                        \"coords\": coords,\n                        \"start\": frameIndex,\n                        \"end\": frameIndex,\n                    }\n                    print(\n                        \"FRAME INDEX: {}\".format(frame_num)\n                    )  # this is the indexable frame. not uuid.\n                    print(\"FRAME AREA: {}\".format(frame_area))\n                    print(\"FRAME COORDS: {}\".format(str(coords)))",
        "type": "code",
        "location": "/tests/video_detector_tests/rectangle_framedifference.py:120-140"
    },
    "5435": {
        "file_id": 707,
        "content": "This code snippet handles frame changes in a video detection process. When a change is detected (needChange), it updates the frame's coordinates, number, and area. It then prints information about the new frame and adds it to the frameDict dictionary with the index as the key.",
        "type": "comment"
    },
    "5436": {
        "file_id": 707,
        "content": "                # allow us to introduce our new frame determinism.\n            else:\n                if reputation > 0:\n                    reputation -= 1\n            if frame_x1 is not None and reputation > 0:\n                # you may choose to keep cutting the frame? with delay though.\n                cv2.rectangle(\n                    frame, (frame_x1, frame_y1), (frame_x2, frame_y2), (255, 0, 0), 2\n                )\n                frameDict[frame_num][\"end\"] = frameIndex\n                # we mark the first and last time to display this frame.\n            # how to stablize this shit?\n        cv2.imshow(\"video\", frame)\n        # just video.\n        # cv2.imshow('img_output', img_output)\n        # cv2.imshow('img_bgmodel', img_bgmodel)\n    else:\n        cv2.waitKey(1000)\n        break\n    if 0xFF & cv2.waitKey(10) == 27:\n        break\ncv2.destroyAllWindows()\nprint(\"FINAL FRAME DETECTIONS:\")\nprint(frameDict)\n# {1: {'coords': ((80, 199), (496, 825)), 'start': 13, 'end': 269}, 2: {'coords': ((80, 381), (483, 644)), 'start': 297, 'end': 601}}",
        "type": "code",
        "location": "/tests/video_detector_tests/rectangle_framedifference.py:141-169"
    },
    "5437": {
        "file_id": 707,
        "content": "This code appears to be part of a video detection and analysis program. It uses OpenCV to display frames from the video and overlay rectangles on frames that have been detected multiple times. The \"frameDict\" stores information about detected frames, including their coordinates, start and end indices in the video, and reputation. The program continues until the user presses ESC or waits too long, then prints the final frame detections.",
        "type": "comment"
    },
    "5438": {
        "file_id": 708,
        "content": "/tests/video_detector_tests/rect_corner_detect_fast.py",
        "type": "filepath"
    },
    "5439": {
        "file_id": 708,
        "content": "This code appears to be exploring various corner detection methods from the OpenCV library (cv2) for potential use with the Canny edge detector. The purpose might be to find the most suitable method for a specific application.",
        "type": "summary"
    },
    "5440": {
        "file_id": 708,
        "content": "# there are some corner detection methods in cv2. not sure what to follow... is it intended to use with canny edge detector or not?",
        "type": "code",
        "location": "/tests/video_detector_tests/rect_corner_detect_fast.py:1-1"
    },
    "5441": {
        "file_id": 708,
        "content": "This code appears to be exploring various corner detection methods from the OpenCV library (cv2) for potential use with the Canny edge detector. The purpose might be to find the most suitable method for a specific application.",
        "type": "comment"
    },
    "5442": {
        "file_id": 709,
        "content": "/tests/video_detector_tests/singleTracker.py",
        "type": "filepath"
    },
    "5443": {
        "file_id": 709,
        "content": "This code utilizes YOLOv5 for object detection and a video tracker to monitor dog movement in frames, identifying dogs and providing bounding box coordinates above threshold. Additionally, it closes OpenCV-created video windows.",
        "type": "summary"
    },
    "5444": {
        "file_id": 709,
        "content": "import cv2\n# import imutils #another dependency?\n# tracker = cv2.TrackerCSRT_create() # outdated tracker.\n# i really don't know what is a dog.\nimport torch\n# don't really know how paddleocr recognize chars.\nlocalModelDir = '/root/Desktop/works/pyjom/pyjom/models/yolov5/ultralytics_yolov5_master/'\nimport os\nos.environ[\"YOLOV5_MODEL_DIR\"] = '/root/Desktop/works/pyjom/pyjom/models/yolov5/'\nmodel = torch.hub.load(localModelDir, 'yolov5s',source=\"local\")\ndef getDogBB(frame,thresh=0.7):\n    img = frame[:,:,::-1].transpose((2,0,1))\n    # Inference\n    # reshape this shit.\n    # img = np.reshape()\n    results = model(img) # pass the image through our model\n    df = results.pandas().xyxy[0]\n    print(df)\n    data = []\n    for index,line in df.iterrows():\n        # print(line)\n        left = (line[\"xmin\"],line[\"ymin\"])\n        right = (line[\"xmax\"],line[\"ymax\"])\n        confidence = line[\"confidence\"]\n        class_ = line[\"class\"]\n        name = line[\"name\"]\n        if name == \"dog\" and confidence >= thresh: # better figure out all output names.",
        "type": "code",
        "location": "/tests/video_detector_tests/singleTracker.py:1-31"
    },
    "5445": {
        "file_id": 709,
        "content": "This code imports necessary libraries, sets the local model directory, and loads a YOLOv5 model for object detection. It defines a function to get the bounding box coordinates of a dog in an image, with the option to set a minimum confidence threshold. The code then performs inference using the loaded model on the input frame image and returns the bounding box information if the detected class is \"dog\" and the confidence meets or exceeds the threshold.",
        "type": "comment"
    },
    "5446": {
        "file_id": 709,
        "content": "            data.append({\"location\":[left,right],\"confidence\":confidence,\"identity\":{\"class\":class_,\"name\":name}})\n    print(data)\n    data = list(sorted(data,key=lambda x: -x[\"confidence\"]))\n    if len(data)>0:\n        target= data[0]\n        xmin,ymin = target[\"location\"][0]\n        xmax,ymax = target[\"location\"][1]\n        return int(xmin),int(ymin),int(xmax-xmin),int(ymax-ymin)\ndef checkDog(frame,thresh=0.5):\n    return getDogBB(frame,thresh=thresh) == None # dog missing.\n# better use something else?\n# tracker = cv2.TrackerMIL_create()\ntracker_types = ['MIL', 'GOTURN', 'DaSiamRPN']\ntracker_type = tracker_types[2]\nbasepath = \"./OpenCV-Object-Tracker-Python-Sample\"\nif tracker_type == 'MIL':\n    tracker = cv2.TrackerMIL_create()\nelif tracker_type == 'DaSiamRPN': # deeplearning.\n    # this tracker is slow as hell. really.\n    params = cv2.TrackerDaSiamRPN_Params()\n    params.model = os.path.join(basepath,\"model/DaSiamRPN/dasiamrpn_model.onnx\")\n    params.kernel_r1 = os.path.join(basepath,\"model/DaSiamRPN/dasiamrpn_kernel_r1.onnx\")",
        "type": "code",
        "location": "/tests/video_detector_tests/singleTracker.py:32-56"
    },
    "5447": {
        "file_id": 709,
        "content": "Code initializes a video tracker using different algorithms and prepares parameters for the 'DaSiamRPN' tracker, which is slower but uses deep learning. It then checks if a dog is present in each frame of a video and returns its bounding box coordinates.",
        "type": "comment"
    },
    "5448": {
        "file_id": 709,
        "content": "    params.kernel_cls1 = os.path.join(basepath,\"model/DaSiamRPN/dasiamrpn_kernel_cls1.onnx\")\n    tracker = cv2.TrackerDaSiamRPN_create(params)\n    # tracker = cv2.TrackerDaSiamRPN_create()\nelif tracker_type == 'GOTURN': #also need config file.\n    # this is bad though.\n    params = cv2.TrackerGOTURN_Params()\n    params.modelTxt = os.path.join(basepath,\"model/GOTURN/goturn.prototxt\") # save this shit without BOM.\n    params.modelBin = os.path.join(basepath,\"model/GOTURN/goturn.caffemodel\")\n    tracker = cv2.TrackerGOTURN_create(params)\n    # tracker = cv2.TrackerGOTURN_create()\n# we have to feed dog coordinates into the shit.\nvideo = cv2.VideoCapture(\"../../samples/video/dog_with_text.mp4\")\n_,frame = video.read()\n# frame = imutils.resize(frame,width=720) #why?\nindex = 0\nyoloRate = 10\ntrack_success = False\nupdate_track = 3\nBB = None\ninit=False\nwhile frame is not None:\n    index +=1\n    _, frame = video.read()\n    if frame is None:\n        print(\"VIDEO END.\")\n        break\n    if index%yoloRate == 0:\n        if BB is None:",
        "type": "code",
        "location": "/tests/video_detector_tests/singleTracker.py:57-86"
    },
    "5449": {
        "file_id": 709,
        "content": "This code initializes a tracker using the cv2.TrackerDaSiamRPN or cv2.TrackerGOTURN methods based on the tracker_type variable. It then creates parameters for the GOTURN tracker and creates a video capture object to read a video file. The loop reads frames from the video, checks if a bounding box is None every yoloRate frames, and if it's None, it initializes the BB variable. This code appears to be part of a video tracking application.",
        "type": "comment"
    },
    "5450": {
        "file_id": 709,
        "content": "            BB = getDogBB(frame)\n        else:\n            x, y, w, h = BB\n            if len(frame.shape) == 3:\n                dogFrame = frame[y:y+h,x:x+w,:]\n            else:\n                dogFrame = frame[y:y+h,x:x+w]\n            result = checkDog(dogFrame)\n            if result: # dog gone missing.\n                BB = getDogBB(frame)\n                init=False\n    if BB is not None:\n        if not init:\n            tracker.init(frame, BB) # how to init this shit?\n            init=True\n    # when lost, we know there is no dog inside the bounding box.\n    # frame = imutils.resize(frame,width=720)\n        if index % update_track == 0:\n            track_success,BB = tracker.update(frame)\n        if track_success and BB:\n            top_left = (int(BB[0]),int(BB[1]))\n            bottom_right = (int(BB[0]+BB[2]), int(BB[1]+BB[3]))\n            cv2.rectangle(frame,top_left,bottom_right,(0,255,0),5)\n    cv2.imshow('Output',frame)\n    key  =  cv2.waitKey(1) & 0xff\n    if key == ord('q'):\n        break\nvideo.release()",
        "type": "code",
        "location": "/tests/video_detector_tests/singleTracker.py:87-114"
    },
    "5451": {
        "file_id": 709,
        "content": "This code initializes a tracker and tracks a dog's movement in video frames. It checks if the dog is present in the bounding box and updates the position accordingly. If the dog goes missing, it reinitializes the tracker with new dog's bounding box. The tracked dog's position is displayed on the frame, which is then shown as output.",
        "type": "comment"
    },
    "5452": {
        "file_id": 709,
        "content": "cv2.destroyAllWindows()",
        "type": "code",
        "location": "/tests/video_detector_tests/singleTracker.py:115-115"
    },
    "5453": {
        "file_id": 709,
        "content": "This code is used to close all the video windows created by OpenCV (cv2).",
        "type": "comment"
    },
    "5454": {
        "file_id": 710,
        "content": "/tests/video_detector_tests/yolo.py",
        "type": "filepath"
    },
    "5455": {
        "file_id": 710,
        "content": "The code reads video frames, uses YOLOv5 model for object detection and text extraction, sets model directories and environment variables, performs inference, and stores results in a DataFrame. It detects objects (likely a dog) and displays image with details before exiting upon key press.",
        "type": "summary"
    },
    "5456": {
        "file_id": 710,
        "content": "image_path = \"../../samples/video/dog_with_text.mp4\"\nimport cv2\nvideo = cv2.VideoCapture(image_path)\nfor _ in range(100):\n    ret, frame = video.read() # first frame is blackout!\nimport torch\n# don't really know how paddleocr recognize chars.\nlocalModelDir = '/root/Desktop/works/pyjom/pyjom/models/yolov5/ultralytics_yolov5_master/'\nimport os\nos.environ[\"YOLOV5_MODEL_DIR\"] = '/root/Desktop/works/pyjom/pyjom/models/yolov5/'\nmodel = torch.hub.load(localModelDir, 'yolov5s',source=\"local\") # the yolov5s.pt file is required when loading the model.\n# Image\n# img = 'https://ultralytics.com/images/zidane.jpg'\nimg = frame[:,:,::-1].transpose((2,0,1))\n# Inference\n# reshape this shit.\n# img = np.reshape()\nresults = model(img) # pass the image through our model\ndf = results.pandas().xyxy[0]\nprint(df)\ndata = []\nfor index,line in df.iterrows():\n    # print(line)\n    left = (line[\"xmin\"],line[\"ymin\"])\n    right = (line[\"xmax\"],line[\"ymax\"])\n    confidence = line[\"confidence\"]\n    class_ = line[\"class\"]\n    name = line[\"name\"]\n  ",
        "type": "code",
        "location": "/tests/video_detector_tests/yolo.py:1-37"
    },
    "5457": {
        "file_id": 710,
        "content": "The code reads a video frame by frame, applies object detection using YOLOv5 model, and extracts text from the detected objects. It sets the local model directory and environment variable for YOLOv5 model loading, loads the model, resizes and preprocesses the frame, performs inference, and stores the resultant bounding boxes with associated data (class, confidence, name) into a DataFrame.",
        "type": "comment"
    },
    "5458": {
        "file_id": 710,
        "content": "  data.append({\"location\":[left,right],\"confidence\":confidence,\"identity\":{\"class\":class_,\"name\":name}})\nprint(data)\ncv2.imshow(\"name\",frame)\ncv2.waitKey(0)\n# found the freaking dog!",
        "type": "code",
        "location": "/tests/video_detector_tests/yolo.py:37-41"
    },
    "5459": {
        "file_id": 710,
        "content": "This code detects an object (likely a dog) using YOLO and appends its location, confidence level, and identity details to the \"data\" list. The image is displayed with the name \"name\" and waits for a key press to exit. The comment celebrates finding the desired object.",
        "type": "comment"
    },
    "5460": {
        "file_id": 711,
        "content": "/tests/video_detector_tests/yolo_norfair.py",
        "type": "filepath"
    },
    "5461": {
        "file_id": 711,
        "content": "This code initializes Detectron2 for instance segmentation on COCO dataset, sets up a YOLO object detector for video frames, and tracks their positions across frames. It processes detection information, updates tracked objects using a tracker, draws circles with names based on estimated positions. The code displays a video frame, waits for 'q' key press to break loop, destroys windows, and writes the frame to file if display is not enabled.",
        "type": "summary"
    },
    "5462": {
        "file_id": 711,
        "content": "import cv2\nimport numpy as np\nfrom detectron2.config import get_cfg\nfrom detectron2.engine import DefaultPredictor\nfrom cocoNames import cocoName\nfrom norfair import Detection, Tracker, Video, draw_tracked_objects\ndef euclidean_distance(detection, tracked_object):\n    return np.linalg.norm(detection.points - tracked_object.estimate)\n# Set up Detectron2 object detector\ncfg = get_cfg()\ncfg.merge_from_file(\"norfair/demos/faster_rcnn_R_50_FPN_3x.yaml\")\ncfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5 # looks like it does not recognize dog.\n# cfg.MODEL.WEIGHTS = \"detectron2://COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl\"\ncfg.MODEL.WEIGHTS = \"/root/Desktop/works/pyjom/tests/video_detector_tests/detectron2_models/model_final_f10217.pkl\"\n# it is stored in s3\n# https://dl.fbaipublicfiles.com/detectron2/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl\n# download cache: /root/.torch/iopath_cache/detectron2/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl",
        "type": "code",
        "location": "/tests/video_detector_tests/yolo_norfair.py:1-22"
    },
    "5463": {
        "file_id": 711,
        "content": "The code imports necessary libraries and sets up the Detectron2 object detector using a pre-trained model for instance segmentation on COCO dataset. The model weight file is located at \"/root/Desktop/works/pyjom/tests/video_detector_tests/detectron2\\_models/model\\_final\\_f10217.pkl\".",
        "type": "comment"
    },
    "5464": {
        "file_id": 711,
        "content": "detector = DefaultPredictor(cfg)\n# what are the classes output by the model?\n# Norfair\n# video_path = \"/root/Desktop/works/pyjom/samples/video/dog_with_text.mp4\"\nvideo_path = \"/root/Desktop/works/pyjom/samples/video/LlfeL29BP.mp4\"\nvideo = Video(input_path=video_path)\ntracker = Tracker(distance_function=euclidean_distance, distance_threshold=200,hit_inertia_min=2,hit_inertia_max=20,initialization_delay=1) # what the heck?\n# you should implement standalone tracker function, optical.\n# maybe you can track the object via framedifference?\ntracked_objects = None\ndisplay=True\nfor index, frame in enumerate(video): # we need to speed up.\n    if index%10 == 0:\n        detections = detector(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n        print(\"original detections:\",detections)\n        instances = detections[\"instances\"]\n        print(\"instances:\",instances)\n        # breakpoint()\n        pred_classes = instances.pred_classes\n        if len(pred_classes) == 0:\n            continue\n        detections2=[]\n        for index,class_ in enumerate(pred_classes):",
        "type": "code",
        "location": "/tests/video_detector_tests/yolo_norfair.py:24-48"
    },
    "5465": {
        "file_id": 711,
        "content": "This code is initializing a YOLO object detector, reading a video file, setting up a tracker, and then iterating through each frame of the video. The detector predicts objects in each frame, and if any are detected, it saves the detections and continues to the next frame. If no objects are detected, it skips that frame. The tracker helps keep track of object positions across frames, but the implementation details are unclear.",
        "type": "comment"
    },
    "5466": {
        "file_id": 711,
        "content": "            print(\"index:\",index)\n            class_ = int(class_.cpu().numpy().tolist())\n            print(\"class:\",class_)\n            box = instances.pred_boxes.tensor[index].cpu().numpy().tolist()\n            box = [int(x) for x in box]\n            score = float(instances.scores[index].cpu().numpy().tolist())\n            print('box:',box)\n            print('score:',score)\n            det = Detection(instances.pred_boxes.get_centers()[index].cpu().numpy(),scores=np.array([score]),data={\"box\":box,\"class\":{\"id\":class_,\"name\":cocoRealName[class_]}})\n            detections2.append(det)\n            # breakpoint()\n        # detections = [Detection(p) for p in instances.pred_boxes.get_centers().cpu().numpy()] # what is this instance anyway?\n        # you would lost data you dick!\n        print(\"detections2\",detections2)\n        tracked_objects = tracker.update(detections=detections2)\n        # print(detections)\n        print(\"tracked objects:\",tracked_objects) # you don't track shit?\n    if tracked_objects is not None:",
        "type": "code",
        "location": "/tests/video_detector_tests/yolo_norfair.py:49-66"
    },
    "5467": {
        "file_id": 711,
        "content": "The code is processing and printing detection information, appending detections to a list (detections2), and updating tracked objects using a tracker. The comment criticizes a previous line of code for potentially losing data. The final if statement checks if any tracked_objects were found.",
        "type": "comment"
    },
    "5468": {
        "file_id": 711,
        "content": "        if tracked_objects!=[]:\n            # there is no bounding box avaliable?\n            for obj in tracked_objects:\n                point = obj.estimate[0]\n                position = tuple(point.astype(int))\n                color = (255,0,0)\n                # breakpoint()\n                name = obj.last_detection.data[\"class\"][\"name\"]\n                cv2.circle(\n                        frame,\n                        position,\n                        radius=100,\n                        color=color,\n                        thickness=2,\n                    )\n                cv2.putText(\n                    frame,\n                    \"[{}][{}]\".format(str(obj.id),name),\n                    (position[0]-100,position[1]),\n                    cv2.FONT_HERSHEY_SIMPLEX,\n                    2,\n                    (0,255,0),\n                    3,\n                    cv2.LINE_AA,\n                )\n            # breakpoint()\n        # i want to draw you in a different way.\n        # draw_tracked_objects(frame, tracked_objects,color=(255,0,0))",
        "type": "code",
        "location": "/tests/video_detector_tests/yolo_norfair.py:67-94"
    },
    "5469": {
        "file_id": 711,
        "content": "The code checks if there are any tracked objects. If there are, it estimates the object's position and draws a circle at that point on the frame. The object's name is also displayed near the circle. Finally, the code calls a function to draw the objects differently using a specific color.",
        "type": "comment"
    },
    "5470": {
        "file_id": 711,
        "content": "    if display:\n        cv2.imshow(\"window\",frame)\n        key  =  cv2.waitKey(1) & 0xff\n        if key == ord('q'):\n            break\n        # maybe we shall print this shit somehow.\n    # video.write(frame) # you write what?\nif display:\n    cv2.destroyAllWindows()",
        "type": "code",
        "location": "/tests/video_detector_tests/yolo_norfair.py:95-103"
    },
    "5471": {
        "file_id": 711,
        "content": "This code snippet displays a video frame on the screen, waits for a key press (specifically 'q'), and breaks the loop if 'q' is pressed. It then destroys all windows created. The frame is written to the video file if display is not enabled.",
        "type": "comment"
    },
    "5472": {
        "file_id": 712,
        "content": "/tests/video_detector_tests/siamMask/setup.sh",
        "type": "filepath"
    },
    "5473": {
        "file_id": 712,
        "content": "Downloading and setting up SiamMask from GitHub, then retrieving pre-trained model files for VOT and DAVIS datasets.",
        "type": "summary"
    },
    "5474": {
        "file_id": 712,
        "content": "git clone https://github.com/foolwood/SiamMask.git && cd SiamMask\nexport SiamMask=$PWD\ncd $SiamMask/experiments/siammask_sharp\nwget http://www.robots.ox.ac.uk/~qwang/SiamMask_VOT.pth\nwget http://www.robots.ox.ac.uk/~qwang/SiamMask_VOT_LD.pth\nwget http://www.robots.ox.ac.uk/~qwang/SiamMask_DAVIS.pth",
        "type": "code",
        "location": "/tests/video_detector_tests/siamMask/setup.sh:1-6"
    },
    "5475": {
        "file_id": 712,
        "content": "Downloading and setting up SiamMask from GitHub, then retrieving pre-trained model files for VOT and DAVIS datasets.",
        "type": "comment"
    },
    "5476": {
        "file_id": 713,
        "content": "/tests/video_detector_tests/siamMask/demo.sh",
        "type": "filepath"
    },
    "5477": {
        "file_id": 713,
        "content": "This script changes directory to \"SiamMask\" and sets environment variables for running a SiamMask demo using Python 3. It resumes from the \"SiamMask_DAVIS.pth\" file with configuration from \"config_davis.json\".",
        "type": "summary"
    },
    "5478": {
        "file_id": 713,
        "content": "cd SiamMask\nexport SiamMask=$PWD\n# cd $SiamMask/experiments/siammask_sharp\n# cd $SiamMask/experiments/siammask_sharp\n# export PYTHONPATH=$PWD:$PYTHONPATH\n# which python3\npython3 -m tools.demo --resume experiments/siammask_sharp/SiamMask_DAVIS.pth --config experiments/siammask_sharp/config_davis.json",
        "type": "code",
        "location": "/tests/video_detector_tests/siamMask/demo.sh:1-7"
    },
    "5479": {
        "file_id": 713,
        "content": "This script changes directory to \"SiamMask\" and sets environment variables for running a SiamMask demo using Python 3. It resumes from the \"SiamMask_DAVIS.pth\" file with configuration from \"config_davis.json\".",
        "type": "comment"
    },
    "5480": {
        "file_id": 714,
        "content": "/tests/taobao_guangguang_download_å“‡å“¦è§†é¢‘_æ·˜å®é€›é€›_tiktok_douyin/frida_globalswitch_apk.js",
        "type": "filepath"
    },
    "5481": {
        "file_id": 714,
        "content": "The code disables SSL-SPDY and SPDY for packet capture debugging, and attempts to print class names using Frida in an APK, but fails to hook 'Response' methods. It uses Java classes in 'mtopsdk.network' to track requests, log details, and initializes ANetworkCallImpl, while modifying 'mtopsdk.mtop.global.SwitchConfig' using Frida for URL logging.",
        "type": "summary"
    },
    "5482": {
        "file_id": 714,
        "content": "////////////////////////////////////////////////////////////////////////\n// try to disable security? disable ssl-spdy and spdy\n////////////////////////////////////////////////////////////////////////\n// try this first anyway.\nsetTimeout(function () {\n    console.log('startâ€”â€”*-*-*-*-*-');\n   Java.perform(function () {\n       var SwitchConfig = Java.use('mtopsdk.mtop.global.SwitchConfig');\n       SwitchConfig.isGlobalSpdySwitchOpen.overload().implementation = function () {\n           var ret = this.isGlobalSpdySwitchOpen.apply(this, arguments);\n           console.log(\"å¼€å¯æŠ“åŒ…\" + ret);\n           return false;\n       }\n       SwitchConfig.isGlobalSpdySslSwitchOpen.overload().implementation = function () {\n        var ret = this.isGlobalSpdySslSwitchOpen.apply(this, arguments);\n        console.log(\"å¼€å¯æŠ“åŒ…\" + ret);\n        return false;\n       }\n   });\n});\n// â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\n// ç‰ˆæƒå£°æ˜Žï¼šæœ¬æ–‡ä¸ºCSDNåšä¸»ã€Œå“ˆé‡Œå“ˆæ°”ã€çš„åŽŸåˆ›æ–‡ç« ï¼Œéµå¾ªCC 4.0 BY-SAç‰ˆæƒåè®®ï¼Œè½¬è½½è¯·é™„ä¸ŠåŽŸæ–‡å‡ºå¤„é“¾æŽ¥åŠæœ¬å£°æ˜Žã€‚\n// åŽŸæ–‡é“¾æŽ¥ï¼šhttps://blog.csdn.net/qq_34067821/article/details/103203549\n////////////////////////////////////////////////////////////////////////",
        "type": "code",
        "location": "/tests/taobao_guangguang_download_å“‡å“¦è§†é¢‘_æ·˜å®é€›é€›_tiktok_douyin/frida_globalswitch_apk.js:2-27"
    },
    "5483": {
        "file_id": 714,
        "content": "This code attempts to disable SSL-SPDY and SPdy by overriding the isGlobalSpdySwitchOpen and isGlobalSpdySslSwitchOpen methods of the SwitchConfig class. It sets both switches to off, effectively disabling them, in order to enable packet capture for debugging purposes. The code is attributed to a CSDN blog post by the author \"å“ˆé‡Œå“ˆæ°”\".",
        "type": "comment"
    },
    "5484": {
        "file_id": 714,
        "content": "// print class names\n////////////////////////////////////////////////////////////////////////\n// var callback = {\n// \t'onMatch': function(cname){\n// \t\t//lets just print out the class name.\n// \t\tconsole.log(cname);\n// \t},\n// \t'onComplete': function() {\n// \t\tconsole.log(\"done\");\n// \t},\n// \t'onError': function(){\n// \t\tconsole.log(\"There is error\");\n// \t}\n// };\n// Java.perform(function(){\n// \tJava.enumerateLoadedClasses(callback);\t//onMatch: function (className)\n// });\n////////////////////////////////////////////////////////////////////////// failed to hook request/response methods as expected\n////////////////////////////////////////////////////////////////////////\n// // Java.perform(function () {\n// //     // Function to hook is defined here\n// //     //æ‰€æœ‰å“åº”\n// in this apk we do not find 'Response' shit.\n// //     var Response = Java.use('mtopsdk.network.domain.Response');\n// //     Response.$init.overload('mtopsdk.network.domain.Response$Builder').implementation = function() {\n// //         //PrintStack()\n// //         console.log(\"Response \" + arguments[0].body)",
        "type": "code",
        "location": "/tests/taobao_guangguang_download_å“‡å“¦è§†é¢‘_æ·˜å®é€›é€›_tiktok_douyin/frida_globalswitch_apk.js:28-59"
    },
    "5485": {
        "file_id": 714,
        "content": "The code aims to print class names using Frida in an APK. It defines a callback with 'onMatch' and 'onComplete' functions, then uses Java.enumerateLoadedClasses() to obtain the class names. The code also attempts to hook 'Response' methods but failed as they were not found in the APK.",
        "type": "comment"
    },
    "5486": {
        "file_id": 714,
        "content": "// //         var ret = this.$init.apply(this, arguments);\n// //         //all request\n// //         console.log(\"Response \" + this.toString())\n// //         return ret;\n// //     };\n// //     //æ‰€æœ‰è¯·æ±‚\n// //     var RequestBuilder = Java.use('mtopsdk.network.domain.Request$Builder');\n// //     RequestBuilder.build.overload().implementation = function() {\n// //         //PrintStack()\n// //         var ret = this.build.apply(this, arguments);\n// //         //all request\n// //         console.log(\"RequestBuilder \" + ret.toString())\n// //         return ret;\n// //     };\n// //     //æ‰€æœ‰è¯·æ±‚\n// //     var ANetworkCallImpl = Java.use('mtopsdk.network.impl.ANetworkCallImpl');\n// //     ANetworkCallImpl.$init.overload('mtopsdk.network.domain.Request', 'android.content.Context').implementation = function() {\n// //         //PrintStack()\n// //         console.log('ANetworkCallImpl ' + arguments[0])\n// //         var ret = this.$init.apply(this, arguments);\n// //         return ret;\n// //     };\n// //     //æ‰€æœ‰è¯·æ±‚url\n// //     var AbstractNetworkConverter = Java.use(",
        "type": "code",
        "location": "/tests/taobao_guangguang_download_å“‡å“¦è§†é¢‘_æ·˜å®é€›é€›_tiktok_douyin/frida_globalswitch_apk.js:60-87"
    },
    "5487": {
        "file_id": 714,
        "content": "This code is manipulating several Java classes in the 'mtopsdk.network' package for tracking all requests, logging the request details and builder objects, and initializing an ANetworkCallImpl with a Request object and Context.",
        "type": "comment"
    },
    "5488": {
        "file_id": 714,
        "content": "// //         'mtopsdk.mtop.protocol.converter.impl.AbstractNetworkConverter'\n// //     );\n// //     AbstractNetworkConverter.buildBaseUrl.overload(\n// //         'mtopsdk.framework.domain.MtopContext',\n// //         'java.lang.String',\n// //         'java.lang.String'\n// //     ).implementation = function() {\n// //         console.log(\"buildBaseUrl \"+arguments[1]+' '+arguments[2])\n// //         var ret = this.buildBaseUrl.apply(this, arguments);\n// //         //url\n// //         console.log(\"buildBaseUrl \"+ret)\n// //         return ret;\n// //     };\n// //     // ç¦ç”¨spdyåè®®\n// //     var SwitchConfig = Java.use('mtopsdk.mtop.global.SwitchConfig');\n// //     SwitchConfig.setGlobalSpdySslSwitchOpen.overload().implementation = function() {\n// //         var ret = this.isGlobalSpdySwitchOpen.apply(this, arguments);\n// //         console.log('isGlobalSpdySwitchOpenl ' + ret)\n// //         return false;\n// //     };\n// // });",
        "type": "code",
        "location": "/tests/taobao_guangguang_download_å“‡å“¦è§†é¢‘_æ·˜å®é€›é€›_tiktok_douyin/frida_globalswitch_apk.js:88-112"
    },
    "5489": {
        "file_id": 714,
        "content": "This code is using Frida to instrument the 'mtopsdk.mtop.global.SwitchConfig' class in an APK. It overrides the 'setGlobalSpdySslSwitchOpen' method to always return false, disabling SPDY protocol. Additionally, it modifies the 'AbstractNetworkConverter.buildBaseUrl' method to log the arguments and base URL.",
        "type": "comment"
    },
    "5490": {
        "file_id": 715,
        "content": "/tests/taobao_guangguang_download_å“‡å“¦è§†é¢‘_æ·˜å®é€›é€›_tiktok_douyin/disable_ssl.js",
        "type": "filepath"
    },
    "5491": {
        "file_id": 715,
        "content": "Bypasses Universal Android SSL Pinning using frida, replacing checkTrustedRecursive implementation to enable SSL communication.",
        "type": "summary"
    },
    "5492": {
        "file_id": 715,
        "content": "// script name: sowdust/universal-android-ssl-pinning-bypass-2\n/* \n   Universal Android SSL Pinning Bypass\n   by Mattia Vinci and Maurizio Agazzini \n   $ frida -U -f org.package.name -l universal-ssl-check-bypass.js --no-pause\n    https://techblog.mediaservice.net/2018/11/universal-android-ssl-check-bypass-2/\n*/\nJava.perform(function() {\n    var array_list = Java.use(\"java.util.ArrayList\");\n    var ApiClient = Java.use('com.android.org.conscrypt.TrustManagerImpl');\n    ApiClient.checkTrustedRecursive.implementation = function(a1, a2, a3, a4, a5, a6) {\n        // console.log('Bypassing SSL Pinning');\n        var k = array_list.$new();\n        return k;\n    }\n}, 0);",
        "type": "code",
        "location": "/tests/taobao_guangguang_download_å“‡å“¦è§†é¢‘_æ·˜å®é€›é€›_tiktok_douyin/disable_ssl.js:1-22"
    },
    "5493": {
        "file_id": 715,
        "content": "Bypasses Universal Android SSL Pinning using frida, replacing checkTrustedRecursive implementation to enable SSL communication.",
        "type": "comment"
    },
    "5494": {
        "file_id": 716,
        "content": "/tests/taobao_guangguang_download_å“‡å“¦è§†é¢‘_æ·˜å®é€›é€›_tiktok_douyin/decodeTaobaoQuery.py",
        "type": "filepath"
    },
    "5495": {
        "file_id": 716,
        "content": "The code decodes a Taobao query string, parses the URL-encoded data into JSON format with various parameters, and prints it using pprint for readability.",
        "type": "summary"
    },
    "5496": {
        "file_id": 716,
        "content": "import urllib.parse\nimport json\na = \"\"\"%7B%22LBS%22%3A%22%7B%5C%22TB%5C%22%3A%5C%22%7B%5C%5C%5C%22stores%5C%5C%5C%22%3A%5B%7B%5C%5C%5C%22code%5C%5C%5C%22%3A%5C%5C%5C%22236736190%5C%5C%5C%22%2C%5C%5C%5C%22bizType%5C%5C%5C%22%3A%5C%5C%5C%222%5C%5C%5C%22%2C%5C%5C%5C%22type%5C%5C%5C%22%3A%5C%5C%5C%2224%5C%5C%5C%22%7D%5D%7D%5C%22%2C%5C%22TMALL_MARKET_B2C%5C%22%3A%5C%22%7B%5C%5C%5C%22stores%5C%5C%5C%22%3A%5B%7B%5C%5C%5C%22code%5C%5C%5C%22%3A%5C%5C%5C%22107%5C%5C%5C%22%2C%5C%5C%5C%22bizType%5C%5C%5C%22%3A%5C%5C%5C%22REGION_TYPE_REGION%5C%5C%5C%22%2C%5C%5C%5C%22addrId%5C%5C%5C%22%3A%5C%5C%5C%229056332332%5C%5C%5C%22%2C%5C%5C%5C%22type%5C%5C%5C%22%3A%5C%5C%5C%22CHOOSE_ADDR%5C%5C%5C%22%7D%5D%7D%5C%22%2C%5C%22TMALL_MARKET_O2O%5C%22%3A%5C%22%7B%5C%5C%5C%22stores%5C%5C%5C%22%3A%5B%7B%5C%5C%5C%22code%5C%5C%5C%22%3A%5C%5C%5C%22235565019%5C%5C%5C%22%2C%5C%5C%5C%22bizType%5C%5C%5C%22%3A%5C%5C%5C%22DELIVERY_TIME_HALF_DAY%5C%5C%5C%22%2C%5C%5C%5C%22addrId%5C%5C%5C%22%3A%5C%5C%5C%229056332332%5C%5C%5C%22%2C%",
        "type": "code",
        "location": "/tests/taobao_guangguang_download_å“‡å“¦è§†é¢‘_æ·˜å®é€›é€›_tiktok_douyin/decodeTaobaoQuery.py:1-3"
    },
    "5497": {
        "file_id": 716,
        "content": "This code is decoding a Taobao query string containing information about various stores and their types, likely used for filtering or search purposes.",
        "type": "comment"
    },
    "5498": {
        "file_id": 716,
        "content": "5C%5C%5C%22type%5C%5C%5C%22%3A%5C%5C%5C%22CHOOSE_ADDR%5C%5C%5C%22%7D%5D%7D%5C%22%7D%22%2C%22URL_REFERER_ORIGIN%22%3A%22%2F%2Fs.m.taobao.com%2Fh5entry%3Fg_channelSrp%3Dvideointeract%26g_tab%3Dtbexperience%26g_pfilter%3Ddaren%26g_closeModues%3Dtab%26closeExpSubTab%3Dtrue%26g_csearchdoor_spm%3Da310p.14955560%26spm%3Da310p.13800399%26launchMode%3Dandroid_new_task%26g_closeExpSubTab%3Dtrue%22%2C%22ad_type%22%3A%221.0%22%2C%22apptimestamp%22%3A%221665607023%22%2C%22areaCode%22%3A%22CN%22%2C%22brand%22%3A%22Xiaomi%22%2C%22canP4pVideoPlay%22%3A%22true%22%2C%22channelSrp%22%3A%22videointeract%22%2C%22cityCode%22%3A%22320100%22%2C%22closeExpSubTab%22%3A%22true%22%2C%22closeModues%22%3A%22tab%22%2C%22countryNum%22%3A%22156%22%2C%22csearchdoor_spm%22%3A%22a310p.14955560%22%2C%22device%22%3A%22Mi+MIX+2%22%2C%22editionCode%22%3A%22CN%22%2C%22from%22%3A%22input%22%2C%22globalLbs%22%3A%22%7B%5C%22biz_common%5C%22%3A%7B%5C%22recommendedAddress%5C%22%3A%7B%5C%22addressId%5C%22%3A%5C%229056332332%5C%22%2",
        "type": "code",
        "location": "/tests/taobao_guangguang_download_å“‡å“¦è§†é¢‘_æ·˜å®é€›é€›_tiktok_douyin/decodeTaobaoQuery.py:3-3"
    },
    "5499": {
        "file_id": 716,
        "content": "This code contains a complex JSON object with various parameters like type, URL_REFERER_ORIGIN, ad_type, areaCode, brand, canP4pVideoPlay, channelSrp, cityCode, closeExpSubTab, closeModues, countryNum, csearchdoor\\_spm, device, editionCode, from, and globalLbs. These parameters are used to describe the source of the request, user's location, device information, and other relevant data for an API call.",
        "type": "comment"
    }
}