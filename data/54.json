{
    "5400": {
        "file_id": 701,
        "content": "            BB = getDogBB(frame)\n        else:\n            x, y, w, h = BB\n            if len(frame.shape) == 3:\n                dogFrame = frame[y:y+h,x:x+w,:]\n            else:\n                dogFrame = frame[y:y+h,x:x+w]\n            result = checkDog(dogFrame)\n            if result: # dog gone missing.\n                BB = getDogBB(frame)\n                init=False\n    if BB is not None:\n        if not init:\n            tracker.init(frame, BB) # how to init this shit?\n            init=True\n    # when lost, we know there is no dog inside the bounding box.\n    # frame = imutils.resize(frame,width=720)\n        if index % update_track == 0:\n            track_success,BB = tracker.update(frame)\n        if track_success and BB:\n            top_left = (int(BB[0]),int(BB[1]))\n            bottom_right = (int(BB[0]+BB[2]), int(BB[1]+BB[3]))\n            cv2.rectangle(frame,top_left,bottom_right,(0,255,0),5)\n    cv2.imshow('Output',frame)\n    key  =  cv2.waitKey(1) & 0xff\n    if key == ord('q'):\n        break\nvideo.release()",
        "type": "code",
        "location": "/tests/video_detector_tests/singleTracker.py:87-114"
    },
    "5401": {
        "file_id": 701,
        "content": "This code initializes a tracker and tracks a dog's movement in video frames. It checks if the dog is present in the bounding box and updates the position accordingly. If the dog goes missing, it reinitializes the tracker with new dog's bounding box. The tracked dog's position is displayed on the frame, which is then shown as output.",
        "type": "comment"
    },
    "5402": {
        "file_id": 701,
        "content": "cv2.destroyAllWindows()",
        "type": "code",
        "location": "/tests/video_detector_tests/singleTracker.py:115-115"
    },
    "5403": {
        "file_id": 701,
        "content": "This code is used to close all the video windows created by OpenCV (cv2).",
        "type": "comment"
    },
    "5404": {
        "file_id": 702,
        "content": "/tests/video_detector_tests/yolo.py",
        "type": "filepath"
    },
    "5405": {
        "file_id": 702,
        "content": "The code reads video frames, uses YOLOv5 model for object detection and text extraction, sets model directories and environment variables, performs inference, and stores results in a DataFrame. It detects objects (likely a dog) and displays image with details before exiting upon key press.",
        "type": "summary"
    },
    "5406": {
        "file_id": 702,
        "content": "image_path = \"../../samples/video/dog_with_text.mp4\"\nimport cv2\nvideo = cv2.VideoCapture(image_path)\nfor _ in range(100):\n    ret, frame = video.read() # first frame is blackout!\nimport torch\n# don't really know how paddleocr recognize chars.\nlocalModelDir = '/root/Desktop/works/pyjom/pyjom/models/yolov5/ultralytics_yolov5_master/'\nimport os\nos.environ[\"YOLOV5_MODEL_DIR\"] = '/root/Desktop/works/pyjom/pyjom/models/yolov5/'\nmodel = torch.hub.load(localModelDir, 'yolov5s',source=\"local\") # the yolov5s.pt file is required when loading the model.\n# Image\n# img = 'https://ultralytics.com/images/zidane.jpg'\nimg = frame[:,:,::-1].transpose((2,0,1))\n# Inference\n# reshape this shit.\n# img = np.reshape()\nresults = model(img) # pass the image through our model\ndf = results.pandas().xyxy[0]\nprint(df)\ndata = []\nfor index,line in df.iterrows():\n    # print(line)\n    left = (line[\"xmin\"],line[\"ymin\"])\n    right = (line[\"xmax\"],line[\"ymax\"])\n    confidence = line[\"confidence\"]\n    class_ = line[\"class\"]\n    name = line[\"name\"]\n  ",
        "type": "code",
        "location": "/tests/video_detector_tests/yolo.py:1-37"
    },
    "5407": {
        "file_id": 702,
        "content": "The code reads a video frame by frame, applies object detection using YOLOv5 model, and extracts text from the detected objects. It sets the local model directory and environment variable for YOLOv5 model loading, loads the model, resizes and preprocesses the frame, performs inference, and stores the resultant bounding boxes with associated data (class, confidence, name) into a DataFrame.",
        "type": "comment"
    },
    "5408": {
        "file_id": 702,
        "content": "  data.append({\"location\":[left,right],\"confidence\":confidence,\"identity\":{\"class\":class_,\"name\":name}})\nprint(data)\ncv2.imshow(\"name\",frame)\ncv2.waitKey(0)\n# found the freaking dog!",
        "type": "code",
        "location": "/tests/video_detector_tests/yolo.py:37-41"
    },
    "5409": {
        "file_id": 702,
        "content": "This code detects an object (likely a dog) using YOLO and appends its location, confidence level, and identity details to the \"data\" list. The image is displayed with the name \"name\" and waits for a key press to exit. The comment celebrates finding the desired object.",
        "type": "comment"
    },
    "5410": {
        "file_id": 703,
        "content": "/tests/video_detector_tests/yolo_norfair.py",
        "type": "filepath"
    },
    "5411": {
        "file_id": 703,
        "content": "This code initializes Detectron2 for instance segmentation on COCO dataset, sets up a YOLO object detector for video frames, and tracks their positions across frames. It processes detection information, updates tracked objects using a tracker, draws circles with names based on estimated positions. The code displays a video frame, waits for 'q' key press to break loop, destroys windows, and writes the frame to file if display is not enabled.",
        "type": "summary"
    },
    "5412": {
        "file_id": 703,
        "content": "import cv2\nimport numpy as np\nfrom detectron2.config import get_cfg\nfrom detectron2.engine import DefaultPredictor\nfrom cocoNames import cocoName\nfrom norfair import Detection, Tracker, Video, draw_tracked_objects\ndef euclidean_distance(detection, tracked_object):\n    return np.linalg.norm(detection.points - tracked_object.estimate)\n# Set up Detectron2 object detector\ncfg = get_cfg()\ncfg.merge_from_file(\"norfair/demos/faster_rcnn_R_50_FPN_3x.yaml\")\ncfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5 # looks like it does not recognize dog.\n# cfg.MODEL.WEIGHTS = \"detectron2://COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl\"\ncfg.MODEL.WEIGHTS = \"/root/Desktop/works/pyjom/tests/video_detector_tests/detectron2_models/model_final_f10217.pkl\"\n# it is stored in s3\n# https://dl.fbaipublicfiles.com/detectron2/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl\n# download cache: /root/.torch/iopath_cache/detectron2/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl",
        "type": "code",
        "location": "/tests/video_detector_tests/yolo_norfair.py:1-22"
    },
    "5413": {
        "file_id": 703,
        "content": "The code imports necessary libraries and sets up the Detectron2 object detector using a pre-trained model for instance segmentation on COCO dataset. The model weight file is located at \"/root/Desktop/works/pyjom/tests/video_detector_tests/detectron2\\_models/model\\_final\\_f10217.pkl\".",
        "type": "comment"
    },
    "5414": {
        "file_id": 703,
        "content": "detector = DefaultPredictor(cfg)\n# what are the classes output by the model?\n# Norfair\n# video_path = \"/root/Desktop/works/pyjom/samples/video/dog_with_text.mp4\"\nvideo_path = \"/root/Desktop/works/pyjom/samples/video/LlfeL29BP.mp4\"\nvideo = Video(input_path=video_path)\ntracker = Tracker(distance_function=euclidean_distance, distance_threshold=200,hit_inertia_min=2,hit_inertia_max=20,initialization_delay=1) # what the heck?\n# you should implement standalone tracker function, optical.\n# maybe you can track the object via framedifference?\ntracked_objects = None\ndisplay=True\nfor index, frame in enumerate(video): # we need to speed up.\n    if index%10 == 0:\n        detections = detector(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n        print(\"original detections:\",detections)\n        instances = detections[\"instances\"]\n        print(\"instances:\",instances)\n        # breakpoint()\n        pred_classes = instances.pred_classes\n        if len(pred_classes) == 0:\n            continue\n        detections2=[]\n        for index,class_ in enumerate(pred_classes):",
        "type": "code",
        "location": "/tests/video_detector_tests/yolo_norfair.py:24-48"
    },
    "5415": {
        "file_id": 703,
        "content": "This code is initializing a YOLO object detector, reading a video file, setting up a tracker, and then iterating through each frame of the video. The detector predicts objects in each frame, and if any are detected, it saves the detections and continues to the next frame. If no objects are detected, it skips that frame. The tracker helps keep track of object positions across frames, but the implementation details are unclear.",
        "type": "comment"
    },
    "5416": {
        "file_id": 703,
        "content": "            print(\"index:\",index)\n            class_ = int(class_.cpu().numpy().tolist())\n            print(\"class:\",class_)\n            box = instances.pred_boxes.tensor[index].cpu().numpy().tolist()\n            box = [int(x) for x in box]\n            score = float(instances.scores[index].cpu().numpy().tolist())\n            print('box:',box)\n            print('score:',score)\n            det = Detection(instances.pred_boxes.get_centers()[index].cpu().numpy(),scores=np.array([score]),data={\"box\":box,\"class\":{\"id\":class_,\"name\":cocoRealName[class_]}})\n            detections2.append(det)\n            # breakpoint()\n        # detections = [Detection(p) for p in instances.pred_boxes.get_centers().cpu().numpy()] # what is this instance anyway?\n        # you would lost data you dick!\n        print(\"detections2\",detections2)\n        tracked_objects = tracker.update(detections=detections2)\n        # print(detections)\n        print(\"tracked objects:\",tracked_objects) # you don't track shit?\n    if tracked_objects is not None:",
        "type": "code",
        "location": "/tests/video_detector_tests/yolo_norfair.py:49-66"
    },
    "5417": {
        "file_id": 703,
        "content": "The code is processing and printing detection information, appending detections to a list (detections2), and updating tracked objects using a tracker. The comment criticizes a previous line of code for potentially losing data. The final if statement checks if any tracked_objects were found.",
        "type": "comment"
    },
    "5418": {
        "file_id": 703,
        "content": "        if tracked_objects!=[]:\n            # there is no bounding box avaliable?\n            for obj in tracked_objects:\n                point = obj.estimate[0]\n                position = tuple(point.astype(int))\n                color = (255,0,0)\n                # breakpoint()\n                name = obj.last_detection.data[\"class\"][\"name\"]\n                cv2.circle(\n                        frame,\n                        position,\n                        radius=100,\n                        color=color,\n                        thickness=2,\n                    )\n                cv2.putText(\n                    frame,\n                    \"[{}][{}]\".format(str(obj.id),name),\n                    (position[0]-100,position[1]),\n                    cv2.FONT_HERSHEY_SIMPLEX,\n                    2,\n                    (0,255,0),\n                    3,\n                    cv2.LINE_AA,\n                )\n            # breakpoint()\n        # i want to draw you in a different way.\n        # draw_tracked_objects(frame, tracked_objects,color=(255,0,0))",
        "type": "code",
        "location": "/tests/video_detector_tests/yolo_norfair.py:67-94"
    },
    "5419": {
        "file_id": 703,
        "content": "The code checks if there are any tracked objects. If there are, it estimates the object's position and draws a circle at that point on the frame. The object's name is also displayed near the circle. Finally, the code calls a function to draw the objects differently using a specific color.",
        "type": "comment"
    },
    "5420": {
        "file_id": 703,
        "content": "    if display:\n        cv2.imshow(\"window\",frame)\n        key  =  cv2.waitKey(1) & 0xff\n        if key == ord('q'):\n            break\n        # maybe we shall print this shit somehow.\n    # video.write(frame) # you write what?\nif display:\n    cv2.destroyAllWindows()",
        "type": "code",
        "location": "/tests/video_detector_tests/yolo_norfair.py:95-103"
    },
    "5421": {
        "file_id": 703,
        "content": "This code snippet displays a video frame on the screen, waits for a key press (specifically 'q'), and breaks the loop if 'q' is pressed. It then destroys all windows created. The frame is written to the video file if display is not enabled.",
        "type": "comment"
    },
    "5422": {
        "file_id": 704,
        "content": "/tests/video_phash_deduplication/README.md",
        "type": "filepath"
    },
    "5423": {
        "file_id": 704,
        "content": "Code detects identical video files and finds repeated intervals within each video file.",
        "type": "summary"
    },
    "5424": {
        "file_id": 704,
        "content": "two main problems, one is to detect identical video files, one is to find 'repeated interval' inside each other.",
        "type": "code",
        "location": "/tests/video_phash_deduplication/README.md:1-1"
    },
    "5425": {
        "file_id": 704,
        "content": "Code detects identical video files and finds repeated intervals within each video file.",
        "type": "comment"
    },
    "5426": {
        "file_id": 705,
        "content": "/tests/video_phash_deduplication/config_milvus.sh",
        "type": "filepath"
    },
    "5427": {
        "file_id": 705,
        "content": "This code sets up Milvus, a vector database, by creating its configuration directory and downloading the necessary server and log configurations before starting the Milvus services.",
        "type": "summary"
    },
    "5428": {
        "file_id": 705,
        "content": "# # Create Milvus file\n# $ mkdir -p /home/$USER/milvus/conf\n# $ cd /home/$USER/milvus/conf\n# $ wget https://raw.githubusercontent.com/milvus-io/milvus/v0.8.0/core/conf/demo/server_config.yaml\n# $ wget https://raw.githubusercontent.com/milvus-io/milvus/v0.8.0/core/conf/demo/log_config.conf\nsudo systemctl start milvus\nsudo systemctl start milvus-etcd\nsudo systemctl start milvus-minio",
        "type": "code",
        "location": "/tests/video_phash_deduplication/config_milvus.sh:1-8"
    },
    "5429": {
        "file_id": 705,
        "content": "This code sets up Milvus, a vector database, by creating its configuration directory and downloading the necessary server and log configurations before starting the Milvus services.",
        "type": "comment"
    },
    "5430": {
        "file_id": 706,
        "content": "/tests/video_phash_deduplication/test_image_hash.py",
        "type": "filepath"
    },
    "5431": {
        "file_id": 706,
        "content": "This code uses the Python Imaging Library (PIL) and imagehash module to calculate average color hash values for images. It opens three images, 'pic_0', 'pic_0_similar', and 'pic_1', calculates their color hash values using the phash function from imagehash, and then calculates the differences between the hashes of each pair of images. The code checks the types of the calculated hashes and prints the distances (dis0 and dis1) between the pairs of images. It also mentions a possible discrepancy in the expected number of digits for the distances.",
        "type": "summary"
    },
    "5432": {
        "file_id": 706,
        "content": "pic_0= 'cat.png'\npic_0_similar = \"cat3.png\"\npic_1 = \"/root/Desktop/works/pyjom/samples/image/dick.png\"\nfrom PIL import Image\n# >>> import imagehash\n# >>> hash = imagehash.average_hash(Image.open\nimport imagehash\npics = [pic_0, pic_0_similar, pic_1]\nhashs = [imagehash.phash(Image.open(pic)) for pic in pics]\ndis0 = hashs[0]-hashs[1]\ndis1 = hashs[1]-hashs[2]\n# 0 24\n# 6 24\n# well, let's check?\nprint([type(h) for h in hashs])\nbreakpoint()\nprint(dis0, dis1)\n# three truth tables.\n# 2^4 = 16, total 2*8 digits=16?",
        "type": "code",
        "location": "/tests/video_phash_deduplication/test_image_hash.py:1-23"
    },
    "5433": {
        "file_id": 706,
        "content": "This code uses the Python Imaging Library (PIL) and imagehash module to calculate average color hash values for images. It opens three images, 'pic_0', 'pic_0_similar', and 'pic_1', calculates their color hash values using the phash function from imagehash, and then calculates the differences between the hashes of each pair of images. The code checks the types of the calculated hashes and prints the distances (dis0 and dis1) between the pairs of images. It also mentions a possible discrepancy in the expected number of digits for the distances.",
        "type": "comment"
    },
    "5434": {
        "file_id": 707,
        "content": "/tests/video_phash_deduplication/test_image_hash_milvus_database_search.py",
        "type": "filepath"
    },
    "5435": {
        "file_id": 707,
        "content": "This code is initializing image paths and loading necessary libraries. It calculates image hashes for three different images using the phash function from imagehash library. It then creates a connection to a Milvus database and loads an existing collection.",
        "type": "summary"
    },
    "5436": {
        "file_id": 707,
        "content": "pic_0 = \"cat.png\"\npic_0_similar = \"cat3.png\"\npic_1 = \"/root/Desktop/works/pyjom/samples/image/dick.png\"\nfrom PIL import Image\n# >>> import imagehash\n# >>> hash = imagehash.average_hash(Image.open\nimport imagehash\npics = [pic_0, pic_0_similar, pic_1]\nhashs = [imagehash.phash(Image.open(pic)) for pic in pics]\n# dis0 = hashs[0]-hashs[1]\n# dis1 = hashs[1]-hashs[2]\n# print(dis0, dis1)\n# 0 24\n# 6 24\n# well, let's check?\n# print(hashs)\n# three truth tables.\n# 2^4 = 16, total 2*8 digits=16?\n# towhee(multimodal search like jina), haystack, milvus\n# import pymilvus\nfrom pymilvus import connections\nconnection = connections.connect(alias=\"default\", host=\"localhost\", port=\"19530\")\nfrom pymilvus import Collection\ncollection = Collection(\"book\")  # Get an existing collection.\ncollection.load()\n# seems hard to setup.\n# not started!",
        "type": "code",
        "location": "/tests/video_phash_deduplication/test_image_hash_milvus_database_search.py:1-37"
    },
    "5437": {
        "file_id": 707,
        "content": "This code is initializing image paths and loading necessary libraries. It calculates image hashes for three different images using the phash function from imagehash library. It then creates a connection to a Milvus database and loads an existing collection.",
        "type": "comment"
    },
    "5438": {
        "file_id": 708,
        "content": "/tests/video_phash_deduplication/test_milvus.py",
        "type": "filepath"
    },
    "5439": {
        "file_id": 708,
        "content": "The code demonstrates Milvus database operations, including creating a \"video\" collection, inserting data and performing searches. It is part of debugging process to retrieve documents by ID. The programmer is stuck and requires further investigation.",
        "type": "summary"
    },
    "5440": {
        "file_id": 708,
        "content": "# duplicate -> remove, do not insert\n# not duplicate -> get the data, insert\n# you want to clear the collection after this run?\n# import pymilvus\nfrom pymilvus import connections\nfrom functools import lru_cache\n@lru_cache(maxsize=1)\ndef connectMilvusDatabase(alias=\"default\", host=\"localhost\", port=\"19530\"):\n\tconnection = connections.connect(alias=alias, host=host, port=port)# can we reconnect?\n\tprint('milvus connected')\nconnectMilvusDatabase()\nconnectMilvusDatabase() # will not connect again.\ncollection_name = \"video_deduplication\"\nfrom pymilvus import Collection\n# Collection(collection_name)\n# remote this thing.\nfrom pymilvus import utility\ntry:\n    if utility.has_collection(collection_name):  # be prudent.\n        utility.drop_collection(collection_name)\nexcept:\n    import traceback\n    traceback.print_exc()\n    print(\"maybe the collection does not exist\")\nfrom pymilvus import CollectionSchema, FieldSchema, DataType\nvideo_semantic_id = FieldSchema(  # how to insert this shit without prior knowledge?\n    name=\"video_semantic_id\",",
        "type": "code",
        "location": "/tests/video_phash_deduplication/test_milvus.py:1-39"
    },
    "5441": {
        "file_id": 708,
        "content": "This code establishes a connection to a Milvus database, checks if the \"video_deduplication\" collection exists, and if so, removes it before creating a new one. The `connectMilvusDatabase` function sets up a connection with specified alias, host, and port (default values used in this code). The `utility.has_collection` and `utility.drop_collection` functions from the `pymilvus` utility module are used to check for and remove an existing collection named \"video_deduplication\". A `CollectionSchema` is defined for the new collection, specifying a field schema named \"video_semantic_id\".",
        "type": "comment"
    },
    "5442": {
        "file_id": 708,
        "content": "    dtype=DataType.INT64,\n    is_primary=True,  # if is primary, will do check for 'not duplicate' or something.\n    auto_id=True,  # no need for id generation.\n)\nvideo_length = FieldSchema(\n    name=\"video_length\",\n    dtype=DataType.FLOAT,\n)\nvideo_phash = FieldSchema(\n    name=\"video_phash\", dtype=DataType.BINARY_VECTOR, dim=64\n)  # 64\n# single dimension? no multi dimension support?\nschema = CollectionSchema(\n    fields=[video_semantic_id, video_length, video_phash],\n    description=\"Test video deduplication\",\n)\n# collection = Collection(\"video\")      # Get an existing collection.\ncollection = Collection(\n    name=collection_name,\n    schema=schema,\n    using=\"default\",\n    shards_num=2,\n)\n# is this demo collection?\n# seems hard to setup.\n# not started!\n# https://milvus.io/docs/v2.0.0/metric.md#binary\n# the metric is important to us.\nsearch_params = {\"metric_type\": \"Jaccard\", \"params\": {\"nprobe\": 10}}\nimport numpy as np\nqueryData = np.array(\n    [\n        [True, True, True, False, False, True, False, True],\n        [True, False, False, True, False, True, True, False],",
        "type": "code",
        "location": "/tests/video_phash_deduplication/test_milvus.py:40-76"
    },
    "5443": {
        "file_id": 708,
        "content": "This code is defining a schema for a Milvus collection, specifying the data types and field names. The schema includes video_semantic_id, video_length, and video_phash fields, which are used in video deduplication. The code creates a collection named \"video\" with 2 shards using the specified schema and sets the metric type for searching as Jaccard with nprobe parameter set to 10. It also imports numpy and creates queryData, which seems to be a binary vector.",
        "type": "comment"
    },
    "5444": {
        "file_id": 708,
        "content": "        [True, False, False, True, True, False, False, True],\n        [True, True, True, True, True, False, False, True],\n        [True, False, False, True, False, True, True, False],\n        [False, True, True, False, False, False, False, True],\n        [True, True, False, False, False, True, True, False],\n        [False, False, True, False, False, True, False, False],\n    ]\n)\nqueryData = queryData.reshape(-1).tolist()\nqueryData = [\"1\" if x else \"0\" for x in queryData]\nimport bitarray\nqueryData = bitarray.bitarray(\"\".join(queryData), endian=\"little\")\nqueryData2 = queryData.copy()\nqueryData2[1:4] = 0\nqueryData3 = queryData2.copy()\nqueryData2 = queryData2.tobytes()\nqueryData3[8:15] = 0\nqueryData3 = queryData3.tobytes()\nqueryData = queryData.tobytes()\n# dimension: 8*8=64\n# collection.insert([[1], [np.float32(3.5)], [queryData]])\n# collection.insert([[np.float32(3.5)], [queryData]])\n# for _ in range(8):\ncollection.insert([[np.float32(3.5)], [queryData]])\ncollection.insert([[np.float32(3.5)], [queryData2]])  # slight difference.",
        "type": "code",
        "location": "/tests/video_phash_deduplication/test_milvus.py:77-102"
    },
    "5445": {
        "file_id": 708,
        "content": "The code is preparing and inserting data into a Milvus collection. It creates binary representations of queryData, queryData2, and queryData3 (slightly different from queryData2), then inserts them along with a float value (np.float32(3.5)) into the collection, representing a 64-dimensional vector.",
        "type": "comment"
    },
    "5446": {
        "file_id": 708,
        "content": "collection.insert([[np.float32(3.5)], [queryData3]])  # more difference.\n# print(len(queryData), len(queryData)*8)\n# # print(queryData.shape)\n# breakpoint()\n# collection.load()\ncollection.insert([[np.float32(3.5)], [queryData]]) # still three.\n# can release even if not loaded.\ncollection.release() # unload.\ncollection.load()\n# make it into some library!\n# insert after load?\n# # 1,64\n# what is wrong? wtf?\n# queryData = queryData.tolist()\nresults = collection.search(\n    data=[queryData],  # this is the float dimension.\n    anns_field=\"video_phash\",\n    param=search_params,\n    output_fields=[\"video_length\"],\n    limit=10,\n    expr=\"video_length > 1.2 and video_length < 4\",\n    # expr='video_length < 1.2',\n)\ntheHit = results[0]\nprint(theHit)\n# so we can perform search without filtering afterwards.\n# results[0][0].entity.get('video_length')\n# print(results[0].ids)\n# now, we want to have the 'distance' parameter.\n# print(results[0])\n# print(theHit)\n# distances = theHit.distances\n# results = [x for x in theHit]\n# hits = len(theHit)",
        "type": "code",
        "location": "/tests/video_phash_deduplication/test_milvus.py:103-139"
    },
    "5447": {
        "file_id": 708,
        "content": "The code is inserting data into a collection, releasing and reloading it, performing a search based on specific parameters, and accessing the results. The purpose seems to be searching for video data within a database based on certain criteria, such as length, and extracting relevant information from the resulting hits.",
        "type": "comment"
    },
    "5448": {
        "file_id": 708,
        "content": "# breakpoint()\n# how to get document by id? wtf",
        "type": "code",
        "location": "/tests/video_phash_deduplication/test_milvus.py:140-141"
    },
    "5449": {
        "file_id": 708,
        "content": "This code appears to be part of a debugging process, where the programmer is trying to understand how to retrieve a document by its ID using the Milvus database. The \"breakpoint()\" comment suggests they are currently stuck or needing to pause execution for further investigation.",
        "type": "comment"
    },
    "5450": {
        "file_id": 709,
        "content": "/tests/video_phash_deduplication/test_milvus_library.py",
        "type": "filepath"
    },
    "5451": {
        "file_id": 709,
        "content": "This code defines a Milvus function for connecting, managing collections, and caching. It creates Collections with specified data types, searches duplicated videos, retrieves video duration/hash, indexes videos, and reloads collection if necessary.",
        "type": "summary"
    },
    "5452": {
        "file_id": 709,
        "content": "# # duplicate -> remove, do not insert\n# # not duplicate -> get the data, insert\n# # you want to clear the collection after this run?\n# from functools import lru_cache\n# from pymilvus import connections\n# @lru_cache(maxsize=1)\n# def connectMilvusDatabase(alias=\"default\", host=\"localhost\", port=\"19530\"):\n#     connection = connections.connect(\n#         alias=alias, host=host, port=port\n#     )  # can we reconnect?\n#     print(\"milvus connected\")\n# # connectMilvusDatabase()\n# # connectMilvusDatabase() # will not connect again.\n# from pymilvus import Collection\n# from pymilvus import utility\n# from pymilvus import CollectionSchema, FieldSchema, DataType\n# import traceback\n# def getMilvusVideoDeduplicationCollection(\n#     get_existing: bool = False,\n# ):  # most of the time we just use the same\n#     collection_name = \"video_deduplication\"\n#     try:\n#         if utility.has_collection(collection_name):  # be prudent.\n#             if get_existing:\n#                 return Collection(collection_name)\n#             utility.drop_collection(collection_name)",
        "type": "code",
        "location": "/tests/video_phash_deduplication/test_milvus_library.py:1-35"
    },
    "5453": {
        "file_id": 709,
        "content": "The code defines a function to connect to a Milvus database, get or remove an existing collection named \"video_deduplication\", and returns the collection if it already exists. The function uses caching and checks if the collection already exists before performing any actions.",
        "type": "comment"
    },
    "5454": {
        "file_id": 709,
        "content": "#     except:\n#         traceback.print_exc()\n#         print(\"maybe the collection does not exist\")\n#     video_semantic_id = FieldSchema(  # how to insert this shit without prior knowledge?\n#         name=\"video_semantic_id\",\n#         dtype=DataType.INT64,\n#         is_primary=True,  # if is primary, will do check for 'not duplicate' or something.\n#         auto_id=True,  # no need for id generation.\n#     )\n#     video_length = FieldSchema(\n#         name=\"video_length\",\n#         dtype=DataType.FLOAT,\n#     )\n#     video_phash = FieldSchema(\n#         name=\"video_phash\", dtype=DataType.BINARY_VECTOR, dim=64\n#     )  # 64\n#     # single dimension? no multi dimension support?\n#     schema = CollectionSchema(\n#         fields=[video_semantic_id, video_length, video_phash],\n#         description=\"Test video deduplication\",\n#     )\n#     collection = Collection(\n#         name=collection_name,\n#         schema=schema,\n#         using=\"default\",\n#         shards_num=2,\n#     )\n#     # is this demo collection?\n#     return collection",
        "type": "code",
        "location": "/tests/video_phash_deduplication/test_milvus_library.py:36-65"
    },
    "5455": {
        "file_id": 709,
        "content": "This code defines a CollectionSchema and Collection for Milvus library. The schema contains fields for video_semantic_id, video_length, and video_phash, with their respective data types and properties. The Collection is created with a name, schema, database usage, and number of shards.",
        "type": "comment"
    },
    "5456": {
        "file_id": 709,
        "content": "# # seems hard to setup.\n# # not started!\n# # https://milvus.io/docs/v2.0.0/metric.md#binary\n# # the metric is important to us.\n# import numpy as np\n# import bitarray\n# @lru_cache(maxsize=1)\n# def transformVideoPhash(videoPhash):\n#     # we need the raw phash.\n#     queryData = videoPhash.hash  # videoPhashTruthTable8x8 or something\n#     queryData = queryData.reshape(-1).tolist()\n#     queryData = [\"1\" if x else \"0\" for x in queryData]\n#     queryData = bitarray.bitarray(\"\".join(queryData), endian=\"little\")\n#     queryData = queryData.tobytes()\n#     return queryData\n# # dimension: 8*8=64\n# def indexVideoWithVideoDurationAndPhash(collection, videoDuration, videoPhash):\n#     queryData = transformVideoPhash(videoPhash)\n#     collection.insert([[np.float32(videoDuration)], [queryData]])\n# # can release even if not loaded.\n# from test_video_hash import getVideoPHash\n# import caer\n# @lru_cache(maxsize=1)\n# def getVideoDurationAndPhashFromFile(videoFilePath):\n#     videoDuration = caer.video.frames_and_fps.get_duration(videoFilePath)",
        "type": "code",
        "location": "/tests/video_phash_deduplication/test_milvus_library.py:68-103"
    },
    "5457": {
        "file_id": 709,
        "content": "Function `transformVideoPhash` takes a video phash and converts it into a binary format for Milvus library indexing. Function `indexVideoWithVideoDurationAndPhash` inserts the video duration and transformed phash into the specified collection. The `getVideoDurationAndPhashFromFile` function retrieves the video duration and corresponding phash of a given video file using caer's video module. All functions are cached to avoid redundant computations.",
        "type": "comment"
    },
    "5458": {
        "file_id": 709,
        "content": "#     videoPhash = getVideoPHash(videoFilePath)\n#     return videoDuration, videoPhash\n# def indexVideoWithVideoDurationAndPhashFromFile(collection, videoFilePath):\n#     videoDuration, videoPhash = getVideoDurationAndPhashFromFile(videoFilePath)\n#     indexVideoWithVideoDurationAndPhash(collection, videoDuration, videoPhash)\n# def reloadMilvusCollection(collection):\n#     collection.release()  # unload.\n#     collection.load()\n# # make it into some library!\n# # insert after load?\n# # # 1,64\n# # what is wrong? wtf?\n# # queryData = queryData.tolist()\n# def getDistancesBySearchingDuplicatedVideoInMilvusByFile(\n#     collection,\n#     videoFilePath,\n#     search_params={\"metric_type\": \"Jaccard\", \"params\": {\"nprobe\": 10}},\n#     autoreload: bool = True,\n#     span: float = 2,\n#     debug: bool = False,\n#     limit: int = 10,\n# ):\n#     if autoreload:\n#         reloadMilvusCollection(collection)\n#     videoDuration, videoPhash = getVideoDurationAndPhashFromFile(videoFilePath)\n#     queryData = transformVideoPhash(videoPhash)",
        "type": "code",
        "location": "/tests/video_phash_deduplication/test_milvus_library.py:104-136"
    },
    "5459": {
        "file_id": 709,
        "content": "This code snippet defines a function for searching duplicated videos in Milvus by their file path, using Jaccard metric. It also includes functions to get video duration and hash from the file, index videos with duration and hash, and reload Milvus collection if necessary. The search parameters include metric type, probe count, span, limit, and whether to enable debug mode.",
        "type": "comment"
    },
    "5460": {
        "file_id": 709,
        "content": "#     minVideoLength = max(0, videoDuration - span)\n#     maxVideoLength = videoDuration + span\n#     results = collection.search(\n#         data=[queryData],  # this is the float dimension.\n#         anns_field=\"video_phash\",\n#         param=search_params,\n#         output_fields=[\"video_length\"],\n#         limit=limit,\n#         expr=\"video_length > {minVideoLength} and video_length < {maxVideoLength}\".format(\n#             minVideoLength=minVideoLength, maxVideoLength=maxVideoLength\n#         ),\n#     )\n#     theHit = results[0]\n#     # print(theHit)\n#     # so we can perform search without filtering afterwards.\n#     # results[0][0].entity.get('video_length')\n#     # print(results[0].ids)\n#     # now, we want to have the 'distance' parameter.\n#     # print(results[0])\n#     # print(theHit)\n#     distances = list(theHit.distances)\n#     if debug:\n#         print(\"distances: %s\" % distances)\n#     return distances\n#     # what is the distance? we need to try.\n#     # returh the closest distance?\n#     # results = [x for x in theHit]",
        "type": "code",
        "location": "/tests/video_phash_deduplication/test_milvus_library.py:137-164"
    },
    "5461": {
        "file_id": 709,
        "content": "This code searches for videos within a specified range of video length in the Milvus library. It sets minimum and maximum lengths based on the query's duration and span, and uses these values to filter results from the search. The closest distance between the query and each result is then returned.",
        "type": "comment"
    },
    "5462": {
        "file_id": 709,
        "content": "#     # hits = len(theHit)\n#     # breakpoint()\n#     # how to get document by id? wtf\n# def checkDuplicatedVideoAndInsertVector(\n#     collection,\n#     videoPath,\n#     threshold: float = 0.15,  # are you sure?\n#     insertDuplicatedVector: bool = True,\n#     debug: bool = True,\n# ):\n#     reloadMilvusCollection(collection)\n#     distances = getDistancesBySearchingDuplicatedVideoInMilvusByFile(\n#         collection, videoPath, debug=debug\n#     )\n#     minDistance = min(distances + [1])  # empty!\n#     duplicated = minDistance < threshold\n#     if insertDuplicatedVector or (not duplicated):\n#         indexVideoWithVideoDurationAndPhashFromFile(\n#             collection, videoPath\n#         )  # anyway let's do this.\n#     return duplicated\n# shall we insert that vector or not, even if we have detected the duplicated media?\n# you choose.\nimport sys\nimport os\n# os.chdir(\"../../\")\nsys.path.append(\"../../\")\n# ignore the global proxy now, we are not going to use that.\nos.environ[\"http_proxy\"] = \"\"\nos.environ[\"https_proxy\"] = \"\"",
        "type": "code",
        "location": "/tests/video_phash_deduplication/test_milvus_library.py:165-200"
    },
    "5463": {
        "file_id": 709,
        "content": "Function checkDuplicatedVideoAndInsertVector checks if a video file exists in Milvus collection and returns whether the video is duplicated or not. If insertDuplicatedVector is True, it indexes the video regardless of duplication status. The function uses getDistancesBySearchingDuplicatedVideoInMilvusByFile to find distances between the new video and existing videos in Milvus.",
        "type": "comment"
    },
    "5464": {
        "file_id": 709,
        "content": "from pyjom.videotoolbox import getMilvusVideoDeduplicationCollection,checkDuplicatedVideoAndInsertVector\nif __name__ == \"__main__\":\n    # connectMilvusDatabase()\n    collection = (\n        getMilvusVideoDeduplicationCollection()\n    )  # will not get existing collections\n    videoPaths = [\n        \"cute_cat_gif.mp4\",\n        \"cute_cat_gif.gif\",\n        \"cat_delogo.gif\",\n        \"/root/Desktop/works/pyjom/samples/video/dog_with_large_text.gif\",\n    ]\n    # for videoPath in videoPaths:\n    from lazero.utils.logger import sprint\n    for videoPath in videoPaths:\n        print(\"filepath: %s\" % videoPath)\n        duplicated = checkDuplicatedVideoAndInsertVector(collection, videoPath)\n        sprint(\"duplicated?\", duplicated)\n\"\"\"\nfilepath: cute_cat_gif.mp4\ndistances: [0.0, 0.11764705926179886, 0.11764705926179886, 0.7200000286102295, 0.7200000286102295, 0.7346938848495483, 0.7659574747085571, 0.7924528121948242]\n______________________________\nfilepath: cute_cat_gif.gif\ndistances: [0.0, 0.11764705926179886, 0.117647",
        "type": "code",
        "location": "/tests/video_phash_deduplication/test_milvus_library.py:201-226"
    },
    "5465": {
        "file_id": 709,
        "content": "The code connects to a Milvus database, retrieves the video deduplication collection, and checks if each given video path is already in the collection. It prints the file paths of the videos and whether they are duplicated or not using `checkDuplicatedVideoAndInsertVector` function from `lazero.utils.logger` module. The distances between the new video and existing ones in the database are also printed.",
        "type": "comment"
    },
    "5466": {
        "file_id": 709,
        "content": "05926179886, 0.7200000286102295, 0.7200000286102295, 0.7346938848495483, 0.7659574747085571, 0.7692307829856873]\n______________________________\nfilepath: cat_delogo.gif\ndistances: [0.0, 0.11764705926179886, 0.11764705926179886, 0.7200000286102295, 0.7200000286102295, 0.7346938848495483, 0.7659574747085571, 0.7924528121948242]\n______________________________\nfilepath: /root/Desktop/works/pyjom/samples/video/dog_with_large_text.gif\ndistances: [0.0, 0.6808510422706604, 0.6938775777816772, 0.6938775777816772, 0.739130437374115, 0.7692307829856873, 0.7924528121948242, 0.7924528121948242]\n______________________________\n\"\"\"",
        "type": "code",
        "location": "/tests/video_phash_deduplication/test_milvus_library.py:226-234"
    },
    "5467": {
        "file_id": 709,
        "content": "The code appears to be storing and comparing distances between video phashes for different files. Each line contains a file path followed by an array of distances, indicating the similarity of that video phash to other video phashes in the system. The lower the distance value, the more similar the videos are.",
        "type": "comment"
    },
    "5468": {
        "file_id": 710,
        "content": "/tests/video_phash_deduplication/test_video_hash.py",
        "type": "filepath"
    },
    "5469": {
        "file_id": 710,
        "content": "The code defines `getVideoPHash` to calculate a video's phash using the `videohashes` tool, testing it by comparing pairwise differences between hash values for different videos and considering duplicates based on a threshold.",
        "type": "summary"
    },
    "5470": {
        "file_id": 710,
        "content": "# use some delogo stuff.\nfrom lazero.program.subprocess import runCommandGetJson\n# these two are similar. can be used as threshold.\n# aaaa3d8a2eaa1f8a delogo\n# aaaa398a2faa5d8a not delogoed.\n# aaaa3c8a2faa5e8a mp4 (very similar to delogoed version)\ndef getVideoPHash(filepath,debug=False, timeout=100):\n    import os\n    import imagehash\n    assert os.path.exists(filepath)\n    assert os.path.isfile(filepath)\n    if not os.path.isabs(filepath):\n        filepath = os.path.abspath(filepath)\n    commandLine = [\n        \"videohashes\", # installed in path.\n        # \"/root/Desktop/works/pyjom/tests/video_phash_deduplication/videohashes/videohashes-linux\",\n        \"-json\",\n        filepath,\n    ]\n    success, myJson = runCommandGetJson(commandLine, debug=debug, timeout=timeout)\n    if debug:\n        print(\"SUCCESS?\", success)\n        print(myJson, type(myJson))\n    if not success:\n        return\n    # breakpoint()\n    phashString = myJson[\"phash\"]\n    phash = imagehash.hex_to_hash(phashString)\n    if debug:\n        print(\"FILEPATH: %s\" % filepath)",
        "type": "code",
        "location": "/tests/video_phash_deduplication/test_video_hash.py:1-32"
    },
    "5471": {
        "file_id": 710,
        "content": "The code defines a function `getVideoPHash` that calculates a video's phash (a unique identifier for an image or video) using the `videohashes` command-line tool. It takes a filepath as input, checks if it exists and is a file, then runs the command to generate the JSON output. The function also converts the returned phash string to a binary hash and optionally prints debug information.",
        "type": "comment"
    },
    "5472": {
        "file_id": 710,
        "content": "        print(myJson)\n        print(\"PHASH:\", phash)\n    # if withDuration:\n    #     duration = myJson[\"duration\"]\n    #     return duration, phash\n    # duration is inaccurate\n    return phash\nif __name__ == \"__main__\":\n    videoPaths = [\n        \"cute_cat_gif.mp4\",\n        \"cute_cat_gif.gif\",\n        \"cat_delogo.gif\",\n        \"/root/Desktop/works/pyjom/samples/video/dog_with_large_text.gif\",\n    ]\n    hashs = [getVideoPHash(filepath,debug=True) for filepath in videoPaths]\n    dis0 = hashs[0] - hashs[1]  # small\n    dis1 = hashs[1] - hashs[2]  # big\n    dis2 = hashs[0] - hashs[2]  # big\n    dis3 = hashs[0] - hashs[3]  # big\n    print(dis0, dis1, dis2, dis3)\n    # 4 4 4\n    # strange. why?\n    # 4 4 4 42\n    # huge difference.\n    # what value do you decide to be duplicate?\n    # phash < 7 (really?)\n    # so how do we run this test?",
        "type": "code",
        "location": "/tests/video_phash_deduplication/test_video_hash.py:33-65"
    },
    "5473": {
        "file_id": 710,
        "content": "This code tests the video hashing function by calculating pairwise differences between hash values for different video files. It then compares the differences to determine potential duplicates and prints the results. The hash difference threshold for considering duplicates is set to 7, but this seems low and may need adjustment based on further testing.",
        "type": "comment"
    },
    "5474": {
        "file_id": 711,
        "content": "/tests/video_script_generation_reconstruction/README.md",
        "type": "filepath"
    },
    "5475": {
        "file_id": 711,
        "content": "This code refers to a project containing various AI models for feature extraction, video summarization, audio classification, image labeling, text extraction, and keyword suggestion. The user needs to write a script before using these tools from PaddleVideo.",
        "type": "summary"
    },
    "5476": {
        "file_id": 711,
        "content": "contains multiple feature extractor, video summarizer, audio classifier, image labeler, text extractor, keyword suggestor. but you need to write one freaking script first. that is your freaking style. you can write that shit in markdown anyway, in reference of existing excellent(?) videos.\nfrom https://github.com/PaddlePaddle/PaddleVideo.\nvideo understanding.",
        "type": "code",
        "location": "/tests/video_script_generation_reconstruction/README.md:1-5"
    },
    "5477": {
        "file_id": 711,
        "content": "This code refers to a project containing various AI models for feature extraction, video summarization, audio classification, image labeling, text extraction, and keyword suggestion. The user needs to write a script before using these tools from PaddleVideo.",
        "type": "comment"
    },
    "5478": {
        "file_id": 712,
        "content": "/tests/video_script_generation_reconstruction/lstm_trial.py",
        "type": "filepath"
    },
    "5479": {
        "file_id": 712,
        "content": "This code initializes a LSTM layer, passes a 3D input tensor through it, and prints the shapes of the input, output, and hidden states.",
        "type": "summary"
    },
    "5480": {
        "file_id": 712,
        "content": "from torch.nn import LSTM\nimport numpy as np\ndata = [[[1,2,3],[2,3,4],[3,5,6]]]\nfrom torch import Tensor\ndata = Tensor(data)\nlayer_lstm = LSTM(3,1)\noutput_1, (hid_1_a,hid_1_b) = layer_lstm(data)\n# print(len(hidden_1))\nprint(data.shape)\nprint(output_1.shape) # [1,3,10]\nprint(hid_1_a.shape,hid_1_b.shape)",
        "type": "code",
        "location": "/tests/video_script_generation_reconstruction/lstm_trial.py:1-17"
    },
    "5481": {
        "file_id": 712,
        "content": "This code initializes a LSTM layer, passes a 3D input tensor through it, and prints the shapes of the input, output, and hidden states.",
        "type": "comment"
    },
    "5482": {
        "file_id": 713,
        "content": "/tests/video_script_generation_reconstruction/raw_data_cut.py",
        "type": "filepath"
    },
    "5483": {
        "file_id": 713,
        "content": "This code initializes shapes, generates cut targets, and ensures correct tensor dimensions. It applies convolution, pooling, activation functions, performs two RNN operations, prints output/hidden state shapes, defines a final linear layer, transposes data, and suggests MaxPool1d for character extraction.",
        "type": "summary"
    },
    "5484": {
        "file_id": 713,
        "content": "# 2d understanding or 3d?\n# what about the freaking audio?\nimport numpy as np\nimport torch\nvideo_shape = (30,100,100) # thirty frames extracted.\naudio_shape = (1,40000) # so batch size is included.\nvideo2_shape = (60,200,200) # thirty frames extracted.\naudio2_shape = (1,80000)\ntarget_cut_shape = (30,2) # choose either beginning or to cut?\ntarget_cut2_shape = (60,2) # choose either beginning or to cut?\nimport random\ntarget_cut = np.array([random.randint(0,1) for _ in range(30)])\ntarget_cut2 = np.array([random.randint(0,1) for _ in range(60)])\nvideo_data = np.array(np.random.random(video_shape))\naudio_data = np.array(np.random.random(audio_shape))\nvideo2_data = np.array(np.random.random(video2_shape))\naudio2_data = np.array(np.random.random(audio2_shape))\n# print(data)\n# I really not caring the freaking data range.\nprint(video_data.shape)\nprint(audio_data.shape)\nprint(target_cut2.shape)\ndevice = torch.device(\"cuda\")\nvideo_data = torch.Tensor([video_data]) # to make sure the first dimension is batchsize\ntarget_cut = torch.Tensor([target_cut])",
        "type": "code",
        "location": "/tests/video_script_generation_reconstruction/raw_data_cut.py:1-33"
    },
    "5485": {
        "file_id": 713,
        "content": "Code initializes various shapes for video and audio data, randomly generates cut targets, and ensures tensor dimensions are correct for GPU usage.",
        "type": "comment"
    },
    "5486": {
        "file_id": 713,
        "content": "audio_data = torch.Tensor(audio_data)\nlayer_1 = torch.nn.Conv2d(30,3,10) # original shape: (30,100,100)\noutput_1 = layer_1(video_data)\nprint(output_1.shape) #(1,3,91,91)\nlayer_2 = torch.nn.Conv2d(3,1,10)\noutput_2 = layer_2(output_1)\nprint(output_2.shape) #([1, 2, 82, 82])\nlayer_3 = torch.nn.MaxPool1d(4)\noutput_3 = layer_3(audio_data)\nprint(output_3.shape) # torch.Size([1, 10000]) # what is this fuck?\nlayer_4 = torch.nn.MaxPool2d(2)\noutput_4 = layer_4(output_2)\nprint(output_4.shape) # 1,2,41,41 freaking bad.\nlayer_5 = torch.nn.Sigmoid()\noutput_5 = layer_5(output_4)\nprint(output_5.shape) # 1,2,41,41\noutput_5 = output_5.reshape(1,41,41)\n# get this reshaped.\noutput_5 = output_5.reshape(1,1,41*41)\nrnn_layer_1 = torch.nn.RNN(41*41,41*41,3) # must have three dimensions.\nrnn_output_1, rnn_hidd_1 = rnn_layer_1(output_5)\nprint(rnn_output_1.shape,rnn_hidd_1.shape) #tuple torch.Size([1, 41, 20]) torch.Size([3, 41, 20])\nrnn_output_2, rnn_hidd_2 = rnn_layer_1(output_5,rnn_hidd_1)\nprint(\"RNN 2:\",rnn_output_2.shape,rnn_hidd_2.shape)",
        "type": "code",
        "location": "/tests/video_script_generation_reconstruction/raw_data_cut.py:34-67"
    },
    "5487": {
        "file_id": 713,
        "content": "This code applies convolution, pooling, and activation functions to the input data. It reshapes the output for RNN processing with a specific structure, and performs two RNN operations.",
        "type": "comment"
    },
    "5488": {
        "file_id": 713,
        "content": "rnn_output_3, rnn_hidd_3 = rnn_layer_1(rnn_output_1,rnn_hidd_1)\nprint(\"RNN 3:\",rnn_output_3.shape,rnn_hidd_3.shape)\n# final_data = \nfinal_layer = torch.nn.Linear(41*41,2) # the final swap.\nfinal_data = final_layer(rnn_output_1)\nprint(final_data.shape)\n# find the max one.\nfinal_data = final_data.transpose(2,1)\nprint(final_data.shape)\n# output_final_layer = torch.nn.MaxPool1d(41) \n# final_data2 = output_final_layer(final_data)\n# print(final_data2.shape) # 40000,1 this is a single character. is it?",
        "type": "code",
        "location": "/tests/video_script_generation_reconstruction/raw_data_cut.py:69-81"
    },
    "5489": {
        "file_id": 713,
        "content": "This code applies an RNN layer, prints the shapes of output and hidden states, defines a final linear layer with 41x41 input size and 2 output sizes, passes RNN output through it, transposes the data, and suggests using MaxPool1d for possible character extraction.",
        "type": "comment"
    },
    "5490": {
        "file_id": 714,
        "content": "/tests/video_script_generation_reconstruction/raw_data_understanding.py",
        "type": "filepath"
    },
    "5491": {
        "file_id": 714,
        "content": "The code defines the shapes of video and audio data, generates random one-hot encoded target sentences, and creates random data for both videos. It then prints the shapes of the generated video and audio data.",
        "type": "summary"
    },
    "5492": {
        "file_id": 714,
        "content": "# 2d understanding or 3d?\n# what about the freaking audio?\nimport numpy as np\nimport torch\nvideo_shape = (30,100,100) # thirty frames extracted.\naudio_shape = (1,40000)\nvideo2_shape = (60,200,200) # thirty frames extracted.\naudio2_shape = (1,80000)\ntarget_sentence_shape = (10,40000) # full charset. you may choose not to speak. when should you freaking speak?\ntarget_sentence2_shape = (15,40000) # full charset. you may choose not to speak. when should you freaking speak?\n# one hot encoding.\nimport random\ntarget_sentence = np.array([random.randint(0,39999) for _ in range(10)])\ntarget_sentence2 = np.array([random.randint(0,39999) for _ in range(15)])\n# i really don't care how. freaking do it!\nvideo_data = np.random.random(video_shape)\naudio_data = np.random.random(audio_shape)\nvideo2_data = np.random.random(video2_shape)\naudio2_data = np.random.random(audio2_shape)\n# print(data)\n# I really not caring the freaking data range.\nprint(video_data.shape)\nprint(audio_data.shape)",
        "type": "code",
        "location": "/tests/video_script_generation_reconstruction/raw_data_understanding.py:1-31"
    },
    "5493": {
        "file_id": 714,
        "content": "The code defines the shapes of video and audio data, generates random one-hot encoded target sentences, and creates random data for both videos. It then prints the shapes of the generated video and audio data.",
        "type": "comment"
    },
    "5494": {
        "file_id": 715,
        "content": "/tests/video_script_generation_reconstruction/spp_any_video.py",
        "type": "filepath"
    },
    "5495": {
        "file_id": 715,
        "content": "The code initializes video and audio data arrays, defines a VideoCutNet model with CNN layers, performs spatial pyramid pooling on video frames, uses LSTM for audio processing, iterates over training loop for gradient descent, and lacks batch size specification.",
        "type": "summary"
    },
    "5496": {
        "file_id": 715,
        "content": "# 2d understanding or 3d?\n# what about the freaking audio?\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nimport torch.nn as nn\nvideo_shape = (20,3,100,100) # thirty frames extracted.\naudio_shape = (1,40000)\nvideo2_shape = (70,3,200,200) # thirty frames extracted. # change it!\naudio2_shape = (2,120000) # no freaking padding game.\ntarget_sentence_shape = (20,2) # full charset. you may choose not to speak. when should you freaking speak?\ntarget_sentence2_shape = (70,2) # full charset. you may choose not to speak. when should you freaking speak?\n# one hot encoding.\nimport random\ntarget_sentence = np.array([random.randint(0,1) for _ in range(20)]) # do one-hot encoding please.\ntarget_sentence2 = np.array([random.randint(0,1) for _ in range(70)])\ntarget_sentence = np.eye(2)[target_sentence]\ntarget_sentence2 = np.eye(2)[target_sentence2]\npad_video_shape_2 = np.zeros((20,3,200,200))\npad_sentence_2 = np.zeros((20,2))\ntarget_sentence2 = np.concatenate([target_sentence2,pad_sentence_2])\n# print(target_sentence2.shape,pad_sentence_2.shape)",
        "type": "code",
        "location": "/tests/video_script_generation_reconstruction/spp_any_video.py:1-30"
    },
    "5497": {
        "file_id": 715,
        "content": "This code defines video and audio shapes for various inputs, creates random target sentences, and pads the data with zeros to maintain consistent shape. It uses numpy array manipulation and one-hot encoding to represent categorical data.",
        "type": "comment"
    },
    "5498": {
        "file_id": 715,
        "content": "# breakpoint()\n# i really don't care how. freaking do it!\nvideo_data = np.random.random(video_shape)\naudio_data = np.random.random(audio_shape)\nvideo2_data = np.random.random(video2_shape)\naudio2_data = np.random.random(audio2_shape)\n# print(data)\nvideo2_data = np.concatenate([video2_data,pad_video_shape_2])\n# I really not caring the freaking data range.\nprint(video_data.shape)\nprint(audio_data.shape)\nfrom spp_module import spatial_pyramid_pool\nclass VideoCutNet(torch.nn.Module):\n    def __init__(self,debug=True):\n        super().__init__()\n        self.debug = debug\n        self.hidden_states=[None]\n        self.audio_hidden_states = [None]\n        self.va_hidden_states = [None,None]\n        self.c2layer_1 = nn.Conv2d(3,4,4)\n        self.c2layer_2 = nn.Conv2d(4,16,20)\n        self.output_num = [20]\n        # print(x.shape,spp.shape) # 1,5120\n        self.cnn_1 = nn.Conv1d(2,20,16,stride=2,padding=8) # you could use this on the audio.\n        self.cnn_2 = nn.Conv1d(20,16,16,stride=2,padding=8)\n        self.cnn_3 = nn.Conv1d(16,30,16,stride=4,padding=8)",
        "type": "code",
        "location": "/tests/video_script_generation_reconstruction/spp_any_video.py:31-63"
    },
    "5499": {
        "file_id": 715,
        "content": "This code snippet initializes random video and audio data arrays with specified shapes, concatenates the second video data array with padding, defines a class for the VideoCutNet model, and sets up various layers such as convolutional layers (CNNs) for processing audio and videos. The debug parameter allows controlling whether or not to print the shapes of the initialized arrays.",
        "type": "comment"
    }
}