{
    "5400": {
        "file_id": 707,
        "content": "# files.sort()\n# for file in files:\n#      if file.endswith(\".tif\")\n# fn = os.path.join(dir, 'original.tif')\nfn = \"IWWS.jpeg\"\nimp = IJ.openImage(fn)\noutput_fn = \"imagej_output_jython.jpg\"\nimp = IJ.openImage(fn)\nFlat.getFastInstance().run(\n    imp, blocksize, histogram_bins, maximum_slope, mask, composite\n)\nIJ.save(imp, output_fn)\nFlat.getFastInstance().run(\n    imp, blocksize, histogram_bins, maximum_slope, mask, composite\n)\n# ImageConverter(imp).convertToGray8()\nIJ.save(imp, \"imagej_double_jython.jpg\")",
        "type": "code",
        "location": "/tests/remove_subtle_watermark_local_contrast_ocr/jython_imagej_test_clahe.py:39-58"
    },
    "5401": {
        "file_id": 707,
        "content": "This code opens an image file, applies contrast enhancement using Flat.getFastInstance(), saves the result as \"imagej_output_jython.jpg\", applies contrast enhancement again (probably unnecessarily), converts the image to grayscale, and saves it as \"imagej_double_jython.jpg\".",
        "type": "comment"
    },
    "5402": {
        "file_id": 708,
        "content": "/tests/remove_subtle_watermark_local_contrast_ocr/init_mclahe_numpy_only.sh",
        "type": "filepath"
    },
    "5403": {
        "file_id": 708,
        "content": "This command installs the latest version of mclahe library from a zip file, specifically optimized for numpy.",
        "type": "summary"
    },
    "5404": {
        "file_id": 708,
        "content": "pip3 install --upgrade https://github.com/VincentStimper/mclahe/archive/numpy.zip",
        "type": "code",
        "location": "/tests/remove_subtle_watermark_local_contrast_ocr/init_mclahe_numpy_only.sh:1-1"
    },
    "5405": {
        "file_id": 708,
        "content": "This command installs the latest version of mclahe library from a zip file, specifically optimized for numpy.",
        "type": "comment"
    },
    "5406": {
        "file_id": 709,
        "content": "/tests/remove_subtle_watermark_local_contrast_ocr/imagej2_pyimagej_test_clahe.py",
        "type": "filepath"
    },
    "5407": {
        "file_id": 709,
        "content": "The code integrates Python and Java using jpype, sets up JVM classpath, applies CLAHE in ImageJ2/PyImageJ for image contrast enhancement, and explores available methods and properties.",
        "type": "summary"
    },
    "5408": {
        "file_id": 709,
        "content": "# source:\n# https://github.com/seung-lab/Alembic/blob/575c8ed2a5f8789e65de652c9349993c530de718/src/archive/import/convert_dir_to_CLAHE.py\n# https://github.com/search?q=mpicbg.ij.clahe&type=code\n# for jpython you need to append all jar absolute paths to sys.path. grammar shall be identical.\nimport jpype\nimport jpype.imports\nfrom jpype.types import *\n# jpype.addClassPath(\"/root/Desktop/works/pyjom/tests/remove_subtle_watermark_local_contrast_ocr/imagej_fiji_linux/Fiji.app/jars/*\")\n# jpype.addClassPath(\"/root/Desktop/works/pyjom/tests/remove_subtle_watermark_local_contrast_ocr/imagej_fiji_linux/Fiji.app/jars/*/*\")\n# jpype.addClassPath(\"/root/Desktop/works/pyjom/tests/remove_subtle_watermark_local_contrast_ocr/imagej_fiji_linux/Fiji.app/plugins/*\")\n# jpype.addClassPath(\"/root/Desktop/works/pyjom/tests/remove_subtle_watermark_local_contrast_ocr/imagej_fiji_linux/Fiji.app/plugins/*/*\")\njpype.startJVM(\n    classpath=[\n        \"/root/Desktop/works/pyjom/tests/remove_subtle_watermark_local_contrast_ocr/imagej_fiji_linux/Fiji.app/jars/*\",",
        "type": "code",
        "location": "/tests/remove_subtle_watermark_local_contrast_ocr/imagej2_pyimagej_test_clahe.py:1-18"
    },
    "5409": {
        "file_id": 709,
        "content": "This code is setting up the JVM classpath for jpype, a tool to integrate Python and Java, by appending various jar absolute paths. These paths may include jars within Fiji's directories. This allows the program to use specific Java classes or libraries that are located in these jar files.",
        "type": "comment"
    },
    "5410": {
        "file_id": 709,
        "content": "        \"/root/Desktop/works/pyjom/tests/remove_subtle_watermark_local_contrast_ocr/imagej_fiji_linux/Fiji.app/plugins/*\",\n    ]\n)\nfrom ij import IJ\nimport os\nfrom mpicbg.ij.clahe import Flat\nfrom ij.process import ImageConverter\n# http://fiji.sc/wiki/index.php/Enhance_Local_Contrast_(CLAHE)\n# http://fiji.sc/cgi-bin/gitweb.cgi?p=mpicbg.git;a=blob;f=mpicbg/ij/clahe/PlugIn.java;h=663153764493547de560c08ee11f2e6b1e7e1a32;hb=HEAD\n# dir = \"/usr/people/tmacrina/seungmount/research/Julimaps/datasets/AIBS_pilot_v1/0_raw/\"\nblocksize = 40\nhistogram_bins = 255\nmaximum_slope = 5\nmask = \"*None*\"\ncomposite = False\nmask = None\n# files = os.listdir(dir)\n# files.sort()\n# for file in files:\n#      if file.endswith(\".tif\")\n# fn = os.path.join(dir, 'original.tif')\nfn = \"IWWS.jpeg\"\nimp = IJ.openImage(fn)\noutput_fn = \"imagej_output.jpg\"\nimp = IJ.openImage(fn)\nFlat.getFastInstance().run(\n    imp, blocksize, histogram_bins, maximum_slope, mask, composite\n)\nIJ.save(imp, output_fn)\nFlat.getFastInstance().run(\n    imp, blocksize, histogram_bins, maximum_slope, mask, composite",
        "type": "code",
        "location": "/tests/remove_subtle_watermark_local_contrast_ocr/imagej2_pyimagej_test_clahe.py:19-58"
    },
    "5411": {
        "file_id": 709,
        "content": "Applies CLAHE (Contrast Limited Adaptive Histogram Equalization) on an input image to enhance local contrast. It takes the input image, adjusts blocksize, histogram bins, maximum slope, mask, and composite parameters to improve image quality. Saves the output image with modified contrast.",
        "type": "comment"
    },
    "5412": {
        "file_id": 709,
        "content": ")\n# ImageConverter(imp).convertToGray8()\nIJ.save(imp, \"imagej_double.jpg\")\n# # Create an ImageJ2 gateway with the newest available version of ImageJ2.\n# # fiji_path = \"/root/Desktop/works/pyjom/tests/remove_subtle_watermark_local_contrast_ocr/imagej_fiji_linux/Fiji.app\"\n# # ij = imagej.init(fiji_path)\n# import scyjava\n# # plugins_dir = '/Applications/Fiji.app/plugins'\n# # plugins_dir = \"/root/Desktop/works/pyjom/tests/remove_subtle_watermark_local_contrast_ocr/imagej_fiji_linux/Fiji.app/plugins\"\n# # scyjava.config.add_option(f'-Dplugins.dir={plugins_dir}')\n# # scyjava.config.add_repositories({'scijava.public': 'https://maven.scijava.org/content/groups/public'})\n# import imagej\n# ij = imagej.init()\n# # Load an image.\n# image_url = \"IWWS.jpeg\"\n# jimage = ij.io().open(image_url)\n# # Convert the image from ImageJ2 to xarray, a package that adds\n# # labeled datasets to numpy (http://xarray.pydata.org/en/stable/).\n# image = ij.py.from_java(jimage)\n# # Display the image (backed by matplotlib).\n# # ij.py.show(image, cmap='gray')",
        "type": "code",
        "location": "/tests/remove_subtle_watermark_local_contrast_ocr/imagej2_pyimagej_test_clahe.py:59-85"
    },
    "5413": {
        "file_id": 709,
        "content": "This code initializes ImageJ2, opens an image, converts it to xarray for labeled datasets in numpy, and displays the image using matplotlib.",
        "type": "comment"
    },
    "5414": {
        "file_id": 709,
        "content": "# # print('IMAGE',image)\n# # d = dir(ij)\n# # print(d)\n# # ['IJ', 'ResultsTable', 'RoiManager', 'WindowManager', '__class__', '__del__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_access_legacy_class', '_check_legacy_active', 'animation', 'app', 'appEvent', 'command', 'compareTo', 'console', 'context', 'convert', 'dataset', 'display', 'dispose', 'equals', 'event', 'eventHistory', 'get', 'getApp', 'getClass', 'getContext', 'getIdentifier', 'getInfo', 'getLocation', 'getPriority', 'getShortName', 'getTitle', 'getVersion', 'hashCode', 'icon', 'imageDisplay', 'input', 'io', 'launch', 'legacy', 'log', 'lut', 'main', 'menu', 'module', 'notebook', 'notify', 'notifyAll', 'object', 'op', 'options', 'overlay', 'pla",
        "type": "code",
        "location": "/tests/remove_subtle_watermark_local_contrast_ocr/imagej2_pyimagej_test_clahe.py:86-89"
    },
    "5415": {
        "file_id": 709,
        "content": "The code is exploring the available methods and properties of the ImageJ2/PyImageJ object (ij) by printing a list of all accessible attributes. However, this specific snippet seems to have been commented out, indicating that the developer may have considered it but eventually decided against including it in the final code.",
        "type": "comment"
    },
    "5416": {
        "file_id": 709,
        "content": "tform', 'plugin', 'prefs', 'py', 'recentFile', 'rendering', 'sampler', 'scifio', 'screenCapture', 'script', 'setContext', 'setInfo', 'setPriority', 'startup', 'status', 'text', 'thread', 'toString', 'tool', 'ui', 'update', 'uploader', 'wait', 'widget', 'window']\n# # p = ij.plugin\n# # print(dir(p))\n# clahe = scyjava.jimport('mpicbg')",
        "type": "code",
        "location": "/tests/remove_subtle_watermark_local_contrast_ocr/imagej2_pyimagej_test_clahe.py:89-92"
    },
    "5417": {
        "file_id": 709,
        "content": "Code imports 'clahe' from 'mpicbg' for ImageJ2 usage.",
        "type": "comment"
    },
    "5418": {
        "file_id": 710,
        "content": "/tests/remove_subtle_watermark_local_contrast_ocr/glche_test.py",
        "type": "filepath"
    },
    "5419": {
        "file_id": 710,
        "content": "This code performs image processing, including contrast normalization, hue preservation, and fusion for color images using nonlinear transformation and CLAHE. It can process multiple images in a specified directory via an optional loop.",
        "type": "summary"
    },
    "5420": {
        "file_id": 710,
        "content": "from PIL import Image\nfrom scipy.optimize import minimize_scalar\nimport numpy as np\nimport cv2\nimport os\ndef linearStretching(x_c, x_max, x_min, l):\n    return (l - 1) * (x_c - x_min) / (x_max - x_min)\ndef mapping(h, l):\n    cum_sum = 0\n    t = np.zeros_like(h, dtype=np.int)\n    for i in range(l):\n        cum_sum += h[i]\n        t[i] = np.ceil((l - 1) * cum_sum + 0.5)\n    return t\ndef f(lam, h_i, h_u, l):\n    h_tilde = 1 / (1 + lam) * h_i + lam / (1 + lam) * h_u\n    t = mapping(h_tilde, l)\n    d = 0\n    for i in range(l):\n        for j in range(i + 1):\n            if h_tilde[i] > 0 and h_tilde[j] > 0 and t[i] == t[j]:\n                d = max(d, i - j)\n    return d\ndef huePreservation(g_i, i, x_hat_c, l):\n    g_i_f = g_i.flatten()\n    i_f = i.flatten()\n    x_hat_c_f = x_hat_c.flatten()\n    g_c = np.zeros(g_i_f.shape)\n    g_c[g_i_f <= i_f] = (g_i_f / i_f * x_hat_c_f)[g_i_f <= i_f]\n    g_c[g_i_f > i_f] = ((l - 1 - g_i_f) / (l - 1 - i_f) * (x_hat_c_f - i_f) + g_i_f)[g_i_f > i_f]\n    return g_c.reshape(i.shape)\ndef fusion(i):",
        "type": "code",
        "location": "/tests/remove_subtle_watermark_local_contrast_ocr/glche_test.py:1-40"
    },
    "5421": {
        "file_id": 710,
        "content": "The code defines several functions for image processing, including linear stretching, hue preservation, and fusion. These functions are likely used to enhance image quality, contrast, or OCR capabilities.",
        "type": "comment"
    },
    "5422": {
        "file_id": 710,
        "content": "    lap = cv2.Laplacian(i.astype(np.uint8), cv2.CV_16S, ksize=3)\n    c_d = np.array(cv2.convertScaleAbs(lap))\n    #print(np.max(np.max(c_d)), np.min(np.min(c_d)))\n    c_d = c_d / np.max(np.max(c_d)) + 0.00001\n    i_scaled = (i - np.min(np.min(i))) / (np.max(np.max(i)) - np.min(np.min(i)))\n    b_d = np.apply_along_axis(lambda x: np.exp(- (x - 0.5) ** 2 / (2 * 0.2 ** 2)), 0, i_scaled.flatten()).reshape(i.shape)\n    w_d = np.minimum(c_d, b_d)\n    return w_d\ndef main(path, name): # no parameter? fuck.\n    x = np.array(Image.open(path)).astype(np.float64)\n    x_r, x_g, x_b = x[:, :, 0], x[:, :, 1], x[:, :, 2]\n    x_max = np.max(np.max(np.max(x)))\n    x_min = np.min(np.min(np.min(x)))\n    l = 256\n    x_hat_r = linearStretching(x_r, x_max, x_min, l)\n    x_hat_g = linearStretching(x_g, x_max, x_min, l)\n    x_hat_b = linearStretching(x_b, x_max, x_min, l)\n    i = (0.299 * x_hat_r + 0.587 * x_hat_g + 0.114 * x_hat_b).astype(np.uint8)\n    h_i = np.bincount(i.flatten())\n    h_i = np.concatenate((h_i, np.zeros(l - h_i.shape[0]))) / (i.shape[0] * i.shape[1])",
        "type": "code",
        "location": "/tests/remove_subtle_watermark_local_contrast_ocr/glche_test.py:41-64"
    },
    "5423": {
        "file_id": 710,
        "content": "The code reads an image and applies linear stretching to the red, green, and blue channels separately. It then combines these channels using YCbCr color space conversion and calculates the histogram of the combined image. The resulting histogram is normalized by dividing it by the total number of pixels. Finally, it returns a stretched and scaled image with watermark detection values.",
        "type": "comment"
    },
    "5424": {
        "file_id": 710,
        "content": "    h_u = np.ones_like(h_i) * 1 / l\n    result = minimize_scalar(f, method = \"brent\", args = (h_i, h_u, l))\n    h_tilde = 1 / (1 + result.x) * h_i + result.x / (1 + result.x) * h_u\n    t = mapping(h_tilde, l)\n    g_i = np.apply_along_axis(lambda x: t[x], 0, i.flatten()).reshape(i.shape)\n    g_r = huePreservation(g_i, i, x_hat_r, l)\n    g_g = huePreservation(g_i, i, x_hat_g, l)\n    g_b = huePreservation(g_i, i, x_hat_b, l)\n    #glo = np.dstack((g_r, g_g, g_b)).astype(np.int)\n    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n    l_i = clahe.apply(i)\n    l_r = huePreservation(l_i, i, x_hat_r, l)\n    l_g = huePreservation(l_i, i, x_hat_g, l)\n    l_b = huePreservation(l_i, i, x_hat_b, l)\n    #loc = np.dstack((l_r, l_g, l_b)).astype(np.int)\n    w_g = fusion(g_i)\n    w_l = fusion(l_i)\n    w_hat_g = w_g / (w_g + w_l)\n    w_hat_l = w_l / (w_g + w_l)\n    y_r = w_hat_g * g_r + w_hat_l * l_r\n    y_g = w_hat_g * g_g + w_hat_l * l_g\n    y_b = w_hat_g * g_b + w_hat_l * l_b\n    y = np.dstack((y_r, y_g, y_b)).astype(np.uint8)",
        "type": "code",
        "location": "/tests/remove_subtle_watermark_local_contrast_ocr/glche_test.py:65-91"
    },
    "5425": {
        "file_id": 710,
        "content": "This code performs contrast normalization, hue preservation, and fusion for color image processing. It uses nonlinear transformation to equalize intensity values, applies CLAHE for local contrast enhancement, and fuses the results using a weighted average based on relative brightness.",
        "type": "comment"
    },
    "5426": {
        "file_id": 710,
        "content": "    img = Image.fromarray(y)\n    img.save(name + '-en.jpg')\nif __name__ == \"__main__\":\n    picPath = \"IWWS.jpeg\"\n    imageName = \"IWWS-glche.jpeg\"\n    main(picPath, imageName)\n#     dirs = '..\\\\'\n#     count = 0\n#     for num in ('9', '14', '43', '45', '99'):\n#         path = dirs + num\n#         pics = os.listdir(path)\n#         path += '\\\\'\n#         for pic in pics:\n#             main(path + pic, pic[: -4])\n#             count += 1\n#             print(count, 'Done!')",
        "type": "code",
        "location": "/tests/remove_subtle_watermark_local_contrast_ocr/glche_test.py:93-109"
    },
    "5427": {
        "file_id": 710,
        "content": "This code reads an image file, applies a function to it, and saves the modified image with a new name. It also has an optional loop that processes multiple images in a specified directory.",
        "type": "comment"
    },
    "5428": {
        "file_id": 711,
        "content": "/tests/video_phash_deduplication/test_video_hash.py",
        "type": "filepath"
    },
    "5429": {
        "file_id": 711,
        "content": "The code defines `getVideoPHash` to calculate a video's phash using the `videohashes` tool, testing it by comparing pairwise differences between hash values for different videos and considering duplicates based on a threshold.",
        "type": "summary"
    },
    "5430": {
        "file_id": 711,
        "content": "# use some delogo stuff.\nfrom lazero.program.subprocess import runCommandGetJson\n# these two are similar. can be used as threshold.\n# aaaa3d8a2eaa1f8a delogo\n# aaaa398a2faa5d8a not delogoed.\n# aaaa3c8a2faa5e8a mp4 (very similar to delogoed version)\ndef getVideoPHash(filepath,debug=False, timeout=100):\n    import os\n    import imagehash\n    assert os.path.exists(filepath)\n    assert os.path.isfile(filepath)\n    if not os.path.isabs(filepath):\n        filepath = os.path.abspath(filepath)\n    commandLine = [\n        \"videohashes\", # installed in path.\n        # \"/root/Desktop/works/pyjom/tests/video_phash_deduplication/videohashes/videohashes-linux\",\n        \"-json\",\n        filepath,\n    ]\n    success, myJson = runCommandGetJson(commandLine, debug=debug, timeout=timeout)\n    if debug:\n        print(\"SUCCESS?\", success)\n        print(myJson, type(myJson))\n    if not success:\n        return\n    # breakpoint()\n    phashString = myJson[\"phash\"]\n    phash = imagehash.hex_to_hash(phashString)\n    if debug:\n        print(\"FILEPATH: %s\" % filepath)",
        "type": "code",
        "location": "/tests/video_phash_deduplication/test_video_hash.py:1-32"
    },
    "5431": {
        "file_id": 711,
        "content": "The code defines a function `getVideoPHash` that calculates a video's phash (a unique identifier for an image or video) using the `videohashes` command-line tool. It takes a filepath as input, checks if it exists and is a file, then runs the command to generate the JSON output. The function also converts the returned phash string to a binary hash and optionally prints debug information.",
        "type": "comment"
    },
    "5432": {
        "file_id": 711,
        "content": "        print(myJson)\n        print(\"PHASH:\", phash)\n    # if withDuration:\n    #     duration = myJson[\"duration\"]\n    #     return duration, phash\n    # duration is inaccurate\n    return phash\nif __name__ == \"__main__\":\n    videoPaths = [\n        \"cute_cat_gif.mp4\",\n        \"cute_cat_gif.gif\",\n        \"cat_delogo.gif\",\n        \"/root/Desktop/works/pyjom/samples/video/dog_with_large_text.gif\",\n    ]\n    hashs = [getVideoPHash(filepath,debug=True) for filepath in videoPaths]\n    dis0 = hashs[0] - hashs[1]  # small\n    dis1 = hashs[1] - hashs[2]  # big\n    dis2 = hashs[0] - hashs[2]  # big\n    dis3 = hashs[0] - hashs[3]  # big\n    print(dis0, dis1, dis2, dis3)\n    # 4 4 4\n    # strange. why?\n    # 4 4 4 42\n    # huge difference.\n    # what value do you decide to be duplicate?\n    # phash < 7 (really?)\n    # so how do we run this test?",
        "type": "code",
        "location": "/tests/video_phash_deduplication/test_video_hash.py:33-65"
    },
    "5433": {
        "file_id": 711,
        "content": "This code tests the video hashing function by calculating pairwise differences between hash values for different video files. It then compares the differences to determine potential duplicates and prints the results. The hash difference threshold for considering duplicates is set to 7, but this seems low and may need adjustment based on further testing.",
        "type": "comment"
    },
    "5434": {
        "file_id": 712,
        "content": "/tests/video_phash_deduplication/test_milvus_library.py",
        "type": "filepath"
    },
    "5435": {
        "file_id": 712,
        "content": "This code defines a Milvus function for connecting, managing collections, and caching. It creates Collections with specified data types, searches duplicated videos, retrieves video duration/hash, indexes videos, and reloads collection if necessary.",
        "type": "summary"
    },
    "5436": {
        "file_id": 712,
        "content": "# # duplicate -> remove, do not insert\n# # not duplicate -> get the data, insert\n# # you want to clear the collection after this run?\n# from functools import lru_cache\n# from pymilvus import connections\n# @lru_cache(maxsize=1)\n# def connectMilvusDatabase(alias=\"default\", host=\"localhost\", port=\"19530\"):\n#     connection = connections.connect(\n#         alias=alias, host=host, port=port\n#     )  # can we reconnect?\n#     print(\"milvus connected\")\n# # connectMilvusDatabase()\n# # connectMilvusDatabase() # will not connect again.\n# from pymilvus import Collection\n# from pymilvus import utility\n# from pymilvus import CollectionSchema, FieldSchema, DataType\n# import traceback\n# def getMilvusVideoDeduplicationCollection(\n#     get_existing: bool = False,\n# ):  # most of the time we just use the same\n#     collection_name = \"video_deduplication\"\n#     try:\n#         if utility.has_collection(collection_name):  # be prudent.\n#             if get_existing:\n#                 return Collection(collection_name)\n#             utility.drop_collection(collection_name)",
        "type": "code",
        "location": "/tests/video_phash_deduplication/test_milvus_library.py:1-35"
    },
    "5437": {
        "file_id": 712,
        "content": "The code defines a function to connect to a Milvus database, get or remove an existing collection named \"video_deduplication\", and returns the collection if it already exists. The function uses caching and checks if the collection already exists before performing any actions.",
        "type": "comment"
    },
    "5438": {
        "file_id": 712,
        "content": "#     except:\n#         traceback.print_exc()\n#         print(\"maybe the collection does not exist\")\n#     video_semantic_id = FieldSchema(  # how to insert this shit without prior knowledge?\n#         name=\"video_semantic_id\",\n#         dtype=DataType.INT64,\n#         is_primary=True,  # if is primary, will do check for 'not duplicate' or something.\n#         auto_id=True,  # no need for id generation.\n#     )\n#     video_length = FieldSchema(\n#         name=\"video_length\",\n#         dtype=DataType.FLOAT,\n#     )\n#     video_phash = FieldSchema(\n#         name=\"video_phash\", dtype=DataType.BINARY_VECTOR, dim=64\n#     )  # 64\n#     # single dimension? no multi dimension support?\n#     schema = CollectionSchema(\n#         fields=[video_semantic_id, video_length, video_phash],\n#         description=\"Test video deduplication\",\n#     )\n#     collection = Collection(\n#         name=collection_name,\n#         schema=schema,\n#         using=\"default\",\n#         shards_num=2,\n#     )\n#     # is this demo collection?\n#     return collection",
        "type": "code",
        "location": "/tests/video_phash_deduplication/test_milvus_library.py:36-65"
    },
    "5439": {
        "file_id": 712,
        "content": "This code defines a CollectionSchema and Collection for Milvus library. The schema contains fields for video_semantic_id, video_length, and video_phash, with their respective data types and properties. The Collection is created with a name, schema, database usage, and number of shards.",
        "type": "comment"
    },
    "5440": {
        "file_id": 712,
        "content": "# # seems hard to setup.\n# # not started!\n# # https://milvus.io/docs/v2.0.0/metric.md#binary\n# # the metric is important to us.\n# import numpy as np\n# import bitarray\n# @lru_cache(maxsize=1)\n# def transformVideoPhash(videoPhash):\n#     # we need the raw phash.\n#     queryData = videoPhash.hash  # videoPhashTruthTable8x8 or something\n#     queryData = queryData.reshape(-1).tolist()\n#     queryData = [\"1\" if x else \"0\" for x in queryData]\n#     queryData = bitarray.bitarray(\"\".join(queryData), endian=\"little\")\n#     queryData = queryData.tobytes()\n#     return queryData\n# # dimension: 8*8=64\n# def indexVideoWithVideoDurationAndPhash(collection, videoDuration, videoPhash):\n#     queryData = transformVideoPhash(videoPhash)\n#     collection.insert([[np.float32(videoDuration)], [queryData]])\n# # can release even if not loaded.\n# from test_video_hash import getVideoPHash\n# import caer\n# @lru_cache(maxsize=1)\n# def getVideoDurationAndPhashFromFile(videoFilePath):\n#     videoDuration = caer.video.frames_and_fps.get_duration(videoFilePath)",
        "type": "code",
        "location": "/tests/video_phash_deduplication/test_milvus_library.py:68-103"
    },
    "5441": {
        "file_id": 712,
        "content": "Function `transformVideoPhash` takes a video phash and converts it into a binary format for Milvus library indexing. Function `indexVideoWithVideoDurationAndPhash` inserts the video duration and transformed phash into the specified collection. The `getVideoDurationAndPhashFromFile` function retrieves the video duration and corresponding phash of a given video file using caer's video module. All functions are cached to avoid redundant computations.",
        "type": "comment"
    },
    "5442": {
        "file_id": 712,
        "content": "#     videoPhash = getVideoPHash(videoFilePath)\n#     return videoDuration, videoPhash\n# def indexVideoWithVideoDurationAndPhashFromFile(collection, videoFilePath):\n#     videoDuration, videoPhash = getVideoDurationAndPhashFromFile(videoFilePath)\n#     indexVideoWithVideoDurationAndPhash(collection, videoDuration, videoPhash)\n# def reloadMilvusCollection(collection):\n#     collection.release()  # unload.\n#     collection.load()\n# # make it into some library!\n# # insert after load?\n# # # 1,64\n# # what is wrong? wtf?\n# # queryData = queryData.tolist()\n# def getDistancesBySearchingDuplicatedVideoInMilvusByFile(\n#     collection,\n#     videoFilePath,\n#     search_params={\"metric_type\": \"Jaccard\", \"params\": {\"nprobe\": 10}},\n#     autoreload: bool = True,\n#     span: float = 2,\n#     debug: bool = False,\n#     limit: int = 10,\n# ):\n#     if autoreload:\n#         reloadMilvusCollection(collection)\n#     videoDuration, videoPhash = getVideoDurationAndPhashFromFile(videoFilePath)\n#     queryData = transformVideoPhash(videoPhash)",
        "type": "code",
        "location": "/tests/video_phash_deduplication/test_milvus_library.py:104-136"
    },
    "5443": {
        "file_id": 712,
        "content": "This code snippet defines a function for searching duplicated videos in Milvus by their file path, using Jaccard metric. It also includes functions to get video duration and hash from the file, index videos with duration and hash, and reload Milvus collection if necessary. The search parameters include metric type, probe count, span, limit, and whether to enable debug mode.",
        "type": "comment"
    },
    "5444": {
        "file_id": 712,
        "content": "#     minVideoLength = max(0, videoDuration - span)\n#     maxVideoLength = videoDuration + span\n#     results = collection.search(\n#         data=[queryData],  # this is the float dimension.\n#         anns_field=\"video_phash\",\n#         param=search_params,\n#         output_fields=[\"video_length\"],\n#         limit=limit,\n#         expr=\"video_length > {minVideoLength} and video_length < {maxVideoLength}\".format(\n#             minVideoLength=minVideoLength, maxVideoLength=maxVideoLength\n#         ),\n#     )\n#     theHit = results[0]\n#     # print(theHit)\n#     # so we can perform search without filtering afterwards.\n#     # results[0][0].entity.get('video_length')\n#     # print(results[0].ids)\n#     # now, we want to have the 'distance' parameter.\n#     # print(results[0])\n#     # print(theHit)\n#     distances = list(theHit.distances)\n#     if debug:\n#         print(\"distances: %s\" % distances)\n#     return distances\n#     # what is the distance? we need to try.\n#     # returh the closest distance?\n#     # results = [x for x in theHit]",
        "type": "code",
        "location": "/tests/video_phash_deduplication/test_milvus_library.py:137-164"
    },
    "5445": {
        "file_id": 712,
        "content": "This code searches for videos within a specified range of video length in the Milvus library. It sets minimum and maximum lengths based on the query's duration and span, and uses these values to filter results from the search. The closest distance between the query and each result is then returned.",
        "type": "comment"
    },
    "5446": {
        "file_id": 712,
        "content": "#     # hits = len(theHit)\n#     # breakpoint()\n#     # how to get document by id? wtf\n# def checkDuplicatedVideoAndInsertVector(\n#     collection,\n#     videoPath,\n#     threshold: float = 0.15,  # are you sure?\n#     insertDuplicatedVector: bool = True,\n#     debug: bool = True,\n# ):\n#     reloadMilvusCollection(collection)\n#     distances = getDistancesBySearchingDuplicatedVideoInMilvusByFile(\n#         collection, videoPath, debug=debug\n#     )\n#     minDistance = min(distances + [1])  # empty!\n#     duplicated = minDistance < threshold\n#     if insertDuplicatedVector or (not duplicated):\n#         indexVideoWithVideoDurationAndPhashFromFile(\n#             collection, videoPath\n#         )  # anyway let's do this.\n#     return duplicated\n# shall we insert that vector or not, even if we have detected the duplicated media?\n# you choose.\nimport sys\nimport os\n# os.chdir(\"../../\")\nsys.path.append(\"../../\")\n# ignore the global proxy now, we are not going to use that.\nos.environ[\"http_proxy\"] = \"\"\nos.environ[\"https_proxy\"] = \"\"",
        "type": "code",
        "location": "/tests/video_phash_deduplication/test_milvus_library.py:165-200"
    },
    "5447": {
        "file_id": 712,
        "content": "Function checkDuplicatedVideoAndInsertVector checks if a video file exists in Milvus collection and returns whether the video is duplicated or not. If insertDuplicatedVector is True, it indexes the video regardless of duplication status. The function uses getDistancesBySearchingDuplicatedVideoInMilvusByFile to find distances between the new video and existing videos in Milvus.",
        "type": "comment"
    },
    "5448": {
        "file_id": 712,
        "content": "from pyjom.videotoolbox import getMilvusVideoDeduplicationCollection,checkDuplicatedVideoAndInsertVector\nif __name__ == \"__main__\":\n    # connectMilvusDatabase()\n    collection = (\n        getMilvusVideoDeduplicationCollection()\n    )  # will not get existing collections\n    videoPaths = [\n        \"cute_cat_gif.mp4\",\n        \"cute_cat_gif.gif\",\n        \"cat_delogo.gif\",\n        \"/root/Desktop/works/pyjom/samples/video/dog_with_large_text.gif\",\n    ]\n    # for videoPath in videoPaths:\n    from lazero.utils.logger import sprint\n    for videoPath in videoPaths:\n        print(\"filepath: %s\" % videoPath)\n        duplicated = checkDuplicatedVideoAndInsertVector(collection, videoPath)\n        sprint(\"duplicated?\", duplicated)\n\"\"\"\nfilepath: cute_cat_gif.mp4\ndistances: [0.0, 0.11764705926179886, 0.11764705926179886, 0.7200000286102295, 0.7200000286102295, 0.7346938848495483, 0.7659574747085571, 0.7924528121948242]\n______________________________\nfilepath: cute_cat_gif.gif\ndistances: [0.0, 0.11764705926179886, 0.117647",
        "type": "code",
        "location": "/tests/video_phash_deduplication/test_milvus_library.py:201-226"
    },
    "5449": {
        "file_id": 712,
        "content": "The code connects to a Milvus database, retrieves the video deduplication collection, and checks if each given video path is already in the collection. It prints the file paths of the videos and whether they are duplicated or not using `checkDuplicatedVideoAndInsertVector` function from `lazero.utils.logger` module. The distances between the new video and existing ones in the database are also printed.",
        "type": "comment"
    },
    "5450": {
        "file_id": 712,
        "content": "05926179886, 0.7200000286102295, 0.7200000286102295, 0.7346938848495483, 0.7659574747085571, 0.7692307829856873]\n______________________________\nfilepath: cat_delogo.gif\ndistances: [0.0, 0.11764705926179886, 0.11764705926179886, 0.7200000286102295, 0.7200000286102295, 0.7346938848495483, 0.7659574747085571, 0.7924528121948242]\n______________________________\nfilepath: /root/Desktop/works/pyjom/samples/video/dog_with_large_text.gif\ndistances: [0.0, 0.6808510422706604, 0.6938775777816772, 0.6938775777816772, 0.739130437374115, 0.7692307829856873, 0.7924528121948242, 0.7924528121948242]\n______________________________\n\"\"\"",
        "type": "code",
        "location": "/tests/video_phash_deduplication/test_milvus_library.py:226-234"
    },
    "5451": {
        "file_id": 712,
        "content": "The code appears to be storing and comparing distances between video phashes for different files. Each line contains a file path followed by an array of distances, indicating the similarity of that video phash to other video phashes in the system. The lower the distance value, the more similar the videos are.",
        "type": "comment"
    },
    "5452": {
        "file_id": 713,
        "content": "/tests/video_phash_deduplication/test_milvus.py",
        "type": "filepath"
    },
    "5453": {
        "file_id": 713,
        "content": "The code demonstrates Milvus database operations, including creating a \"video\" collection, inserting data and performing searches. It is part of debugging process to retrieve documents by ID. The programmer is stuck and requires further investigation.",
        "type": "summary"
    },
    "5454": {
        "file_id": 713,
        "content": "# duplicate -> remove, do not insert\n# not duplicate -> get the data, insert\n# you want to clear the collection after this run?\n# import pymilvus\nfrom pymilvus import connections\nfrom functools import lru_cache\n@lru_cache(maxsize=1)\ndef connectMilvusDatabase(alias=\"default\", host=\"localhost\", port=\"19530\"):\n\tconnection = connections.connect(alias=alias, host=host, port=port)# can we reconnect?\n\tprint('milvus connected')\nconnectMilvusDatabase()\nconnectMilvusDatabase() # will not connect again.\ncollection_name = \"video_deduplication\"\nfrom pymilvus import Collection\n# Collection(collection_name)\n# remote this thing.\nfrom pymilvus import utility\ntry:\n    if utility.has_collection(collection_name):  # be prudent.\n        utility.drop_collection(collection_name)\nexcept:\n    import traceback\n    traceback.print_exc()\n    print(\"maybe the collection does not exist\")\nfrom pymilvus import CollectionSchema, FieldSchema, DataType\nvideo_semantic_id = FieldSchema(  # how to insert this shit without prior knowledge?\n    name=\"video_semantic_id\",",
        "type": "code",
        "location": "/tests/video_phash_deduplication/test_milvus.py:1-39"
    },
    "5455": {
        "file_id": 713,
        "content": "This code establishes a connection to a Milvus database, checks if the \"video_deduplication\" collection exists, and if so, removes it before creating a new one. The `connectMilvusDatabase` function sets up a connection with specified alias, host, and port (default values used in this code). The `utility.has_collection` and `utility.drop_collection` functions from the `pymilvus` utility module are used to check for and remove an existing collection named \"video_deduplication\". A `CollectionSchema` is defined for the new collection, specifying a field schema named \"video_semantic_id\".",
        "type": "comment"
    },
    "5456": {
        "file_id": 713,
        "content": "    dtype=DataType.INT64,\n    is_primary=True,  # if is primary, will do check for 'not duplicate' or something.\n    auto_id=True,  # no need for id generation.\n)\nvideo_length = FieldSchema(\n    name=\"video_length\",\n    dtype=DataType.FLOAT,\n)\nvideo_phash = FieldSchema(\n    name=\"video_phash\", dtype=DataType.BINARY_VECTOR, dim=64\n)  # 64\n# single dimension? no multi dimension support?\nschema = CollectionSchema(\n    fields=[video_semantic_id, video_length, video_phash],\n    description=\"Test video deduplication\",\n)\n# collection = Collection(\"video\")      # Get an existing collection.\ncollection = Collection(\n    name=collection_name,\n    schema=schema,\n    using=\"default\",\n    shards_num=2,\n)\n# is this demo collection?\n# seems hard to setup.\n# not started!\n# https://milvus.io/docs/v2.0.0/metric.md#binary\n# the metric is important to us.\nsearch_params = {\"metric_type\": \"Jaccard\", \"params\": {\"nprobe\": 10}}\nimport numpy as np\nqueryData = np.array(\n    [\n        [True, True, True, False, False, True, False, True],\n        [True, False, False, True, False, True, True, False],",
        "type": "code",
        "location": "/tests/video_phash_deduplication/test_milvus.py:40-76"
    },
    "5457": {
        "file_id": 713,
        "content": "This code is defining a schema for a Milvus collection, specifying the data types and field names. The schema includes video_semantic_id, video_length, and video_phash fields, which are used in video deduplication. The code creates a collection named \"video\" with 2 shards using the specified schema and sets the metric type for searching as Jaccard with nprobe parameter set to 10. It also imports numpy and creates queryData, which seems to be a binary vector.",
        "type": "comment"
    },
    "5458": {
        "file_id": 713,
        "content": "        [True, False, False, True, True, False, False, True],\n        [True, True, True, True, True, False, False, True],\n        [True, False, False, True, False, True, True, False],\n        [False, True, True, False, False, False, False, True],\n        [True, True, False, False, False, True, True, False],\n        [False, False, True, False, False, True, False, False],\n    ]\n)\nqueryData = queryData.reshape(-1).tolist()\nqueryData = [\"1\" if x else \"0\" for x in queryData]\nimport bitarray\nqueryData = bitarray.bitarray(\"\".join(queryData), endian=\"little\")\nqueryData2 = queryData.copy()\nqueryData2[1:4] = 0\nqueryData3 = queryData2.copy()\nqueryData2 = queryData2.tobytes()\nqueryData3[8:15] = 0\nqueryData3 = queryData3.tobytes()\nqueryData = queryData.tobytes()\n# dimension: 8*8=64\n# collection.insert([[1], [np.float32(3.5)], [queryData]])\n# collection.insert([[np.float32(3.5)], [queryData]])\n# for _ in range(8):\ncollection.insert([[np.float32(3.5)], [queryData]])\ncollection.insert([[np.float32(3.5)], [queryData2]])  # slight difference.",
        "type": "code",
        "location": "/tests/video_phash_deduplication/test_milvus.py:77-102"
    },
    "5459": {
        "file_id": 713,
        "content": "The code is preparing and inserting data into a Milvus collection. It creates binary representations of queryData, queryData2, and queryData3 (slightly different from queryData2), then inserts them along with a float value (np.float32(3.5)) into the collection, representing a 64-dimensional vector.",
        "type": "comment"
    },
    "5460": {
        "file_id": 713,
        "content": "collection.insert([[np.float32(3.5)], [queryData3]])  # more difference.\n# print(len(queryData), len(queryData)*8)\n# # print(queryData.shape)\n# breakpoint()\n# collection.load()\ncollection.insert([[np.float32(3.5)], [queryData]]) # still three.\n# can release even if not loaded.\ncollection.release() # unload.\ncollection.load()\n# make it into some library!\n# insert after load?\n# # 1,64\n# what is wrong? wtf?\n# queryData = queryData.tolist()\nresults = collection.search(\n    data=[queryData],  # this is the float dimension.\n    anns_field=\"video_phash\",\n    param=search_params,\n    output_fields=[\"video_length\"],\n    limit=10,\n    expr=\"video_length > 1.2 and video_length < 4\",\n    # expr='video_length < 1.2',\n)\ntheHit = results[0]\nprint(theHit)\n# so we can perform search without filtering afterwards.\n# results[0][0].entity.get('video_length')\n# print(results[0].ids)\n# now, we want to have the 'distance' parameter.\n# print(results[0])\n# print(theHit)\n# distances = theHit.distances\n# results = [x for x in theHit]\n# hits = len(theHit)",
        "type": "code",
        "location": "/tests/video_phash_deduplication/test_milvus.py:103-139"
    },
    "5461": {
        "file_id": 713,
        "content": "The code is inserting data into a collection, releasing and reloading it, performing a search based on specific parameters, and accessing the results. The purpose seems to be searching for video data within a database based on certain criteria, such as length, and extracting relevant information from the resulting hits.",
        "type": "comment"
    },
    "5462": {
        "file_id": 713,
        "content": "# breakpoint()\n# how to get document by id? wtf",
        "type": "code",
        "location": "/tests/video_phash_deduplication/test_milvus.py:140-141"
    },
    "5463": {
        "file_id": 713,
        "content": "This code appears to be part of a debugging process, where the programmer is trying to understand how to retrieve a document by its ID using the Milvus database. The \"breakpoint()\" comment suggests they are currently stuck or needing to pause execution for further investigation.",
        "type": "comment"
    },
    "5464": {
        "file_id": 714,
        "content": "/tests/video_phash_deduplication/test_image_hash_milvus_database_search.py",
        "type": "filepath"
    },
    "5465": {
        "file_id": 714,
        "content": "This code is initializing image paths and loading necessary libraries. It calculates image hashes for three different images using the phash function from imagehash library. It then creates a connection to a Milvus database and loads an existing collection.",
        "type": "summary"
    },
    "5466": {
        "file_id": 714,
        "content": "pic_0 = \"cat.png\"\npic_0_similar = \"cat3.png\"\npic_1 = \"/root/Desktop/works/pyjom/samples/image/dick.png\"\nfrom PIL import Image\n# >>> import imagehash\n# >>> hash = imagehash.average_hash(Image.open\nimport imagehash\npics = [pic_0, pic_0_similar, pic_1]\nhashs = [imagehash.phash(Image.open(pic)) for pic in pics]\n# dis0 = hashs[0]-hashs[1]\n# dis1 = hashs[1]-hashs[2]\n# print(dis0, dis1)\n# 0 24\n# 6 24\n# well, let's check?\n# print(hashs)\n# three truth tables.\n# 2^4 = 16, total 2*8 digits=16?\n# towhee(multimodal search like jina), haystack, milvus\n# import pymilvus\nfrom pymilvus import connections\nconnection = connections.connect(alias=\"default\", host=\"localhost\", port=\"19530\")\nfrom pymilvus import Collection\ncollection = Collection(\"book\")  # Get an existing collection.\ncollection.load()\n# seems hard to setup.\n# not started!",
        "type": "code",
        "location": "/tests/video_phash_deduplication/test_image_hash_milvus_database_search.py:1-37"
    },
    "5467": {
        "file_id": 714,
        "content": "This code is initializing image paths and loading necessary libraries. It calculates image hashes for three different images using the phash function from imagehash library. It then creates a connection to a Milvus database and loads an existing collection.",
        "type": "comment"
    },
    "5468": {
        "file_id": 715,
        "content": "/tests/video_phash_deduplication/test_image_hash.py",
        "type": "filepath"
    },
    "5469": {
        "file_id": 715,
        "content": "This code uses the Python Imaging Library (PIL) and imagehash module to calculate average color hash values for images. It opens three images, 'pic_0', 'pic_0_similar', and 'pic_1', calculates their color hash values using the phash function from imagehash, and then calculates the differences between the hashes of each pair of images. The code checks the types of the calculated hashes and prints the distances (dis0 and dis1) between the pairs of images. It also mentions a possible discrepancy in the expected number of digits for the distances.",
        "type": "summary"
    },
    "5470": {
        "file_id": 715,
        "content": "pic_0= 'cat.png'\npic_0_similar = \"cat3.png\"\npic_1 = \"/root/Desktop/works/pyjom/samples/image/dick.png\"\nfrom PIL import Image\n# >>> import imagehash\n# >>> hash = imagehash.average_hash(Image.open\nimport imagehash\npics = [pic_0, pic_0_similar, pic_1]\nhashs = [imagehash.phash(Image.open(pic)) for pic in pics]\ndis0 = hashs[0]-hashs[1]\ndis1 = hashs[1]-hashs[2]\n# 0 24\n# 6 24\n# well, let's check?\nprint([type(h) for h in hashs])\nbreakpoint()\nprint(dis0, dis1)\n# three truth tables.\n# 2^4 = 16, total 2*8 digits=16?",
        "type": "code",
        "location": "/tests/video_phash_deduplication/test_image_hash.py:1-23"
    },
    "5471": {
        "file_id": 715,
        "content": "This code uses the Python Imaging Library (PIL) and imagehash module to calculate average color hash values for images. It opens three images, 'pic_0', 'pic_0_similar', and 'pic_1', calculates their color hash values using the phash function from imagehash, and then calculates the differences between the hashes of each pair of images. The code checks the types of the calculated hashes and prints the distances (dis0 and dis1) between the pairs of images. It also mentions a possible discrepancy in the expected number of digits for the distances.",
        "type": "comment"
    },
    "5472": {
        "file_id": 716,
        "content": "/tests/video_phash_deduplication/README.md",
        "type": "filepath"
    },
    "5473": {
        "file_id": 716,
        "content": "Code detects identical video files and finds repeated intervals within each video file.",
        "type": "summary"
    },
    "5474": {
        "file_id": 716,
        "content": "two main problems, one is to detect identical video files, one is to find 'repeated interval' inside each other.",
        "type": "code",
        "location": "/tests/video_phash_deduplication/README.md:1-1"
    },
    "5475": {
        "file_id": 716,
        "content": "Code detects identical video files and finds repeated intervals within each video file.",
        "type": "comment"
    },
    "5476": {
        "file_id": 717,
        "content": "/tests/video_phash_deduplication/config_milvus.sh",
        "type": "filepath"
    },
    "5477": {
        "file_id": 717,
        "content": "This code sets up Milvus, a vector database, by creating its configuration directory and downloading the necessary server and log configurations before starting the Milvus services.",
        "type": "summary"
    },
    "5478": {
        "file_id": 717,
        "content": "# # Create Milvus file\n# $ mkdir -p /home/$USER/milvus/conf\n# $ cd /home/$USER/milvus/conf\n# $ wget https://raw.githubusercontent.com/milvus-io/milvus/v0.8.0/core/conf/demo/server_config.yaml\n# $ wget https://raw.githubusercontent.com/milvus-io/milvus/v0.8.0/core/conf/demo/log_config.conf\nsudo systemctl start milvus\nsudo systemctl start milvus-etcd\nsudo systemctl start milvus-minio",
        "type": "code",
        "location": "/tests/video_phash_deduplication/config_milvus.sh:1-8"
    },
    "5479": {
        "file_id": 717,
        "content": "This code sets up Milvus, a vector database, by creating its configuration directory and downloading the necessary server and log configurations before starting the Milvus services.",
        "type": "comment"
    },
    "5480": {
        "file_id": 718,
        "content": "/tests/title_cover_generator/tokenizer.py",
        "type": "filepath"
    },
    "5481": {
        "file_id": 718,
        "content": "This code defines a custom tokenizer class, T5PegasusTokenizer, which extends the BertTokenizer. It takes an optional pre_tokenizer function as input and initializes the base tokenizer. The _tokenize method splits the text into tokens using the pre_tokenizer, and if the token is in the vocabulary, it adds it to split_tokens. If not, it extends split_tokens with tokens generated by the base tokenizer's _tokenize method.",
        "type": "summary"
    },
    "5482": {
        "file_id": 718,
        "content": "import jieba\nfrom transformers import BertTokenizer\n# alike structure as DianJing. but is it for gpt2?\nclass T5PegasusTokenizer(BertTokenizer):\n    def __init__(self, pre_tokenizer=lambda x: jieba.cut(x, HMM=False), *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.pre_tokenizer = pre_tokenizer\n    def _tokenize(self, text, *arg, **kwargs):\n        split_tokens = []\n        for text in self.pre_tokenizer(text):\n            if text in self.vocab:\n                split_tokens.append(text)\n            else:\n                split_tokens.extend(super()._tokenize(text))\n        return split_tokens",
        "type": "code",
        "location": "/tests/title_cover_generator/tokenizer.py:1-17"
    },
    "5483": {
        "file_id": 718,
        "content": "This code defines a custom tokenizer class, T5PegasusTokenizer, which extends the BertTokenizer. It takes an optional pre_tokenizer function as input and initializes the base tokenizer. The _tokenize method splits the text into tokens using the pre_tokenizer, and if the token is in the vocabulary, it adds it to split_tokens. If not, it extends split_tokens with tokens generated by the base tokenizer's _tokenize method.",
        "type": "comment"
    },
    "5484": {
        "file_id": 719,
        "content": "/tests/title_cover_generator/spacy_word_swapper.py",
        "type": "filepath"
    },
    "5485": {
        "file_id": 719,
        "content": "The code uses Spacy and Jieba tokenizer to check if a string contains English, removes non-English elements, and prints the tokens along with their POS and dependency tags. Proper nouns list may be updated and improvements are potential.",
        "type": "summary"
    },
    "5486": {
        "file_id": 719,
        "content": "# just use some simple analysis to extract the template. may not be cost effective like DianJing also you can try the freaking gpt2 model, or pegasus.\nfrom commons import sample_data\n# first assume all to be freaking chinese.\n# import nltk\nimport spacy\nimport jieba\nfrom spacy.lang.zh.examples import sentences \nimport re\ndef recursiveCompiledSearch(compiledRegex, pattern,initPos=0,resultTotal = []):\n    result = compiledRegex.search(pattern)\n    if result !=None:\n        match = result[0]\n        span = result.span()\n        realSpan = (span[0]+initPos, span[1]+initPos)\n        # initialSpan = span[0]\n        endSpan = span[1]\n        initPos += endSpan\n        mresult = {\"match\":match, \"span\":realSpan}\n        resultTotal.append(mresult)\n        newPattern = pattern[endSpan:]\n        return recursiveCompiledSearch(compiledRegex,newPattern,initPos,resultTotal)\n    else: return resultTotal\nnlp = spacy.load(\"zh_core_web_sm\")\n# proper_nouns = ['守望先锋','第五人格']\n# whatever. we can always change shit.\n# nlp.tokenizer.pkuseg_update_user_dict(proper_nouns)",
        "type": "code",
        "location": "/tests/title_cover_generator/spacy_word_swapper.py:1-30"
    },
    "5487": {
        "file_id": 719,
        "content": "The code is importing necessary libraries and defining a function for recursive text search. It uses the Spacy library for Chinese language processing, but it seems to be in progress as it mentions potential improvements and updates. The proper nouns list may be updated or changed later.",
        "type": "comment"
    },
    "5488": {
        "file_id": 719,
        "content": "# this is imoortant.\nenglish = re.compile(r\"([a-zA-Z]+([ \\-,\\.:;?!]+)?)+\")\ndef check_has_language(string,language_re): result = recursiveCompiledSearch(language_re,string,resultTotal=[]); return len(result) >0\nfor elem in sample_data:\n    hasSpace = False\n    # we need to eliminate some english things.\n    # we also have some spaces. remove them before proceed.\n    if \" \" in elem:\n        hasSpace = True\n        elem = elem.replace(\" \", \"\")\n    # some flashy text will never be accepted. if outside of english, chinese we accept nothing.\n    # english is not included in spacy.\n    data = [x for x in jieba.cut(elem)] # contradictory.\n    english_check = check_has_language(elem,english)\n    if english_check:\n        print(\"HAS ENGLISH\")\n        print(elem)\n        continue\n    # check if words contains english. remove these titles.\n    # print(data)\n    nlp.tokenizer.pkuseg_update_user_dict(data)\n    doc = nlp(elem)\n    print(doc.text)\n    for token in doc:\n        print(token.text, token.pos_, token.dep_)",
        "type": "code",
        "location": "/tests/title_cover_generator/spacy_word_swapper.py:31-56"
    },
    "5489": {
        "file_id": 719,
        "content": "This code checks if a given string contains English language, removes spaces and non-English elements using Jieba tokenizer and Spacy, and then prints the tokens along with their part of speech (POS) and dependency tags for further analysis.",
        "type": "comment"
    },
    "5490": {
        "file_id": 720,
        "content": "/tests/title_cover_generator/pyltp_server.py",
        "type": "filepath"
    },
    "5491": {
        "file_id": 720,
        "content": "The code initializes LTP models for NLP tasks, offering functions for segmentation, part-of-speech tagging, named entity recognition, and dependency syntax parsing. It uses PyLTL to extract subject-predicate-object triples from sentences, identifies relationships, and appends them to Dynamic_relation if applicable.",
        "type": "summary"
    },
    "5492": {
        "file_id": 720,
        "content": "# !/usr/bin/env python3\n# -*- coding:utf-8 -*-\n# create on 5/26/20\n__author__ = \"sinsa\"\nimport os\nimport logging\nfrom logging import info, error, warn\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s - PID:%(process)d - %(levelname)s: %(message)s\",\n)\nfrom pyltp import Segmentor\nfrom pyltp import Postagger\nfrom pyltp import NamedEntityRecognizer\nfrom pyltp import Parser\nfrom pyltp import SentenceSplitter\nclass LTP_MODEL:\n    def __init__(self):\n        LTP_DATA_DIR = \"./pyltp_data/ltp_data_v3.4.0\"  # ltp模型目录的路径\n        info(\"loading models ...\")\n        self.cws_model_path = os.path.join(\n            LTP_DATA_DIR, \"cws.model\"\n        )  # 分词模型路径，模型名称为`cws.model`\n        self.segmentor = Segmentor(self.cws_model_path)  # 初始化实例\n        # self.segmentor.load(self.cws_model_path)  # 加载模型\n        info(\"has loaded 分词模型\")\n        self.pos_model_path = os.path.join(\n            LTP_DATA_DIR, \"pos.model\"\n        )  # 词性标注模型路径，模型名称为`pos.model`\n        self.postaggers = Postagger(self.pos_model_path)  # 初始化实例",
        "type": "code",
        "location": "/tests/title_cover_generator/pyltp_server.py:1-35"
    },
    "5493": {
        "file_id": 720,
        "content": "This code initializes the necessary LTP (Language Technology Platform) models for Natural Language Processing tasks. It sets the LTP data directory, loads and initializes models for word segmentation, part-of-speech tagging, named entity recognition, and parsing. The logger is configured to provide status updates during the loading process.",
        "type": "comment"
    },
    "5494": {
        "file_id": 720,
        "content": "        # self.postaggers.load(self.pos_model_path)  # 加载模型\n        info(\"has loaded 词性标注模型\")\n        self.ner_model_path = os.path.join(\n            LTP_DATA_DIR, \"ner.model\"\n        )  # 命名实体识别模型路径，模型名称为`pos.model`\n        self.recognizer = NamedEntityRecognizer(self.ner_model_path)  # 初始化实例\n        # self.recognizer.load(self.ner_model_path)  # 加载模型\n        info(\"has loaded 命名实体识别模型\")\n        self.par_model_path = os.path.join(\n            LTP_DATA_DIR, \"parser.model\"\n        )  # 依存句法分析模型路径，模型名称为`parser.model`\n        self.parser = Parser(self.par_model_path)  # 初始化实例\n        # self.parser.load(self.par_model_path)  # 加载模型\n        info(\"has loaded 依存句法分析模型\")\n    def __release__(self):\n        self.segmentor.release()  # 释放模型\n        self.postaggers.release()  # 释放模型\n        self.recognizer.release()  # 释放模型\n        self.parser.release()  # 释放模型\n    def SplitSentence(self, sentence):\n        sents_list = SentenceSplitter.split(sentence)  # 分句\n        return list(sents_list)\n    def segment(self, input_list):",
        "type": "code",
        "location": "/tests/title_cover_generator/pyltp_server.py:36-61"
    },
    "5495": {
        "file_id": 720,
        "content": "The code loads three models (POS tagging, Named Entity Recognition, and Dependency Parsing) and initializes corresponding recognizers or parsers for each model. It also provides methods to release the models when finished and split a sentence into individual sentences.",
        "type": "comment"
    },
    "5496": {
        "file_id": 720,
        "content": "        \"\"\"\n        功能：实现分词文本的分词\n        返回值：每个文本的形成一个列表[['word1','word2'],['word1','word3'],……]\n        \"\"\"\n        segmented_text_list = []\n        for text in input_list:\n            words = self.segmentor.segment(text)  # 分词\n            segmented_text_list.append(list(words))\n        return segmented_text_list\n    def postagger(self, input_list, return_words_list=False):\n        \"\"\"\n        功能：实现文本中每个词的词性标注\n        返回值：每个文本是一个列表，列表中的每个词也是个列表[[['word1',u'O'],['word2',u'O']],[['word2',u'O'],['word5',u'O']],……]\n        \"\"\"\n        postagger_text_list = []\n        words_list = self.segment(input_list)\n        postags_list = []\n        for words in words_list:\n            postags = self.postaggers.postag(words)  # 词性标注\n            postags_list.append(list(postags))\n            words_postags = list(zip(words, list(postags)))\n            postagger_text_list.append(words_postags)\n        if return_words_list:\n            return words_list, postags_list\n        else:\n            return postagger_text_list\n    def NamedEntityRecognizer(self, input_list, Entity_dist=False, repead=False):",
        "type": "code",
        "location": "/tests/title_cover_generator/pyltp_server.py:62-90"
    },
    "5497": {
        "file_id": 720,
        "content": "This code defines three functions: \"segment\", \"postagger\", and \"NamedEntityRecognizer\". The \"segment\" function takes a list of texts as input, performs segmentation on each text to obtain a list of words, and returns the segmented text as a list of lists. The \"postagger\" function takes a list of texts and performs part-of-speech tagging on each word in the list. It then returns the tagged text as a list of lists. If the \"return_words_list\" parameter is True, it also returns the original words list. The \"NamedEntityRecognizer\" function recognizes named entities in the input texts based on the provided parameters (\"Entity_dist\" and \"repead\").",
        "type": "comment"
    },
    "5498": {
        "file_id": 720,
        "content": "        \"\"\"\n        功能：识别文本中的命名实体：地名，组织名和机构名\n        参数repead：表示是否进行去重处理 ，默认是不去重\n        参数Entity_dist：表示每个文本，返回的识别后的列表，还是抽取后的实体字典，默认返回的是列表\n        返回值的形式：1.[[['word1',u'O'],['word2',u'O'],['word3',u'O']],[['word2',u'O'],['word3',u'O'],['word4',u'O']],……]\n                        2.[{'person':[],'place':[],'organization':[]},{'person':[],'place':[],'organization':[]},{'person':[],'place':[],'organization':[]},……]\n        \"\"\"\n        words_list, postags_list = self.postagger(input_list, return_words_list=True)\n        entity_text_list = []\n        for words, postags in zip(words_list, postags_list):\n            netags = self.recognizer.recognize(\n                words, postags\n            )  # 命名实体识别 人名（Nh）、地名（Ns）、机构名（Ni）\n            text = list(zip(words, netags))\n            entity_text_list.append(text)\n        if Entity_dist:\n            extract_entity_list = []\n            for words_entity_note_list in entity_text_list:\n                extract_entity_list.append(\n                    self.get_entity_dict(words_entity_note_list, repead)",
        "type": "code",
        "location": "/tests/title_cover_generator/pyltp_server.py:91-113"
    },
    "5499": {
        "file_id": 720,
        "content": "This code snippet is responsible for identifying named entities in a given text, such as person names, place names, and organization names. It uses the postagger to identify words and their parts of speech (POS) and then applies the recognizer to recognize named entities based on these POS tags. If Entity_dist is set to True, it extracts entities into a dictionary format.",
        "type": "comment"
    }
}