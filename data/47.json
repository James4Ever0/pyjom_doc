{
    "4700": {
        "file_id": 605,
        "content": "    else:\n        raise Exception(\"unknown objective: %s\" % objective)\n    # print(len(data)) # 589\n    data = dataDict[\"data\"]\n    defaultWidth, defaultHeight = dataDict[\"width\"], dataDict[\"height\"]\n    if objective in [\"continual\", 'continual_najie']:\n        finalCommandDict = kalmanStablePipRegionExporter(\n            data, defaultWidth, defaultHeight\n        )\n    else:\n        finalCommandDict = sampledStablePipRegionExporter(\n            data, defaultWidth, defaultHeight\n        )\n    import matplotlib.pyplot as plt\n    from matplotlib.patches import Rectangle\n    fig, ax = plt.subplots()\n    def plotRect(ax, x, y, width, height, facecolor):\n        ax.add_patch(\n            Rectangle((x, y), width, height, facecolor=facecolor, fill=True, alpha=0.5)\n        )  # in 0-1\n    ax.plot([[0, 0], [defaultWidth, defaultHeight]])\n    plotRect(ax, 0, 0, defaultWidth, defaultHeight, \"black\")\n    colors = [\"red\", \"yellow\", \"blue\",'orange','white','purple']\n    for index, key in enumerate(finalCommandDict.keys()):\n        import parse",
        "type": "code",
        "location": "/tests/video_detector_tests/pip_meanVariance_stablize.py:388-419"
    },
    "4701": {
        "file_id": 605,
        "content": "This code is handling an objective and preparing a plot for stable PIP region exporter. It raises an exception if the objective is unknown. Depending on the objective, it uses kalmanStablePipRegionExporter or sampledStablePipRegionExporter. It then plots a rectangle on the figure using matplotlib's Rectangle class. Finally, it sets colors for each region in the plot.",
        "type": "comment"
    },
    "4702": {
        "file_id": 605,
        "content": "        commandArguments = parse.parse(\"crop_{x:d}_{y:d}_{w:d}_{h:d}\", key)\n        color = colors[index%len(colors)]\n        rect = [int(commandArguments[name]) for name in [\"x\", \"y\", \"w\", \"h\"]]\n        print(\"RECT\", rect, color, \"SPAN\", finalCommandDict[key])\n        plotRect(ax, *rect, color)\n    # breakpoint()\n    plt.show()",
        "type": "code",
        "location": "/tests/video_detector_tests/pip_meanVariance_stablize.py:421-427"
    },
    "4703": {
        "file_id": 605,
        "content": "The code is creating a rectangular plot using matplotlib. It takes commandArguments as input and uses them to define the x, y, w, and h values of the rectangle. The color of the rectangle is randomly chosen from a list of colors. It then prints the coordinates of the rectangle, the chosen color, and the span of the finalCommandDict[key]. Lastly, it displays the plot using plt.show().",
        "type": "comment"
    },
    "4704": {
        "file_id": 606,
        "content": "/tests/video_detector_tests/motion_gpl.sh",
        "type": "filepath"
    },
    "4705": {
        "file_id": 606,
        "content": "Kills motion process and attempts to capture video from webcam using ffmpeg, then starts motion with the mconfig.conf configuration file. Not suitable for media file processing.",
        "type": "summary"
    },
    "4706": {
        "file_id": 606,
        "content": "killall -s KILL motion\n# ffmpeg -re -i ../../samples/video/LlfeL29BP.mp4 -f v4l2 /dev/video0 &\nmotion -c mconfig.conf\n# to conclude, this is only useful for webcams, not for media file processing.\n# are you sure if you want to capture shits over webcams by this?",
        "type": "code",
        "location": "/tests/video_detector_tests/motion_gpl.sh:1-6"
    },
    "4707": {
        "file_id": 606,
        "content": "Kills motion process and attempts to capture video from webcam using ffmpeg, then starts motion with the mconfig.conf configuration file. Not suitable for media file processing.",
        "type": "comment"
    },
    "4708": {
        "file_id": 607,
        "content": "/tests/video_detector_tests/motion_github.py",
        "type": "filepath"
    },
    "4709": {
        "file_id": 607,
        "content": "This code imports libraries, initializes a motion detector algorithm and sets up video capture. It continuously reads frames from the source, applies an algorithm to create output images, displays them in separate windows, and runs until a frame is not ready or Esc key pressed.",
        "type": "summary"
    },
    "4710": {
        "file_id": 607,
        "content": "# motion detectors are used to track objects. though you may want to separate objects with it.\nimport numpy as np\nimport cv2\nimport pybgs as bgs\n# suspect by static image analysis, and then create bounding box over the thing.\n# check image quality.\nalgorithm = bgs.FrameDifference() # track object we need that.\n# algorithm = bgs.SuBSENSE()\n# video_file = \"../../samples/video/highway_car.avi\"\n# video_file = \"../../samples/video/dog_with_text.mp4\"\nvideo_file = \"../../samples/video/LiEIfnsvn.mp4\" # this one with cropped boundaries. \n# video_file = \"../../samples/video/LlfeL29BP.mp4\"\n# maybe we should consider something else to crop the thing? or not?\n# accumulate the delta over time to see the result?\n# use static detection method.\ncapture = cv2.VideoCapture(video_file)\nwhile not capture.isOpened():\n  capture = cv2.VideoCapture(video_file)\n  cv2.waitKey(1000)\n  print(\"Wait for the header\")\n#pos_frame = capture.get(cv2.cv.CV_CAP_PROP_POS_FRAMES)\n#pos_frame = capture.get(cv2.CV_CAP_PROP_POS_FRAMES)\npos_frame = capture.get(1)",
        "type": "code",
        "location": "/tests/video_detector_tests/motion_github.py:1-29"
    },
    "4711": {
        "file_id": 607,
        "content": "This code imports necessary libraries and initializes a motion detector algorithm (FrameDifference) to track objects in a video. It also defines the video file path and sets up a VideoCapture object. The code waits for the video header, retrieves the current frame position, and is ready to process frames using the motion detection algorithm.",
        "type": "comment"
    },
    "4712": {
        "file_id": 607,
        "content": "while True:\n  flag, frame = capture.read()\n  if flag:\n    cv2.imshow('video', frame)\n    #pos_frame = capture.get(cv2.cv.CV_CAP_PROP_POS_FRAMES)\n    #pos_frame = capture.get(cv2.CV_CAP_PROP_POS_FRAMES)\n    pos_frame = capture.get(1)\n    #print str(pos_frame)+\" frames\"\n    img_output = algorithm.apply(frame)\n    img_bgmodel = algorithm.getBackgroundModel()\n    cv2.imshow('img_output', img_output)\n    cv2.imshow('img_bgmodel', img_bgmodel)\n  else:\n    #capture.set(cv2.cv.CV_CAP_PROP_POS_FRAMES, pos_frame-1)\n    #capture.set(cv2.CV_CAP_PROP_POS_FRAMES, pos_frame-1)\n    #capture.set(1, pos_frame-1)\n    #print \"Frame is not ready\"\n    cv2.waitKey(1000)\n    break\n  if 0xFF & cv2.waitKey(10) == 27:\n    break\n  #if capture.get(cv2.cv.CV_CAP_PROP_POS_FRAMES) == capture.get(cv2.cv.CV_CAP_PROP_FRAME_COUNT):\n  #if capture.get(cv2.CV_CAP_PROP_POS_FRAMES) == capture.get(cv2.CV_CAP_PROP_FRAME_COUNT):\n  #if capture.get(1) == capture.get(cv2.CV_CAP_PROP_FRAME_COUNT):\n    #break\ncv2.destroyAllWindows()",
        "type": "code",
        "location": "/tests/video_detector_tests/motion_github.py:30-62"
    },
    "4713": {
        "file_id": 607,
        "content": "The code continuously reads frames from a video source and displays them. It captures the current frame position, applies an algorithm to create output images, and shows the output and background model images in separate windows. It keeps running until a frame is not ready or the user presses Esc key, closing all windows at the end.",
        "type": "comment"
    },
    "4714": {
        "file_id": 608,
        "content": "/tests/video_detector_tests/mathlib.py",
        "type": "filepath"
    },
    "4715": {
        "file_id": 608,
        "content": "The code uses Sympy to merge overlapping intervals in a list of tuples, creates unified boundaries, and returns final results as merged continual mappings.",
        "type": "summary"
    },
    "4716": {
        "file_id": 608,
        "content": "# not overriding math.\n# do some ranged stuff here...\ndef getContinualNonSympyMergeResult(inputMSetCandidates):\n    # basically the same example.\n    # assume no overlapping here.\n    import sympy\n    def unionToTupleList(myUnion):\n        unionBoundaries = list(myUnion.boundary)\n        unionBoundaries.sort()\n        leftBoundaries = unionBoundaries[::2]\n        rightBoundaries = unionBoundaries[1::2]\n        return list(zip(leftBoundaries, rightBoundaries))\n    def tupleSetToUncertain(mSet):\n        mUncertain = None\n        for start, end in mSet:\n            if mUncertain is None:\n                mUncertain = sympy.Interval(start, end)\n            else:\n                mUncertain += sympy.Interval(start, end)\n        typeUncertain = type(mUncertain)\n        return mUncertain, typeUncertain\n    def mergeOverlappedInIntervalTupleList(intervalTupleList):\n        mUncertain, _ = tupleSetToUncertain(intervalTupleList)\n        mUncertainBoundaryList = list(mUncertain.boundary)\n        mUncertainBoundaryList.sort()",
        "type": "code",
        "location": "/tests/video_detector_tests/mathlib.py:1-28"
    },
    "4717": {
        "file_id": 608,
        "content": "This code defines three functions for set operations involving intervals. The functions are getContinualNonSympyMergeResult, unionToTupleList, and mergeOverlappedInIntervalTupleList. The code uses Sympy library to handle mathematical operations on intervals and merges non-overlapping intervals into a single uncertain variable. It sorts and converts intervals into tuples for further processing.",
        "type": "comment"
    },
    "4718": {
        "file_id": 608,
        "content": "        mergedIntervalTupleList = list(\n            zip(mUncertainBoundaryList[::2], mUncertainBoundaryList[1::2])\n        )\n        return mergedIntervalTupleList\n    # mSet = mergeOverlappedInIntervalTupleList([(0, 1), (2, 3)])\n    # mSet2 = mergeOverlappedInIntervalTupleList([(0.5, 1.5), (1.6, 2.5)])\n    # print(\"MSET\", mSet)\n    # print(\"MSET2\", mSet2)\n    mSetCandidates = [\n        mergeOverlappedInIntervalTupleList(x) for x in inputMSetCandidates\n    ]\n    mSetUnified = [x for y in mSetCandidates for x in y]\n    leftBoundaryList = set([x[0] for x in mSetUnified])\n    rightBoundaryList = set([x[1] for x in mSetUnified])\n    # they may freaking overlap.\n    # if want nearby-merge strategy, simply just expand all intervals, merge them with union and shrink the individual intervals inside union respectively.\n    markers = {\n        \"enter\": {k: [] for k in leftBoundaryList},\n        \"exit\": {k: [] for k in rightBoundaryList},\n    }\n    for index, mSetCandidate in enumerate(mSetCandidates):\n        leftBoundaryListOfCandidate = [x[0] for x in mSetCandidate]",
        "type": "code",
        "location": "/tests/video_detector_tests/mathlib.py:29-55"
    },
    "4719": {
        "file_id": 608,
        "content": "This code defines a function `mergeOverlappedInIntervalTupleList` which takes a list of interval tuples, merges overlapping intervals, and returns the merged list. The main purpose is to unify all the data in the same scope with potential overlap. It first creates a set of left and right boundaries from the unified data, then initializes a `markers` dictionary with \"enter\" and \"exit\" markers for each left boundary. Finally, it iterates over each candidate set, extracting the left boundaries and using them to update the marker dictionary. The final merged interval tuples are not explicitly calculated or returned here, but can be derived from the information in `markers`.",
        "type": "comment"
    },
    "4720": {
        "file_id": 608,
        "content": "        rightBoundaryListOfCandidate = [x[1] for x in mSetCandidate]\n        for leftBoundaryOfCandidate in leftBoundaryListOfCandidate:\n            markers[\"enter\"][leftBoundaryOfCandidate].append(index)  # remap this thing!\n        for rightBoundaryOfCandidate in rightBoundaryListOfCandidate:\n            markers[\"exit\"][rightBoundaryOfCandidate].append(index)  # remap this thing!\n    # now, iterate through the boundaries of mSetUnified.\n    unifiedBoundaryList = leftBoundaryList.union(\n        rightBoundaryList\n    )  # call me a set instead of a list please? now we must sort this thing\n    unifiedBoundaryList = list(unifiedBoundaryList)\n    unifiedBoundaryList.sort()\n    unifiedBoundaryMarks = {}\n    finalMappings = {}\n    # print(\"MARKERS\", markers)\n    # breakpoint()\n    for index, boundary in enumerate(unifiedBoundaryList):\n        previousMark = unifiedBoundaryMarks.get(index - 1, [])\n        enterList = markers[\"enter\"].get(boundary, [])\n        exitList = markers[\"exit\"].get(boundary, [])\n        currentMark = set(previousMark + enterList).difference(set(exitList))",
        "type": "code",
        "location": "/tests/video_detector_tests/mathlib.py:56-77"
    },
    "4721": {
        "file_id": 608,
        "content": "This code creates a set of unified boundaries and maps the markers accordingly. It first gathers the \"enter\" and \"exit\" markers for each boundary, then forms the final mappings by taking the difference between the \"enter\" and \"exit\" lists. The code also sorts the boundaries and retrieves previous marks to form the current mark set.",
        "type": "comment"
    },
    "4722": {
        "file_id": 608,
        "content": "        currentMark = list(currentMark)\n        unifiedBoundaryMarks.update({index: currentMark})\n        # now, handle the change? or not?\n        # let's just deal those empty ones, shall we?\n        if previousMark == []:  # inside it is empty range.\n            # elif currentMark == []:\n            if index == 0:\n                continue  # just the start, no need to note this down.\n            else:\n                finalMappings.update(\n                    {\n                        \"empty\": finalMappings.get(\"empty\", [])\n                        + [(unifiedBoundaryList[index - 1], boundary)]\n                    }\n                )\n            # the end of previous mark! this interval belongs to previousMark\n        else:\n            key = previousMark.copy()\n            key.sort()\n            key = tuple(key)\n            finalMappings.update(\n                {\n                    key: finalMappings.get(key, [])\n                    + [(unifiedBoundaryList[index - 1], boundary)]\n                }\n            )",
        "type": "code",
        "location": "/tests/video_detector_tests/mathlib.py:78-103"
    },
    "4723": {
        "file_id": 608,
        "content": "This code checks if the current mark is empty and updates the finalMappings accordingly. If previousMark is empty, it skips noting down just the start of a range. Otherwise, it sorts and makes a unique key using previousMark, then adds the interval to finalMappings for that key.",
        "type": "comment"
    },
    "4724": {
        "file_id": 608,
        "content": "            # also the end of previous mark! belongs to previousMark.\n    ### NOW THE FINAL OUTPUT ###\n    finalCats = {}\n    for key, value in finalMappings.items():\n        # value is an array containing subInterval tuples.\n        value = mergeOverlappedInIntervalTupleList(value)\n        finalCats.update({key: value})\n    # print(\"______________FINAL CATS______________\")\n    # print(finalCats)\n    return finalCats\ndef getContinualMappedNonSympyMergeResult(mRangesDict, concatSymbol=\"|\", noEmpty=True):\n    mKeyMaps = list(mRangesDict.keys())\n    mSetCandidates = [mRangesDict[key] for key in mKeyMaps]\n    # the next step will automatically merge all overlapped candidates.\n    finalCats = getContinualNonSympyMergeResult(mSetCandidates)\n    finalCatsMapped = {\n        concatSymbol.join([mKeyMaps[k] for k in mTuple]): finalCats[mTuple]\n        for mTuple in finalCats.keys()\n        if type(mTuple) == tuple\n    }\n    if not noEmpty:\n        finalCatsMapped.update(\n            {k: finalCats[k] for k in finalCats.keys() if type(k) != tuple}",
        "type": "code",
        "location": "/tests/video_detector_tests/mathlib.py:104-130"
    },
    "4725": {
        "file_id": 608,
        "content": "This code calculates merged, continual results for a dictionary of sets. It maps the results to a format using a specified concatenation symbol, and allows for empty sets if requested. It uses functions like getContinualNonSympyMergeResult and mergeOverlappedInIntervalTupleList to merge overlapping intervals. The final result is returned as a dictionary of merged continual mappings.",
        "type": "comment"
    },
    "4726": {
        "file_id": 608,
        "content": "        )\n    return finalCatsMapped\n    # default not to output empty set?\ndef getContinualMappedNonSympyMergeResultWithRangedEmpty(\n    mRangesDict, start, end, concatSymbol=\"|\"\n):\n    import uuid\n    emptySetName = str(uuid.uuid4())\n    newRangesDict = mRangesDict.copy()\n    newRangesDict.update({emptySetName: [(start, end)]})\n    newRangesDict = getContinualMappedNonSympyMergeResult(\n        newRangesDict, concatSymbol=\"|\", noEmpty=True\n    )\n    newRangesDict = {\n        key: [\n            (mStart, mEnd)\n            for mStart, mEnd in newRangesDict[key]\n            if mStart >= start and mEnd <= end\n        ]\n        for key in newRangesDict.keys()\n    }\n    newRangesDict = {\n        key: newRangesDict[key]\n        for key in newRangesDict.keys()\n        if newRangesDict[key] != []\n    }\n    finalNewRangesDict = {}\n    for key in newRangesDict.keys():\n        mergedEmptySetName = \"{}{}\".format(concatSymbol, emptySetName)\n        if mergedEmptySetName in key:\n            newKey = key.replace(mergedEmptySetName,\"\")",
        "type": "code",
        "location": "/tests/video_detector_tests/mathlib.py:131-164"
    },
    "4727": {
        "file_id": 608,
        "content": "Function to get a continual mapped non-Sympy merge result with range based on input parameters. It creates a new dictionary with an empty set named UUID, then updates the existing dictionary with this new one. Filters out any ranges that do not fall within the given start and end values. Removes any keys in the newRangesDict dictionary if their corresponding value is an empty list. Finally, iterates over each key in newRangesDict and checks if mergedEmptySetName exists; if it does, it replaces the key with an empty string.",
        "type": "comment"
    },
    "4728": {
        "file_id": 608,
        "content": "            finalNewRangesDict.update({newKey:newRangesDict[key]})\n        elif key == emptySetName:\n            finalNewRangesDict.update({'empty':newRangesDict[key]})\n        else:\n            finalNewRangesDict.update({key:newRangesDict[key]})\n    return finalNewRangesDict\ndef mergedRangesToSequential(renderDict):\n    renderList = []\n    for renderCommandString in renderDict.keys():\n        commandTimeSpans = renderDict[renderCommandString].copy()\n        # commandTimeSpan.sort(key=lambda x: x[0])\n        for commandTimeSpan in commandTimeSpans:\n            renderList.append([renderCommandString, commandTimeSpan].copy())\n    renderList.sort(key=lambda x: x[1][0])\n    return renderList\n    # for renderCommandString, commandTimeSpan in renderList:\n    #     print(renderCommandString, commandTimeSpan)\ndef sequentialToMergedRanges(sequence):\n    mergedRanges = {}\n    for commandString, commandTimeSpan in sequence:\n        mergedRanges.update({commandString: mergedRanges.get(commandString,[])+[commandTimeSpan]})",
        "type": "code",
        "location": "/tests/video_detector_tests/mathlib.py:165-188"
    },
    "4729": {
        "file_id": 608,
        "content": "Function `mergedRangesToSequential` takes a dictionary where keys are commands and values are time spans, sorts them by start time in ascending order, and returns the sorted list of commands with their respective time spans.\n\nFunction `sequentialToMergedRanges` takes a list of command strings and their corresponding start times, groups them by command string, and produces a dictionary with commands as keys and lists of time spans as values.",
        "type": "comment"
    },
    "4730": {
        "file_id": 608,
        "content": "    mergedRanges = getContinualMappedNonSympyMergeResult(mergedRanges)\n    return mergedRanges",
        "type": "code",
        "location": "/tests/video_detector_tests/mathlib.py:189-190"
    },
    "4731": {
        "file_id": 608,
        "content": "This code block retrieves the merged ranges of continuous numbers using getContinualMappedNonSympyMergeResult function and assigns it to variable 'mergedRanges'. Finally, it returns the mergedRanges.",
        "type": "comment"
    },
    "4732": {
        "file_id": 609,
        "content": "/tests/video_detector_tests/frameDifference.py",
        "type": "filepath"
    },
    "4733": {
        "file_id": 609,
        "content": "This function detects motion in a video by comparing frames, calculating differences, applying thresholding and morphology operations, and identifying contours. The code displays two consecutive frames side-by-side using OpenCV and stops when 'Esc' is pressed.",
        "type": "summary"
    },
    "4734": {
        "file_id": 609,
        "content": "import cv2 as cv\nimport numpy as np\nfrom matplotlib import pyplot as plt\ndef motionDetection(videoPath):\n    cap = cv.VideoCapture(videoPath)\n    ret, frame1 = cap.read()\n    ret, frame2 = cap.read()\n    while cap.isOpened():\n        if frame1 is not None and frame2 is not None:\n            pass\n        else:\n            break\n        diff = cv.absdiff(frame1, frame2)\n        diff_gray = cv.cvtColor(diff, cv.COLOR_BGR2GRAY)\n        blur = cv.GaussianBlur(diff_gray, (5, 5), 0)\n        _, thresh = cv.threshold(blur, 20, 255, cv.THRESH_BINARY)\n        dilated = cv.dilate(thresh, None, iterations=3)\n        contours, _ = cv.findContours(\n            dilated, cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE)\n        for contour in contours:\n            (x, y, w, h) = cv.boundingRect(contour)\n            if cv.contourArea(contour) < 900:\n                continue\n            cv.rectangle(frame1, (x, y), (x+w, y+h), (0, 255, 0), 2)\n            cv.putText(frame1, \"Status: {}\".format('Movement'), (10, 20), cv.FONT_HERSHEY_SIMPLEX,\n                        1, (255, 0, 0), 3)",
        "type": "code",
        "location": "/tests/video_detector_tests/frameDifference.py:1-30"
    },
    "4735": {
        "file_id": 609,
        "content": "This function performs motion detection by comparing successive frames in a video, calculates the difference, applies thresholding and morphology operations, and finally detects contours to identify areas with significant changes.",
        "type": "comment"
    },
    "4736": {
        "file_id": 609,
        "content": "        # cv.drawContours(frame1, contours, -1, (0, 255, 0), 2)\n        cv.imshow(\"Video\", frame1)\n        frame1 = frame2\n        ret, frame2 = cap.read()\n        if cv.waitKey(50) == 27:\n            break\n    cap.release()\n    cv.destroyAllWindows()\nif __name__ == \"__main__\":\n    motionDetection(\"../../samples/video/LiEIfnsvn.mp4\")",
        "type": "code",
        "location": "/tests/video_detector_tests/frameDifference.py:32-46"
    },
    "4737": {
        "file_id": 609,
        "content": "This code displays two consecutive frames of a video side by side, highlighting the difference between them using OpenCV. It reads frames from a video file and continuously checks for user input to break the loop when key 'Esc' is pressed.",
        "type": "comment"
    },
    "4738": {
        "file_id": 610,
        "content": "/tests/video_detector_tests/detectron2_norfair.py",
        "type": "filepath"
    },
    "4739": {
        "file_id": 610,
        "content": "The code uses Detectron2 for object detection, tracks \"person\" or \"dog\", updates tracked_objects, and displays bounding boxes. It utilizes OpenCV for video display and waits for 'q' to terminate, closing windows upon exiting.",
        "type": "summary"
    },
    "4740": {
        "file_id": 610,
        "content": "import cv2\nimport numpy as np\nfrom detectron2.config import get_cfg\nfrom detectron2.engine import DefaultPredictor\nfrom cocoNames import cocoRealName\nfrom norfair import Detection, Tracker, Video, draw_tracked_objects\ndef euclidean_distance(detection, tracked_object):\n    return np.linalg.norm(detection.points - tracked_object.estimate)\n# Set up Detectron2 object detector\ncfg = get_cfg()\ncfg.merge_from_file(\"norfair/demos/faster_rcnn_R_50_FPN_3x.yaml\")\ncfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5 # looks like it does not recognize dog.\n# cfg.MODEL.WEIGHTS = \"detectron2://COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl\"\ncfg.MODEL.WEIGHTS = \"/root/Desktop/works/pyjom/tests/video_detector_tests/detectron2_models/model_final_f10217.pkl\"\n# it is stored in s3\n# https://dl.fbaipublicfiles.com/detectron2/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl\n# download cache: /root/.torch/iopath_cache/detectron2/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl",
        "type": "code",
        "location": "/tests/video_detector_tests/detectron2_norfair.py:1-21"
    },
    "4741": {
        "file_id": 610,
        "content": "The code imports necessary libraries and sets up a Detectron2 object detector using pre-trained weights for instance segmentation. It also defines a function to calculate Euclidean distance between detection and tracked objects. The configuration file specifies the model architecture, which is R_50_FPN with three stages and the specific weights (model_final_f10217.pkl) to be used for detection. The weights can either be downloaded from a public S3 storage or retrieved from the local cache if already downloaded.",
        "type": "comment"
    },
    "4742": {
        "file_id": 610,
        "content": "detector = DefaultPredictor(cfg)\n# what are the classes output by the model?\n# Norfair\nvideo_path = \"/root/Desktop/works/pyjom/samples/video/dog_with_text.mp4\"\n# video_path = \"/root/Desktop/works/pyjom/samples/video/LlfeL29BP.mp4\"\nvideo = Video(input_path=video_path)\ntracker = Tracker(distance_function=euclidean_distance, distance_threshold=400,hit_inertia_min=2,hit_inertia_max=20,initialization_delay=1) # what the heck?\ntracked_objects = None\ndisplay=True\nfor index, frame in enumerate(video): # we need to speed up.\n    if index%10 == 0:\n        detections = detector(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n        print(\"original detections:\",detections)\n        instances = detections[\"instances\"]\n        print(\"instances:\",instances)\n        # breakpoint()\n        pred_classes = instances.pred_classes\n        if len(pred_classes) == 0:\n            continue\n        detections2=[]\n        for index,class_ in enumerate(pred_classes):\n            print(\"index:\",index)\n            class_ = int(class_.cpu().numpy().tolist())",
        "type": "code",
        "location": "/tests/video_detector_tests/detectron2_norfair.py:23-48"
    },
    "4743": {
        "file_id": 610,
        "content": "The code initializes a video detector using Detector class and then processes each frame of the video. It predicts instances in each frame, prints detected classes and instances, and continues only if there are predictions. The tracker is used to track objects over frames, but its parameters might need clarification. Speedup is mentioned as needed, which implies potential optimizations.",
        "type": "comment"
    },
    "4744": {
        "file_id": 610,
        "content": "            print(\"class:\",class_)\n            box = instances.pred_boxes.tensor[index].cpu().numpy().tolist()\n            box = [int(x) for x in box]\n            score = float(instances.scores[index].cpu().numpy().tolist())\n            print('box:',box)\n            print('score:',score)\n            className = cocoRealName[class_]\n            # we filter our targets.\n            if className not in [\"person\",\"dog\"]:\n                continue\n            mdata = {\"box\":box,\"class\":{\"id\":class_,\"name\":className}}\n            det = Detection(instances.pred_boxes.get_centers()[index].cpu().numpy(),scores=np.array([score]),data=mdata)\n            detections2.append(det)\n            # breakpoint()\n        # detections = [Detection(p) for p in instances.pred_boxes.get_centers().cpu().numpy()] # what is this instance anyway?\n        # you would lost data you dick!\n        print(\"detections2\",detections2)\n        tracked_objects = tracker.update(detections=detections2)\n        # print(detections)\n        print(\"tracked objects:\",tracked_objects) # you don't track shit?",
        "type": "code",
        "location": "/tests/video_detector_tests/detectron2_norfair.py:49-68"
    },
    "4745": {
        "file_id": 610,
        "content": "This code is filtering and creating detection objects for \"person\" or \"dog\" instances from a given dataset. It prints the box coordinates, score, and class name before adding it to the detections2 list. The code then updates tracked_objects using the tracker function with the detections2 list.",
        "type": "comment"
    },
    "4746": {
        "file_id": 610,
        "content": "    if tracked_objects is not None:\n        if tracked_objects!=[]:\n            # there is no bounding box avaliable?\n            for obj in tracked_objects:\n                point = obj.estimate[0]\n                position = tuple(point.astype(int))\n                color = (255,0,0)\n                # breakpoint()\n                name = obj.last_detection.data[\"class\"][\"name\"]\n                cv2.circle(\n                        frame,\n                        position,\n                        radius=100,\n                        color=color,\n                        thickness=2,\n                    )\n                cv2.putText(\n                    frame,\n                    \"[{}][{}]\".format(str(obj.id),name),\n                    (position[0]-100,position[1]),\n                    cv2.FONT_HERSHEY_SIMPLEX,\n                    2,\n                    (0,255,0),\n                    3,\n                    cv2.LINE_AA,\n                )\n            # breakpoint()\n        # i want to draw you in a different way.\n        # draw_tracked_objects(frame, tracked_objects,color=(255,0,0))",
        "type": "code",
        "location": "/tests/video_detector_tests/detectron2_norfair.py:69-97"
    },
    "4747": {
        "file_id": 610,
        "content": "The code checks if there are any tracked objects and then proceeds to draw bounding boxes around them, add labels for the objects' class names, and display their IDs on the frame using OpenCV functions. Additionally, it offers an alternative way to draw the objects in a different color.",
        "type": "comment"
    },
    "4748": {
        "file_id": 610,
        "content": "    if display:\n        cv2.imshow(\"window\",frame)\n        key  =  cv2.waitKey(1) & 0xff\n        if key == ord('q'):\n            break\n        # maybe we shall print this shit somehow.\n    # video.write(frame) # you write what?\nif display:\n    cv2.destroyAllWindows()",
        "type": "code",
        "location": "/tests/video_detector_tests/detectron2_norfair.py:98-106"
    },
    "4749": {
        "file_id": 610,
        "content": "The code displays a frame from a video using OpenCV's imshow function and waits for a user input (key) to terminate. The key input is checked if it matches the character 'q', which signals a break in the loop. OpenCV's destroyAllWindows() is called to close the window when display is enabled.",
        "type": "comment"
    },
    "4750": {
        "file_id": 611,
        "content": "/tests/video_detector_tests/detectron2_model_zoo_url.py",
        "type": "filepath"
    },
    "4751": {
        "file_id": 611,
        "content": "This code maps Detectron2 COCO model names to checkpoint files, defining configurations for trained parameters and providing pretrained model URLs and checkpoints.",
        "type": "summary"
    },
    "4752": {
        "file_id": 611,
        "content": "from typing import Optional\nclass _ModelZooUrls(object):\n    \"\"\"\n    Mapping from names to officially released Detectron2 pre-trained models.\n    \"\"\"\n    S3_PREFIX = \"https://dl.fbaipublicfiles.com/detectron2/\"\n    # format: {config_path.yaml} -> model_id/model_final_{commit}.pkl\n    CONFIG_PATH_TO_URL_SUFFIX = {\n        # COCO Detection with Faster R-CNN\n        \"COCO-Detection/faster_rcnn_R_50_C4_1x\": \"137257644/model_final_721ade.pkl\",\n        \"COCO-Detection/faster_rcnn_R_50_DC5_1x\": \"137847829/model_final_51d356.pkl\",\n        \"COCO-Detection/faster_rcnn_R_50_FPN_1x\": \"137257794/model_final_b275ba.pkl\",\n        \"COCO-Detection/faster_rcnn_R_50_C4_3x\": \"137849393/model_final_f97cb7.pkl\",\n        \"COCO-Detection/faster_rcnn_R_50_DC5_3x\": \"137849425/model_final_68d202.pkl\",\n        \"COCO-Detection/faster_rcnn_R_50_FPN_3x\": \"137849458/model_final_280758.pkl\",\n        \"COCO-Detection/faster_rcnn_R_101_C4_3x\": \"138204752/model_final_298dad.pkl\",\n        \"COCO-Detection/faster_rcnn_R_101_DC5_3x\": \"138204841/model_final_3e0943.pkl\",",
        "type": "code",
        "location": "/tests/video_detector_tests/detectron2_model_zoo_url.py:2-20"
    },
    "4753": {
        "file_id": 611,
        "content": "The code defines a class \"_ModelZooUrls\" that provides a mapping between Detectron2 pre-trained model names and their respective URLs. It uses the \"S3_PREFIX\" to specify the base URL for downloading models and stores each model's path in the \"CONFIG_PATH_TO_URL_SUFFIX\" dictionary. The code includes various pre-trained COCO Detection models, such as Faster R-CNN with different architectures and scales.",
        "type": "comment"
    },
    "4754": {
        "file_id": 611,
        "content": "        \"COCO-Detection/faster_rcnn_R_101_FPN_3x\": \"137851257/model_final_f6e8b1.pkl\",\n        \"COCO-Detection/faster_rcnn_X_101_32x8d_FPN_3x\": \"139173657/model_final_68b088.pkl\",\n        # COCO Detection with RetinaNet\n        \"COCO-Detection/retinanet_R_50_FPN_1x\": \"190397773/model_final_bfca0b.pkl\",\n        \"COCO-Detection/retinanet_R_50_FPN_3x\": \"190397829/model_final_5bd44e.pkl\",\n        \"COCO-Detection/retinanet_R_101_FPN_3x\": \"190397697/model_final_971ab9.pkl\",\n        # COCO Detection with RPN and Fast R-CNN\n        \"COCO-Detection/rpn_R_50_C4_1x\": \"137258005/model_final_450694.pkl\",\n        \"COCO-Detection/rpn_R_50_FPN_1x\": \"137258492/model_final_02ce48.pkl\",\n        \"COCO-Detection/fast_rcnn_R_50_FPN_1x\": \"137635226/model_final_e5f7ce.pkl\",\n        # COCO Instance Segmentation Baselines with Mask R-CNN\n        \"COCO-InstanceSegmentation/mask_rcnn_R_50_C4_1x\": \"137259246/model_final_9243eb.pkl\",\n        \"COCO-InstanceSegmentation/mask_rcnn_R_50_DC5_1x\": \"137260150/model_final_4f86c3.pkl\",\n        \"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_1x\": \"137260431/model_final_a54504.pkl\",",
        "type": "code",
        "location": "/tests/video_detector_tests/detectron2_model_zoo_url.py:21-34"
    },
    "4755": {
        "file_id": 611,
        "content": "This code maps various Detectron2 models (e.g., faster_rcnn, retinanet, mask_rcnn) to their corresponding pre-trained model files stored in specific URLs or locations. These models are used for tasks like instance segmentation and detection on the COCO dataset.",
        "type": "comment"
    },
    "4756": {
        "file_id": 611,
        "content": "        \"COCO-InstanceSegmentation/mask_rcnn_R_50_C4_3x\": \"137849525/model_final_4ce675.pkl\",\n        \"COCO-InstanceSegmentation/mask_rcnn_R_50_DC5_3x\": \"137849551/model_final_84107b.pkl\",\n        \"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x\": \"137849600/model_final_f10217.pkl\",\n        \"COCO-InstanceSegmentation/mask_rcnn_R_101_C4_3x\": \"138363239/model_final_a2914c.pkl\",\n        \"COCO-InstanceSegmentation/mask_rcnn_R_101_DC5_3x\": \"138363294/model_final_0464b7.pkl\",\n        \"COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x\": \"138205316/model_final_a3ec72.pkl\",\n        \"COCO-InstanceSegmentation/mask_rcnn_X_101_32x8d_FPN_3x\": \"139653917/model_final_2d9806.pkl\",  # noqa\n        # New baselines using Large-Scale Jitter and Longer Training Schedule\n        \"new_baselines/mask_rcnn_R_50_FPN_100ep_LSJ\": \"42047764/model_final_bb69de.pkl\",\n        \"new_baselines/mask_rcnn_R_50_FPN_200ep_LSJ\": \"42047638/model_final_89a8d3.pkl\",\n        \"new_baselines/mask_rcnn_R_50_FPN_400ep_LSJ\": \"42019571/model_final_14d201.pkl\",",
        "type": "code",
        "location": "/tests/video_detector_tests/detectron2_model_zoo_url.py:35-45"
    },
    "4757": {
        "file_id": 611,
        "content": "This code maps different model names to their corresponding checkpoint files in the Detectron2 Model Zoo. It includes COCO instance segmentation models and new baselines with Large-Scale Jitter and longer training schedules.",
        "type": "comment"
    },
    "4758": {
        "file_id": 611,
        "content": "        \"new_baselines/mask_rcnn_R_101_FPN_100ep_LSJ\": \"42025812/model_final_4f7b58.pkl\",\n        \"new_baselines/mask_rcnn_R_101_FPN_200ep_LSJ\": \"42131867/model_final_0bb7ae.pkl\",\n        \"new_baselines/mask_rcnn_R_101_FPN_400ep_LSJ\": \"42073830/model_final_f96b26.pkl\",\n        \"new_baselines/mask_rcnn_regnetx_4gf_dds_FPN_100ep_LSJ\": \"42047771/model_final_b7fbab.pkl\",  # noqa\n        \"new_baselines/mask_rcnn_regnetx_4gf_dds_FPN_200ep_LSJ\": \"42132721/model_final_5d87c1.pkl\",  # noqa\n        \"new_baselines/mask_rcnn_regnetx_4gf_dds_FPN_400ep_LSJ\": \"42025447/model_final_f1362d.pkl\",  # noqa\n        \"new_baselines/mask_rcnn_regnety_4gf_dds_FPN_100ep_LSJ\": \"42047784/model_final_6ba57e.pkl\",  # noqa\n        \"new_baselines/mask_rcnn_regnety_4gf_dds_FPN_200ep_LSJ\": \"42047642/model_final_27b9c1.pkl\",  # noqa\n        \"new_baselines/mask_rcnn_regnety_4gf_dds_FPN_400ep_LSJ\": \"42045954/model_final_ef3a80.pkl\",  # noqa\n        # COCO Person Keypoint Detection Baselines with Keypoint R-CNN\n        \"COCO-Keypoints/keypoint_rcnn_R_50_FPN_1x\": \"137261548/model_final_04e291.pkl\",",
        "type": "code",
        "location": "/tests/video_detector_tests/detectron2_model_zoo_url.py:46-56"
    },
    "4759": {
        "file_id": 611,
        "content": "This code defines a mapping of model names to their corresponding final pickle files. The models are Detectron2 COCO Person Keypoint Detection Baselines and include variations of Mask R-CNN, Mask R-CNN with RegNetX/Y, and the COCO-Keypoints Keypoint R-CNN.",
        "type": "comment"
    },
    "4760": {
        "file_id": 611,
        "content": "        \"COCO-Keypoints/keypoint_rcnn_R_50_FPN_3x\": \"137849621/model_final_a6e10b.pkl\",\n        \"COCO-Keypoints/keypoint_rcnn_R_101_FPN_3x\": \"138363331/model_final_997cc7.pkl\",\n        \"COCO-Keypoints/keypoint_rcnn_X_101_32x8d_FPN_3x\": \"139686956/model_final_5ad38f.pkl\",\n        # COCO Panoptic Segmentation Baselines with Panoptic FPN\n        \"COCO-PanopticSegmentation/panoptic_fpn_R_50_1x\": \"139514544/model_final_dbfeb4.pkl\",\n        \"COCO-PanopticSegmentation/panoptic_fpn_R_50_3x\": \"139514569/model_final_c10459.pkl\",\n        \"COCO-PanopticSegmentation/panoptic_fpn_R_101_3x\": \"139514519/model_final_cafdb1.pkl\",\n        # LVIS Instance Segmentation Baselines with Mask R-CNN\n        \"LVISv0.5-InstanceSegmentation/mask_rcnn_R_50_FPN_1x\": \"144219072/model_final_571f7c.pkl\",  # noqa\n        \"LVISv0.5-InstanceSegmentation/mask_rcnn_R_101_FPN_1x\": \"144219035/model_final_824ab5.pkl\",  # noqa\n        \"LVISv0.5-InstanceSegmentation/mask_rcnn_X_101_32x8d_FPN_1x\": \"144219108/model_final_5e3439.pkl\",  # noqa",
        "type": "code",
        "location": "/tests/video_detector_tests/detectron2_model_zoo_url.py:57-67"
    },
    "4761": {
        "file_id": 611,
        "content": "This code is a dictionary mapping model names to their corresponding checkpoint files. These models are for Detectron2's object detection, keypoint estimation, and segmentation tasks on COCO and LVIS datasets. The checkpoint files store the trained model parameters for each specific configuration.",
        "type": "comment"
    },
    "4762": {
        "file_id": 611,
        "content": "        # Cityscapes & Pascal VOC Baselines\n        \"Cityscapes/mask_rcnn_R_50_FPN\": \"142423278/model_final_af9cf5.pkl\",\n        \"PascalVOC-Detection/faster_rcnn_R_50_C4\": \"142202221/model_final_b1acc2.pkl\",\n        # Other Settings\n        \"Misc/mask_rcnn_R_50_FPN_1x_dconv_c3-c5\": \"138602867/model_final_65c703.pkl\",\n        \"Misc/mask_rcnn_R_50_FPN_3x_dconv_c3-c5\": \"144998336/model_final_821d0b.pkl\",\n        \"Misc/cascade_mask_rcnn_R_50_FPN_1x\": \"138602847/model_final_e9d89b.pkl\",\n        \"Misc/cascade_mask_rcnn_R_50_FPN_3x\": \"144998488/model_final_480dd8.pkl\",\n        \"Misc/mask_rcnn_R_50_FPN_3x_syncbn\": \"169527823/model_final_3b3c51.pkl\",\n        \"Misc/mask_rcnn_R_50_FPN_3x_gn\": \"138602888/model_final_dc5d9e.pkl\",\n        \"Misc/scratch_mask_rcnn_R_50_FPN_3x_gn\": \"138602908/model_final_01ca85.pkl\",\n        \"Misc/scratch_mask_rcnn_R_50_FPN_9x_gn\": \"183808979/model_final_da7b4c.pkl\",\n        \"Misc/scratch_mask_rcnn_R_50_FPN_9x_syncbn\": \"184226666/model_final_5ce33e.pkl\",\n        \"Misc/panoptic_fpn_R_101_dconv_cascade_gn_3x\": \"139797668/model_final_be35db.pkl\",",
        "type": "code",
        "location": "/tests/video_detector_tests/detectron2_model_zoo_url.py:68-81"
    },
    "4763": {
        "file_id": 611,
        "content": "This code defines model configurations and their corresponding checkpoint file paths for various tasks like Cityscapes, Pascal VOC detection, and panoptic segmentation. These configurations include different architectures and training strategies such as syncBN and gn. The checkpoint files store the trained model parameters which can be loaded to replicate the results.",
        "type": "comment"
    },
    "4764": {
        "file_id": 611,
        "content": "        \"Misc/cascade_mask_rcnn_X_152_32x8d_FPN_IN5k_gn_dconv\": \"18131413/model_0039999_e76410.pkl\",  # noqa\n        # D1 Comparisons\n        \"Detectron1-Comparisons/faster_rcnn_R_50_FPN_noaug_1x\": \"137781054/model_final_7ab50c.pkl\",  # noqa\n        \"Detectron1-Comparisons/mask_rcnn_R_50_FPN_noaug_1x\": \"137781281/model_final_62ca52.pkl\",  # noqa\n        \"Detectron1-Comparisons/keypoint_rcnn_R_50_FPN_1x\": \"137781195/model_final_cce136.pkl\",\n    }\n    @staticmethod\n    def query(config_path: str) -> Optional[str]:\n        \"\"\"\n        Args:\n            config_path: relative config filename\n        \"\"\"\n        name = config_path.replace(\".yaml\", \"\").replace(\".py\", \"\")\n        if name in _ModelZooUrls.CONFIG_PATH_TO_URL_SUFFIX:\n            suffix = _ModelZooUrls.CONFIG_PATH_TO_URL_SUFFIX[name]\n            return _ModelZooUrls.S3_PREFIX + name + \"/\" + suffix\n        return None\ndef get_checkpoint_url(config_path):\n    \"\"\"\n    Returns the URL to the model trained using the given config\n    Args:\n        config_path (str): config file name relative to detectron2's \"configs/\"",
        "type": "code",
        "location": "/tests/video_detector_tests/detectron2_model_zoo_url.py:82-107"
    },
    "4765": {
        "file_id": 611,
        "content": "This code provides a function to query the model URL and checkpoint from a given configuration path. It maps specific configurations to their respective URL suffixes and uses them to generate the model's URL, including a prefix. The function returns the URL if a valid mapping is found; otherwise, it returns None.",
        "type": "comment"
    },
    "4766": {
        "file_id": 611,
        "content": "            directory, e.g., \"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_1x.yaml\"\n    Returns:\n        str: a URL to the model\n    \"\"\"\n    url = _ModelZooUrls.query(config_path)\n    if url is None:\n        raise RuntimeError(\"Pretrained model for {} is not available!\".format(config_path))\n    return url\nif __name__ == \"__main__\":\n    test_config = \"Detectron1-Comparisons/mask_rcnn_R_50_FPN_noaug_1x\"\n    url = get_checkpoint_url(test_config)\n    print(\"model name:\",test_config)\n    print(\"model url:\",url)",
        "type": "code",
        "location": "/tests/video_detector_tests/detectron2_model_zoo_url.py:108-122"
    },
    "4767": {
        "file_id": 611,
        "content": "This code defines a function `get_checkpoint_url` that returns the URL of a pretrained model based on its configuration path. It also checks if a valid URL exists for the given config, and raises an error if not. The provided example demonstrates how to use this function with a specific config, printing the model name and URL.",
        "type": "comment"
    },
    "4768": {
        "file_id": 612,
        "content": "/tests/video_detector_tests/cocoNames.py",
        "type": "filepath"
    },
    "4769": {
        "file_id": 612,
        "content": "The code defines two dictionaries, \"cocoName\" and \"cocoRealName\", used for image classification tasks based on the MS COCO dataset. It maps labels to object names and indexes respectively, correcting 0-indexing in dataset labels.",
        "type": "summary"
    },
    "4770": {
        "file_id": 612,
        "content": "cocoName = {0: '__background__',\n\t 1: 'person',\n\t 2: 'bicycle',\n\t 3: 'car',\n\t 4: 'motorcycle',\n\t 5: 'airplane',\n\t 6: 'bus',\n\t 7: 'train',\n\t 8: 'truck',\n\t 9: 'boat',\n\t 10: 'traffic light',\n\t 11: 'fire hydrant',\n\t 12: 'stop sign',\n\t 13: 'parking meter',\n\t 14: 'bench',\n\t 15: 'bird',\n\t 16: 'cat',\n\t 17: 'dog',\n\t 18: 'horse',\n\t 19: 'sheep',\n\t 20: 'cow',\n\t 21: 'elephant',\n\t 22: 'bear',\n\t 23: 'zebra',\n\t 24: 'giraffe',\n\t 25: 'backpack',\n\t 26: 'umbrella',\n\t 27: 'handbag',\n\t 28: 'tie',\n\t 29: 'suitcase',\n\t 30: 'frisbee',\n\t 31: 'skis',\n\t 32: 'snowboard',\n\t 33: 'sports ball',\n\t 34: 'kite',\n\t 35: 'baseball bat',\n\t 36: 'baseball glove',\n\t 37: 'skateboard',\n\t 38: 'surfboard',\n\t 39: 'tennis racket',\n\t 40: 'bottle',\n\t 41: 'wine glass',\n\t 42: 'cup',\n\t 43: 'fork',\n\t 44: 'knife',\n\t 45: 'spoon',\n\t 46: 'bowl',\n\t 47: 'banana',\n\t 48: 'apple',\n\t 49: 'sandwich',\n\t 50: 'orange',\n\t 51: 'broccoli',\n\t 52: 'carrot',\n\t 53: 'hot dog',\n\t 54: 'pizza',\n\t 55: 'donut',\n\t 56: 'cake',\n\t 57: 'chair',\n\t 58: 'couch',\n\t 59: 'potted plant',\n\t 60: 'bed',\n\t 61: 'dining table',\n\t 62: 'toilet',\n\t 63: 'tv',",
        "type": "code",
        "location": "/tests/video_detector_tests/cocoNames.py:1-64"
    },
    "4771": {
        "file_id": 612,
        "content": "This code defines a dictionary named \"cocoName\" that maps integer labels to object names, used for image classification tasks based on the MS COCO dataset.",
        "type": "comment"
    },
    "4772": {
        "file_id": 612,
        "content": "\t 64: 'laptop',\n\t 65: 'mouse',\n\t 66: 'remote',\n\t 67: 'keyboard',\n\t 68: 'cell phone',\n\t 69: 'microwave',\n\t 70: 'oven',\n\t 71: 'toaster',\n\t 72: 'sink',\n\t 73: 'refrigerator',\n\t 74: 'book',\n\t 75: 'clock',\n\t 76: 'vase',\n\t 77: 'scissors',\n\t 78: 'teddy bear',\n\t 79: 'hair drier',\n\t 80: 'toothbrush'}\ncocoRealName = {k-1:cocoName[k] for k in cocoName.keys()}",
        "type": "code",
        "location": "/tests/video_detector_tests/cocoNames.py:65-83"
    },
    "4773": {
        "file_id": 612,
        "content": "The code defines a dictionary named \"cocoRealName\" that maps object names to corresponding COCO indexes. It uses a dictionary comprehension to subtract 1 from each key in the original \"cocoName\" dictionary, which assumes an offset of 0-indexing in the dataset labels.",
        "type": "comment"
    },
    "4774": {
        "file_id": 613,
        "content": "/tests/video_detector_tests/siamMask/setup.sh",
        "type": "filepath"
    },
    "4775": {
        "file_id": 613,
        "content": "Downloading and setting up SiamMask from GitHub, then retrieving pre-trained model files for VOT and DAVIS datasets.",
        "type": "summary"
    },
    "4776": {
        "file_id": 613,
        "content": "git clone https://github.com/foolwood/SiamMask.git && cd SiamMask\nexport SiamMask=$PWD\ncd $SiamMask/experiments/siammask_sharp\nwget http://www.robots.ox.ac.uk/~qwang/SiamMask_VOT.pth\nwget http://www.robots.ox.ac.uk/~qwang/SiamMask_VOT_LD.pth\nwget http://www.robots.ox.ac.uk/~qwang/SiamMask_DAVIS.pth",
        "type": "code",
        "location": "/tests/video_detector_tests/siamMask/setup.sh:1-6"
    },
    "4777": {
        "file_id": 613,
        "content": "Downloading and setting up SiamMask from GitHub, then retrieving pre-trained model files for VOT and DAVIS datasets.",
        "type": "comment"
    },
    "4778": {
        "file_id": 614,
        "content": "/tests/video_detector_tests/siamMask/demo.sh",
        "type": "filepath"
    },
    "4779": {
        "file_id": 614,
        "content": "This script changes directory to \"SiamMask\" and sets environment variables for running a SiamMask demo using Python 3. It resumes from the \"SiamMask_DAVIS.pth\" file with configuration from \"config_davis.json\".",
        "type": "summary"
    },
    "4780": {
        "file_id": 614,
        "content": "cd SiamMask\nexport SiamMask=$PWD\n# cd $SiamMask/experiments/siammask_sharp\n# cd $SiamMask/experiments/siammask_sharp\n# export PYTHONPATH=$PWD:$PYTHONPATH\n# which python3\npython3 -m tools.demo --resume experiments/siammask_sharp/SiamMask_DAVIS.pth --config experiments/siammask_sharp/config_davis.json",
        "type": "code",
        "location": "/tests/video_detector_tests/siamMask/demo.sh:1-7"
    },
    "4781": {
        "file_id": 614,
        "content": "This script changes directory to \"SiamMask\" and sets environment variables for running a SiamMask demo using Python 3. It resumes from the \"SiamMask_DAVIS.pth\" file with configuration from \"config_davis.json\".",
        "type": "comment"
    },
    "4782": {
        "file_id": 615,
        "content": "/tests/tkinter_tag_toggle_button/toggle_button.py",
        "type": "filepath"
    },
    "4783": {
        "file_id": 615,
        "content": "This code uses Tkinter to create buttons for toggling video tags and feeds the information into the main logic using mlt xml format.",
        "type": "summary"
    },
    "4784": {
        "file_id": 615,
        "content": "# Import Module\nfrom tkinter import *\n# Create Object\nroot = Tk()\n# Add Title\nroot.title('On/Off Switch!')\n# Add Geometry\nroot.geometry(\"500x300\")\n# Keep track of the button state on/off\n#global is_on\nis_on = {\"myTag\":False,\"myTag2\":False,\"myTag3\":False}\n# Create Label\n# Define our switch function\ndef switch(key, buttons, index, is_on):\n    button = buttons[index]\n    if is_on[key]:\n        button.config(text=key ,bg = \"grey\",fg=\"black\")\n        is_on[key] = False\n    else:\n        button.config(text = key,bg = \"green\",fg=\"white\")\n        is_on[key] = True\n# Define Our Images\n# on = PhotoImage(file = \"on.png\")\n# off = PhotoImage(file = \"off.png\")\n# Create A Button\non_buttons = []\nmfunctions = []\n# for j in range(n):\n#     e = Button(my_w, text=j) \n#     e.grid(row=i, column=j) \ndef getSwitchLambda(text, on_buttons, index, is_on):\n    return lambda:switch(text, on_buttons, index, is_on)\nfor index, text in enumerate(is_on.keys()):\n    # print(\"TEXT:\", text)\n    on_buttons.append(Button(root, text=text, bd = 0,bg=\"grey\",fg=\"black\"))",
        "type": "code",
        "location": "/tests/tkinter_tag_toggle_button/toggle_button.py:1-46"
    },
    "4785": {
        "file_id": 615,
        "content": "Code imports the Tkinter module, creates a root window with title and geometry, defines a global dictionary to track button states, and defines a function to switch button text and colors based on its state. It also includes a placeholder for creating buttons with images for \"on\" and \"off\" states, but they are currently not implemented. A separate function is defined to create buttons with lambda functions that call the switch function when clicked. The loop creates buttons with their respective texts and sets initial state according to the dictionary.",
        "type": "comment"
    },
    "4786": {
        "file_id": 615,
        "content": "    mfunctions.append(getSwitchLambda(text, on_buttons, index, is_on))\n    on_buttons[index].config(command=mfunctions[index])\n    on_buttons[index].grid(row=1, column=0+index)\n# for x in mfunctions: x()\n# def getLambda(x): return lambda:print(x)\n# # great. much fucking better.\n# for y in [getLambda(x) for x in range(3)]: y()\n# so that is what's weird about the freaking lambda!\n# on_button1 = Button(root, text=\"myTag2\", bd = 0,bg=\"grey\",fg=\"black\")\n# # on_button1.command = lambda:switch(key=\"myTag\", button=on_button1)\n# on_button1.config(command=lambda:switch(key=\"myTag2\", button=on_button1))\n# on_button1.pack(pady = 50)\n# Execute Tkinter\nroot.mainloop()\n# so we would also like to use shotcut to manually cut videos and feed that info into the main production logic, by means of mlt xml.",
        "type": "code",
        "location": "/tests/tkinter_tag_toggle_button/toggle_button.py:47-67"
    },
    "4787": {
        "file_id": 615,
        "content": "This code creates Tkinter buttons for toggling tags and configures them with lambda functions. It then executes the Tkinter event loop to display the buttons. The purpose is to allow users to manually cut videos and feed that information into the main production logic using mlt xml format.",
        "type": "comment"
    },
    "4788": {
        "file_id": 616,
        "content": "/tests/title_rewrite_paraphrase/test_local.py",
        "type": "filepath"
    },
    "4789": {
        "file_id": 616,
        "content": "The code defines a paraphrasing function using tokenizer, model, sample and top_p options. The displayed elapsed time indicates acceptable performance for the task.",
        "type": "summary"
    },
    "4790": {
        "file_id": 616,
        "content": "# \nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\nmodelID = \"ClueAI/PromptCLUE-base-v1-5\"\n# https://github.com/James4Ever0/transformers/blob/main/src/transformers/models/auto/configuration_auto.py\n# https://github.com/James4Ever0/transformers/blob/main/src/transformers/modeling_utils.py (need change)\ntokenizer = T5Tokenizer.from_pretrained(modelID, local_files_first=True)\nmodel = T5ForConditionalGeneration.from_pretrained(\n    modelID, local_files_first=True\n)  # oh shit! 1G model\n# print(\"TOKENIZER?\", tokenizer) # always cpu. no \"device\" attribute.\n# print(\"_\"*20)\n# print(\"MODEL?\", model.device)\n# breakpoint()\n# what are these devices? all default CPU?\ndef preprocess(text):\n    return text.replace(\"\\n\", \"_\")\ndef postprocess(text):\n    return text.replace(\"_\", \"\\n\")\ndef answer(text, sample=True, top_p=0.8, device=\"cpu\"):\n    \"\"\"sampleTrue;\n    top_p0-1\"\"\"\n    text = preprocess(text)\n    encoding = tokenizer(\n        text=[text], truncation=True, padding=True, max_length=768, return_tensors=\"pt\"",
        "type": "code",
        "location": "/tests/title_rewrite_paraphrase/test_local.py:1-33"
    },
    "4791": {
        "file_id": 616,
        "content": "Loading T5 tokenizer and model with specified ID from local files first. Preprocessing function replaces newline characters with underscores, while postprocessing does the opposite. Function answer generates text using tokenizer, model, sample option, top_p value, and specified device (default: CPU).",
        "type": "comment"
    },
    "4792": {
        "file_id": 616,
        "content": "    ).to(device)\n    if not sample:\n        out = model.generate(\n            **encoding,\n            return_dict_in_generate=True,\n            output_scores=False,\n            max_length=128,\n            num_beams=4,\n            length_penalty=1\n        )\n    else:\n        out = model.generate(  # check \"generate_config\" in test.py?\n            **encoding,\n            return_dict_in_generate=True,\n            output_scores=False,\n            max_length=128,\n            min_length=5,\n            do_sample=True,\n            length_penalty=1,\n            num_beams=4,\n            top_p=top_p\n        )\n    out_text = tokenizer.batch_decode(out[\"sequences\"], skip_special_tokens=True)\n    return postprocess(out_text[0])\ndef my_function():\n    # Function code goes here\n    q = \"\"\"\n\n\n\"\"\"  # i think this model just doesn't get it.\n    output = answer(q)\n    print(\"Output:\", output)\nimport timeit\n# Time the function\nelapsed_time = timeit.timeit(my_function, number=1)\nprint(\"Elapsed time:\", elapsed_time)",
        "type": "code",
        "location": "/tests/title_rewrite_paraphrase/test_local.py:34-75"
    },
    "4793": {
        "file_id": 616,
        "content": "The code defines a function that generates text using a model, specifically for the purpose of paraphrasing or rewriting sentences. The model takes an input sentence and outputs a generated response. The function also measures the elapsed time to execute the code.",
        "type": "comment"
    },
    "4794": {
        "file_id": 616,
        "content": "# Elapsed time: 10.513529631891288\n# not too bad?",
        "type": "code",
        "location": "/tests/title_rewrite_paraphrase/test_local.py:76-77"
    },
    "4795": {
        "file_id": 616,
        "content": "These lines are displaying the elapsed time for a certain task or operation, and indicating that it was completed within an acceptable range. The comment suggests that the performance of this specific action is considered satisfactory by the developer.",
        "type": "comment"
    },
    "4796": {
        "file_id": 617,
        "content": "/tests/title_rewrite_paraphrase/test_baidu_paraphrase.py",
        "type": "filepath"
    },
    "4797": {
        "file_id": 617,
        "content": "The code imports Baidu language models, defines functions for detecting and translating languages, and uses them to paraphrase text by randomly selecting intermediate languages. It employs the baiduParaphraserByTranslation function for iterative translation through multiple languages, with optional depth limit and debug mode.",
        "type": "summary"
    },
    "4798": {
        "file_id": 617,
        "content": "from functools import lru_cache\nimport paddlehub as hub\n@lru_cache(maxsize=1)\ndef getBaiduLanguageTranslationModel():\n    language_translation_model = hub.Module(name=\"baidu_translate\")\n    return language_translation_model\n@lru_cache(maxsize=1)\ndef getBaiduLanguageRecognitionModel():\n    language_recognition_model = hub.Module(name=\"baidu_language_recognition\")\n    return language_recognition_model\nBAIDU_API_SLEEP_TIME = 1\nBAIDU_TRANSLATOR_LOCK_FILE = (\n    \"/root/Desktop/works/pyjom/tests/karaoke_effects/baidu_translator.lock\"\n)\ndef baidu_lang_detect(\n    content: str, sleep=BAIDU_API_SLEEP_TIME, lock_file=BAIDU_TRANSLATOR_LOCK_FILE\n):  # target language must be chinese.\n    import filelock\n    lock = filelock.FileLock(lock_file)\n    with lock:\n        import time\n        time.sleep(sleep)\n        language_recognition_model = getBaiduLanguageRecognitionModel()\n        langid = language_recognition_model.recognize(content)\n        return langid\ndef baidu_translate(\n    content: str,\n    source: str,\n    target: str,",
        "type": "code",
        "location": "/tests/title_rewrite_paraphrase/test_baidu_paraphrase.py:1-41"
    },
    "4799": {
        "file_id": 617,
        "content": "The code imports necessary modules, caches Baidu language translation and recognition models for efficient usage, and defines two functions: `baidu_lang_detect` for detecting the language of a given content, and `baidu_translate` for translating source text to target text using the cached Baidu language translation model. The code also includes variables for API sleep time and lock file path.",
        "type": "comment"
    }
}