{
    "4300": {
        "file_id": 545,
        "content": "#     # hits = len(theHit)\n#     # breakpoint()\n#     # how to get document by id? wtf\n# def checkDuplicatedVideoAndInsertVector(\n#     collection,\n#     videoPath,\n#     threshold: float = 0.15,  # are you sure?\n#     insertDuplicatedVector: bool = True,\n#     debug: bool = True,\n# ):\n#     reloadMilvusCollection(collection)\n#     distances = getDistancesBySearchingDuplicatedVideoInMilvusByFile(\n#         collection, videoPath, debug=debug\n#     )\n#     minDistance = min(distances + [1])  # empty!\n#     duplicated = minDistance < threshold\n#     if insertDuplicatedVector or (not duplicated):\n#         indexVideoWithVideoDurationAndPhashFromFile(\n#             collection, videoPath\n#         )  # anyway let's do this.\n#     return duplicated\n# shall we insert that vector or not, even if we have detected the duplicated media?\n# you choose.\nimport sys\nimport os\n# os.chdir(\"../../\")\nsys.path.append(\"../../\")\n# ignore the global proxy now, we are not going to use that.\nos.environ[\"http_proxy\"] = \"\"\nos.environ[\"https_proxy\"] = \"\"",
        "type": "code",
        "location": "/tests/video_phash_deduplication/test_milvus_library.py:165-200"
    },
    "4301": {
        "file_id": 545,
        "content": "Function checkDuplicatedVideoAndInsertVector checks if a video file exists in Milvus collection and returns whether the video is duplicated or not. If insertDuplicatedVector is True, it indexes the video regardless of duplication status. The function uses getDistancesBySearchingDuplicatedVideoInMilvusByFile to find distances between the new video and existing videos in Milvus.",
        "type": "comment"
    },
    "4302": {
        "file_id": 545,
        "content": "from pyjom.videotoolbox import getMilvusVideoDeduplicationCollection,checkDuplicatedVideoAndInsertVector\nif __name__ == \"__main__\":\n    # connectMilvusDatabase()\n    collection = (\n        getMilvusVideoDeduplicationCollection()\n    )  # will not get existing collections\n    videoPaths = [\n        \"cute_cat_gif.mp4\",\n        \"cute_cat_gif.gif\",\n        \"cat_delogo.gif\",\n        \"/root/Desktop/works/pyjom/samples/video/dog_with_large_text.gif\",\n    ]\n    # for videoPath in videoPaths:\n    from lazero.utils.logger import sprint\n    for videoPath in videoPaths:\n        print(\"filepath: %s\" % videoPath)\n        duplicated = checkDuplicatedVideoAndInsertVector(collection, videoPath)\n        sprint(\"duplicated?\", duplicated)\n\"\"\"\nfilepath: cute_cat_gif.mp4\ndistances: [0.0, 0.11764705926179886, 0.11764705926179886, 0.7200000286102295, 0.7200000286102295, 0.7346938848495483, 0.7659574747085571, 0.7924528121948242]\n______________________________\nfilepath: cute_cat_gif.gif\ndistances: [0.0, 0.11764705926179886, 0.117647",
        "type": "code",
        "location": "/tests/video_phash_deduplication/test_milvus_library.py:201-226"
    },
    "4303": {
        "file_id": 545,
        "content": "The code connects to a Milvus database, retrieves the video deduplication collection, and checks if each given video path is already in the collection. It prints the file paths of the videos and whether they are duplicated or not using `checkDuplicatedVideoAndInsertVector` function from `lazero.utils.logger` module. The distances between the new video and existing ones in the database are also printed.",
        "type": "comment"
    },
    "4304": {
        "file_id": 545,
        "content": "05926179886, 0.7200000286102295, 0.7200000286102295, 0.7346938848495483, 0.7659574747085571, 0.7692307829856873]\n______________________________\nfilepath: cat_delogo.gif\ndistances: [0.0, 0.11764705926179886, 0.11764705926179886, 0.7200000286102295, 0.7200000286102295, 0.7346938848495483, 0.7659574747085571, 0.7924528121948242]\n______________________________\nfilepath: /root/Desktop/works/pyjom/samples/video/dog_with_large_text.gif\ndistances: [0.0, 0.6808510422706604, 0.6938775777816772, 0.6938775777816772, 0.739130437374115, 0.7692307829856873, 0.7924528121948242, 0.7924528121948242]\n______________________________\n\"\"\"",
        "type": "code",
        "location": "/tests/video_phash_deduplication/test_milvus_library.py:226-234"
    },
    "4305": {
        "file_id": 545,
        "content": "The code appears to be storing and comparing distances between video phashes for different files. Each line contains a file path followed by an array of distances, indicating the similarity of that video phash to other video phashes in the system. The lower the distance value, the more similar the videos are.",
        "type": "comment"
    },
    "4306": {
        "file_id": 546,
        "content": "/tests/video_phash_deduplication/test_milvus.py",
        "type": "filepath"
    },
    "4307": {
        "file_id": 546,
        "content": "The code demonstrates Milvus database operations, including creating a \"video\" collection, inserting data and performing searches. It is part of debugging process to retrieve documents by ID. The programmer is stuck and requires further investigation.",
        "type": "summary"
    },
    "4308": {
        "file_id": 546,
        "content": "# duplicate -> remove, do not insert\n# not duplicate -> get the data, insert\n# you want to clear the collection after this run?\n# import pymilvus\nfrom pymilvus import connections\nfrom functools import lru_cache\n@lru_cache(maxsize=1)\ndef connectMilvusDatabase(alias=\"default\", host=\"localhost\", port=\"19530\"):\n\tconnection = connections.connect(alias=alias, host=host, port=port)# can we reconnect?\n\tprint('milvus connected')\nconnectMilvusDatabase()\nconnectMilvusDatabase() # will not connect again.\ncollection_name = \"video_deduplication\"\nfrom pymilvus import Collection\n# Collection(collection_name)\n# remote this thing.\nfrom pymilvus import utility\ntry:\n    if utility.has_collection(collection_name):  # be prudent.\n        utility.drop_collection(collection_name)\nexcept:\n    import traceback\n    traceback.print_exc()\n    print(\"maybe the collection does not exist\")\nfrom pymilvus import CollectionSchema, FieldSchema, DataType\nvideo_semantic_id = FieldSchema(  # how to insert this shit without prior knowledge?\n    name=\"video_semantic_id\",",
        "type": "code",
        "location": "/tests/video_phash_deduplication/test_milvus.py:1-39"
    },
    "4309": {
        "file_id": 546,
        "content": "This code establishes a connection to a Milvus database, checks if the \"video_deduplication\" collection exists, and if so, removes it before creating a new one. The `connectMilvusDatabase` function sets up a connection with specified alias, host, and port (default values used in this code). The `utility.has_collection` and `utility.drop_collection` functions from the `pymilvus` utility module are used to check for and remove an existing collection named \"video_deduplication\". A `CollectionSchema` is defined for the new collection, specifying a field schema named \"video_semantic_id\".",
        "type": "comment"
    },
    "4310": {
        "file_id": 546,
        "content": "    dtype=DataType.INT64,\n    is_primary=True,  # if is primary, will do check for 'not duplicate' or something.\n    auto_id=True,  # no need for id generation.\n)\nvideo_length = FieldSchema(\n    name=\"video_length\",\n    dtype=DataType.FLOAT,\n)\nvideo_phash = FieldSchema(\n    name=\"video_phash\", dtype=DataType.BINARY_VECTOR, dim=64\n)  # 64\n# single dimension? no multi dimension support?\nschema = CollectionSchema(\n    fields=[video_semantic_id, video_length, video_phash],\n    description=\"Test video deduplication\",\n)\n# collection = Collection(\"video\")      # Get an existing collection.\ncollection = Collection(\n    name=collection_name,\n    schema=schema,\n    using=\"default\",\n    shards_num=2,\n)\n# is this demo collection?\n# seems hard to setup.\n# not started!\n# https://milvus.io/docs/v2.0.0/metric.md#binary\n# the metric is important to us.\nsearch_params = {\"metric_type\": \"Jaccard\", \"params\": {\"nprobe\": 10}}\nimport numpy as np\nqueryData = np.array(\n    [\n        [True, True, True, False, False, True, False, True],\n        [True, False, False, True, False, True, True, False],",
        "type": "code",
        "location": "/tests/video_phash_deduplication/test_milvus.py:40-76"
    },
    "4311": {
        "file_id": 546,
        "content": "This code is defining a schema for a Milvus collection, specifying the data types and field names. The schema includes video_semantic_id, video_length, and video_phash fields, which are used in video deduplication. The code creates a collection named \"video\" with 2 shards using the specified schema and sets the metric type for searching as Jaccard with nprobe parameter set to 10. It also imports numpy and creates queryData, which seems to be a binary vector.",
        "type": "comment"
    },
    "4312": {
        "file_id": 546,
        "content": "        [True, False, False, True, True, False, False, True],\n        [True, True, True, True, True, False, False, True],\n        [True, False, False, True, False, True, True, False],\n        [False, True, True, False, False, False, False, True],\n        [True, True, False, False, False, True, True, False],\n        [False, False, True, False, False, True, False, False],\n    ]\n)\nqueryData = queryData.reshape(-1).tolist()\nqueryData = [\"1\" if x else \"0\" for x in queryData]\nimport bitarray\nqueryData = bitarray.bitarray(\"\".join(queryData), endian=\"little\")\nqueryData2 = queryData.copy()\nqueryData2[1:4] = 0\nqueryData3 = queryData2.copy()\nqueryData2 = queryData2.tobytes()\nqueryData3[8:15] = 0\nqueryData3 = queryData3.tobytes()\nqueryData = queryData.tobytes()\n# dimension: 8*8=64\n# collection.insert([[1], [np.float32(3.5)], [queryData]])\n# collection.insert([[np.float32(3.5)], [queryData]])\n# for _ in range(8):\ncollection.insert([[np.float32(3.5)], [queryData]])\ncollection.insert([[np.float32(3.5)], [queryData2]])  # slight difference.",
        "type": "code",
        "location": "/tests/video_phash_deduplication/test_milvus.py:77-102"
    },
    "4313": {
        "file_id": 546,
        "content": "The code is preparing and inserting data into a Milvus collection. It creates binary representations of queryData, queryData2, and queryData3 (slightly different from queryData2), then inserts them along with a float value (np.float32(3.5)) into the collection, representing a 64-dimensional vector.",
        "type": "comment"
    },
    "4314": {
        "file_id": 546,
        "content": "collection.insert([[np.float32(3.5)], [queryData3]])  # more difference.\n# print(len(queryData), len(queryData)*8)\n# # print(queryData.shape)\n# breakpoint()\n# collection.load()\ncollection.insert([[np.float32(3.5)], [queryData]]) # still three.\n# can release even if not loaded.\ncollection.release() # unload.\ncollection.load()\n# make it into some library!\n# insert after load?\n# # 1,64\n# what is wrong? wtf?\n# queryData = queryData.tolist()\nresults = collection.search(\n    data=[queryData],  # this is the float dimension.\n    anns_field=\"video_phash\",\n    param=search_params,\n    output_fields=[\"video_length\"],\n    limit=10,\n    expr=\"video_length > 1.2 and video_length < 4\",\n    # expr='video_length < 1.2',\n)\ntheHit = results[0]\nprint(theHit)\n# so we can perform search without filtering afterwards.\n# results[0][0].entity.get('video_length')\n# print(results[0].ids)\n# now, we want to have the 'distance' parameter.\n# print(results[0])\n# print(theHit)\n# distances = theHit.distances\n# results = [x for x in theHit]\n# hits = len(theHit)",
        "type": "code",
        "location": "/tests/video_phash_deduplication/test_milvus.py:103-139"
    },
    "4315": {
        "file_id": 546,
        "content": "The code is inserting data into a collection, releasing and reloading it, performing a search based on specific parameters, and accessing the results. The purpose seems to be searching for video data within a database based on certain criteria, such as length, and extracting relevant information from the resulting hits.",
        "type": "comment"
    },
    "4316": {
        "file_id": 546,
        "content": "# breakpoint()\n# how to get document by id? wtf",
        "type": "code",
        "location": "/tests/video_phash_deduplication/test_milvus.py:140-141"
    },
    "4317": {
        "file_id": 546,
        "content": "This code appears to be part of a debugging process, where the programmer is trying to understand how to retrieve a document by its ID using the Milvus database. The \"breakpoint()\" comment suggests they are currently stuck or needing to pause execution for further investigation.",
        "type": "comment"
    },
    "4318": {
        "file_id": 547,
        "content": "/tests/video_phash_deduplication/test_image_hash_milvus_database_search.py",
        "type": "filepath"
    },
    "4319": {
        "file_id": 547,
        "content": "This code is initializing image paths and loading necessary libraries. It calculates image hashes for three different images using the phash function from imagehash library. It then creates a connection to a Milvus database and loads an existing collection.",
        "type": "summary"
    },
    "4320": {
        "file_id": 547,
        "content": "pic_0 = \"cat.png\"\npic_0_similar = \"cat3.png\"\npic_1 = \"/root/Desktop/works/pyjom/samples/image/dick.png\"\nfrom PIL import Image\n# >>> import imagehash\n# >>> hash = imagehash.average_hash(Image.open\nimport imagehash\npics = [pic_0, pic_0_similar, pic_1]\nhashs = [imagehash.phash(Image.open(pic)) for pic in pics]\n# dis0 = hashs[0]-hashs[1]\n# dis1 = hashs[1]-hashs[2]\n# print(dis0, dis1)\n# 0 24\n# 6 24\n# well, let's check?\n# print(hashs)\n# three truth tables.\n# 2^4 = 16, total 2*8 digits=16?\n# towhee(multimodal search like jina), haystack, milvus\n# import pymilvus\nfrom pymilvus import connections\nconnection = connections.connect(alias=\"default\", host=\"localhost\", port=\"19530\")\nfrom pymilvus import Collection\ncollection = Collection(\"book\")  # Get an existing collection.\ncollection.load()\n# seems hard to setup.\n# not started!",
        "type": "code",
        "location": "/tests/video_phash_deduplication/test_image_hash_milvus_database_search.py:1-37"
    },
    "4321": {
        "file_id": 547,
        "content": "This code is initializing image paths and loading necessary libraries. It calculates image hashes for three different images using the phash function from imagehash library. It then creates a connection to a Milvus database and loads an existing collection.",
        "type": "comment"
    },
    "4322": {
        "file_id": 548,
        "content": "/tests/video_phash_deduplication/test_image_hash.py",
        "type": "filepath"
    },
    "4323": {
        "file_id": 548,
        "content": "This code uses the Python Imaging Library (PIL) and imagehash module to calculate average color hash values for images. It opens three images, 'pic_0', 'pic_0_similar', and 'pic_1', calculates their color hash values using the phash function from imagehash, and then calculates the differences between the hashes of each pair of images. The code checks the types of the calculated hashes and prints the distances (dis0 and dis1) between the pairs of images. It also mentions a possible discrepancy in the expected number of digits for the distances.",
        "type": "summary"
    },
    "4324": {
        "file_id": 548,
        "content": "pic_0= 'cat.png'\npic_0_similar = \"cat3.png\"\npic_1 = \"/root/Desktop/works/pyjom/samples/image/dick.png\"\nfrom PIL import Image\n# >>> import imagehash\n# >>> hash = imagehash.average_hash(Image.open\nimport imagehash\npics = [pic_0, pic_0_similar, pic_1]\nhashs = [imagehash.phash(Image.open(pic)) for pic in pics]\ndis0 = hashs[0]-hashs[1]\ndis1 = hashs[1]-hashs[2]\n# 0 24\n# 6 24\n# well, let's check?\nprint([type(h) for h in hashs])\nbreakpoint()\nprint(dis0, dis1)\n# three truth tables.\n# 2^4 = 16, total 2*8 digits=16?",
        "type": "code",
        "location": "/tests/video_phash_deduplication/test_image_hash.py:1-23"
    },
    "4325": {
        "file_id": 548,
        "content": "This code uses the Python Imaging Library (PIL) and imagehash module to calculate average color hash values for images. It opens three images, 'pic_0', 'pic_0_similar', and 'pic_1', calculates their color hash values using the phash function from imagehash, and then calculates the differences between the hashes of each pair of images. The code checks the types of the calculated hashes and prints the distances (dis0 and dis1) between the pairs of images. It also mentions a possible discrepancy in the expected number of digits for the distances.",
        "type": "comment"
    },
    "4326": {
        "file_id": 549,
        "content": "/tests/video_phash_deduplication/README.md",
        "type": "filepath"
    },
    "4327": {
        "file_id": 549,
        "content": "Code detects identical video files and finds repeated intervals within each video file.",
        "type": "summary"
    },
    "4328": {
        "file_id": 549,
        "content": "two main problems, one is to detect identical video files, one is to find 'repeated interval' inside each other.",
        "type": "code",
        "location": "/tests/video_phash_deduplication/README.md:1-1"
    },
    "4329": {
        "file_id": 549,
        "content": "Code detects identical video files and finds repeated intervals within each video file.",
        "type": "comment"
    },
    "4330": {
        "file_id": 550,
        "content": "/tests/video_phash_deduplication/config_milvus.sh",
        "type": "filepath"
    },
    "4331": {
        "file_id": 550,
        "content": "This code sets up Milvus, a vector database, by creating its configuration directory and downloading the necessary server and log configurations before starting the Milvus services.",
        "type": "summary"
    },
    "4332": {
        "file_id": 550,
        "content": "# # Create Milvus file\n# $ mkdir -p /home/$USER/milvus/conf\n# $ cd /home/$USER/milvus/conf\n# $ wget https://raw.githubusercontent.com/milvus-io/milvus/v0.8.0/core/conf/demo/server_config.yaml\n# $ wget https://raw.githubusercontent.com/milvus-io/milvus/v0.8.0/core/conf/demo/log_config.conf\nsudo systemctl start milvus\nsudo systemctl start milvus-etcd\nsudo systemctl start milvus-minio",
        "type": "code",
        "location": "/tests/video_phash_deduplication/config_milvus.sh:1-8"
    },
    "4333": {
        "file_id": 550,
        "content": "This code sets up Milvus, a vector database, by creating its configuration directory and downloading the necessary server and log configurations before starting the Milvus services.",
        "type": "comment"
    },
    "4334": {
        "file_id": 551,
        "content": "/tests/still_watermark_auto_removal/test_auto_video_watermark_detection.sh",
        "type": "filepath"
    },
    "4335": {
        "file_id": 551,
        "content": "cd into the automatic-watermark-detection directory and execute the video_watermark_detection.py script for watermark detection in videos.",
        "type": "summary"
    },
    "4336": {
        "file_id": 551,
        "content": "cd automatic-watermark-detection\npython3 video_watermark_detection.py",
        "type": "code",
        "location": "/tests/still_watermark_auto_removal/test_auto_video_watermark_detection.sh:1-3"
    },
    "4337": {
        "file_id": 551,
        "content": "cd into the automatic-watermark-detection directory and execute the video_watermark_detection.py script for watermark detection in videos.",
        "type": "comment"
    },
    "4338": {
        "file_id": 552,
        "content": "/tests/still_watermark_auto_removal/maxRectangleSolver.py",
        "type": "filepath"
    },
    "4339": {
        "file_id": 552,
        "content": "The code defines checkOverlap for point validation and solves the maximum rectangle problem, iterating through candidate rectangles, detecting overlaps, and displaying the best candidate in red. It also sorts and prints top 5 areas.",
        "type": "summary"
    },
    "4340": {
        "file_id": 552,
        "content": "import sympy\nimport json\ndata = json.loads(open(\"test_special.json\", \"r\").read())\ncanvas = data[\"canvas\"]\nrectangles = data[\"rectangles\"]\ncanvasWidth, canvasHeight = canvas\nxValid = [0, canvasWidth]\nyValid = [0, canvasHeight]\nmRects = []\ndef checkContains(rect, point):\n    xPoints = [p[0] for p in rect]\n    yPoints = [p[1] for p in rect]\n    maxX, minX = max(xPoints), min(xPoints)\n    maxY, minY = max(yPoints), min(yPoints)\n    x, y = point\n    return x > minX and x < maxX and y > minY and y < maxY\n# def checkOverlapAsymmetric(rect0, rect1):\n#     for point in rect0:\n#         if checkContains(rect1, point):\n#             return True\n#         # also check for intersections?\n#     return False\n# Python program to check if rectangles overlap\nclass D2Point:\n    def __init__(self, x, y):\n        self.x = x\n        self.y = y\ndef getRectDiagonalPoints(rect):\n    xPoints = [p[0] for p in rect]\n    yPoints = [p[1] for p in rect]\n    maxX, minX = max(xPoints), min(xPoints)\n    maxY, minY = max(yPoints), min(yPoints)\n    p0, p1 = D2Point(minX, minY), D2Point(maxX, maxY)",
        "type": "code",
        "location": "/tests/still_watermark_auto_removal/maxRectangleSolver.py:1-43"
    },
    "4341": {
        "file_id": 552,
        "content": "Code reads JSON data containing a canvas and rectangles. It validates points, creates a function to check if two rectangles overlap, and defines D2Point class for storing 2D point coordinates.",
        "type": "comment"
    },
    "4342": {
        "file_id": 552,
        "content": "    return p0, p1\ndef do_overlap(l1, r1, l2, r2):\n    # if rectangle has area 0, no overlap\n    if l1.x == r1.x or l1.y == r1.y or r2.x == l2.x or l2.y == r2.y:\n        return False\n    # If one rectangle is on left side of other\n    if l1.x >= r2.x or l2.x >= r1.x:\n        return False\n    if l1.y >= r2.y or l2.y >= r1.y:\n        return False\n    return True\ndef checkOverlap(rect0, rect1):\n    return do_overlap(*getRectDiagonalPoints(rect0),*getRectDiagonalPoints(rect1))\nfor x, y, mWidth, mHeight in rectangles:\n    xValid.append(x)\n    xValid.append(x + mWidth)\n    yValid.append(y)\n    yValid.append(y + mHeight)\n    p0, p1, p2, p3 = (\n        (x, y),\n        (x + mWidth, y),\n        (x + mWidth, y + mHeight),\n        (x, y + mHeight),\n    )\n    # mRectangle = sympy.Polygon(p0,p1,p2,p3)\n    mRectangle = [p0, p1, p2, p3]\n    mRects.append(mRectangle)\ndef purify(xValid):\n    xValid = list(set(xValid))\n    xValid.sort()\n    return xValid\ndef checkOverlapAgainstRectList(rect, rectList):\n    for testRect in rectList:\n        if checkOverlap(rect, testRect):",
        "type": "code",
        "location": "/tests/still_watermark_auto_removal/maxRectangleSolver.py:44-85"
    },
    "4343": {
        "file_id": 552,
        "content": "This code defines a function checkOverlap which takes two rectangles and checks if they overlap. The function do_overlap is used to determine if the rectangles have any area of overlap. It also includes utility functions like getRectDiagonalPoints, purify, and checkOverlapAgainstRectList for manipulating and comparing rectangles. The code creates a list of rectangles and checks for overlaps between each rectangle and others in a separate list.",
        "type": "comment"
    },
    "4344": {
        "file_id": 552,
        "content": "            return True\n    return False\nxValid = purify(xValid)\nyValid = purify(yValid)\ntotalCandidates = []\ndef getRectArea(rect):\n    xPoints = [p[0] for p in rect]\n    yPoints = [p[1] for p in rect]\n    maxX, minX = max(xPoints), min(xPoints)\n    maxY, minY = max(yPoints), min(yPoints)\n    return (maxX - minX) * (maxY - minY)\nbestCandidate = None\nbestArea = 0\nfor ix0 in range(0, len(xValid)-1):\n    for ix1 in range(ix0+1, len(xValid)):\n        for iy0 in range(0, len(yValid)-1):\n            for iy1 in range(iy0+1, len(yValid)):\n                x0, x1, y0, y1 = xValid[ix0], xValid[ix1], yValid[iy0], yValid[iy1]\n                x, y = x0, y0\n                mWidth, mHeight = x1 - x, y1 - y\n                p0, p1, p2, p3 = (\n                    (x, y),\n                    (x + mWidth, y),\n                    (x + mWidth, y + mHeight),\n                    (x, y + mHeight),\n                )\n                rectCandidate = [p0, p1, p2, p3]\n                area = getRectArea(rectCandidate)\n                if area <= bestArea:",
        "type": "code",
        "location": "/tests/still_watermark_auto_removal/maxRectangleSolver.py:86-120"
    },
    "4345": {
        "file_id": 552,
        "content": "The code is implementing a maximum rectangle solver algorithm. It iterates over the x and y validated points to generate all possible rectangles, calculating their areas using the getRectArea function. The best candidate with the highest area is stored.",
        "type": "comment"
    },
    "4346": {
        "file_id": 552,
        "content": "                    continue\n                if checkOverlapAgainstRectList(rectCandidate, mRects):\n                    break\n                bestCandidate = rectCandidate.copy()\n                bestArea = area\n                # print(\"UPDATING:\",bestCandidate)\n                # print('AREA:', bestArea)\n                # totalCandidates.append(rectCandidate.copy())\nprint(\"final candidate:\", bestCandidate)\n# plot this?\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Rectangle\nfig, ax = plt.subplots()\nax.plot([canvasWidth, canvasHeight])\n# add rectangle to plot\ndef plotRect(ax, x, y, width, height, facecolor):\n    ax.add_patch(Rectangle((x, y), width, height, facecolor=facecolor, fill=True))\ndef rectToXYWH(rect):\n    xPoints = [p[0] for p in rect]\n    yPoints = [p[1] for p in rect]\n    maxX, minX = max(xPoints), min(xPoints)\n    maxY, minY = max(yPoints), min(yPoints)\n    x, y = minX, minY\n    width, height = (maxX - minX), (maxY - minY)\n    return x, y, width, height\nplotRect(ax,0,0,canvasWidth, canvasHeight,'black')",
        "type": "code",
        "location": "/tests/still_watermark_auto_removal/maxRectangleSolver.py:121-151"
    },
    "4347": {
        "file_id": 552,
        "content": "This code finds the maximum rectangle by iterating through candidate rectangles and checking for overlaps. It updates the best candidate and area if a better one is found, then plots the final candidate rectangle on a plot.",
        "type": "comment"
    },
    "4348": {
        "file_id": 552,
        "content": "for rect in mRects:\n    x,y, width, height = rectToXYWH(rect)\n    plotRect(ax,x,y,width,height,'white')\nplotRect(ax,*rectToXYWH(bestCandidate),'red')\n# display plot\nplt.show()\n# totalCandidates.sort(key = lambda rect: -getRectArea(rect))\n# for rect in totalCandidates[:5]:\n#     print(rect)",
        "type": "code",
        "location": "/tests/still_watermark_auto_removal/maxRectangleSolver.py:153-161"
    },
    "4349": {
        "file_id": 552,
        "content": "The code generates and plots rectangles using given coordinates and dimensions, with the best candidate rectangle displayed in red. It also sorts the total list of candidate rectangles by area and prints the top 5 areas.",
        "type": "comment"
    },
    "4350": {
        "file_id": 553,
        "content": "/tests/still_watermark_auto_removal/EAST-Detector-for-text-detection-using-OpenCV-master/README.md",
        "type": "filepath"
    },
    "4351": {
        "file_id": 553,
        "content": "EAST Detector code in OpenCV enables real-time, accurate text detection in natural scenes, addressing challenges like angles, lighting, and non-ideal surfaces. The project welcomes improvements via pull requests and is licensed under MIT.",
        "type": "summary"
    },
    "4352": {
        "file_id": 553,
        "content": "# EAST Detector for Text Detection\nOpenCV’s EAST(Efficient and Accurate Scene Text Detection ) text detector is a deep learning model, based on a novel architecture and training pattern. It is capable of \n- running at near real-time at 13 FPS on 720p images and \n- obtains state-of-the-art text detection accuracy.\n[Link to paper](https://arxiv.org/pdf/1704.03155.pdf)\nOpenCV’s text detector implementation of EAST is quite robust, capable of localizing text even when it’s blurred, reflective, or partially obscured.\nThere are many natural scene text detection challenges that have been described by Celine Mancas-Thillou and Bernard Gosselin in their excellent 2017 paper, [Natural Scene Text Understanding](https://www.tcts.fpms.ac.be/publications/regpapers/2007/VS_cmtbg2007.pdf) below:\n- **Image/sensor noise**: Sensor noise from a handheld camera is typically higher than that of a traditional scanner. Additionally, low-priced cameras will typically interpolate the pixels of raw sensors to produce real colors.",
        "type": "code",
        "location": "/tests/still_watermark_auto_removal/EAST-Detector-for-text-detection-using-OpenCV-master/README.md:1-13"
    },
    "4353": {
        "file_id": 553,
        "content": "Code for OpenCV's EAST Detector implementation for text detection, capable of real-time performance and high accuracy. Based on a novel architecture and training pattern, it can detect text even when blurred, reflective or partially obscured. The code addresses challenges like sensor noise from handheld cameras.",
        "type": "comment"
    },
    "4354": {
        "file_id": 553,
        "content": "- **Viewing angles**: Natural scene text can naturally have viewing angles that are not parallel to the text, making the text harder to recognize.\nBlurring: Uncontrolled environments tend to have blur, especially if the end user is utilizing a smartphone that does not have some form of stabilization.\n- **Lighting conditions**: We cannot make any assumptions regarding our lighting conditions in natural scene images. It may be near dark, the flash on the camera may be on, or the sun may be shining brightly, saturating the entire image.\n- **Resolution**: Not all cameras are created equal — we may be dealing with cameras with sub-par resolution.\n- **Non-paper objects**: Most, but not all, paper is not reflective (at least in context of paper you are trying to scan). Text in natural scenes may be reflective, including logos, signs, etc.\n- **Non-planar objects**: Consider what happens when you wrap text around a bottle — the text on the surface becomes distorted and deformed. While humans may sti",
        "type": "code",
        "location": "/tests/still_watermark_auto_removal/EAST-Detector-for-text-detection-using-OpenCV-master/README.md:15-24"
    },
    "4355": {
        "file_id": 553,
        "content": "The code discusses various challenges in natural scene text detection, such as viewing angles, lighting conditions, resolution, non-paper objects (like reflective surfaces), and non-planar objects (such as distorted text on curved surfaces). These factors make it difficult to recognize and extract text from natural scenes.",
        "type": "comment"
    },
    "4356": {
        "file_id": 553,
        "content": "ll be able to easily “detect” and read the text, our algorithms will struggle. We need to be able to handle such use cases.\n- **Unknown layout**: We cannot use any a priori information to give our algorithms “clues” as to where the text resides.\n## Contributing\nPull requests are welcome. For major changes, please open an issue first to discuss what you would like to change.\n### Thanks to [Adrian's Blog](https://www.pyimagesearch.com/2018/08/20/opencv-text-detection-east-text-detector/) for a comprehensive blog on EAST Detector.\n## License\n[MIT](https://choosealicense.com/licenses/mit/)",
        "type": "code",
        "location": "/tests/still_watermark_auto_removal/EAST-Detector-for-text-detection-using-OpenCV-master/README.md:24-35"
    },
    "4357": {
        "file_id": 553,
        "content": "This code is from the README file of a project that utilizes EAST Detector, an OpenCV-based text detection algorithm. The project aims to improve its algorithms' ability to handle complex text layouts and use cases. It welcomes pull requests for major changes and is licensed under MIT.",
        "type": "comment"
    },
    "4358": {
        "file_id": 554,
        "content": "/tests/still_watermark_auto_removal/EAST-Detector-for-text-detection-using-OpenCV-master/opencv_text_detection_image.py",
        "type": "filepath"
    },
    "4359": {
        "file_id": 554,
        "content": "The code loads an input image, preprocesses it, resizes using argparse, detects text with OpenCV and EAST Detector, calculates prediction time, filters low-confidence detections, extracts scores & geometrical data, applies non-maxima suppression, scales coordinates, draws bounding boxes on the original image, and displays it.",
        "type": "summary"
    },
    "4360": {
        "file_id": 554,
        "content": "# USAGE\n# python3 opencv_text_detection_image.py --image images/lebron_james.jpg --east frozen_east_text_detection.pb\n# import the necessary packages\nfrom imutils.object_detection import non_max_suppression\nimport numpy as np\nimport argparse\nimport time\nimport cv2\n# construct the argument parser and parse the arguments\nap = argparse.ArgumentParser()\nap.add_argument(\"-i\", \"--image\", type=str,\n                help=\"path to input image\")\nap.add_argument(\"-east\", \"--east\", type=str,\n                help=\"path to input EAST text detector\")\nap.add_argument(\"-c\", \"--min-confidence\", type=float, default=0.5,\n                help=\"minimum probability required to inspect a region\")\nap.add_argument(\"-w\", \"--width\", type=int, default=320,\n                help=\"resized image width (should be multiple of 32)\")\nap.add_argument(\"-e\", \"--height\", type=int, default=320,\n                help=\"resized image height (should be multiple of 32)\")\nargs = vars(ap.parse_args())\n# load the input image and grab the image dimensions\nimage = cv2.imread(args[\"image\"])",
        "type": "code",
        "location": "/tests/still_watermark_auto_removal/EAST-Detector-for-text-detection-using-OpenCV-master/opencv_text_detection_image.py:1-26"
    },
    "4361": {
        "file_id": 554,
        "content": "This code loads an input image and applies preprocessing steps. It utilizes the argparse module to accept command-line arguments, allowing users to specify the input image path and the East text detector model file. It also sets default values for confidence threshold and resized image dimensions, which can be overridden by command-line options.",
        "type": "comment"
    },
    "4362": {
        "file_id": 554,
        "content": "orig = image.copy()\n(H, W) = image.shape[:2]\n# set the new width and height and then determine the ratio in change\n# for both the width and height\n(newW, newH) = (args[\"width\"], args[\"height\"])\nrW = W / float(newW)\nrH = H / float(newH)\n# resize the image and grab the new image dimensions\nimage = cv2.resize(image, (newW, newH))\n(H, W) = image.shape[:2]\n# define the two output layer names for the EAST detector model that\n# we are interested -- the first is the output probabilities and the\n# second can be used to derive the bounding box coordinates of text\nlayerNames = [\n    \"feature_fusion/Conv_7/Sigmoid\",\n    \"feature_fusion/concat_3\"]\n# load the pre-trained EAST text detector\nprint(\"[INFO] loading EAST text detector...\")\nnet = cv2.dnn.readNet(args[\"east\"])\n# construct a blob from the image and then perform a forward pass of\n# the model to obtain the two output layer sets\nblob = cv2.dnn.blobFromImage(image, 1.0, (W, H),\n                             (123.68, 116.78, 103.94), swapRB=True, crop=False)\nstart = time.time()",
        "type": "code",
        "location": "/tests/still_watermark_auto_removal/EAST-Detector-for-text-detection-using-OpenCV-master/opencv_text_detection_image.py:27-55"
    },
    "4363": {
        "file_id": 554,
        "content": "This code performs image preprocessing, resizing and loads the EAST text detector model for text detection. It sets the original image copy, calculates new width and height based on arguments, resizes the image, gets updated dimensions, defines output layer names for the model, loads the pre-trained EAST text detector model, constructs a blob from the image, performs a forward pass of the model to obtain two output layers.",
        "type": "comment"
    },
    "4364": {
        "file_id": 554,
        "content": "net.setInput(blob)\n(scores, geometry) = net.forward(layerNames)\nend = time.time()\n# show timing information on text prediction\nprint(\"[INFO] text detection took {:.6f} seconds\".format(end - start))\n# grab the number of rows and columns from the scores volume, then\n# initialize our set of bounding box rectangles and corresponding\n# confidence scores\n(numRows, numCols) = scores.shape[2:4]\nrects = []\nconfidences = []\n# loop over the number of rows\nfor y in range(0, numRows):\n    # extract the scores (probabilities), followed by the geometrical\n    # data used to derive potential bounding box coordinates that\n    # surround text\n    scoresData = scores[0, 0, y]\n    xData0 = geometry[0, 0, y]\n    xData1 = geometry[0, 1, y]\n    xData2 = geometry[0, 2, y]\n    xData3 = geometry[0, 3, y]\n    anglesData = geometry[0, 4, y]\n    # loop over the number of columns\n    for x in range(0, numCols):\n        # if our score does not have sufficient probability, ignore it\n        if scoresData[x] < args[\"min_confidence\"]:\n            continue",
        "type": "code",
        "location": "/tests/still_watermark_auto_removal/EAST-Detector-for-text-detection-using-OpenCV-master/opencv_text_detection_image.py:56-86"
    },
    "4365": {
        "file_id": 554,
        "content": "Code performs text detection using OpenCV and EAST Detector. It calculates the time taken for text prediction, extracts scores and geometrical data, filters out low-confidence detections, and stores bounding box rectangles and corresponding confidence scores in lists.",
        "type": "comment"
    },
    "4366": {
        "file_id": 554,
        "content": "        # compute the offset factor as our resulting feature maps will\n        # be 4x smaller than the input image\n        (offsetX, offsetY) = (x * 4.0, y * 4.0)\n        # extract the rotation angle for the prediction and then\n        # compute the sin and cosine\n        angle = anglesData[x]\n        cos = np.cos(angle)\n        sin = np.sin(angle)\n        # use the geometry volume to derive the width and height of\n        # the bounding box\n        h = xData0[x] + xData2[x]\n        w = xData1[x] + xData3[x]\n        # compute both the starting and ending (x, y)-coordinates for\n        # the text prediction bounding box\n        endX = int(offsetX + (cos * xData1[x]) + (sin * xData2[x]))\n        endY = int(offsetY - (sin * xData1[x]) + (cos * xData2[x]))\n        startX = int(endX - w)\n        startY = int(endY - h)\n        # add the bounding box coordinates and probability score to\n        # our respective lists\n        rects.append((startX, startY, endX, endY))\n        confidences.append(scoresData[x])\n# apply non-maxima suppression to suppress weak, overlapping bounding",
        "type": "code",
        "location": "/tests/still_watermark_auto_removal/EAST-Detector-for-text-detection-using-OpenCV-master/opencv_text_detection_image.py:88-115"
    },
    "4367": {
        "file_id": 554,
        "content": "This code computes the bounding box coordinates and confidence scores for text predictions, using input data such as angles, offsets, xData values. It then applies non-maxima suppression to suppress weak overlapping bounding boxes, likely for further processing or object detection purposes.",
        "type": "comment"
    },
    "4368": {
        "file_id": 554,
        "content": "# boxes\nboxes = non_max_suppression(np.array(rects), probs=confidences)\n# loop over the bounding boxes\nfor (startX, startY, endX, endY) in boxes:\n    # scale the bounding box coordinates based on the respective\n    # ratios\n    startX = int(startX * rW)\n    startY = int(startY * rH)\n    endX = int(endX * rW)\n    endY = int(endY * rH)\n    # draw the bounding box on the image\n    cv2.rectangle(orig, (startX, startY), (endX, endY), (0, 255, 0), 2)\n# show the output image\ncv2.imshow(\"Text Detection\", orig)\ncv2.waitKey(0)",
        "type": "code",
        "location": "/tests/still_watermark_auto_removal/EAST-Detector-for-text-detection-using-OpenCV-master/opencv_text_detection_image.py:116-133"
    },
    "4369": {
        "file_id": 554,
        "content": "This code performs non-maximum suppression on bounding box coordinates, scales the coordinates based on image ratios, draws bounding boxes on the original image using OpenCV, and displays the resulting image.",
        "type": "comment"
    },
    "4370": {
        "file_id": 555,
        "content": "/tests/still_watermark_auto_removal/automatic-watermark-detection/video_watermark_detection.py",
        "type": "filepath"
    },
    "4371": {
        "file_id": 555,
        "content": "The code employs OpenCV and Deep Learning for watermark detection, estimation, and removal. It detects video watermarks using adaptive thresholding, applies Gaussian blur, scales, and draws boxes before saving the information to a JSON file.",
        "type": "summary"
    },
    "4372": {
        "file_id": 555,
        "content": "# sample few images from a video.\nimport random\n## we import our version of cv2 here? or uninstall and reinstall opencv-python with custom things?\nimport pathlib\nimport sys\nsite_path = pathlib.Path(\"/usr/local/lib/python3.9/site-packages\")\ncv2_libs_dir = site_path / 'cv2' / f'python-{sys.version_info.major}.{sys.version_info.minor}'\nprint(cv2_libs_dir)\ncv2_libs = sorted(cv2_libs_dir.glob(\"*.so\"))\nif len(cv2_libs) == 1:\n    print(\"INSERTING:\",cv2_libs[0].parent)\n    sys.path.insert(1, str(cv2_libs[0].parent))\nimport cv2\nimport progressbar as pb\nvideoPaths = [\n    \"/root/Desktop/works/pyjom/tests/still_watermark_auto_removal/kunfu_cat.mp4\", # bilibili animal video compilation\n    \"/root/Desktop/works/pyjom/tests/bilibili_practices/bilibili_video_translate/japan_day.webm\", # youtube animation with watermark\n    \"/root/Desktop/works/pyjom/samples/video/LiGHT3ZCi.mp4\", # animal video compilation with pip and large area of watermark\n]  # his watermark. scorpa.\nvideo_path = videoPaths[2]\n# will change this shit.\n# shall we downscale this thing?",
        "type": "code",
        "location": "/tests/still_watermark_auto_removal/automatic-watermark-detection/video_watermark_detection.py:1-25"
    },
    "4373": {
        "file_id": 555,
        "content": "The code imports necessary libraries, checks and inserts custom OpenCV library paths, defines a list of video paths, and selects the third video for processing.",
        "type": "comment"
    },
    "4374": {
        "file_id": 555,
        "content": "# video = cv2.\n# video_path = \"\"\n# long loading time since we are backing up.\nsample_count = 60\nvideo_cap = cv2.VideoCapture(video_path)\nfps = video_cap.get(cv2.CAP_PROP_FPS)  # 60.\nframe_count = int(video_cap.get(cv2.CAP_PROP_FRAME_COUNT))\nprint(frame_count)\nsample_indexs = [x for x in range(frame_count)]\nsample_indexs = random.sample(sample_indexs, sample_count)\n# import copy\nimageSet = []\nfor frame_index_counter in pb.progressbar(range(frame_count)):  # are you sure?\n    success, frame = video_cap.read()\n    if not success:\n        break\n    if frame_index_counter in sample_indexs:\n        imageSet.append(frame.copy())\nfrom src import *\ngx, gy, gxlist, gylist = estimate_watermark_imgSet(imageSet)\n# print(len(imageSet))\ncropped_gx, cropped_gy, watermark_location = crop_watermark(gx, gy, location=True)\nW_m = poisson_reconstruct(cropped_gx, cropped_gy)\nW_full = poisson_reconstruct(gx, gy)\nprint(cropped_gx.shape, cropped_gy.shape, W_m.shape)  # (50, 137, 3) may vary.\nprint(watermark_location)  # ((1022, 21), (1072, 158)) inverted x,y! hell.",
        "type": "code",
        "location": "/tests/still_watermark_auto_removal/automatic-watermark-detection/video_watermark_detection.py:27-62"
    },
    "4375": {
        "file_id": 555,
        "content": "This code reads frames from a video, randomly selects some frames to analyze for watermark detection, and then estimates the watermark using poisson reconstruction. The code also outputs the shape of the detected watermark and its location on the frames. It may not use progress bar properly and has an issue with inverted x and y coordinates for watermark location.",
        "type": "comment"
    },
    "4376": {
        "file_id": 555,
        "content": "# cv2.imshow(\"WATERMARK\",W_m)\n# cv2.imshow(\"WATERMARK_FULL\",W_full)\n# # remove the freaking watermark please?\n# cv2.waitKey(0)\n# east_net = \"/media/root/help/pyjom/tests/still_watermark_auto_removal/EAST-Detector-for-text-detection-using-OpenCV-master/frozen_east_text_detection.pb\"\n# net = cv2.dnn.readNet(east_net)\n# H,W = W_full.shape[:2]\n# newH = (H//32)*32\n# newW = (W//32)*32\n# rH, rW = H/float(newH), W/float(newW)\n# W_full = cv2.resize(W_full,(newW,newH))\nmaxval, minval = np.max(W_full), np.min(W_full)\nW_full = (W_full - minval) * (255 / (maxval - minval))  # is that necessary?\n# # print(,W_full.shape,W_full.dtype)\nW_full = W_full.astype(np.uint8)\n# # breakpoint()\n# newH,newW = W_full.shape[:2]\n# # 14.122540090957173 -17.575702620638673 (1080, 1920, 3) float64\n# # you even have negative values. what the fuck?\n# blob = cv2.dnn.blobFromImage(W_full, 1.0, (newW, newH), (123.68, 116.78, 103.94), swapRB=True, crop=False)\n# # start = time.time()\n# net.setInput(blob)\n# layerNames = [\n# \t\"feature_fusion/Conv_7/Sigmoid\",",
        "type": "code",
        "location": "/tests/still_watermark_auto_removal/automatic-watermark-detection/video_watermark_detection.py:64-91"
    },
    "4377": {
        "file_id": 555,
        "content": "The code is using OpenCV and TensorFlow to detect and remove a watermark from an input image. It resizes the input image, normalizes pixel values, preprocesses the image with a DNN (Deep Neural Network), and then sets the input for the network's feature extraction layer.",
        "type": "comment"
    },
    "4378": {
        "file_id": 555,
        "content": "# \t\"feature_fusion/concat_3\"]\n# (scores, geometry) = net.forward(layerNames)\n# def decode_predictions(scores, geometry,min_confidence=0.5):\n# \t# grab the number of rows and columns from the scores volume, then\n# \t# initialize our set of bounding box rectangles and corresponding\n# \t# confidence scores\n# \t(numRows, numCols) = scores.shape[2:4]\n# \trects = []\n# \tconfidences = []\n# \t# loop over the number of rows\n# \tfor y in range(0, numRows):\n# \t\t# extract the scores (probabilities), followed by the\n# \t\t# geometrical data used to derive potential bounding box\n# \t\t# coordinates that surround text\n# \t\tscoresData = scores[0, 0, y]\n# \t\txData0 = geometry[0, 0, y]\n# \t\txData1 = geometry[0, 1, y]\n# \t\txData2 = geometry[0, 2, y]\n# \t\txData3 = geometry[0, 3, y]\n# \t\tanglesData = geometry[0, 4, y]\n# \t\t# loop over the number of columns\n# \t\tfor x in range(0, numCols):\n# \t\t\t# if our score does not have sufficient probability,\n# \t\t\t# ignore it\n# \t\t\tif scoresData[x] < min_confidence:\n# \t\t\t\tcontinue\n# \t\t\t# compute the offset factor as our resulting feature",
        "type": "code",
        "location": "/tests/still_watermark_auto_removal/automatic-watermark-detection/video_watermark_detection.py:92-119"
    },
    "4379": {
        "file_id": 555,
        "content": "The code is decoding the predictions of a deep learning model for watermark detection. It iterates through the scores and geometrical data to extract bounding box coordinates and angles, discarding low-confidence predictions.",
        "type": "comment"
    },
    "4380": {
        "file_id": 555,
        "content": "# \t\t\t# maps will be 4x smaller than the input image\n# \t\t\t(offsetX, offsetY) = (x * 4.0, y * 4.0)\n# \t\t\t# extract the rotation angle for the prediction and\n# \t\t\t# then compute the sin and cosine\n# \t\t\tangle = anglesData[x]\n# \t\t\tcos = np.cos(angle)\n# \t\t\tsin = np.sin(angle)\n# \t\t\t# use the geometry volume to derive the width and height\n# \t\t\t# of the bounding box\n# \t\t\th = xData0[x] + xData2[x]\n# \t\t\tw = xData1[x] + xData3[x]\n# \t\t\t# compute both the starting and ending (x, y)-coordinates\n# \t\t\t# for the text prediction bounding box\n# \t\t\tendX = int(offsetX + (cos * xData1[x]) + (sin * xData2[x]))\n# \t\t\tendY = int(offsetY - (sin * xData1[x]) + (cos * xData2[x]))\n# \t\t\tstartX = int(endX - w)\n# \t\t\tstartY = int(endY - h)\n# \t\t\t# add the bounding box coordinates and probability score\n# \t\t\t# to our respective lists\n# \t\t\trects.append((startX, startY, endX, endY))\n# \t\t\tconfidences.append(scoresData[x])\n# \t# return a tuple of the bounding boxes and associated confidences\n# \treturn (rects, confidences)\n# (rects, confidences) = decode_predictions(scores, geometry)",
        "type": "code",
        "location": "/tests/still_watermark_auto_removal/automatic-watermark-detection/video_watermark_detection.py:120-144"
    },
    "4381": {
        "file_id": 555,
        "content": "This code block extracts the rotation angle, computes sin and cosine values, derives bounding box width and height from geometry volume, calculates starting and ending coordinates of text prediction bounding boxes, appends these coordinates to rects list, and probability scores to confidences list. Finally, it returns a tuple containing the bounding boxes and associated confidences.",
        "type": "comment"
    },
    "4382": {
        "file_id": 555,
        "content": "# from imutils.object_detection import non_max_suppression\n# boxes = non_max_suppression(np.array(rects), probs=confidences)\n# rW=rH=1\n# no box painting.\n# for (startX, startY, endX, endY) in boxes:\n#     # scale the bounding box coordinates based on the respective\n#     # ratios\n#     startX = int(startX * rW)\n#     startY = int(startY * rH)\n#     endX = int(endX * rW)\n#     endY = int(endY * rH)\n#     # draw the bounding box on the frame\n#     cv2.rectangle(W_full, (startX, startY), (endX, endY), (0, 255, 0), 2)\n# # you could implement your own watermark detector network so far. it is easy.\n# # maybe directly using optical flow and gradients will be prettier?\n# W_full\nsrc = W_full\nscale_percent = 50\n# calculate the 50 percent of original dimensions\nwidth = int(src.shape[1] * scale_percent / 100)\nheight = int(src.shape[0] * scale_percent / 100)\n# dsize\ndsize = (width, height)\n# resize image\noutput = cv2.resize(src, dsize)\ngray_output = cv2.cvtColor(output, cv2.COLOR_BGR2GRAY)\ngray_output = cv2.GaussianBlur(gray_output, (11, 3), 0)",
        "type": "code",
        "location": "/tests/still_watermark_auto_removal/automatic-watermark-detection/video_watermark_detection.py:146-181"
    },
    "4383": {
        "file_id": 555,
        "content": "The code performs non-maximum suppression on rectangles, scales bounding box coordinates based on aspect ratios, draws bounding boxes on frames, and resizes the image with 50% scale while converting it to grayscale and applying Gaussian blur.",
        "type": "comment"
    },
    "4384": {
        "file_id": 555,
        "content": "thresh_output = cv2.adaptiveThreshold(\n    gray_output, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2\n)\nthresh_output = 255 - thresh_output\n# cnts, hierachy = cv2.findContours(thresh_output,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE) # really freaking bad. we should invert this.\ncnts, hierachy = cv2.findContours(\n    thresh_output, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n)  # really freaking bad. we should invert this.\n# cv2.RETR_EXTERNAL\n[a, b] = output.shape[:2]\nmyMask = np.zeros(shape=[a, b], dtype=np.uint8)\n# this is for video watermarks. how about pictures? do we need to cut corners? how to find the freaking watermark again?\nfor cnt in cnts:\n    x, y, w, h = cv2.boundingRect(cnt)  # Draw the bounding box image=\n    # cv2.rectangle(output, (x,y), (x+w,y+h), (0,0,255),2)\n    cv2.rectangle(myMask, (x, y), (x + w, y + h), 255, -1)\ndilated_mask = cv2.GaussianBlur(myMask, (11, 11), 0)\ncv2.threshold(dilated_mask, 256 / 2, 255, cv2.THRESH_BINARY, dilated_mask)\ncnts2, hierachy2 = cv2.findContours(",
        "type": "code",
        "location": "/tests/still_watermark_auto_removal/automatic-watermark-detection/video_watermark_detection.py:183-206"
    },
    "4385": {
        "file_id": 555,
        "content": "This code applies adaptive thresholding to detect watermarks in a video. It then identifies contours and creates a mask using bounding boxes, applies Gaussian blur, and finally finds the contours again for further processing.",
        "type": "comment"
    },
    "4386": {
        "file_id": 555,
        "content": "    dilated_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n)\nmyMask2 = np.zeros(shape=[a, b], dtype=np.uint8)\n# this is for video watermarks. how about pictures? do we need to cut corners? how to find the freaking watermark again?\nheight, width = myMask2.shape[:2]\nrectangles = []\nfor cnt in cnts2:\n    x, y, w, h = cv2.boundingRect(cnt)  # Draw the bounding box image=\n    # cv2.rectangle(output, (x,y), (x+w,y+h), (0,0,255),2)\n    rectangles.append((x,y,w,h))\n    cv2.rectangle(myMask2, (x, y), (x + w, y + h), 255, -1)\nimport json\ndata = {\"canvas\":(width, height), 'rectangles':rectangles}\ndataString = json.dumps(data)\nwith open(\"test.json\", 'w+') as f: f.write(dataString)\nprint(\"TOTAL {} CONTOURS.\".format(len(cnts2)))  # paint those contours.\n# cv2.imshow(\"IMAGE\",thresh_output)\ncv2.imshow(\"MPICTURE\", myMask2)\ncv2.waitKey(0)\n# fill those areas and you will get it.\n# how do we remove this shit?\n# also how do we remove other weird things? like floating watermarks?\n# print(imageSet[0].shape)\n# breakpoint()",
        "type": "code",
        "location": "/tests/still_watermark_auto_removal/automatic-watermark-detection/video_watermark_detection.py:207-239"
    },
    "4387": {
        "file_id": 555,
        "content": "Code creates a mask for video watermarks by detecting contours and drawing bounding boxes. It saves the mask information to a JSON file. The code also displays the mask as an image. The code is not specifically designed for images; modifications are needed to handle other formats like images with floating watermarks.",
        "type": "comment"
    },
    "4388": {
        "file_id": 556,
        "content": "/tests/taobao_guangguang_download_哇哦视频_淘宝逛逛_tiktok_douyin/wtf_is_this_shit.sh",
        "type": "filepath"
    },
    "4389": {
        "file_id": 556,
        "content": "Downloading and unzipping main.json from a URL, saving it as wtf_is_this2.",
        "type": "summary"
    },
    "4390": {
        "file_id": 556,
        "content": "# curl -o - https://g.alicdn.com/tnode/fullpageshortvideo/2.1.0/main.json.json | gunzip -c > wtf_is_this2\n# file wtf_is_this # data\n# gzipped.",
        "type": "code",
        "location": "/tests/taobao_guangguang_download_哇哦视频_淘宝逛逛_tiktok_douyin/wtf_is_this_shit.sh:1-3"
    },
    "4391": {
        "file_id": 556,
        "content": "Downloading and unzipping main.json from a URL, saving it as wtf_is_this2.",
        "type": "comment"
    },
    "4392": {
        "file_id": 557,
        "content": "/tests/taobao_guangguang_download_哇哦视频_淘宝逛逛_tiktok_douyin/view_weishi_capture_data.sh",
        "type": "filepath"
    },
    "4393": {
        "file_id": 557,
        "content": "This code runs MITM proxy with logs from \"logs.log\" and hits the URL for getting Weishi play page in a H5 environment.",
        "type": "summary"
    },
    "4394": {
        "file_id": 557,
        "content": "mitmproxy -r logs.log\n# http://h5.weishi.qq.com/webapp/json/weishi/WSH5GetPlayPage",
        "type": "code",
        "location": "/tests/taobao_guangguang_download_哇哦视频_淘宝逛逛_tiktok_douyin/view_weishi_capture_data.sh:1-2"
    },
    "4395": {
        "file_id": 557,
        "content": "This code runs MITM proxy with logs from \"logs.log\" and hits the URL for getting Weishi play page in a H5 environment.",
        "type": "comment"
    },
    "4396": {
        "file_id": 558,
        "content": "/tests/taobao_guangguang_download_哇哦视频_淘宝逛逛_tiktok_douyin/taobao_query.sh",
        "type": "filepath"
    },
    "4397": {
        "file_id": 558,
        "content": "The JSON data includes store and category information, login requirements, addresses, types for TMALL_MARKET_O2O, and user location settings/preferences with device info, geolocation details, and nested service options for online platforms.",
        "type": "summary"
    },
    "4398": {
        "file_id": 558,
        "content": "## require login?\n# {\n#   \"ret\": [\n#     \"FAIL_SYS_USER_VALIDATE::登陆成功后请点击重试获得查询结果!\"\n#   ],\n#   \"data\": {}\n# }⏎       \n# use h5api?\n# http://59.82.113.156/gw/\n# seems all the same.\ncurl -H \"x-sign: azYBCM002xAAKW%2BhAZ1epg3vUDRc2W%2BpbLkKnJg3oIA%2FZg%2Bl%2Be3cZFL5P4FTFDk8Lia77SVlqaBU41ulP5grHc7tzTlviW%2Bpb4lvqW\" -H \"x-sgext: JAF6pNmPrGQougwhMeo8qQ%3D%3D\" \"http://h5api.m.taobao.com/h5/mtop.taobao.wsearch.appsearch/1.0/?data=%7B%22LBS%22%3A%22%7B%5C%22TB%5C%22%3A%5C%22%7B%5C%5C%5C%22stores%5C%5C%5C%22%3A%5B%7B%5C%5C%5C%22code%5C%5C%5C%22%3A%5C%5C%5C%22236736190%5C%5C%5C%22%2C%5C%5C%5C%22bizType%5C%5C%5C%22%3A%5C%5C%5C%222%5C%5C%5C%22%2C%5C%5C%5C%22type%5C%5C%5C%22%3A%5C%5C%5C%2224%5C%5C%5C%22%7D%5D%7D%5C%22%2C%5C%22TMALL_MARKET_B2C%5C%22%3A%5C%22%7B%5C%5C%5C%22stores%5C%5C%5C%22%3A%5B%7B%5C%5C%5C%22code%5C%5C%5C%22%3A%5C%5C%5C%22107%5C%5C%5C%22%2C%5C%5C%5C%22bizType%5C%5C%5C%22%3A%5C%5C%5C%22REGION_TYPE_REGION%5C%5C%5C%22%2C%5C%5C%5C%22addrId%5C%5C%5C%22%3A%5C%5C%5C%229056332332%5C%5C%5C%22%2C%5C%",
        "type": "code",
        "location": "/tests/taobao_guangguang_download_哇哦视频_淘宝逛逛_tiktok_douyin/taobao_query.sh:1-11"
    },
    "4399": {
        "file_id": 558,
        "content": "Checking if login is required and using the h5api with the provided URL.",
        "type": "comment"
    }
}