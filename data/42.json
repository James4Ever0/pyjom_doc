{
    "4200": {
        "file_id": 531,
        "content": "    mod_x_half, mod_y_half = mod_x / 2, mod_y / 2\n    x, y, w, h = block_x - mod_x_half, block_y - mod_y_half, mod_x, mod_y\n    return tuple([int(elem) for elem in [x, y, w, h]])\ndef getBlockWeightFromBlockCenterCoordinates(blockCenterCoordinates):\n    mod_x, mod_y = getModXModYFromBlockCenterCoordinates(blockCenterCoordinates)\n    weights = mod_x * mod_y / 8 / 8\n    return weights\nimport progressbar\nimport numpy as np\n# max_dst_x, max_dst_y = 0,0\ndef averageMotionVectors(motion_vector_list):\n    if len(motion_vector_list) == 0:\n        average_tuple = (0, 0)\n    if len(motion_vector_list) > 1:\n        marray = np.array(motion_vector_list)\n        # print(\"MAKING AVERAGE:\")\n        # print(marray)\n        average = np.average(marray, axis=0)\n        # breakpoint()\n        average_tuple = tuple(average)\n    else:\n        average_tuple = tuple(motion_vector_list[0])\n    return average_tuple\nmotion_area_ratio_array = []\n# average_weighted_motion_vector_array = []\n# average_global_weighted_motion_vector_array = []\naverage_weighted_motion_vector_cartesian_array = []",
        "type": "code",
        "location": "/tests/motion_vector_estimation/optical_flow_colored_blocks_convolution.py:107-142"
    },
    "4201": {
        "file_id": 531,
        "content": "Function `getBlockWeightFromBlockCenterCoordinates` calculates the weight of a block based on its center coordinates.\nThe `averageMotionVectors` function calculates the average motion vector from a list of motion vectors.\n`motion_area_ratio_array` is used to store area ratios for blocks, which will be used in calculations later.",
        "type": "comment"
    },
    "4202": {
        "file_id": 531,
        "content": "average_global_weighted_motion_vector_cartesian_array = []\naverage_weighted_motion_vectors_filtered_cartesian_distance_array = []\naverage_global_weighted_motion_vectors_filtered_cartesian_distance_array = []\nfor _ in progressbar.progressbar(range(framesCount)):\n    success, frame, motion_vectors, frame_type, timestamp = cap.read()\n    height, width, channels = frame.shape\n    # breakpoint()\n    if success:\n        # what is the content of this motion vector?\n        # print(motion_vectors)\n        # import pandas as pd\n        # df = pd.DataFrame(motion_vectors)\n        # df = pd.DataFrame(motion_vectors,index=['source_index','unk0','unk1','src_x','src_y','dst_x','dst_y','motion_x','motion_y','motion_scale'])\n        # breakpoint()\n        # print()\n        # print(\"_____________________________\")\n        condition = motion_vectors[:, 0] < 0\n        # print(condition)\n        # print(condition.shape)\n        # breakpoint()\n        motion_vectors_simplified = motion_vectors[condition, :][:, [0, 5, 6, 7, 8, 9]]",
        "type": "code",
        "location": "/tests/motion_vector_estimation/optical_flow_colored_blocks_convolution.py:143-164"
    },
    "4203": {
        "file_id": 531,
        "content": "This code calculates the motion vectors from video frames and filters them based on a condition. The condition checks if the x-component of the motion vector is less than 0, which may indicate horizontal movement. The code then selects specific columns (x, y coordinates, and scale) from the motion vectors that meet this condition for further processing.",
        "type": "comment"
    },
    "4204": {
        "file_id": 531,
        "content": "        motion_vectors_scale = motion_vectors_simplified[:, [5]]\n        motion_vectors_scale_inversed = 1 / motion_vectors_scale\n        motion_vectors_with_scale = motion_vectors_simplified[:, [3, 4]]\n        motion_vectors_scale_inversed_stacked = np.hstack(\n            [motion_vectors_scale_inversed] * 2\n        )\n        motion_vectors_restored = (\n            motion_vectors_scale_inversed_stacked * motion_vectors_with_scale\n        )  # just element wise?\n        # print('STACKED:', motion_vectors_scale_inversed_stacked.shape)\n        # print(\"WITH SCALE:\", motion_vectors_with_scale.shape)\n        # print(\"RESTORED:\",motion_vectors_restored.shape)\n        # print(motion_vectors_simplified.shape)\n        # print(motion_vectors_scale.shape)\n        # breakpoint()\n        motion_vectors_dest_coords_restored = np.hstack(\n            [motion_vectors_simplified[:, [1, 2]], motion_vectors_restored]\n        )\n        # motion_vectors_simplified = motion_vectors[:,[0,5,6,7,8]]\n        # motion_vectors_simplified_unique = np.unique(motion_vectors_simplified, axis=0)",
        "type": "code",
        "location": "/tests/motion_vector_estimation/optical_flow_colored_blocks_convolution.py:165-184"
    },
    "4205": {
        "file_id": 531,
        "content": "This code segment is involved in optical flow calculation. It performs scaling, inverse scaling, and stacking of motion vectors to restore the original motion vector array. The code then concatenates the destination coordinates with restored motion vectors. This process helps in improving the accuracy of motion vector estimation.",
        "type": "comment"
    },
    "4206": {
        "file_id": 531,
        "content": "        # print(motion_vectors_simplified_unique.shape, motion_vectors.shape)\n        # breakpoint()\n        motion_vectors_dict = {}\n        for mv in motion_vectors_dest_coords_restored:\n            # drop duplicates first!\n            (\n                dst_x,  # corresponding macro block.\n                dst_y,  # for destination only\n                motion_x,\n                motion_y,\n                # motion_scale,  # don't know what the fuck is wrong with the motion scale\n            ) = mv.tolist()\n            # say we just want source_index <0, aka mv compared to previous frame\n            # try:\n            #     assert motion_x / motion_scale == src_x - dst_x\n            #     assert motion_y / motion_scale == src_y - dst_y\n            # except:\n            #     print(src_x, dst_x, motion_x, motion_scale)\n            #     print(src_y, dst_y, motion_y, motion_scale)\n            #     print(\"*\" * 20)\n            # it will be inaccurate if we abandon this subpixel precision.\n            # if source_index >= 0:",
        "type": "code",
        "location": "/tests/motion_vector_estimation/optical_flow_colored_blocks_convolution.py:185-206"
    },
    "4207": {
        "file_id": 531,
        "content": "This code segment is extracting and processing motion vectors from a set of coordinates. It is checking for duplicates, performing calculations with source and destination coordinates, and possibly handling inaccuracies caused by subpixel precision. The code seems to be part of a larger process, as it includes debugging statements and references to variables that are not explicitly defined within the provided segment.",
        "type": "comment"
    },
    "4208": {
        "file_id": 531,
        "content": "            #     continue\n            # if dst_x>max_dst_x:\n            #     max_dst_x = dst_x\n            # if dst_y>max_dst_y:\n            #     max_dst_y = dst_y\n            destCoord = (dst_x, dst_y)\n            motion_vector = (motion_x, motion_y)\n            # print(destCoord)\n            # breakpoint()\n            if motion_vector == (0, 0):\n                # print(\"zero motion vector detected. skipping\")\n                # breakpoint()\n                continue\n            # print('destination coords:',destCoord)\n            # print('motion vector:',motion_vector)\n            motion_vectors_dict.update(\n                {destCoord: motion_vectors_dict.get(destCoord, []) + [motion_vector]}\n            )\n            # you know, different frame sources may lead to different results.\n            # these vectors could overlap. which one you want to keep? the smaller ones or the bigger ones?\n            # if destCoord in destCoords:\n            #     print(\"SKIPPING DUPLICATE DESTCOORD:\", destCoord)\n            #     print(\"PREVIOUS MV\",prevMV)",
        "type": "code",
        "location": "/tests/motion_vector_estimation/optical_flow_colored_blocks_convolution.py:207-230"
    },
    "4209": {
        "file_id": 531,
        "content": "This code iterates over motion vectors and updates a dictionary with the destination coordinates and corresponding motion vectors. It skips zero vectors and handles duplicate destinations, but doesn't specify which motion vector to keep in case of overlapping coordinates.",
        "type": "comment"
    },
    "4210": {
        "file_id": 531,
        "content": "            #     print(\"CURRENT MV\", mv)\n            #     continue\n            # else:\n            #     destCoords.add(destCoord)\n            # prevMV = mv\n            # try:\n            #     # src_x, src_y may not apply the same rule.\n            #     # assert src_x % 16 == 8\n            #     # assert src_y % 16 == 8\n            #     assert checkMacroBlock(dst_x) is not None\n            #     assert checkMacroBlock(dst_y) is not None\n            #     # assert dst_x<=res_x # dst_x can go beyond the res_x\n            #     # assert dst_y<=res_y\n            #     # so all rules applied.\n            # except:\n            #     # print('source',src_x, src_y)\n            #     print(\"res\", res_x, res_y)\n            #     print('destionation',dst_x, dst_y)\n            #     print('motion',motion_x, motion_y)\n            #     print(\"scale\",motion_scale)\n        motion_vectors_dict_averaged = {\n            key: averageMotionVectors(motion_vectors_dict[key])\n            for key in motion_vectors_dict.keys()\n        }",
        "type": "code",
        "location": "/tests/motion_vector_estimation/optical_flow_colored_blocks_convolution.py:231-254"
    },
    "4211": {
        "file_id": 531,
        "content": "This code is filtering and averaging motion vectors for macroblocks within a certain range. It checks if the source coordinates follow a specific rule, asserts that valid macroblock check functions are not None, and ensures the destination coordinates do not exceed the resolution limits. If any of these conditions fail, it prints debug information and continues execution. Finally, it calculates the averaged motion vectors for each macroblock in the dictionary.",
        "type": "comment"
    },
    "4212": {
        "file_id": 531,
        "content": "        # assuming no duplicates?\n        weighted_motion_vectors = []\n        weights = []\n        rectangles = []\n        motion_vectors_filtered = []  # for getting data later?\n        for (\n            blockCenterCoordinates,\n            average_motion_vector,\n        ) in motion_vectors_dict_averaged.items():\n            if average_motion_vector == (0, 0):\n                continue\n                # wtf is this? why fucking zero?\n                # print('skipping zero average motion vector')\n                # print(\"destination coords\", key)\n                # print('average motion vector', average_motion_vector)\n            else:\n                m_x, m_y = average_motion_vector\n                motion_vectors_filtered.append(average_motion_vector)\n                rectangle_XYWH = getRectangleXYWHFromBlockCenterCoordinates(\n                    blockCenterCoordinates\n                )\n                rectangles.append(rectangle_XYWH)\n                blockWeight = getBlockWeightFromBlockCenterCoordinates(\n                    blockCenterCoordinates",
        "type": "code",
        "location": "/tests/motion_vector_estimation/optical_flow_colored_blocks_convolution.py:255-278"
    },
    "4213": {
        "file_id": 531,
        "content": "This code filters out motion vectors with an average of (0, 0) and stores the remaining vectors in a list. It also extracts relevant information from the blockCenterCoordinates and calculates the weight for each block. Rectangles are created using the getRectangleXYWHFromBlockCenterCoordinates function, and weights are obtained through getBlockWeightFromBlockCenterCoordinates. The motion_vectors_filtered list keeps track of filtered motion vectors for later use.",
        "type": "comment"
    },
    "4214": {
        "file_id": 531,
        "content": "                )\n                weights.append(blockWeight)\n                weighted_motion_vectors.append(\n                    (\n                        m_x * blockWeight / frame_common_divisor,\n                        m_y * blockWeight / frame_common_divisor,\n                    )\n                )\n        weighted_motion_vectors = np.array(weighted_motion_vectors)\n        sum_weighted_motion_vector = np.sum(weighted_motion_vectors, axis=0)\n        average_global_weighted_motion_vector = (\n            sum_weighted_motion_vector / total_block_weights\n        )\n        sum_weights = sum(weights)\n        average_weighted_motion_vector = sum_weighted_motion_vector / sum_weights\n        motion_area_ratio = sum_weights / total_block_weights\n        # print(motion_vectors.shape)\n        motion_vectors_filtered_cartesian_distance = [\n            cartesianDistance(vector) for vector in motion_vectors_filtered\n        ] + [\n            0\n        ]  # to avoid errors.\n        motion_vectors_filtered_cartesian_distance = np.array(",
        "type": "code",
        "location": "/tests/motion_vector_estimation/optical_flow_colored_blocks_convolution.py:279-301"
    },
    "4215": {
        "file_id": 531,
        "content": "This code calculates the average global weighted motion vector and average weighted motion vector, as well as the motion area ratio. It also filters and stores cartesian distances for each motion vector.",
        "type": "comment"
    },
    "4216": {
        "file_id": 531,
        "content": "            motion_vectors_filtered_cartesian_distance\n        )\n        cartesianWeights = weights + [0]\n        cartesianWeights = np.array(cartesianWeights)\n        cartesianWeightsSum = np.sum(cartesianWeights)\n        weighted_motion_vectors_filtered_cartesian_distance = (\n            motion_vectors_filtered_cartesian_distance * cartesianWeights\n        )\n        sum_weighted_motion_vectors_filtered_cartesian_distance = np.sum(\n            weighted_motion_vectors_filtered_cartesian_distance\n        )\n        # print(\"SUM\", sum_weighted_motion_vectors_filtered_cartesian_distance)\n        # breakpoint()\n        average_weighted_motion_vectors_filtered_cartesian_distance = (\n            sum_weighted_motion_vectors_filtered_cartesian_distance / cartesianWeightsSum\n        )\n        average_global_weighted_motion_vectors_filtered_cartesian_distance = (\n            sum_weighted_motion_vectors_filtered_cartesian_distance\n            / total_block_weights # this is a number, not array!\n        )\n        min_cartesian = min(motion_vectors_filtered_cartesian_distance)",
        "type": "code",
        "location": "/tests/motion_vector_estimation/optical_flow_colored_blocks_convolution.py:302-328"
    },
    "4217": {
        "file_id": 531,
        "content": "This code calculates weighted average motion vectors by multiplying the distance of each vector with its corresponding weight, summing them, and dividing by the total weight. The minimum cartesian distance is also found.",
        "type": "comment"
    },
    "4218": {
        "file_id": 531,
        "content": "        max_cartesian = max(motion_vectors_filtered_cartesian_distance)\n        motion_area_ratio_array.append(motion_area_ratio)\n        # print()\n        # print(average_weighted_motion_vector)\n        # print(average_global_weighted_motion_vector)\n        # breakpoint()\n        average_weighted_motion_vector_cartesian=cartesianDistance(average_weighted_motion_vector)\n        average_weighted_motion_vector_cartesian_array.append(average_weighted_motion_vector_cartesian)\n        average_global_weighted_motion_vector_cartesian = cartesianDistance(average_global_weighted_motion_vector)\n        average_global_weighted_motion_vector_cartesian_array.append(\n        average_global_weighted_motion_vector_cartesian\n        )\n        average_weighted_motion_vectors_filtered_cartesian_distance_array.append(\n            average_weighted_motion_vectors_filtered_cartesian_distance\n        )\n        average_global_weighted_motion_vectors_filtered_cartesian_distance_array.append(\n            average_global_weighted_motion_vectors_filtered_cartesian_distance",
        "type": "code",
        "location": "/tests/motion_vector_estimation/optical_flow_colored_blocks_convolution.py:329-346"
    },
    "4219": {
        "file_id": 531,
        "content": "Calculates the average weighted motion vector and global weighted motion vector in Cartesian distance, then appends them to corresponding arrays. It also computes and appends filtered Cartesian distances of both types of vectors to their respective arrays. No print statements or breakpoints are executed.",
        "type": "comment"
    },
    "4220": {
        "file_id": 531,
        "content": "        )\n        if motion_vectors_dict_averaged != {}:\n            # breakpoint()\n            if visualize:\n                print(\"motion_area_ratio\", motion_area_ratio)\n                print(\"average_weighted_motion_vector_cartesian\", average_weighted_motion_vector_cartesian)\n                print(\n                    \"average_global_weighted_motion_vecto_cartesianr\",\n                    average_global_weighted_motion_vector_cartesian,\n                )\n                print(\n                    \"average_weighted_motion_vectors_filtered_cartesian_distance\",\n                    average_weighted_motion_vectors_filtered_cartesian_distance,\n                )\n                print(\n                    \"average_global_weighted_motion_vectors_filtered_cartesian_distance\",\n                    average_global_weighted_motion_vectors_filtered_cartesian_distance,\n                )\n                motion_mask = np.zeros(\n                    (motion_render_frame[1], motion_render_frame[0], 1)\n                )\n                for index, (x, y, w, h) in enumerate(rectangles):",
        "type": "code",
        "location": "/tests/motion_vector_estimation/optical_flow_colored_blocks_convolution.py:347-369"
    },
    "4221": {
        "file_id": 531,
        "content": "The code checks if there are any motion vectors in the dictionary and prints various motion-related information and creates a motion mask with zeros.",
        "type": "comment"
    },
    "4222": {
        "file_id": 531,
        "content": "                    pt1, pt2 = XYWHToDiagonal(x, y, w, h)\n                    # print(pt1, pt2)\n                    current_cartesian = motion_vectors_filtered_cartesian_distance[\n                        index\n                    ]\n                    # print(type(pt1), type(pt1[0]))\n                    relative_motion_cartesian = (current_cartesian - min_cartesian) / (\n                        max_cartesian - min_cartesian\n                    )  # must from 0 to 1 so we can plot this,\n                    # relative_motion_cartesian = 255*((current_cartesian-min_cartesian)/(max_cartesian-min_cartesian))\n                    # relative_motion_cartesian = int(relative_motion_cartesian)\n                    # relative_motion_cartesian = min(255,max(0, relative_motion_cartesian))\n                    # breakpoint()\n                    cv2.rectangle(\n                        motion_mask,\n                        pt1,\n                        pt2,\n                        color=(relative_motion_cartesian,),\n                        thickness=-1,",
        "type": "code",
        "location": "/tests/motion_vector_estimation/optical_flow_colored_blocks_convolution.py:370-388"
    },
    "4223": {
        "file_id": 531,
        "content": "This code calculates the relative motion vector cartesian distance and draws a rectangle on an image using OpenCV's `cv2.rectangle` function. The rectangle dimensions are based on the input x, y, w, and h parameters, and its color is determined by the relative motion vector cartesian distance, converted to a range of 0-255 for image intensity values.",
        "type": "comment"
    },
    "4224": {
        "file_id": 531,
        "content": "                    )\n                # should we gaussian blur, threshold this, do convolution and then apply bounding box on it?\n                # # visualize this.\n                if show_picture:\n                    cv2.imshow(\"motion_mask\", motion_mask)\n                    cv2.waitKey(100)\n            # may you create bounding box for this? for tracking motion? or not?\n        # breakpoint()\n    else:\n        break\n# print('max_dst_x', max_dst_x)\n# print('max_dst_y', max_dst_y)\nimport matplotlib.pyplot as plt\n# plt.style.use('dark_background')\na, b = 5, 1\nfigure, axis = plt.subplots(a, b)\ndata = [\n    motion_area_ratio_array,\n    # average_weighted_motion_vector_array,\n    # average_global_weighted_motion_vector_array,\n    average_weighted_motion_vector_cartesian_array,\n    average_global_weighted_motion_vector_cartesian_array,\n    average_weighted_motion_vectors_filtered_cartesian_distance_array,\n    average_global_weighted_motion_vectors_filtered_cartesian_distance_array,\n]\ntitles = [\n    \"motion_area_ratio\",",
        "type": "code",
        "location": "/tests/motion_vector_estimation/optical_flow_colored_blocks_convolution.py:389-419"
    },
    "4225": {
        "file_id": 531,
        "content": "The code is visualizing and analyzing motion data using image processing techniques. It displays a motion mask, creates a bounding box for motion tracking, and plots various motion-related data on subplots. The code also includes options to blur, threshold, apply convolution, and display the results.",
        "type": "comment"
    },
    "4226": {
        "file_id": 531,
        "content": "    # \"average_weighted_motion_vector\",\n    # \"average_global_weighted_motion_vector\",\n    \"average_weighted_motion_vector_cartesian\",\n    \"average_global_weighted_motion_vector_cartesian\",\n    \"average_weighted_motion_vectors_filtered_cartesian_distance\",\n    \"average_global_weighted_motion_vectors_filtered_cartesian_distance\",\n]\n# breakpoint()\nassert len(titles) == len(data)\nassert a*b >= len(titles)\nfor _a in range(a):\n    for _b in range(b):\n        index = _a * b + _b\n        if index > len(data) - 1:\n            break\n        if a == 1:\n            if b == 1:\n                axis[0].plot(data[index])\n                axis[0].set_title(titles[index])\n            else:\n                axis[_b].plot(data[index])\n                axis[_b].set_title(titles[index])\n        elif b == 1:\n            axis[_a].plot(data[index])\n            axis[_a].set_title(titles[index])\n        else:\n            axis[_a, _b].plot(data[index])\n            axis[_a, _b].set_title(titles[index])\nplt.show()",
        "type": "code",
        "location": "/tests/motion_vector_estimation/optical_flow_colored_blocks_convolution.py:420-449"
    },
    "4227": {
        "file_id": 531,
        "content": "This code is plotting multiple sets of data onto a graph, with each set corresponding to an item in two lists of titles and data. The code asserts that the lengths of both lists are equal, and then iterates through each element of the lists using nested for loops. If a single plot is desired, it plots and labels one line of data at a time. If multiple plots are desired, it creates and labels a separate plot for each line of data. Finally, it displays the graph.",
        "type": "comment"
    },
    "4228": {
        "file_id": 532,
        "content": "/tests/motion_vector_estimation/mpegflow/test.sh",
        "type": "filepath"
    },
    "4229": {
        "file_id": 532,
        "content": "The code sets the VIDEO variable to a specific video file path, runs the mpegflow command on that file and redirects output to output.txt, but the .so file is fake so it doesn't work well. The code also provides an alternative method using Docker or downloading a real .so file from the web, creating an examples/vis_dump directory, and piping the mpegflow command's output into ./vis with the video file path.",
        "type": "summary"
    },
    "4230": {
        "file_id": 532,
        "content": "VIDEO=\"/root/Desktop/works/pyjom/samples/video/cute_cat_gif.mp4\"\n# ./mpegflow $VIDEO > output.txt\n# it does not help because the .so file is fake. you need a real one.\n# you may download it from web, or just use docker\n# mkdir -p examples/vis_dump && ./mpegflow $VIDEO | ./vis $VIDEO examples/vis_dump\n# maybe this shit is not good at all...",
        "type": "code",
        "location": "/tests/motion_vector_estimation/mpegflow/test.sh:1-10"
    },
    "4231": {
        "file_id": 532,
        "content": "The code sets the VIDEO variable to a specific video file path, runs the mpegflow command on that file and redirects output to output.txt, but the .so file is fake so it doesn't work well. The code also provides an alternative method using Docker or downloading a real .so file from the web, creating an examples/vis_dump directory, and piping the mpegflow command's output into ./vis with the video file path.",
        "type": "comment"
    },
    "4232": {
        "file_id": 533,
        "content": "/tests/motion_vector_estimation/mpegflow/init.sh",
        "type": "filepath"
    },
    "4233": {
        "file_id": 533,
        "content": "Downloading 'mpegflow' and 'vis' from the 1.0 release of vadimkantorov/mpegflow repository.",
        "type": "summary"
    },
    "4234": {
        "file_id": 533,
        "content": "curl -L -O https://github.com/vadimkantorov/mpegflow/releases/download/1.0/mpegflow\ncurl -L -O https://github.com/vadimkantorov/mpegflow/releases/download/1.0/vis",
        "type": "code",
        "location": "/tests/motion_vector_estimation/mpegflow/init.sh:1-2"
    },
    "4235": {
        "file_id": 533,
        "content": "Downloading 'mpegflow' and 'vis' from the 1.0 release of vadimkantorov/mpegflow repository.",
        "type": "comment"
    },
    "4236": {
        "file_id": 534,
        "content": "/tests/ffmpeg_audio_volume_detect_adjust/test_output_file_error.py",
        "type": "filepath"
    },
    "4237": {
        "file_id": 534,
        "content": "The code imports modules, defines paths and fetches media. It detects volume average, adjusts volume if no error occurs, and warns about potential clipping while displaying normalization stats for the output file.",
        "type": "summary"
    },
    "4238": {
        "file_id": 534,
        "content": "import test # for appending path only.\nfrom pyjom.audiotoolbox import detect_volume_average, adjustVolumeInMedia\noutput_path = \"volDetect_test.mp4\"\n# detect_volume_average(output_path, debug=True)\nnormalizedOutputPath = \"normalized.mp4\"\n# Output extension mp4 does not support PCM audio. Please choose a suitable audio codec with the -c:a option.\n# wtf are you talking about?\nonline_fetched_media = \"/root/Desktop/works/pyjom/tests/calculate_separate_video_scene_duration_in_dog_video_with_bgm/sample.mp4\"\n# is this the standard?\ntargets, error = detect_volume_average(online_fetched_media, debug=True)\n# at least let me see this shit.\n# breakpoint()\n# {'mean': -10.6, 'max': 0.0}\n# according to the volume, it seems that everyone agree with this 'industrial standard'\nif not error:\n    adjustVolumeInMedia(\n        output_path, normalizedOutputPath, overwrite_output=True, targets=targets\n    )\n    detect_volume_average(normalizedOutputPath, debug=True)\nelse:\n    print(\"error when detecting volume in media: %s\" % online_fetched_media)",
        "type": "code",
        "location": "/tests/ffmpeg_audio_volume_detect_adjust/test_output_file_error.py:1-23"
    },
    "4239": {
        "file_id": 534,
        "content": "The code imports the necessary modules and defines output path, normalized output path, and online fetched media. It attempts to detect volume average from the media file and adjusts the volume if no error occurs, otherwise it prints an error message.",
        "type": "comment"
    },
    "4240": {
        "file_id": 534,
        "content": "    # what is cliping?\n    # WARNING: Adjusting will lead to clipping of 4.209296 dB                                 \n# even worse with default settings.\n# VOLUME NORMALIZATION SUCCESSFUL\n# MEDIA PATH: normalized.mp4\n# VOLUME: {'mean': -25.1, 'max': -8.8}\n# ERROR STATUS: False\n# 'mean' -> target level\n# 'max' -> true peak (really?)",
        "type": "code",
        "location": "/tests/ffmpeg_audio_volume_detect_adjust/test_output_file_error.py:24-32"
    },
    "4241": {
        "file_id": 534,
        "content": "The code is warning about potential clipping due to audio adjustment, showing successful volume normalization with mean and max levels for the output file.",
        "type": "comment"
    },
    "4242": {
        "file_id": 535,
        "content": "/tests/ffmpeg_audio_volume_detect_adjust/test_get_media_bitrate.py",
        "type": "filepath"
    },
    "4243": {
        "file_id": 535,
        "content": "The code imports necessary modules and defines a list of media file paths. It then iterates through the list, printing each media path and using the getAudioBitrate function to obtain the bitrate of each audio file. The result is printed for each file.",
        "type": "summary"
    },
    "4244": {
        "file_id": 535,
        "content": "import test\nfrom pyjom.audiotoolbox import getAudioBitrate\nmediaPaths = [\n    \"/root/Desktop/works/pyjom/tests/music_analysis/exciting_bgm.mp3\", # 320000\n    \"/root/Desktop/works/pyjom/tests/ffmpeg_audio_volume_detect_adjust/normalized.mp4\", # 130770\n]\nfor mediaPath in mediaPaths:\n    print(\"media path:\", mediaPath)\n    result = getAudioBitrate(mediaPath)\n    print(\"RESULT:\", result)",
        "type": "code",
        "location": "/tests/ffmpeg_audio_volume_detect_adjust/test_get_media_bitrate.py:1-11"
    },
    "4245": {
        "file_id": 535,
        "content": "The code imports necessary modules and defines a list of media file paths. It then iterates through the list, printing each media path and using the getAudioBitrate function to obtain the bitrate of each audio file. The result is printed for each file.",
        "type": "comment"
    },
    "4246": {
        "file_id": 536,
        "content": "/tests/ffmpeg_audio_volume_detect_adjust/test_get_audio_length.py",
        "type": "filepath"
    },
    "4247": {
        "file_id": 536,
        "content": "This code imports the necessary modules, defines an audio file path, retrieves its duration using getAudioDuration function and prints the floating point duration.",
        "type": "summary"
    },
    "4248": {
        "file_id": 536,
        "content": "import test\nfrom pyjom.audiotoolbox import getAudioDuration\naudioPath = \"/root/Desktop/works/pyjom/tests/music_analysis/exciting_bgm.mp3\"\naudioDuration = getAudioDuration(audioPath)\nprint(\"audioDuration:\", audioDuration)\n# audioDuration: 302.915918367\n# obviously floating point duration.",
        "type": "code",
        "location": "/tests/ffmpeg_audio_volume_detect_adjust/test_get_audio_length.py:1-9"
    },
    "4249": {
        "file_id": 536,
        "content": "This code imports the necessary modules, defines an audio file path, retrieves its duration using getAudioDuration function and prints the floating point duration.",
        "type": "comment"
    },
    "4250": {
        "file_id": 537,
        "content": "/tests/ffmpeg_audio_volume_detect_adjust/test.py",
        "type": "filepath"
    },
    "4251": {
        "file_id": 537,
        "content": "The code utilizes pyjom library to generate black videos for testing purposes, and it detects and adjusts audio volume of a media file named volDetect_test.mp4 with mean volume -16.8 and max volume -2.0 without any errors.",
        "type": "summary"
    },
    "4252": {
        "file_id": 537,
        "content": "# ffmpeg -i video.avi -af \"volumedetect\"\n# shall we get the output?\n# we can also detect if the stream does not have audio stream.\nimport sys\npyjom_path = \"/root/Desktop/works/pyjom\"\nsys.path.append(pyjom_path)\nfrom pyjom.audiotoolbox import getAudioDuration\nfrom pyjom.medialang.processors.dotProcessor.videoProcessor import executeEditlyScript\nfrom pyjom.videotoolbox import createPureColorVideo\n# for test only.\ndef create_black_video_without_audio(duration, mediapath):\n    createPureColorVideo(duration, mediapath)\n# this is for test only. not for work.\n# another editly script for another video. please?\ndef create_test_video_with_editly(audio):  # length is calculated by the audio length.\n    audio_duration = getAudioDuration(audio)\n    fast = True\n    output_path = \"volDetect_test.mp4\"\n    videoFilePath = \"black_video_with_equal_length_of_audio.mp4\"\n    create_black_video_without_audio(audio_duration, videoFilePath)\n    editly_json = {\n        \"width\": 1920,\n        \"height\": 1080,\n        \"fast\": fast,\n        \"fps\": 60,",
        "type": "code",
        "location": "/tests/ffmpeg_audio_volume_detect_adjust/test.py:1-28"
    },
    "4253": {
        "file_id": 537,
        "content": "This code imports functions from the pyjom library to create a black video without audio, and provides two functions: one for creating a test video with Editly script based on an audio file's duration, and another for creating a black video with equal length as the given audio. It is intended for testing purposes only and not for actual work processes.",
        "type": "comment"
    },
    "4254": {
        "file_id": 537,
        "content": "        \"outPath\": output_path,\n        \"defaults\": {\"transition\": None},\n        \"clips\": [],\n    }\n    editly_json.update({\"audioFilePath\": audio})\n    duration = cutTo = audio_duration\n    cutFrom = 0\n    mute = True\n    clip = {\n        \"duration\": duration,\n        \"layers\": [],\n    }\n    layer = {\n        \"type\": \"video\",\n        \"path\": videoFilePath,\n        \"resizeMode\": \"contain\",\n        \"cutFrom\": cutFrom,\n        \"cutTo\": cutTo,\n        # that's how we mute it.\n        \"mixVolume\": 1 - int(mute),\n    }\n    clip[\"layers\"].append(layer)\n    editly_json[\"clips\"].append(clip)\n    # execute the thing.\n    executeEditlyScript(\".\", editly_json)\n    print(\"media saved to: %s\" % output_path)\n    return output_path\nfrom pyjom.audiotoolbox import detect_volume_average\nif __name__ == \"__main__\":\n    # perform our test.\n    # are you sure this won't change the volume?\n    audiopath = \"/root/Desktop/works/pyjom/tests/music_analysis/exciting_bgm.mp3\"\n    detect_volume_average(audiopath, debug=True)\n    # MEDIA PATH: /root/Desktop/works/pyjom/tests/music_analysis/exciting_bgm.mp3",
        "type": "code",
        "location": "/tests/ffmpeg_audio_volume_detect_adjust/test.py:29-65"
    },
    "4255": {
        "file_id": 537,
        "content": "This code generates a video with a muted audio track by creating an Editly JSON configuration. It first checks the audio volume and then executes the script to save the resulting media at the specified output path. The provided audio path is \"/root/Desktop/works/pyjom/tests/music_analysis/exciting_bgm.mp3\".",
        "type": "comment"
    },
    "4256": {
        "file_id": 537,
        "content": "    # VOLUME: {'mean': -10.8, 'max': 0.0}\n    # ERROR STATUS: False\n    # ______________________________\n    output_path = create_test_video_with_editly(audiopath)\n    detect_volume_average(output_path, debug=True)\n    # volume changed!\n    # MEDIA PATH: volDetect_test.mp4\n    # VOLUME: {'mean': -16.8, 'max': -2.0}\n    # ERROR STATUS: False\n    # how to adjust the volume accordingly?",
        "type": "code",
        "location": "/tests/ffmpeg_audio_volume_detect_adjust/test.py:66-75"
    },
    "4257": {
        "file_id": 537,
        "content": "The code is detecting and adjusting the audio volume of a media file (volDetect_test.mp4) using the function `detect_volume_average`. The current mean volume is -16.8 with a max volume of -2.0. The error status is False, indicating no issues during the process. The next step might be to adjust the volume according to these values.",
        "type": "comment"
    },
    "4258": {
        "file_id": 538,
        "content": "/tests/tencent_video_recommendation_extraction/requests_html_test.py",
        "type": "filepath"
    },
    "4259": {
        "file_id": 538,
        "content": "Imports pyppeteer and initializes an HTMLSession object, sets the URL to a QQ video page, uses session.get() to fetch the page's content, then iterates through all absolute links in the fetched HTML using r.html.absolute_links. Prints each link.",
        "type": "summary"
    },
    "4260": {
        "file_id": 538,
        "content": "from requests_html import HTMLSession # use pyppeteer.\nsession = HTMLSession()\n# url='https://www.baidu.com/'\nurl = 'http://v.qq.com/x/page/m0847y71q98.html'\nr = session.get(url)\nfor link in r.html.absolute_links:\n    print(link)",
        "type": "code",
        "location": "/tests/tencent_video_recommendation_extraction/requests_html_test.py:1-8"
    },
    "4261": {
        "file_id": 538,
        "content": "Imports pyppeteer and initializes an HTMLSession object, sets the URL to a QQ video page, uses session.get() to fetch the page's content, then iterates through all absolute links in the fetched HTML using r.html.absolute_links. Prints each link.",
        "type": "comment"
    },
    "4262": {
        "file_id": 539,
        "content": "/tests/tencent_video_recommendation_extraction/playwright_with_elinks.sh",
        "type": "filepath"
    },
    "4263": {
        "file_id": 539,
        "content": "This code executes a Python script and an elinks command to extract recommended video IDs from a webpage, searching for them in specific HTML elements.",
        "type": "summary"
    },
    "4264": {
        "file_id": 539,
        "content": "python3 dump_page.py\nelinks -dump dump.html > dump.log\n# please find recommended video id in <div data-vid=\"<vid>\">\n# or in <img src=\"//puui.qpic.cn/vpic_cover/<vid>/<vid>_old_ori.jpg/s640x360?max_age=7776000\">",
        "type": "code",
        "location": "/tests/tencent_video_recommendation_extraction/playwright_with_elinks.sh:1-5"
    },
    "4265": {
        "file_id": 539,
        "content": "This code executes a Python script and an elinks command to extract recommended video IDs from a webpage, searching for them in specific HTML elements.",
        "type": "comment"
    },
    "4266": {
        "file_id": 540,
        "content": "/tests/tencent_video_recommendation_extraction/phantomjs_test.js",
        "type": "filepath"
    },
    "4267": {
        "file_id": 540,
        "content": "This code uses PhantomJS and creates a new page. It opens the specified URL, checks if the status is \"success\", then retrieves the HTML of the page using evaluate() and outputs it to the console before exiting.",
        "type": "summary"
    },
    "4268": {
        "file_id": 540,
        "content": "var page = require('webpage').create();\npage.open('http://v.qq.com/x/page/m0847y71q98.html', function(status) {\n    //console.log(\"Status: \" + status);\n    if (status === \"success\") {\n        //\tpage.render('example.png');\n        result = page.evaluate(() => document.body.innerHTML);\n        console.log(result)\n    }\n    phantom.exit();\n});",
        "type": "code",
        "location": "/tests/tencent_video_recommendation_extraction/phantomjs_test.js:1-10"
    },
    "4269": {
        "file_id": 540,
        "content": "This code uses PhantomJS and creates a new page. It opens the specified URL, checks if the status is \"success\", then retrieves the HTML of the page using evaluate() and outputs it to the console before exiting.",
        "type": "comment"
    },
    "4270": {
        "file_id": 541,
        "content": "/tests/tencent_video_recommendation_extraction/loop_all_tags.py",
        "type": "filepath"
    },
    "4271": {
        "file_id": 541,
        "content": "This code reads the \"dump.html\" file, parses it with BeautifulSoup, and then iterates over all elements to check if their attributes include 'src' or 'href'. If so, it prints the key-value pair for further examination.",
        "type": "summary"
    },
    "4272": {
        "file_id": 541,
        "content": "from bs4 import BeautifulSoup\ndata = open(\"dump.html\",'r').read()\nsoup = BeautifulSoup(data)\nfor elem in soup.find_all():\n    # print(elem.attrs)\n    # for further examination\n    attrs = elem.attrs\n    for key in ['src', 'href']:\n        if key in attrs.keys():\n            print(key, attrs[key])",
        "type": "code",
        "location": "/tests/tencent_video_recommendation_extraction/loop_all_tags.py:1-14"
    },
    "4273": {
        "file_id": 541,
        "content": "This code reads the \"dump.html\" file, parses it with BeautifulSoup, and then iterates over all elements to check if their attributes include 'src' or 'href'. If so, it prints the key-value pair for further examination.",
        "type": "comment"
    },
    "4274": {
        "file_id": 542,
        "content": "/tests/tencent_video_recommendation_extraction/dump_page.py",
        "type": "filepath"
    },
    "4275": {
        "file_id": 542,
        "content": "This code uses Playwright library to launch a headless Chromium browser, navigates to a QQ video page, waits for the DOM content loaded state, retrieves the HTML content, writes it to a \"dump.html\" file, and then closes the browser.",
        "type": "summary"
    },
    "4276": {
        "file_id": 542,
        "content": "from playwright.sync_api import sync_playwright\ndef run(playwright):\n    webkit = playwright.chromium\n    browser = webkit.launch(headless=True)\n    context = browser.new_context()\n    page = context.new_page()\n    page.goto(\"https://v.qq.com/x/page/m0847y71q98.html\")\n    page.wait_for_load_state(\"domcontentloaded\")\n    content = page.content()\n    with open(\"dump.html\", 'w+') as f: f.write(content)\n    print(\"content write to dump.html\")\n    browser.close()\nwith sync_playwright() as playwright:\n    run(playwright)",
        "type": "code",
        "location": "/tests/tencent_video_recommendation_extraction/dump_page.py:1-16"
    },
    "4277": {
        "file_id": 542,
        "content": "This code uses Playwright library to launch a headless Chromium browser, navigates to a QQ video page, waits for the DOM content loaded state, retrieves the HTML content, writes it to a \"dump.html\" file, and then closes the browser.",
        "type": "comment"
    },
    "4278": {
        "file_id": 543,
        "content": "/tests/spatial_temporal_slice_pip/test.py",
        "type": "filepath"
    },
    "4279": {
        "file_id": 543,
        "content": "This code is loading a video file and reading frames from it using OpenCV's VideoCapture class. It continues to read frames until the frame cannot be retrieved, at which point it breaks out of the loop. The code seems to have some issues with low speed and possibly dealing with videos that have been detected as problematic by PIP (presumably a different part of the codebase).",
        "type": "summary"
    },
    "4280": {
        "file_id": 543,
        "content": "target_video = \"/media/root/help/pyjom/samples/video/LiGlReJ4i.mp4\" # 娜姐驾到 卡成傻逼\n# you should quit those which has unexpected long frame processing loops.\n# mask the area which has text on it. fill the area and blur the boundary.\n# you could also trash those videos with pip detected.\nimport cv2\n# shit it has low speed... canny\ncap = cv2.VideoCapture(target_video)\nret = 1\nwhile True:\n    ret, frame = cap.read()\n    if ret is None: break",
        "type": "code",
        "location": "/tests/spatial_temporal_slice_pip/test.py:1-19"
    },
    "4281": {
        "file_id": 543,
        "content": "This code is loading a video file and reading frames from it using OpenCV's VideoCapture class. It continues to read frames until the frame cannot be retrieved, at which point it breaks out of the loop. The code seems to have some issues with low speed and possibly dealing with videos that have been detected as problematic by PIP (presumably a different part of the codebase).",
        "type": "comment"
    },
    "4282": {
        "file_id": 544,
        "content": "/tests/video_phash_deduplication/test_video_hash.py",
        "type": "filepath"
    },
    "4283": {
        "file_id": 544,
        "content": "The code defines `getVideoPHash` to calculate a video's phash using the `videohashes` tool, testing it by comparing pairwise differences between hash values for different videos and considering duplicates based on a threshold.",
        "type": "summary"
    },
    "4284": {
        "file_id": 544,
        "content": "# use some delogo stuff.\nfrom lazero.program.subprocess import runCommandGetJson\n# these two are similar. can be used as threshold.\n# aaaa3d8a2eaa1f8a delogo\n# aaaa398a2faa5d8a not delogoed.\n# aaaa3c8a2faa5e8a mp4 (very similar to delogoed version)\ndef getVideoPHash(filepath,debug=False, timeout=100):\n    import os\n    import imagehash\n    assert os.path.exists(filepath)\n    assert os.path.isfile(filepath)\n    if not os.path.isabs(filepath):\n        filepath = os.path.abspath(filepath)\n    commandLine = [\n        \"videohashes\", # installed in path.\n        # \"/root/Desktop/works/pyjom/tests/video_phash_deduplication/videohashes/videohashes-linux\",\n        \"-json\",\n        filepath,\n    ]\n    success, myJson = runCommandGetJson(commandLine, debug=debug, timeout=timeout)\n    if debug:\n        print(\"SUCCESS?\", success)\n        print(myJson, type(myJson))\n    if not success:\n        return\n    # breakpoint()\n    phashString = myJson[\"phash\"]\n    phash = imagehash.hex_to_hash(phashString)\n    if debug:\n        print(\"FILEPATH: %s\" % filepath)",
        "type": "code",
        "location": "/tests/video_phash_deduplication/test_video_hash.py:1-32"
    },
    "4285": {
        "file_id": 544,
        "content": "The code defines a function `getVideoPHash` that calculates a video's phash (a unique identifier for an image or video) using the `videohashes` command-line tool. It takes a filepath as input, checks if it exists and is a file, then runs the command to generate the JSON output. The function also converts the returned phash string to a binary hash and optionally prints debug information.",
        "type": "comment"
    },
    "4286": {
        "file_id": 544,
        "content": "        print(myJson)\n        print(\"PHASH:\", phash)\n    # if withDuration:\n    #     duration = myJson[\"duration\"]\n    #     return duration, phash\n    # duration is inaccurate\n    return phash\nif __name__ == \"__main__\":\n    videoPaths = [\n        \"cute_cat_gif.mp4\",\n        \"cute_cat_gif.gif\",\n        \"cat_delogo.gif\",\n        \"/root/Desktop/works/pyjom/samples/video/dog_with_large_text.gif\",\n    ]\n    hashs = [getVideoPHash(filepath,debug=True) for filepath in videoPaths]\n    dis0 = hashs[0] - hashs[1]  # small\n    dis1 = hashs[1] - hashs[2]  # big\n    dis2 = hashs[0] - hashs[2]  # big\n    dis3 = hashs[0] - hashs[3]  # big\n    print(dis0, dis1, dis2, dis3)\n    # 4 4 4\n    # strange. why?\n    # 4 4 4 42\n    # huge difference.\n    # what value do you decide to be duplicate?\n    # phash < 7 (really?)\n    # so how do we run this test?",
        "type": "code",
        "location": "/tests/video_phash_deduplication/test_video_hash.py:33-65"
    },
    "4287": {
        "file_id": 544,
        "content": "This code tests the video hashing function by calculating pairwise differences between hash values for different video files. It then compares the differences to determine potential duplicates and prints the results. The hash difference threshold for considering duplicates is set to 7, but this seems low and may need adjustment based on further testing.",
        "type": "comment"
    },
    "4288": {
        "file_id": 545,
        "content": "/tests/video_phash_deduplication/test_milvus_library.py",
        "type": "filepath"
    },
    "4289": {
        "file_id": 545,
        "content": "This code defines a Milvus function for connecting, managing collections, and caching. It creates Collections with specified data types, searches duplicated videos, retrieves video duration/hash, indexes videos, and reloads collection if necessary.",
        "type": "summary"
    },
    "4290": {
        "file_id": 545,
        "content": "# # duplicate -> remove, do not insert\n# # not duplicate -> get the data, insert\n# # you want to clear the collection after this run?\n# from functools import lru_cache\n# from pymilvus import connections\n# @lru_cache(maxsize=1)\n# def connectMilvusDatabase(alias=\"default\", host=\"localhost\", port=\"19530\"):\n#     connection = connections.connect(\n#         alias=alias, host=host, port=port\n#     )  # can we reconnect?\n#     print(\"milvus connected\")\n# # connectMilvusDatabase()\n# # connectMilvusDatabase() # will not connect again.\n# from pymilvus import Collection\n# from pymilvus import utility\n# from pymilvus import CollectionSchema, FieldSchema, DataType\n# import traceback\n# def getMilvusVideoDeduplicationCollection(\n#     get_existing: bool = False,\n# ):  # most of the time we just use the same\n#     collection_name = \"video_deduplication\"\n#     try:\n#         if utility.has_collection(collection_name):  # be prudent.\n#             if get_existing:\n#                 return Collection(collection_name)\n#             utility.drop_collection(collection_name)",
        "type": "code",
        "location": "/tests/video_phash_deduplication/test_milvus_library.py:1-35"
    },
    "4291": {
        "file_id": 545,
        "content": "The code defines a function to connect to a Milvus database, get or remove an existing collection named \"video_deduplication\", and returns the collection if it already exists. The function uses caching and checks if the collection already exists before performing any actions.",
        "type": "comment"
    },
    "4292": {
        "file_id": 545,
        "content": "#     except:\n#         traceback.print_exc()\n#         print(\"maybe the collection does not exist\")\n#     video_semantic_id = FieldSchema(  # how to insert this shit without prior knowledge?\n#         name=\"video_semantic_id\",\n#         dtype=DataType.INT64,\n#         is_primary=True,  # if is primary, will do check for 'not duplicate' or something.\n#         auto_id=True,  # no need for id generation.\n#     )\n#     video_length = FieldSchema(\n#         name=\"video_length\",\n#         dtype=DataType.FLOAT,\n#     )\n#     video_phash = FieldSchema(\n#         name=\"video_phash\", dtype=DataType.BINARY_VECTOR, dim=64\n#     )  # 64\n#     # single dimension? no multi dimension support?\n#     schema = CollectionSchema(\n#         fields=[video_semantic_id, video_length, video_phash],\n#         description=\"Test video deduplication\",\n#     )\n#     collection = Collection(\n#         name=collection_name,\n#         schema=schema,\n#         using=\"default\",\n#         shards_num=2,\n#     )\n#     # is this demo collection?\n#     return collection",
        "type": "code",
        "location": "/tests/video_phash_deduplication/test_milvus_library.py:36-65"
    },
    "4293": {
        "file_id": 545,
        "content": "This code defines a CollectionSchema and Collection for Milvus library. The schema contains fields for video_semantic_id, video_length, and video_phash, with their respective data types and properties. The Collection is created with a name, schema, database usage, and number of shards.",
        "type": "comment"
    },
    "4294": {
        "file_id": 545,
        "content": "# # seems hard to setup.\n# # not started!\n# # https://milvus.io/docs/v2.0.0/metric.md#binary\n# # the metric is important to us.\n# import numpy as np\n# import bitarray\n# @lru_cache(maxsize=1)\n# def transformVideoPhash(videoPhash):\n#     # we need the raw phash.\n#     queryData = videoPhash.hash  # videoPhashTruthTable8x8 or something\n#     queryData = queryData.reshape(-1).tolist()\n#     queryData = [\"1\" if x else \"0\" for x in queryData]\n#     queryData = bitarray.bitarray(\"\".join(queryData), endian=\"little\")\n#     queryData = queryData.tobytes()\n#     return queryData\n# # dimension: 8*8=64\n# def indexVideoWithVideoDurationAndPhash(collection, videoDuration, videoPhash):\n#     queryData = transformVideoPhash(videoPhash)\n#     collection.insert([[np.float32(videoDuration)], [queryData]])\n# # can release even if not loaded.\n# from test_video_hash import getVideoPHash\n# import caer\n# @lru_cache(maxsize=1)\n# def getVideoDurationAndPhashFromFile(videoFilePath):\n#     videoDuration = caer.video.frames_and_fps.get_duration(videoFilePath)",
        "type": "code",
        "location": "/tests/video_phash_deduplication/test_milvus_library.py:68-103"
    },
    "4295": {
        "file_id": 545,
        "content": "Function `transformVideoPhash` takes a video phash and converts it into a binary format for Milvus library indexing. Function `indexVideoWithVideoDurationAndPhash` inserts the video duration and transformed phash into the specified collection. The `getVideoDurationAndPhashFromFile` function retrieves the video duration and corresponding phash of a given video file using caer's video module. All functions are cached to avoid redundant computations.",
        "type": "comment"
    },
    "4296": {
        "file_id": 545,
        "content": "#     videoPhash = getVideoPHash(videoFilePath)\n#     return videoDuration, videoPhash\n# def indexVideoWithVideoDurationAndPhashFromFile(collection, videoFilePath):\n#     videoDuration, videoPhash = getVideoDurationAndPhashFromFile(videoFilePath)\n#     indexVideoWithVideoDurationAndPhash(collection, videoDuration, videoPhash)\n# def reloadMilvusCollection(collection):\n#     collection.release()  # unload.\n#     collection.load()\n# # make it into some library!\n# # insert after load?\n# # # 1,64\n# # what is wrong? wtf?\n# # queryData = queryData.tolist()\n# def getDistancesBySearchingDuplicatedVideoInMilvusByFile(\n#     collection,\n#     videoFilePath,\n#     search_params={\"metric_type\": \"Jaccard\", \"params\": {\"nprobe\": 10}},\n#     autoreload: bool = True,\n#     span: float = 2,\n#     debug: bool = False,\n#     limit: int = 10,\n# ):\n#     if autoreload:\n#         reloadMilvusCollection(collection)\n#     videoDuration, videoPhash = getVideoDurationAndPhashFromFile(videoFilePath)\n#     queryData = transformVideoPhash(videoPhash)",
        "type": "code",
        "location": "/tests/video_phash_deduplication/test_milvus_library.py:104-136"
    },
    "4297": {
        "file_id": 545,
        "content": "This code snippet defines a function for searching duplicated videos in Milvus by their file path, using Jaccard metric. It also includes functions to get video duration and hash from the file, index videos with duration and hash, and reload Milvus collection if necessary. The search parameters include metric type, probe count, span, limit, and whether to enable debug mode.",
        "type": "comment"
    },
    "4298": {
        "file_id": 545,
        "content": "#     minVideoLength = max(0, videoDuration - span)\n#     maxVideoLength = videoDuration + span\n#     results = collection.search(\n#         data=[queryData],  # this is the float dimension.\n#         anns_field=\"video_phash\",\n#         param=search_params,\n#         output_fields=[\"video_length\"],\n#         limit=limit,\n#         expr=\"video_length > {minVideoLength} and video_length < {maxVideoLength}\".format(\n#             minVideoLength=minVideoLength, maxVideoLength=maxVideoLength\n#         ),\n#     )\n#     theHit = results[0]\n#     # print(theHit)\n#     # so we can perform search without filtering afterwards.\n#     # results[0][0].entity.get('video_length')\n#     # print(results[0].ids)\n#     # now, we want to have the 'distance' parameter.\n#     # print(results[0])\n#     # print(theHit)\n#     distances = list(theHit.distances)\n#     if debug:\n#         print(\"distances: %s\" % distances)\n#     return distances\n#     # what is the distance? we need to try.\n#     # returh the closest distance?\n#     # results = [x for x in theHit]",
        "type": "code",
        "location": "/tests/video_phash_deduplication/test_milvus_library.py:137-164"
    },
    "4299": {
        "file_id": 545,
        "content": "This code searches for videos within a specified range of video length in the Milvus library. It sets minimum and maximum lengths based on the query's duration and span, and uses these values to filter results from the search. The closest distance between the query and each result is then returned.",
        "type": "comment"
    }
}