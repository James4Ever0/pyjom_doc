{
    "4200": {
        "file_id": 530,
        "content": "    sleep: int = BAIDU_API_SLEEP_TIME,\n    lock_file: str = BAIDU_TRANSLATOR_LOCK_FILE,\n):  # target language must be chinese.\n    import filelock\n    lock = filelock.FileLock(lock_file)\n    with lock:\n        import time\n        time.sleep(sleep)\n        language_translation_model = getBaiduLanguageTranslationModel()\n        translated_content = language_translation_model.translate(\n            content, source, target\n        )\n        return translated_content\nfrom typing import Iterable, Union\nimport random\ndef baiduParaphraserByTranslation(\n    content: str,\n    debug: bool = False,\n    paraphrase_depth: Union[\n        int, Iterable\n    ] = 1,  # only 1 intermediate language, default.\n    suggested_middle_languages: list[str] = [\n        \"zh\",\n        \"en\",\n        \"jp\",\n    ],  # english, japanese, chinese\n):\n    if issubclass(type(paraphrase_depth), Iterable):\n        paraphrase_depth = random.choice(paraphrase_depth)\n    target_language_id = baidu_lang_detect(content)\n    all_middle_languages = list(set(suggested_middle_languages + [target_language_id]))",
        "type": "code",
        "location": "/tests/title_rewrite_paraphrase/test_baidu_paraphrase.py:42-81"
    },
    "4201": {
        "file_id": 530,
        "content": "This code defines a function called `baiduParaphraserByTranslation` that paraphrases text using the Baidu API. It first detects the target language, then randomly selects one or more intermediate languages from a list of suggested middle languages. The function uses the getBaiduLanguageTranslationModel() to translate the content through each intermediate language, resulting in a paraphrased version of the original text. The translation is done in multiple steps with a sleep time between each step.",
        "type": "comment"
    },
    "4202": {
        "file_id": 530,
        "content": "    assert paraphrase_depth > 0\n    if paraphrase_depth > 1:\n        assert len(all_middle_languages) >= 3\n    current_language_id = target_language_id\n    middle_content = content\n    head_tail_indexs = set([0, paraphrase_depth - 1])\n    intermediate_languages = []\n    for loop_id in range(paraphrase_depth):\n        forbid_langs = set([current_language_id])\n        if loop_id in head_tail_indexs:\n            forbid_langs.add(target_language_id)\n        non_target_middle_languages = [\n            langid for langid in all_middle_languages if langid not in forbid_langs\n        ]\n        if debug:\n            print(f\"INDEX: {loop_id} INTERMEDIATE LANGS: {non_target_middle_languages}\")\n        middle_language_id = random.choice(non_target_middle_languages)\n        middle_content = baidu_translate(\n            middle_content, source=current_language_id, target=middle_language_id\n        )\n        current_language_id = middle_language_id\n        intermediate_languages.append(middle_language_id)\n    output_content = baidu_translate(",
        "type": "code",
        "location": "/tests/title_rewrite_paraphrase/test_baidu_paraphrase.py:83-109"
    },
    "4203": {
        "file_id": 530,
        "content": "This code performs a paraphrasing operation by iteratively translating the content through multiple languages, excluding the target language. It checks for a minimum paraphrase depth and the number of available middle languages. The output is obtained by translating the initial content through each intermediate language, resulting in a paraphrased version of the original text.",
        "type": "comment"
    },
    "4204": {
        "file_id": 530,
        "content": "        middle_content, source=current_language_id, target=target_language_id\n    )\n    success = output_content.strip() != content.strip()\n    if debug:\n        print(\"SOURCE LANGUAGE:\", target_language_id)\n        print(\"USING INTERMEDIATE LANGUAGES:\", intermediate_languages)\n        print(\"PARAPHRASED:\", output_content)\n        print(\"paraphrase success?\", success)\n    return output_content, success\n# content = \"世上所有小猫都是天使变的！\"\ncontent =  \"支持几十个不同类型的任务，具有较好的零样本学习能力和少样本学习能力。\"\noutput, success = baiduParaphraserByTranslation(content, paraphrase_depth=3, debug=True)",
        "type": "code",
        "location": "/tests/title_rewrite_paraphrase/test_baidu_paraphrase.py:110-124"
    },
    "4205": {
        "file_id": 530,
        "content": "The code is calling the baiduParaphraserByTranslation function to paraphrase a given content. It passes the content, specifies the maximum depth of paraphrasing (3), and sets the debug mode to True for printing additional information about the process. If successful, it returns the paraphrased output and a boolean success flag. The content in this case is \"支持几十个不同类型的任务，具有较好的零样本学习能力和少样本学习能力。\".",
        "type": "comment"
    },
    "4206": {
        "file_id": 531,
        "content": "/tests/title_rewrite_paraphrase/test_api.py",
        "type": "filepath"
    },
    "4207": {
        "file_id": 531,
        "content": "This code is a function that uses an API to paraphrase Chinese text. It takes input content, debug flag, target ID, timeout, and providers' URLs as parameters. The function sends a POST request to the selected provider's URL with the content data and retrieves the response. If the output is not equal to the original content (after removing leading/trailing spaces), it considers the paraphrasing successful. The debug flag controls whether to print the output, and the function returns the output and success status. The given code uses this function to paraphrase a specific Chinese text.",
        "type": "summary"
    },
    "4208": {
        "file_id": 531,
        "content": "import requests\ndef chineseParaphraserAPI(    content:str,\ndebug:bool=False,\n    target_id:int =0,\n    timeout:int=10,\n    providers:list[str]=[\"http://www.wzwyc.com/api.php?key=\", \"http://ai.guiyigs.com/api.php?key=\"] # it is about to close! fuck. \"本站于2023年2月19日关站\" buy code from \"1900373358\"\n    ):\n    target = providers[\n        target_id\n    ]  # all the same?\n    data = {\"info\": content}\n    # target = \"http://www.xiaofamaoai.com/result.php\"\n    # xfm_uid = \"342206661e655450c1c37836d23dc3eb\"\n    # data = {\"contents\":content, \"xfm_uid\":xfm_uid, \"agreement\":\"on\"}\n    # nothing? fuck?\n    r = requests.post(target, data=data,timeout=timeout)\n    output = r.text\n    success = output.strip()!= content.strip()\n    if debug:\n        print(output)\n    return output, success\ncontent =  \"支持几十个不同类型的任务，具有较好的零样本学习能力和少样本学习能力。\"\n# content = \"hello world\"\n# it is clearly translation based.\n# since it did not detect source language. well that's just for fun.\noutput,success =chineseParaphraserAPI(content,debug=True)",
        "type": "code",
        "location": "/tests/title_rewrite_paraphrase/test_api.py:1-32"
    },
    "4209": {
        "file_id": 531,
        "content": "This code is a function that uses an API to paraphrase Chinese text. It takes input content, debug flag, target ID, timeout, and providers' URLs as parameters. The function sends a POST request to the selected provider's URL with the content data and retrieves the response. If the output is not equal to the original content (after removing leading/trailing spaces), it considers the paraphrasing successful. The debug flag controls whether to print the output, and the function returns the output and success status. The given code uses this function to paraphrase a specific Chinese text.",
        "type": "comment"
    },
    "4210": {
        "file_id": 532,
        "content": "/tests/title_rewrite_paraphrase/test.py",
        "type": "filepath"
    },
    "4211": {
        "file_id": 532,
        "content": "This code initializes a ClueAI client for paraphrasing, handles errors, and utilizes LRU cache. It generates paraphrased sentences using OpenAI's GPT2 model and allows configuration options. The \"clueai-base\" model is used to predict prompts and check if they are paraphrases of titles. Debug mode prints predicted text and success status, with an option to return scores.",
        "type": "summary"
    },
    "4212": {
        "file_id": 532,
        "content": "# use our free api first. yes?\nimport yaml\nwith open(\"clueai_api.yaml\", \"r\") as f:\n    apiKey = yaml.load(f, Loader=yaml.FullLoader)[\"api_key\"]\n    print(\"Key?\", apiKey)\nimport clueai\n# initialize the Clueai Client with an API Key\n# 微调用户finetune_user=True\n# cl = clueai.Client(apiKey)\n# print(cl.check_usage(finetune_user=False))\n# shit. we are on trial.\n# {'使用量': 0, '剩余量': 5000, '用户类型': '免费用户'}\nfrom functools import lru_cache\n@lru_cache(maxsize=1)\ndef getClueAIClient(apiKey: str):\n    if apiKey == \"\":\n        return clueai.Client(\"\", check_api_key=False)\n    else:\n        return clueai.Client(apiKey)\ndef clueAIParaphraser(\n    title: str,\n    apiKey: str = \"\",\n    generate_config: dict = {\n        \"do_sample\": True,\n        \"top_p\": 0.8,\n        \"max_length\": 128,  # notice! not too long.\n        \"min_length\": 5,\n        \"length_penalty\": 1.0,\n        \"num_beams\": 1,\n    },\n    prompt_template: str = \"\"\"\n生成与下列文字相同意思的句子：\n{}\n答案：\n\"\"\",\n    debug: bool = False,\n):\n    cl = getClueAIClient(apiKey)  # good without API key\n    prompt = prompt_template.format(title)  # shit.",
        "type": "code",
        "location": "/tests/title_rewrite_paraphrase/test.py:1-46"
    },
    "4213": {
        "file_id": 532,
        "content": "The code initializes a ClueAI client using an API key and provides a function for generating paraphrased sentences. It also includes error handling for cases when no API key is provided or when the trial quota has been exceeded. The code uses LRU cache to store the ClueAI client instance, ensuring that subsequent calls will use the cached instance rather than creating a new one each time. The `clueAIParaphraser` function generates a paraphrased sentence using OpenAI's GPT2 model and provides options for configuring the generation process.",
        "type": "comment"
    },
    "4214": {
        "file_id": 532,
        "content": "    # generate a prediction for a prompt\n    # 如果需要自由调整参数自由采样生成，添加额外参数信息设置方式：generate_config=generate_config\n    prediction = cl.generate(\n        model_name=\"clueai-base\", prompt=prompt, generate_config=generate_config\n    )\n    # 需要返回得分的话，指定return_likelihoods=\"GENERATION\"\n    output = prediction.generations[0].text\n    success = title.strip() != output.strip()\n    if debug:\n        # print the predicted text\n        print(\"prediction: {}\".format(output))\n        print(\"paraphrase success?\", success)\n    return output, success\n# title = \"世上所有小猫都是天使变的！\"\n# title = \"支持几十个不同类型的任务，具有较好的零样本学习能力和少样本学习能力。\"\ntitle = \"十只猫九只都拆家 ！\"\n# title = \"猫：脑子是个好东西但是我没有O.o\"\noutput, success = clueAIParaphraser(title, debug=True)",
        "type": "code",
        "location": "/tests/title_rewrite_paraphrase/test.py:47-67"
    },
    "4215": {
        "file_id": 532,
        "content": "This code generates a prediction for a given prompt using the \"clueai-base\" model and checks if it is a paraphrase of the provided title. It also has an optional parameter \"generate_config\" to adjust sampling and allows returning scores with \"return_likelihoods\". The code uses debug mode to print predicted text and success status.",
        "type": "comment"
    },
    "4216": {
        "file_id": 533,
        "content": "/tests/title_cover_generator/tokenizer.py",
        "type": "filepath"
    },
    "4217": {
        "file_id": 533,
        "content": "This code defines a custom tokenizer class, T5PegasusTokenizer, which extends the BertTokenizer. It takes an optional pre_tokenizer function as input and initializes the base tokenizer. The _tokenize method splits the text into tokens using the pre_tokenizer, and if the token is in the vocabulary, it adds it to split_tokens. If not, it extends split_tokens with tokens generated by the base tokenizer's _tokenize method.",
        "type": "summary"
    },
    "4218": {
        "file_id": 533,
        "content": "import jieba\nfrom transformers import BertTokenizer\n# alike structure as DianJing. but is it for gpt2?\nclass T5PegasusTokenizer(BertTokenizer):\n    def __init__(self, pre_tokenizer=lambda x: jieba.cut(x, HMM=False), *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.pre_tokenizer = pre_tokenizer\n    def _tokenize(self, text, *arg, **kwargs):\n        split_tokens = []\n        for text in self.pre_tokenizer(text):\n            if text in self.vocab:\n                split_tokens.append(text)\n            else:\n                split_tokens.extend(super()._tokenize(text))\n        return split_tokens",
        "type": "code",
        "location": "/tests/title_cover_generator/tokenizer.py:1-17"
    },
    "4219": {
        "file_id": 533,
        "content": "This code defines a custom tokenizer class, T5PegasusTokenizer, which extends the BertTokenizer. It takes an optional pre_tokenizer function as input and initializes the base tokenizer. The _tokenize method splits the text into tokens using the pre_tokenizer, and if the token is in the vocabulary, it adds it to split_tokens. If not, it extends split_tokens with tokens generated by the base tokenizer's _tokenize method.",
        "type": "comment"
    },
    "4220": {
        "file_id": 534,
        "content": "/tests/title_cover_generator/spacy_word_swapper.py",
        "type": "filepath"
    },
    "4221": {
        "file_id": 534,
        "content": "The code uses Spacy and Jieba tokenizer to check if a string contains English, removes non-English elements, and prints the tokens along with their POS and dependency tags. Proper nouns list may be updated and improvements are potential.",
        "type": "summary"
    },
    "4222": {
        "file_id": 534,
        "content": "# just use some simple analysis to extract the template. may not be cost effective like DianJing also you can try the freaking gpt2 model, or pegasus.\nfrom commons import sample_data\n# first assume all to be freaking chinese.\n# import nltk\nimport spacy\nimport jieba\nfrom spacy.lang.zh.examples import sentences \nimport re\ndef recursiveCompiledSearch(compiledRegex, pattern,initPos=0,resultTotal = []):\n    result = compiledRegex.search(pattern)\n    if result !=None:\n        match = result[0]\n        span = result.span()\n        realSpan = (span[0]+initPos, span[1]+initPos)\n        # initialSpan = span[0]\n        endSpan = span[1]\n        initPos += endSpan\n        mresult = {\"match\":match, \"span\":realSpan}\n        resultTotal.append(mresult)\n        newPattern = pattern[endSpan:]\n        return recursiveCompiledSearch(compiledRegex,newPattern,initPos,resultTotal)\n    else: return resultTotal\nnlp = spacy.load(\"zh_core_web_sm\")\n# proper_nouns = ['守望先锋','第五人格']\n# whatever. we can always change shit.\n# nlp.tokenizer.pkuseg_update_user_dict(proper_nouns)",
        "type": "code",
        "location": "/tests/title_cover_generator/spacy_word_swapper.py:1-30"
    },
    "4223": {
        "file_id": 534,
        "content": "The code is importing necessary libraries and defining a function for recursive text search. It uses the Spacy library for Chinese language processing, but it seems to be in progress as it mentions potential improvements and updates. The proper nouns list may be updated or changed later.",
        "type": "comment"
    },
    "4224": {
        "file_id": 534,
        "content": "# this is imoortant.\nenglish = re.compile(r\"([a-zA-Z]+([ \\-,\\.:;?!]+)?)+\")\ndef check_has_language(string,language_re): result = recursiveCompiledSearch(language_re,string,resultTotal=[]); return len(result) >0\nfor elem in sample_data:\n    hasSpace = False\n    # we need to eliminate some english things.\n    # we also have some spaces. remove them before proceed.\n    if \" \" in elem:\n        hasSpace = True\n        elem = elem.replace(\" \", \"\")\n    # some flashy text will never be accepted. if outside of english, chinese we accept nothing.\n    # english is not included in spacy.\n    data = [x for x in jieba.cut(elem)] # contradictory.\n    english_check = check_has_language(elem,english)\n    if english_check:\n        print(\"HAS ENGLISH\")\n        print(elem)\n        continue\n    # check if words contains english. remove these titles.\n    # print(data)\n    nlp.tokenizer.pkuseg_update_user_dict(data)\n    doc = nlp(elem)\n    print(doc.text)\n    for token in doc:\n        print(token.text, token.pos_, token.dep_)",
        "type": "code",
        "location": "/tests/title_cover_generator/spacy_word_swapper.py:31-56"
    },
    "4225": {
        "file_id": 534,
        "content": "This code checks if a given string contains English language, removes spaces and non-English elements using Jieba tokenizer and Spacy, and then prints the tokens along with their part of speech (POS) and dependency tags for further analysis.",
        "type": "comment"
    },
    "4226": {
        "file_id": 535,
        "content": "/tests/title_cover_generator/pyltp_server.py",
        "type": "filepath"
    },
    "4227": {
        "file_id": 535,
        "content": "The code initializes LTP models for NLP tasks, offering functions for segmentation, part-of-speech tagging, named entity recognition, and dependency syntax parsing. It uses PyLTL to extract subject-predicate-object triples from sentences, identifies relationships, and appends them to Dynamic_relation if applicable.",
        "type": "summary"
    },
    "4228": {
        "file_id": 535,
        "content": "# !/usr/bin/env python3\n# -*- coding:utf-8 -*-\n# create on 5/26/20\n__author__ = \"sinsa\"\nimport os\nimport logging\nfrom logging import info, error, warn\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s - PID:%(process)d - %(levelname)s: %(message)s\",\n)\nfrom pyltp import Segmentor\nfrom pyltp import Postagger\nfrom pyltp import NamedEntityRecognizer\nfrom pyltp import Parser\nfrom pyltp import SentenceSplitter\nclass LTP_MODEL:\n    def __init__(self):\n        LTP_DATA_DIR = \"./pyltp_data/ltp_data_v3.4.0\"  # ltp模型目录的路径\n        info(\"loading models ...\")\n        self.cws_model_path = os.path.join(\n            LTP_DATA_DIR, \"cws.model\"\n        )  # 分词模型路径，模型名称为`cws.model`\n        self.segmentor = Segmentor(self.cws_model_path)  # 初始化实例\n        # self.segmentor.load(self.cws_model_path)  # 加载模型\n        info(\"has loaded 分词模型\")\n        self.pos_model_path = os.path.join(\n            LTP_DATA_DIR, \"pos.model\"\n        )  # 词性标注模型路径，模型名称为`pos.model`\n        self.postaggers = Postagger(self.pos_model_path)  # 初始化实例",
        "type": "code",
        "location": "/tests/title_cover_generator/pyltp_server.py:1-35"
    },
    "4229": {
        "file_id": 535,
        "content": "This code initializes the necessary LTP (Language Technology Platform) models for Natural Language Processing tasks. It sets the LTP data directory, loads and initializes models for word segmentation, part-of-speech tagging, named entity recognition, and parsing. The logger is configured to provide status updates during the loading process.",
        "type": "comment"
    },
    "4230": {
        "file_id": 535,
        "content": "        # self.postaggers.load(self.pos_model_path)  # 加载模型\n        info(\"has loaded 词性标注模型\")\n        self.ner_model_path = os.path.join(\n            LTP_DATA_DIR, \"ner.model\"\n        )  # 命名实体识别模型路径，模型名称为`pos.model`\n        self.recognizer = NamedEntityRecognizer(self.ner_model_path)  # 初始化实例\n        # self.recognizer.load(self.ner_model_path)  # 加载模型\n        info(\"has loaded 命名实体识别模型\")\n        self.par_model_path = os.path.join(\n            LTP_DATA_DIR, \"parser.model\"\n        )  # 依存句法分析模型路径，模型名称为`parser.model`\n        self.parser = Parser(self.par_model_path)  # 初始化实例\n        # self.parser.load(self.par_model_path)  # 加载模型\n        info(\"has loaded 依存句法分析模型\")\n    def __release__(self):\n        self.segmentor.release()  # 释放模型\n        self.postaggers.release()  # 释放模型\n        self.recognizer.release()  # 释放模型\n        self.parser.release()  # 释放模型\n    def SplitSentence(self, sentence):\n        sents_list = SentenceSplitter.split(sentence)  # 分句\n        return list(sents_list)\n    def segment(self, input_list):",
        "type": "code",
        "location": "/tests/title_cover_generator/pyltp_server.py:36-61"
    },
    "4231": {
        "file_id": 535,
        "content": "The code loads three models (POS tagging, Named Entity Recognition, and Dependency Parsing) and initializes corresponding recognizers or parsers for each model. It also provides methods to release the models when finished and split a sentence into individual sentences.",
        "type": "comment"
    },
    "4232": {
        "file_id": 535,
        "content": "        \"\"\"\n        功能：实现分词文本的分词\n        返回值：每个文本的形成一个列表[['word1','word2'],['word1','word3'],……]\n        \"\"\"\n        segmented_text_list = []\n        for text in input_list:\n            words = self.segmentor.segment(text)  # 分词\n            segmented_text_list.append(list(words))\n        return segmented_text_list\n    def postagger(self, input_list, return_words_list=False):\n        \"\"\"\n        功能：实现文本中每个词的词性标注\n        返回值：每个文本是一个列表，列表中的每个词也是个列表[[['word1',u'O'],['word2',u'O']],[['word2',u'O'],['word5',u'O']],……]\n        \"\"\"\n        postagger_text_list = []\n        words_list = self.segment(input_list)\n        postags_list = []\n        for words in words_list:\n            postags = self.postaggers.postag(words)  # 词性标注\n            postags_list.append(list(postags))\n            words_postags = list(zip(words, list(postags)))\n            postagger_text_list.append(words_postags)\n        if return_words_list:\n            return words_list, postags_list\n        else:\n            return postagger_text_list\n    def NamedEntityRecognizer(self, input_list, Entity_dist=False, repead=False):",
        "type": "code",
        "location": "/tests/title_cover_generator/pyltp_server.py:62-90"
    },
    "4233": {
        "file_id": 535,
        "content": "This code defines three functions: \"segment\", \"postagger\", and \"NamedEntityRecognizer\". The \"segment\" function takes a list of texts as input, performs segmentation on each text to obtain a list of words, and returns the segmented text as a list of lists. The \"postagger\" function takes a list of texts and performs part-of-speech tagging on each word in the list. It then returns the tagged text as a list of lists. If the \"return_words_list\" parameter is True, it also returns the original words list. The \"NamedEntityRecognizer\" function recognizes named entities in the input texts based on the provided parameters (\"Entity_dist\" and \"repead\").",
        "type": "comment"
    },
    "4234": {
        "file_id": 535,
        "content": "        \"\"\"\n        功能：识别文本中的命名实体：地名，组织名和机构名\n        参数repead：表示是否进行去重处理 ，默认是不去重\n        参数Entity_dist：表示每个文本，返回的识别后的列表，还是抽取后的实体字典，默认返回的是列表\n        返回值的形式：1.[[['word1',u'O'],['word2',u'O'],['word3',u'O']],[['word2',u'O'],['word3',u'O'],['word4',u'O']],……]\n                        2.[{'person':[],'place':[],'organization':[]},{'person':[],'place':[],'organization':[]},{'person':[],'place':[],'organization':[]},……]\n        \"\"\"\n        words_list, postags_list = self.postagger(input_list, return_words_list=True)\n        entity_text_list = []\n        for words, postags in zip(words_list, postags_list):\n            netags = self.recognizer.recognize(\n                words, postags\n            )  # 命名实体识别 人名（Nh）、地名（Ns）、机构名（Ni）\n            text = list(zip(words, netags))\n            entity_text_list.append(text)\n        if Entity_dist:\n            extract_entity_list = []\n            for words_entity_note_list in entity_text_list:\n                extract_entity_list.append(\n                    self.get_entity_dict(words_entity_note_list, repead)",
        "type": "code",
        "location": "/tests/title_cover_generator/pyltp_server.py:91-113"
    },
    "4235": {
        "file_id": 535,
        "content": "This code snippet is responsible for identifying named entities in a given text, such as person names, place names, and organization names. It uses the postagger to identify words and their parts of speech (POS) and then applies the recognizer to recognize named entities based on these POS tags. If Entity_dist is set to True, it extracts entities into a dictionary format.",
        "type": "comment"
    },
    "4236": {
        "file_id": 535,
        "content": "                )\n            return extract_entity_list\n        else:\n            return entity_text_list\n    def get_entity_dict(self, words_entity_note_list, repead):\n        \"\"\"\n        功能：根据实体识别的标志，统计文本中的命名实体\n        参数repead：表示是否进行去重处理 ，默认是不去重\n        返回值：{'person':[],'place':[],'organization':[]}\n        \"\"\"\n        \"\"\"\n        O：这个词不是NE\n        S：这个词单独构成一个NE\n        B：这个词为一个NE的开始\n        I：这个词为一个NE的中间\n        E：这个词位一个NE的结尾\n        Nh：人名\n        Ni：机构名\n        Ns：地名\n        \"\"\"\n        name_entity_dist = {}\n        # 存储不同实体的列表\n        name_entity_list = []\n        place_entity_list = []\n        organization_entity_list = []\n        ntag_E_Nh = \"\"\n        ntag_E_Ni = \"\"\n        ntag_E_Ns = \"\"\n        for word, ntag in words_entity_note_list:\n            # print word+\"/\"+ntag,\n            if ntag[0] != \"O\":\n                if ntag[0] == \"S\":\n                    if ntag[-2:] == \"Nh\":\n                        name_entity_list.append(word)\n                    elif ntag[-2:] == \"Ni\":\n                        organization_entity_list.append(word)",
        "type": "code",
        "location": "/tests/title_cover_generator/pyltp_server.py:114-151"
    },
    "4237": {
        "file_id": 535,
        "content": "This code segment is part of a class method that identifies and categorizes named entities such as persons, places, and organizations from a given list. The code iterates through the list of words along with their corresponding entity tags (O, S, B, I, E) and adds them to separate lists based on the type of entity they represent. If the repead parameter is True, it performs deduplication on the final result.",
        "type": "comment"
    },
    "4238": {
        "file_id": 535,
        "content": "                    else:\n                        place_entity_list.append(word)\n                elif ntag[0] == \"B\":\n                    if ntag[-2:] == \"Nh\":\n                        ntag_E_Nh = ntag_E_Nh + word\n                    elif ntag[-2:] == \"Ni\":\n                        ntag_E_Ni = ntag_E_Ni + word\n                    else:\n                        ntag_E_Ns = ntag_E_Ns + word\n                elif ntag[0] == \"I\":\n                    if ntag[-2:] == \"Nh\":\n                        ntag_E_Nh = ntag_E_Nh + word\n                    elif ntag[-2:] == \"Ni\":\n                        ntag_E_Ni = ntag_E_Ni + word\n                    else:\n                        ntag_E_Ns = ntag_E_Ns + word\n                else:\n                    if ntag[-2:] == \"Nh\":\n                        ntag_E_Nh = ntag_E_Nh + word\n                        name_entity_list.append(ntag_E_Nh)\n                        ntag_E_Nh = \"\"\n                    elif ntag[-2:] == \"Ni\":\n                        ntag_E_Ni = ntag_E_Ni + word\n                        organization_entity_list.append(ntag_E_Ni)",
        "type": "code",
        "location": "/tests/title_cover_generator/pyltp_server.py:152-175"
    },
    "4239": {
        "file_id": 535,
        "content": "The code is segmenting named entities (name and organization) using the NER (Named Entity Recognition) model. It appends words to separate variables based on their tags, forming name and organization lists when encountering \"Nh\" or \"Ni\". If no entity is detected, it simply adds the word to the place entity list.",
        "type": "comment"
    },
    "4240": {
        "file_id": 535,
        "content": "                        ntag_E_Ni = \"\"\n                    else:\n                        ntag_E_Ns = ntag_E_Ns + word\n                        place_entity_list.append(ntag_E_Ns)\n                        ntag_E_Ns = \"\"\n        if repead:\n            name_entity_dist[\"person\"] = list(set(name_entity_list))\n            name_entity_dist[\"organization\"] = list(set(organization_entity_list))\n            name_entity_dist[\"place\"] = list(set(place_entity_list))\n        else:\n            name_entity_dist[\"person\"] = name_entity_list\n            name_entity_dist[\"organization\"] = organization_entity_list\n            name_entity_dist[\"place\"] = place_entity_list\n        return name_entity_dist\n    def SyntaxParser(self, input_list, return_words_pos=False):\n        \"\"\"\n        # head = parent+1\n        # relation = relate  可以从中间抽取head 和 relation 构成LTP 的标准输出，但是为了根据自己的情况，直接输出返回的全部的信息\n        功能：实现依存句法分析\n        返回值：每个文本的形成一个列表\n        [[{u'relate': u'WP', u'cont': u'\\uff0c', u'id': 4, u'parent': 3, u'pos': u'wp'},{u'relate': u'RAD', u'cont': u'\\u7684', u'id': 1, u'parent': 0, u'pos': u'u'}],……]",
        "type": "code",
        "location": "/tests/title_cover_generator/pyltp_server.py:176-198"
    },
    "4241": {
        "file_id": 535,
        "content": "This code defines a function `name_entity_dist` that handles named entity recognition and extraction. It identifies named entities (person, organization, place) and stores them in separate lists. The function then adds these lists to a dictionary called `name_entity_dist`, which is returned at the end. Additionally, there's another function `SyntaxParser` that performs dependency syntax parsing on the input list and returns a list of parsed relations between words.",
        "type": "comment"
    },
    "4242": {
        "file_id": 535,
        "content": "        \"\"\"\n        words_list, postags_list = self.postagger(input_list, return_words_list=True)\n        syntaxparser_text_list = []\n        for words, postags in zip(words_list, postags_list):\n            arcs = self.parser.parse(words, postags)  # 句法分析\n            # res = [(arc.head, arc.relation) for arc in arcs]\n            res = [arc for arc in arcs] # arguable.\n            # for arc in arcs:\n            #     print(arc)\n            # breakpoint()\n            text = []\n            for i in range(len(words)):\n                tt = {\n                    \"id\": i,\n                    \"cont\": words[i],\n                    \"pos\": postags[i],\n                    \"parent\": res[i][0],\n                    \"relate\": res[i][1],\n                }\n                text.append(tt)\n            syntaxparser_text_list.append(text)\n        if return_words_pos:\n            return words_list, postags_list, syntaxparser_text_list\n        else:\n            return syntaxparser_text_list\n    def triple_extract(self, intput_list):\n        \"\"\"",
        "type": "code",
        "location": "/tests/title_cover_generator/pyltp_server.py:199-229"
    },
    "4243": {
        "file_id": 535,
        "content": "The code is performing syntax parsing using a parser. It takes an input list of words and their corresponding part-of-speech tags, then applies the parser to generate a syntactic parse tree for each word in the input list. The resulting parse trees are stored as a list of dictionaries with information about each word's id, content, part-of-speech tag, parent, and relation. If 'return_words_pos' is True, it returns the words list, postags list, and syntaxparser_text_list. Otherwise, it only returns the syntaxparser_text_list.",
        "type": "comment"
    },
    "4244": {
        "file_id": 535,
        "content": "        功能: 对于给定的句子进行事实三元组抽取\n        Args:\n            sentence: 要处理的语句\n                        形式是：'真实的句子'\n        \"\"\"\n        Subjective_guest = []  # 主谓宾关系(e1,r,e2)\n        Dynamic_relation = []  # 动宾关系\n        Guest = []  # 介宾关系\n        Name_entity_relation = []  # 命名实体之间的关系\n        # 分词后词的列表 words，词性列表 postags，实体标志列表 netags，语法分析列表 arcs\n        words = []\n        postags = []\n        netags = []\n        arcs = []\n        syntaxparser_text_list = self.SyntaxParser(intput_list)\n        entity_list = self.NamedEntityRecognizer(intput_list)\n        for words_property_list in syntaxparser_text_list[0]:\n            words.append(words_property_list[\"cont\"])\n            postags.append(words_property_list[\"pos\"])\n            arcs.append(\n                {\n                    \"head\": words_property_list[\"parent\"],\n                    \"relation\": words_property_list[\"relate\"],\n                }\n            )\n        for words_entity_list in entity_list[0]:\n            netags.append(words_entity_list[1])\n        child_dict_list = self.build_parse_child_dict(words, postags, arcs)",
        "type": "code",
        "location": "/tests/title_cover_generator/pyltp_server.py:230-258"
    },
    "4245": {
        "file_id": 535,
        "content": "This function performs triplet extraction for a given sentence. It initializes various lists for different relationships and then extracts words, postags, arcs (syntax), and netags (named entities) using the input list. Finally, it builds a dictionary of child relationships from the extracted data.",
        "type": "comment"
    },
    "4246": {
        "file_id": 535,
        "content": "        for index in range(len(postags)):\n            # 抽取以谓词为中心的事实三元组\n            if postags[index] == \"v\":\n                child_dict = child_dict_list[index]\n                # 主谓宾\n                if \"SBV\" in child_dict and \"VOB\" in child_dict:\n                    e1 = self.complete_e(\n                        words, postags, child_dict_list, child_dict[\"SBV\"][0]\n                    )\n                    r = words[index]\n                    e2 = self.complete_e(\n                        words, postags, child_dict_list, child_dict[\"VOB\"][0]\n                    )\n                    Subjective_guest.append((e1, r, e2))\n                # 定语后置，动宾关系\n                if arcs[index][\"relation\"] == \"ATT\":\n                    if \"VOB\" in child_dict:\n                        e1 = self.complete_e(\n                            words, postags, child_dict_list, arcs[index][\"head\"] - 1\n                        )\n                        r = words[index]\n                        e2 = self.complete_e(\n                            words, postags, child_dict_list, child_dict[\"VOB\"][0]",
        "type": "code",
        "location": "/tests/title_cover_generator/pyltp_server.py:260-284"
    },
    "4247": {
        "file_id": 535,
        "content": "This code is extracting subject-predicate-object (SPO) triples from a natural language sentence using PyLTP library. It identifies the verb as the center of the triple and checks for two possible structures: \"SBV\" followed by \"VOB\" or \"ATT\" relation after the verb. The code fills in the subject, predicate, and object entities based on the identified positions in the sentence.",
        "type": "comment"
    },
    "4248": {
        "file_id": 535,
        "content": "                        )\n                        temp_string = r + e2\n                        if temp_string == e1[: len(temp_string)]:\n                            e1 = e1[len(temp_string) :]\n                        if temp_string not in e1:\n                            Dynamic_relation.append((e1, r, e2))\n                # 含有介宾关系的主谓动补关系\n                if \"SBV\" in child_dict and \"CMP\" in child_dict:\n                    # e1 = words[child_dict['SBV'][0]]\n                    e1 = self.complete_e(\n                        words, postags, child_dict_list, child_dict[\"SBV\"][0]\n                    )\n                    cmp_index = child_dict[\"CMP\"][0]\n                    r = words[index] + words[cmp_index]\n                    if \"POB\" in child_dict_list[cmp_index]:\n                        e2 = self.complete_e(\n                            words,\n                            postags,\n                            child_dict_list,\n                            child_dict_list[cmp_index][\"POB\"][0],\n                        )",
        "type": "code",
        "location": "/tests/title_cover_generator/pyltp_server.py:285-306"
    },
    "4249": {
        "file_id": 535,
        "content": "This code checks for a specific relationship between the subject, verb, object, and complement in a sentence. It appends the relationship (e1, r, e2) to Dynamic_relation if it meets certain conditions such as not containing an existing temporary string or being part of the original text.",
        "type": "comment"
    },
    "4250": {
        "file_id": 535,
        "content": "                        Guest.append((e1, r, e2))\n            # 尝试抽取命名实体有关的三元组\n            if netags[index][0] == \"S\" or netags[index][0] == \"B\":\n                ni = index\n                if netags[ni][0] == \"B\":\n                    while netags[ni][0] != \"E\":\n                        ni += 1\n                    e1 = \"\".join(words[index : ni + 1])\n                else:\n                    e1 = words[ni]\n                # 上面是抽取实体，没有判断是什么类型的实体。。\n                if (\n                    arcs[ni][\"relation\"] == \"ATT\"\n                    and postags[arcs[ni][\"head\"] - 1] == \"n\"\n                    and netags[arcs[ni][\"head\"] - 1] == \"O\"\n                ):\n                    r = self.complete_e(\n                        words, postags, child_dict_list, arcs[ni][\"head\"] - 1\n                    )\n                    if e1 in r:\n                        r = r[(r.index(e1) + len(e1)) :]\n                    if (\n                        arcs[arcs[ni][\"head\"] - 1][\"relation\"] == \"ATT\"\n                        and netags[arcs[arcs[ni][\"head\"] - 1][\"head\"] - 1] != \"O\"",
        "type": "code",
        "location": "/tests/title_cover_generator/pyltp_server.py:307-331"
    },
    "4251": {
        "file_id": 535,
        "content": "This code attempts to extract named entity triples. It checks if the current tag is a start or begin tag, and then extracts the named entity based on that. If it meets specific conditions involving \"ATT\" relation and certain postags, it completes the entity and checks if the extracted entity is in the result.",
        "type": "comment"
    },
    "4252": {
        "file_id": 535,
        "content": "                    ):\n                        e2 = self.complete_e(\n                            words,\n                            postags,\n                            child_dict_list,\n                            arcs[arcs[ni][\"head\"] - 1][\"head\"] - 1,\n                        )\n                        mi = arcs[arcs[ni][\"head\"] - 1][\"head\"] - 1\n                        li = mi\n                        if netags[mi][0] == \"B\":\n                            while netags[mi][0] != \"E\":\n                                mi += 1\n                            e = \"\".join(words[li + 1 : mi + 1])\n                            e2 += e\n                        if r in e2:\n                            e2 = e2[(e2.index(r) + len(r)) :]\n                        if r + e2 in sentence:\n                            Name_entity_relation.append((e1, r, e2))\n        return Subjective_guest, Dynamic_relation, Guest, Name_entity_relation\n    def build_parse_child_dict(self, words, postags, arcs):\n        \"\"\"\n        功能：为句子中的每个词语维护一个保存句法依存儿子节点的字典",
        "type": "code",
        "location": "/tests/title_cover_generator/pyltp_server.py:332-354"
    },
    "4253": {
        "file_id": 535,
        "content": "The code defines a function called `build_parse_child_dict` which takes in the words, postags, and arcs of a sentence. It creates a dictionary for each word in the sentence that stores its syntactic dependency children. If a relation word exists between two named entities, it is added to the Name_entity_relation list. The function returns four variables: Subjective_guest, Dynamic_relation, Guest, and Name_entity_relation",
        "type": "comment"
    },
    "4254": {
        "file_id": 535,
        "content": "        Args:\n            words: 分词列表\n            postags: 词性列表\n            arcs: 句法依存列表\n        \"\"\"\n        child_dict_list = []\n        for index in range(len(words)):\n            child_dict = dict()\n            for arc_index in range(len(arcs)):\n                if arcs[arc_index][\"head\"] == index + 1:\n                    if arcs[arc_index][\"relation\"] in child_dict:\n                        child_dict[arcs[arc_index][\"relation\"]].append(arc_index)\n                    else:\n                        child_dict[arcs[arc_index][\"relation\"]] = []\n                        child_dict[arcs[arc_index][\"relation\"]].append(arc_index)\n            child_dict_list.append(child_dict)\n        return child_dict_list\n    def complete_e(self, words, postags, child_dict_list, word_index):\n        \"\"\"\n        功能：完善识别的部分实体\n        \"\"\"\n        child_dict = child_dict_list[word_index]\n        prefix = \"\"\n        if \"ATT\" in child_dict:\n            for i in range(len(child_dict[\"ATT\"])):\n                prefix += self.complete_e(\n                    words, postags, child_dict_list, child_dict[\"ATT\"][i]",
        "type": "code",
        "location": "/tests/title_cover_generator/pyltp_server.py:355-383"
    },
    "4255": {
        "file_id": 535,
        "content": "This function takes in a list of words, their respective parts of speech (postags), and syntactic dependency relations (arcs) as input. It organizes the arcs into a dictionary structure for each word in the list, and returns this dictionary list. The next function aims to further refine or \"complete\" part of the identified entities by recursively calling itself with the appropriate parameters.",
        "type": "comment"
    },
    "4256": {
        "file_id": 535,
        "content": "                )\n        postfix = \"\"\n        if postags[word_index] == \"v\":\n            if \"VOB\" in child_dict:\n                postfix += self.complete_e(\n                    words, postags, child_dict_list, child_dict[\"VOB\"][0]\n                )\n            if \"SBV\" in child_dict:\n                prefix = (\n                    self.complete_e(\n                        words, postags, child_dict_list, child_dict[\"SBV\"][0]\n                    )\n                    + prefix\n                )\n        return prefix + words[word_index] + postfix\nif __name__ == \"__main__\":\n    # intput_list = [\"中国自称为炎黄子孙、龙的传人\"]\n    # incorrect name spliters.\n    from commons import sample_data\n    intput_list = sample_data\n    model = LTP_MODEL()\n    input_sentence = \"雅生活服务的物业管理服务。\"\n    # print(model.SplitSentence(input_sentence))\n    # print(model.segment(intput_list))\n    # print(model.postagger(intput_list))\n    # print(model.NamedEntityRecognizer(intput_list, Entity_dist=True))\n    print(model.NamedEntityRecognizer(intput_list))",
        "type": "code",
        "location": "/tests/title_cover_generator/pyltp_server.py:384-414"
    },
    "4257": {
        "file_id": 535,
        "content": "This code segment is a part of the Named Entity Recognizer function in a Chinese language processing model. It takes an input list containing sentences, and based on postags (part-of-speech tags), it identifies named entities within the text and returns them. The code snippet handles verbs with \"VOB\" or \"SBV\" child nodes differently by appending prefixes accordingly, and then combines prefix, word, and postfix to generate the final output.",
        "type": "comment"
    },
    "4258": {
        "file_id": 535,
        "content": "    # print(model.SyntaxParser(intput_list))\n    (\n        Subjective_guest,\n        Dynamic_relation,\n        Guest,\n        Name_entity_relation,\n    ) = model.triple_extract(intput_list)\n    print(\"=\" * 30)\n    print(Subjective_guest, Dynamic_relation, Guest, Name_entity_relation)\n    model.__release__()",
        "type": "code",
        "location": "/tests/title_cover_generator/pyltp_server.py:415-426"
    },
    "4259": {
        "file_id": 535,
        "content": "Extracting triples from input list using the model's triple_extract method, then printing them and releasing resources.",
        "type": "comment"
    },
    "4260": {
        "file_id": 536,
        "content": "/tests/title_cover_generator/pegasus_trainer.py",
        "type": "filepath"
    },
    "4261": {
        "file_id": 536,
        "content": "This code trains a PEGASUS machine translation model using MT5ForConditionalGeneration, loads data, tokenizes text, and saves weights every 5000 updates for 1000 epochs. It includes optimization steps, error handling, and backup saving functionality. The trainer is dissatisfied with the current performance.",
        "type": "summary"
    },
    "4262": {
        "file_id": 536,
        "content": "from commons import load_train_data_core, import_word\nWord = import_word()\n# print(Word)\n# break()\n#importing the PEGASUS Transformer model\nimport torch\nfrom transformers import MT5ForConditionalGeneration\nfrom tokenizer import T5PegasusTokenizer\nmodel_path = \"./pegasus_title_generation/pegasus_1\" # trained on paraphrase tasks.\n# model_name = './t5_pegasus_training/t5_pegasus'\nmodel_name = model_path\nmodel_name_or_path = model_name\ntorch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntokenizer = T5PegasusTokenizer.from_pretrained(model_name)\nmodel = MT5ForConditionalGeneration.from_pretrained(model_name).to(torch_device)\n# import random\n# import progressbar\ndef mydataset(len_threshold = 2,batchsize=1): # train till you fucking die. this almost depleted my VRAM. better train this shit elsewhere.\n    for a,b in load_train_data_core(len_threshold = 2,batchsize=1): yield a,b # freaking shit.\ndef get_train_data(batchsize=2,max_length=1024):",
        "type": "code",
        "location": "/tests/title_cover_generator/pegasus_trainer.py:2-27"
    },
    "4263": {
        "file_id": 536,
        "content": "Code imports necessary libraries for loading training data, tokenizing text with T5PegasusTokenizer and initializing the PEGASUS Transformer model. The MT5ForConditionalGeneration model is loaded from a pre-trained checkpoint located at `model_path` or `model_name`. The code also specifies the device to use for training (either CPU or CUDA-enabled GPU). A function named `mydataset` is defined, which generates a dataset from load_train_data_core with specified batch size and length threshold. Another function `get_train_data` takes in batch size and maximum sequence length as inputs. This code seems to be used for training the PEGASUS model on specific tasks.",
        "type": "comment"
    },
    "4264": {
        "file_id": 536,
        "content": "    for source_sentences, target_sentences in mydataset(batchsize=batchsize):\n        # targetSentence = [\"今天天气不错\",\"你吃了没有\"]\n        batchsize = len(source_sentences)\n        if batchsize >0:\n        # print([source_sentence,target_sentence])\n            input_ids = tokenizer.batch_encode_plus(source_sentences,max_length=max_length,padding=True,truncation=True, return_tensors=\"pt\").input_ids.to(device)\n            labels = tokenizer.batch_encode_plus(target_sentences,return_tensors=\"pt\",padding=True,truncation=True,max_length=max_length,).input_ids.to(device) # what is the freaking max_length?\n            yield input_ids, labels\n# from torch.optim import SGD\n# from torch.optim import ASGD as SGD\nfrom torch.optim import RMSprop as SGD\nbatchsize = 2\n# optimizer = SGD(model.parameters(), momentum=0.9, lr=0.000001*batchsize, weight_decay=0.0001)\noptimizer = SGD(model.parameters(), lr=0.00001*batchsize, weight_decay=0.0001)\nloss_mean = []\nmean_loss_period = 100\nepochs = 1000\nmsaveperiod = 5000 # wtf is 30000\nupdate_period = 1 # hell man.",
        "type": "code",
        "location": "/tests/title_cover_generator/pegasus_trainer.py:28-49"
    },
    "4265": {
        "file_id": 536,
        "content": "This code is training a model with batch size 2. The maximum length of input sentences is not defined. The optimizer is using RMSprop algorithm, with a learning rate adjusted by the batch size. The mean loss over the last 100 batches is stored in 'loss_mean' list. Training continues for 1000 epochs and model weights are saved every 5000 updates. Updates to the model occur once per iteration.",
        "type": "comment"
    },
    "4266": {
        "file_id": 536,
        "content": "#setting up the model\n# def get_response(input_text):\n#   batch = tokenizer.encode(input_text, return_tensors=\"pt\").to(torch_device)\n#   translated = model.generate(batch,decoder_start_token_id=tokenizer.cls_token_id,eos_token_id=tokenizer.sep_token_id,max_length=30).cpu().numpy()[0]\n#   tgt_text = ''.join(tokenizer.decode(translated[1:])).replace(' ', '')\n#   return tgt_text\n# not so bad?\n# can you train this shit?\n# print(get_response(\"你吃了没有\"))\nfor epoch in range(epochs):\n    print(\"STARTING EPOCH {} TOTAL {}\".format(epoch,epochs))\n    for index, (input_ids, labels) in enumerate(get_train_data(batchsize=batchsize)):\n        try:\n            if index%update_period == 0:\n                optimizer.zero_grad()\n            # print([input_ids, labels])\n            outputs = model(input_ids=input_ids, labels=labels)\n            loss = outputs.loss\n            floss = loss.tolist()\n            loss_mean.append(floss)\n            if len(loss_mean) == mean_loss_period:\n                mloss = sum(loss_mean)/mean_loss_period",
        "type": "code",
        "location": "/tests/title_cover_generator/pegasus_trainer.py:52-77"
    },
    "4267": {
        "file_id": 536,
        "content": "This code sets up a model for generating translations, trains it using train data, and calculates the loss. It uses a tokenizer to encode input texts and generate translations, then calculates the mean loss over a specified period.",
        "type": "comment"
    },
    "4268": {
        "file_id": 536,
        "content": "                print(\"EPOCH {} TOTAL {}\".format(epoch,epochs))\n                print(\"MEAN LOSS OVER {} SAMPLES: {}\".format(mean_loss_period,str(mloss)[:5]))\n                loss_mean = []\n            loss.backward()\n            # logits = outputs.logits\n            if index % update_period == 0:\n                optimizer.step() # this is shit. i should run this shit in kaggle.\n        except:\n            import traceback\n            traceback.print_exc()\n            print(\"POSSIBLY OOM\")\n        if index > (msaveperiod - 1) and index%msaveperiod == 0:\n            print(\"SAVING MODEL AT {} SAMPLES\".format(index))\n            model.save_pretrained(model_name_or_path)\n            # shutil.copy(model_name_or_path,model_name_or_path+\"-backup\")\n            model.save_pretrained(model_name_or_path+\"-backup\")\n            ## it is working.",
        "type": "code",
        "location": "/tests/title_cover_generator/pegasus_trainer.py:78-94"
    },
    "4269": {
        "file_id": 536,
        "content": "This code appears to be part of a training loop for a machine learning model. It keeps track of the mean loss over a certain period and saves the model after a set number of samples. The code includes an optimization step, error handling, and backup saving functionality. The trainer seems frustrated with the line \"this is shit. i should run this shit in kaggle.\"",
        "type": "comment"
    },
    "4270": {
        "file_id": 537,
        "content": "/tests/title_cover_generator/paddlenlp_word_label.py",
        "type": "filepath"
    },
    "4271": {
        "file_id": 537,
        "content": "This code imports Taskflow from paddlenlp and commons.sample_data. It iterates through sample data, applying various tasks (word segmentation, NER, POS tagging, etc.) to each element using Taskflow. If the flow is information extraction, it uses a schema for processing. After processing, it deletes the Taskflow object and prints the task name along with the processed data.",
        "type": "summary"
    },
    "4272": {
        "file_id": 537,
        "content": "from paddlenlp import  Taskflow\nfrom commons import sample_data\n# LAC 词语重要性\nfor elem in sample_data:\n    flows = [\"word_segmentation\",\"ner\",\"pos_tagging\",\"dependency_parsing\",\"information_extraction\",\"sentiment_analysis\",\"text_correction\",\"knowledge_mining\"]\n    for flow in flows:\n        if flow !=\"information_extraction\":\n            seg = Taskflow(flow) # need schema for information extraction.\n        else:\n            schema = [\"主语\",\"谓语\",\"宾语\"]\n            seg = Taskflow(flow, schema=schema) # need schema for information extraction\n        data = seg(elem)\n        del seg\n        print(flow,data)",
        "type": "code",
        "location": "/tests/title_cover_generator/paddlenlp_word_label.py:1-16"
    },
    "4273": {
        "file_id": 537,
        "content": "This code imports Taskflow from paddlenlp and commons.sample_data. It iterates through sample data, applying various tasks (word segmentation, NER, POS tagging, etc.) to each element using Taskflow. If the flow is information extraction, it uses a schema for processing. After processing, it deletes the Taskflow object and prints the task name along with the processed data.",
        "type": "comment"
    },
    "4274": {
        "file_id": 538,
        "content": "/tests/title_cover_generator/gpt2_train.sh",
        "type": "filepath"
    },
    "4275": {
        "file_id": 538,
        "content": "This code navigates into the \"GPT2-NewsTitle\" directory and creates a new directory named \"output_dir\". It then runs the Python script \"train.py\" to train a GPT-2 model for generating news titles or covers.",
        "type": "summary"
    },
    "4276": {
        "file_id": 538,
        "content": "cd GPT2-NewsTitle\nmkdir output_dir\npython3 train.py",
        "type": "code",
        "location": "/tests/title_cover_generator/gpt2_train.sh:1-3"
    },
    "4277": {
        "file_id": 538,
        "content": "This code navigates into the \"GPT2-NewsTitle\" directory and creates a new directory named \"output_dir\". It then runs the Python script \"train.py\" to train a GPT-2 model for generating news titles or covers.",
        "type": "comment"
    },
    "4278": {
        "file_id": 539,
        "content": "/tests/title_cover_generator/gpt2_title_data_prep.py",
        "type": "filepath"
    },
    "4279": {
        "file_id": 539,
        "content": "Code copies train data as test data, creates a new data directory if it doesn't exist, writes train data to a JSON file, and then copies the train file to the test file.",
        "type": "summary"
    },
    "4280": {
        "file_id": 539,
        "content": "# simply copy train shit as test shit.\nfrom commons import load_train_data_core, import_word\nWord = import_word()\nimport json\ndata = []\nimport os\ndata_dir = \"/media/root/help/pyjom/tests/title_cover_generator/GPT2-NewsTitle/data_dir\"\nif not os.path.exists(data_dir):\n    os.mkdir(data_dir)\ntrain_file = os.path.join(data_dir,\"train_data.json\")\ntest_file = os.path.join(data_dir,\"test_data.json\")\nfor content, title in load_train_data_core():\n    sample = {\"title\": title[0],\"content\":content[0]}\n    data.append(sample) # is that necessary?\nwith open(train_file,\"w+\",encoding=\"utf8\") as f:\n    f.write(json.dumps(data,ensure_ascii=False,indent=4))\nimport shutil\nshutil.copy(train_file, test_file)",
        "type": "code",
        "location": "/tests/title_cover_generator/gpt2_title_data_prep.py:1-26"
    },
    "4281": {
        "file_id": 539,
        "content": "Code copies train data as test data, creates a new data directory if it doesn't exist, writes train data to a JSON file, and then copies the train file to the test file.",
        "type": "comment"
    },
    "4282": {
        "file_id": 540,
        "content": "/tests/title_cover_generator/commons.py",
        "type": "filepath"
    },
    "4283": {
        "file_id": 540,
        "content": "The code loads and preprocesses training data using load_train_data_core function, iterating through indexes of text chunks, transforming words, and creating Word class instances. It applies shuffle and progress bar for efficient data access.",
        "type": "summary"
    },
    "4284": {
        "file_id": 540,
        "content": "sample_data = [\"【翎伶】world.execute;(me);\", \"【封校日常】沙拉制作\", \"【Blender场景动画】新代 : 城市【VictoryLuode】\", \"历时733天! 圆了挖机梦，我独立造了一台可遥控小型挖机\", \"【难懂的数学】傅里叶、拉普拉斯、卷积、欧拉方程、梯度散度、拉格朗日方程、奈奎斯特采样、虚数等抽象难懂数学一网打尽\", \"这些up主是中学生和大学生的救星啊啊啊啊啊！！！学习方法｜免费课程｜兴趣技能｜生涯规划\", \"【不止游戏】游戏和电影中的M4，究竟有多经典？\", \"Steam++ 新版v2.7发布 新功能介绍\", \"手绘503张！还原数码宝贝OP\", \"好可爱鸭~ summertime\", \"男室友偷偷逛站酷网，毕设惊艳全校！\", \"对不起，我笑得真的很大声！【第一届立直麻将联赛】\", \"在南京每天画画一小时，在家接单养活自己！\", \"没有什么事情是一个纸团解决不了的，如果有那就用很多个\", \"到底是什么让我能在公园大爷面前如此的自信？\", \"欲拔山城寨，先过五虎将\", \"杨侃最下饭｜27 杨毅：经纪人不能太贪心\", \"【深渊的呼唤V】全球总决赛-决赛 Wolves vs SST\", \"【安特卫普MAJOR】亚洲区预选赛 TYLOO vs Renegades\", \"狼队第五人格分部成立两周年啦！\", \"【守望先锋联赛】英雄崛起!准备好迎接2022赛季!\"]\nimport progressbar\nimport random\ndef load_train_data_core(shuffle=True,batchsize=1,len_threshold = 2,no_unk=True):\n    filepath = \"/media/root/help/pyjom/tests/title_cover_generator/DianJing/data/basic_data_80k_v2.pkl\"\n    # warning...\n    import pickle\n    fobj = open(filepath, 'rb')\n    # print([fobj])\n    # breakpoint()\n    class Word:\n        def __init__(self,val,tf,df):\n            self.val = val",
        "type": "code",
        "location": "/tests/title_cover_generator/commons.py:1-17"
    },
    "4285": {
        "file_id": 540,
        "content": "The code imports the progressbar and random libraries, defines a function load_train_data_core which takes optional parameters shuffle, batchsize, len_threshold, and no_unk. The filepath variable stores the path to a pickle file containing data for training. The function opens the file using pickle's open function in read binary mode and does not perform any additional operations on its contents before returning.",
        "type": "comment"
    },
    "4286": {
        "file_id": 540,
        "content": "            self.tf = tf\n            self.df = df\n        def __repr__(self):\n            pass\n    _, word2idx, idx2word, targets, srcs= pickle.load(fobj) # freaking swap.\n    # titles, abstracts\n    # print(titles) # these are not freaking words. numbers.\n    # print(abstracts)\n    for key in idx2word:\n        elem = idx2word[key]\n        if elem.startswith('<') and elem.endswith('>'):\n            elem = elem[1:-1].upper()\n            elem = \"[{}]\".format(elem)\n            idx2word[key] =elem\n    # you can freaking get the data.\n    # title = titles[0]\n    len_indexs = len(targets)\n    # indexs = [x for x in range(indexs)]\n        # random.shuffle(indexs)\n    randomIdx = [x for x in range(len_indexs)]\n    if shuffle:\n        random.shuffle(randomIdx)\n    randomIdx2 = [randomIdx[x*batchsize:(x+1)*batchsize] for x in range(len(randomIdx)//batchsize+1)]\n    len_srcs = len(srcs)\n    len_targets = len(targets)\n    # mfilter = lambda x: x.replace(\" \",\"\").replace(\"\\n\",\"\")\n    for indexs in progressbar.progressbar(randomIdx2):",
        "type": "code",
        "location": "/tests/title_cover_generator/commons.py:18-44"
    },
    "4287": {
        "file_id": 540,
        "content": "The code is loading a pickle file, extracting relevant data including titles and abstracts. It then modifies some elements in the idx2word dictionary by replacing specific characters with formatted strings. The code provides random indexes for accessing the data and applies a shuffle if required. Lastly, it uses a progress bar for iterating over the shuffled indexes to access the data.",
        "type": "comment"
    },
    "4288": {
        "file_id": 540,
        "content": "        src_result=[]\n        target_result=[]\n        for index in indexs:\n            if index < len_srcs and index < len_targets:\n                src, target = srcs[index], targets[index]\n                src, target = [idx2word[x] for x in src], [idx2word[x] for x in target]\n                src, target = \"\".join(src),\"\".join(target)\n                if no_unk:\n                    src, target = src.replace(\"[UNK]\",\"\"), target.replace(\"[UNK]\",\"\")\n                # src, target = mfilter(src), mfilter(target)\n                if max(len(src),len(target)) > len_threshold:\n                    src_result.append(src)\n                    target_result.append(target)\n        if len(src_result) >0:\n            yield src_result,target_result\n    # for index in indexs:\n    #     title = titles[index]\n    #     mytitle = [idx2word[x] for x in title]\n    #     abstract = abstracts[index]\n    #     myabstract = [idx2word[x] for x in abstract]\n    #     if join:\n    #         yield \"\".join(mytitle), \"\".join(myabstract)\n    #     else: yield mytitle, myabstract",
        "type": "code",
        "location": "/tests/title_cover_generator/commons.py:45-67"
    },
    "4289": {
        "file_id": 540,
        "content": "This code is iterating through indexes in two lists of text chunks, transforming them to word form, joining the words into strings, and removing [UNK] tokens if specified. If any resulting string exceeds a certain length threshold, it appends them to two result lists. The code yields these two result lists if there are at least one entry.",
        "type": "comment"
    },
    "4290": {
        "file_id": 540,
        "content": "    # print(mytitle)\n    # breakpoint()\ndef import_word():\n    # if __name__ == \"__main__\":\n    class Word:\n        def __init__(self,val,tf,df):\n            self.val = val\n            self.tf = tf\n            self.df = df\n        def __repr__(self):\n            pass\n    return Word\nif __name__ == '__main__':\n    Word = import_word()\n    for title, abstract in load_train_data_core():\n        print(title)\n        print(abstract) # we have <unk> tokens. how do we freaking deal with it?\n        breakpoint()",
        "type": "code",
        "location": "/tests/title_cover_generator/commons.py:68-87"
    },
    "4291": {
        "file_id": 540,
        "content": "The code defines a function `import_word` that returns a class named Word. The class has attributes `val`, `tf`, and `df`. The code then checks if it is being run as the main program and creates instances of the Word class from loaded data, printing title and abstract. It encounters a breakpoint to debug or inspect the handling of tokens in the code.",
        "type": "comment"
    },
    "4292": {
        "file_id": 541,
        "content": "/tests/vapoursynth_linux_test/view_test.py",
        "type": "filepath"
    },
    "4293": {
        "file_id": 541,
        "content": "The code imports VapourSynth library functions, creates a Video object with FFMS2 source and option to transpose, but generating previews isn't working as vspipe uses existing APIs and can only generate raw frame data. OpenCV might help; example at https://github.com/UniversalAl/view.",
        "type": "summary"
    },
    "4294": {
        "file_id": 541,
        "content": "videoPath = \"/root/Desktop/works/pyjom/samples/video/dog_with_text.mp4\"\n# videoPath = \"/Users/jamesbrown/desktop/works/pyjom_remote/samples/video/dog_with_text.mp4\"\n# reference: http://www.vapoursynth.com/doc/pythonreference.html\n# The VideoFrame and AudioFrame classes contains one picture/audio chunk and all the metadata associated with it. It is possible to access the raw data using either get_read_ptr(plane) or get_write_ptr(plane) and get_stride(plane) with ctypes.\n# A more Python friendly wrapping is also available where each plane/channel can be accessed as a Python array using frame[plane/channel].\n# To get a frame simply call get_frame(n) on a clip. Should you desire to get all frames in a clip, use this code:\n# for frame in clip.frames():\n#     # Do stuff with your frame\n#     pass\nfrom vapoursynth import core\nvideo = core.ffms2.Source(source=videoPath)\n# video = core.std.Transpose(video)\n# video.set_output()\n# from viewKali import Preview\n# clip = vs.core.lsmas.LibavSMASHSource('source.mp4')",
        "type": "code",
        "location": "/tests/vapoursynth_linux_test/view_test.py:1-23"
    },
    "4295": {
        "file_id": 541,
        "content": "The code imports the necessary functions from the VapourSynth library and defines a video path. It then creates a Video object using the FFMS2 source and provides an option to transpose the video if needed, but it's currently commented out. Additionally, there is a reference to another file called \"viewKali\" where a Preview function may be used, but it's not implemented yet.",
        "type": "comment"
    },
    "4296": {
        "file_id": 541,
        "content": "# seems not working\n# Preview(video)\n# vspipe is a wrapper around existing apis. vapoursynth can only generate raw frame data so we cannot encode video here alone. maybe we need opencv for this?\n# opencv preview https://github.com/UniversalAl/view",
        "type": "code",
        "location": "/tests/vapoursynth_linux_test/view_test.py:24-28"
    },
    "4297": {
        "file_id": 541,
        "content": "This code appears to attempt creating a preview using VapourSynth, but the functionality isn't working. It suggests that vspipe is a wrapper around existing APIs and VapourSynth can only generate raw frame data, so encoding a video alone might not be possible. OpenCV might help in generating previews, and there's an example at this GitHub link: https://github.com/UniversalAl/view.",
        "type": "comment"
    },
    "4298": {
        "file_id": 542,
        "content": "/tests/vapoursynth_linux_test/test_ffmpeg_docker.sh",
        "type": "filepath"
    },
    "4299": {
        "file_id": 542,
        "content": "Code snippet attempts to download a video file 'flower_cif.y4m' using wget, and then applies various filters with ffmpeg-tensorflow Docker container to upscale the video resolution by 2x and save it as 'flower_cif_2x.mp4'. The code also provides an alias for easier execution of ffmpeg-tensorflow command and specifies video filter complexities within the ffmpeg command.",
        "type": "summary"
    }
}