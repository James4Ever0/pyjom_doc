{
    "1500": {
        "file_id": 122,
        "content": "from ad_template_2_functional import removeAndInsertQRCode\nimport cv2\ndef test_main():\n    images = [\n        \"/root/Desktop/works/pyjom/samples/image/qrcode_test/no_qrcode.jpg\",\n        \"/root/Desktop/works/pyjom/samples/image/qrcode_test/with_qrcode.jpg\",\n    ]  # convert to compatible formats first.\n    qrcode_path = \"/root/Desktop/works/pyjom/tests/bilibili_video_recommendation_server/ebegging_template.png\"\n    for img in images:\n        output = removeAndInsertQRCode(img, qrcode_path, None)\n        cv2.imshow(\"IMG\", output)\n        cv2.waitKey(0)\nif __name__ == \"__main__\":\n    test_main()",
        "type": "code",
        "location": "/tasks/qq/qq_red_packet_collect/test_qrcode_insert_replace.py:1-20"
    },
    "1501": {
        "file_id": 122,
        "content": "This code imports a function to remove and insert QR codes, then tests it with two image inputs and a specific qrcode_path. It displays the output images in a window before exiting.",
        "type": "comment"
    },
    "1502": {
        "file_id": 123,
        "content": "/tasks/qq/qq_red_packet_collect/botoy_redpacket_collect.py",
        "type": "filepath"
    },
    "1503": {
        "file_id": 123,
        "content": "The code manages functions like ad sending, chat APIs, and sentiment analysis in opqbot. It ensures proper messaging practices by checking for banned words and repetition, schedules periodic message sending, sets up a weighted random reply yielder, monitors group chats, categorizes messages about cats and dogs, updates the database, manages ad counters and penalties, processes media messages, handles Chinese conversion, logs for GPT training, filters message types, writes chat cursor data, and manages red packets asynchronously.",
        "type": "summary"
    },
    "1504": {
        "file_id": 123,
        "content": "# for arm64 version of opqbot\n# disable that 复读机 plugin.\nimport os\nos.environ['HTTP_PROXY'] = \"\"\nos.environ['HTTPS_PROXY'] = \"\"\n# shall you analyze the logs/redPacketLog_*.log to get topics from groups and individuals.\nfrom chat_local import *\nfrom adtools import sendCatOrDogAdToQQGroup, checkCatOrDog, makeCatOrDogConnections\nfrom chatApis import getChatApiReply\nfrom base_opq import *\nimport schedule\nfrom chat_local import getAbsSentiment\nfrom censorApis import censorReplyAbsSentiment\nfrom commons import (\n    weightedRandomYielder,\n    generatedSentenceFixer,\n    keywordDecorator,\n    removeDuplicateWords,\n    replaceDuplicateChar,\n)\nAD_INIT_COUNTER = 1\ngroupChatCursor = None\ngroupMsgSendStatus = {}\ngroupChatReplyHistory = []\ngroupNoReplyStack = {}  # 防止连续对一个群持续输出\n# qq群最多可以添加500个群 1500个好友 其中群可加的数量 = max(0,500 - 已加入群数量 - 好友数量)\n# 可以退出一些安静的群 不发红包的群 删除好友\n# action.getClusterInfo\n# \"\"\"获取当前集群信息\"\"\"\n# this is to get the current server running status. i suspect.\ndef groupMsgRepeater(msg: str, sentiment_threshold=0.7):\n    sentiment = getAbsSentiment(msg)",
        "type": "code",
        "location": "/tasks/qq/qq_red_packet_collect/botoy_redpacket_collect.py:1-45"
    },
    "1505": {
        "file_id": 123,
        "content": "This code is for the arm64 version of opqbot, disabling the 复读机 plugin, and managing various functions like ad sending, chat APIs, base opq operations, sentiment analysis, censor replies, and more. It handles group messages, repeating them if necessary, while considering sentiment scores and maintaining a log for red packet collection.",
        "type": "comment"
    },
    "1506": {
        "file_id": 123,
        "content": "    if sentiment > sentiment_threshold:\n        return msg\ndef checkGroupMsgSendStatus(group_id, decrease=True):\n    if group_id in groupMsgSendStatus.keys():\n        if decrease:\n            groupMsgSendStatus[group_id] -= 1  # the feedback shall be elsewhere.\n        if groupMsgSendStatus[group_id] <= 0:\n            del groupMsgSendStatus[group_id]\n            return True\n        else:\n            return False\n    return True\n# now async.\n@asyncThread\ndef sendBotGroupTextMsg(\n    replyGetterYielder,\n    groupBannedErrorBuffer=100,  # 被禁言之后的buffer\n    retry=3,\n    min_reply_length=3,  # some impirical value.\n    delay_time_range=(5, 15),\n    context_size_range=(1, 3),  # maybe we do not need no context. or not?\n    maxRepeatRange=(2, 5),\n    noReplyThreshold=3,\n    noReplyBuffer=75,\n):  # the context parameter may lead to OOM.\n    global groupChatCursor\n    # will clear cursor after sending\n    if groupChatCursor is not None:\n        # do work here.\n        group_id = groupChatCursor[\"group_id\"]\n        # groupChatCursor = None",
        "type": "code",
        "location": "/tasks/qq/qq_red_packet_collect/botoy_redpacket_collect.py:46-80"
    },
    "1507": {
        "file_id": 123,
        "content": "This function, when called with a replyGetterYielder object and other parameters such as groupBannedErrorBuffer, retry, delay_time_range, context_size_range, maxRepeatRange, noReplyThreshold, and noReplyBuffer, sends a message to a QQ group chat using an async thread. It also manages the groupMsgSendStatus dictionary to keep track of the number of messages sent to each group. The function checks if the group is currently banned or not, retries sending the message a certain number of times if necessary, and adjusts the send status accordingly.",
        "type": "comment"
    },
    "1508": {
        "file_id": 123,
        "content": "        # return\n        result = checkGroupMsgSendStatus(group_id, decrease=False)  # failsafe.\n        if not result:\n            return\n        # modify this textMessage somehow? with context.\n        context = random.randint(*context_size_range)\n        textMessage = groupChatCursor[\"msg\"]\n        groupChatCursorWithContext = groupChatCursor.copy()\n        messageContext = chat_stack[group_id][-context:-1] + [\n            textMessage\n        ]  # include the last message.\n        messageContext = \" \".join(messageContext)  # just use space.\n        groupChatCursorWithContext[\"msg\"] = messageContext\n        for (\n            replyGetter,\n            argumentList,\n            flag,\n            needContext,\n            enableRetryFlag,\n        ) in replyGetterYielder:  # use all methods.\n            if exit_event.is_set():\n                break\n            retried = False\n            for _ in range(retry):  # retry for three times.\n                if exit_event.is_set():\n                    break\n                extraFlags = {}",
        "type": "code",
        "location": "/tasks/qq/qq_red_packet_collect/botoy_redpacket_collect.py:81-109"
    },
    "1509": {
        "file_id": 123,
        "content": "The code checks if the group message sending status is successful, then sets a context size range and uses it to construct a new text message. It joins all messages together with spaces and assigns the new message to the group chat cursor. Finally, it loops through multiple reply getters, trying them in order, with a maximum of three retries if the event is not set.",
        "type": "comment"
    },
    "1510": {
        "file_id": 123,
        "content": "                if enableRetryFlag:\n                    extraFlags.update({\"retryFlag\": retried})\n                # stderrPrint(extraFlags,replyGetter)\n                if needContext:\n                    reply = replyGetter(\n                        *[groupChatCursorWithContext[key] for key in argumentList],\n                        **extraFlags\n                    )\n                else:\n                    reply = replyGetter(\n                        *[groupChatCursor[key] for key in argumentList], **extraFlags\n                    )\n                if reply is not None:\n                    retried = True  # only plus one on retryIndex when there is no error during generation.\n                    maxRepeat = random.randint(*maxRepeatRange)\n                    reply = generatedSentenceFixer(\n                        reply, maxRepeat=maxRepeat\n                    )  # fix this reply first.\n                    # add a new filter here.\n                    reply = removeDuplicateWords(reply)\n                    if reply in groupChatReplyHistory or len(reply) < min_reply_length:",
        "type": "code",
        "location": "/tasks/qq/qq_red_packet_collect/botoy_redpacket_collect.py:110-132"
    },
    "1511": {
        "file_id": 123,
        "content": "This code block retrieves a reply from a replyGetter function, depending on whether context is needed or not. If the reply is not None, it updates retryFlag and generates a new random maxRepeat value for generatedSentenceFixer to fix the reply. The reply is then passed through removeDuplicateWords filter before checking if it's already in groupChatReplyHistory or shorter than min_reply_length.",
        "type": "comment"
    },
    "1512": {
        "file_id": 123,
        "content": "                        continue  # do not send repeated messages or unusually short messages.\n                    else:\n                        update_stack(groupChatReplyHistory, reply)\n                    # 句子里面不能有违禁词语 不然就不能输出\n                    reply = censorReplyAbsSentiment(reply)\n                    if reply is None:\n                        continue  # skip too vulgar sentences.\n                    if reply.count(\"*\") > 3:  # too much censor will make it unreadable.\n                        continue  # retry to get a better thing.\n                    # do reply.\n                    # stderrPrint(\"PROCESSING GROUP MESSAGE CURSOR:\", groupChatCursor)\n                    stderrPrint(flag, reply)\n                    # must control this shit. 如果被禁言了该如何处理 一般需要缓冲30次\n                    groupChatCursor = None  # remove it only one reply was to be made.\n                    delay = random.randint(*delay_time_range)\n                    time.sleep(delay)  # to make it more humane.\n                    sendMessageStatus = action.sendGroupText(",
        "type": "code",
        "location": "/tasks/qq/qq_red_packet_collect/botoy_redpacket_collect.py:133-152"
    },
    "1513": {
        "file_id": 123,
        "content": "Code checks if the message contains banned words, censors them if necessary, and ensures messages are not too short or repeated. If a suitable reply is found, it sends the message after a random delay to avoid flooding and make it appear more human-like.",
        "type": "comment"
    },
    "1514": {
        "file_id": 123,
        "content": "                        group=group_id, content=reply\n                    )\n                    # stderrPrint(\"SENT MESSAGE STATUS:\",sendMessageStatus)\n                    if not (\n                        sendMessageStatus[\"ErrMsg\"] == \"\"\n                        and sendMessageStatus[\"Ret\"] == 0\n                    ):\n                        # some shit had happened. cannot send message without error.\n                        groupMsgSendStatus.update({group_id: groupBannedErrorBuffer})\n                    else:\n                        # no shit happened.\n                        groupNoReplyStack.update(\n                            {group_id: 1 + groupNoReplyStack.get(group_id, 0)}\n                        )\n                        # stderrPrint(\"UPDATE NOREPLYSTACK\", groupNoReplyStack)\n                        noReply = groupNoReplyStack.get(group_id, 0)\n                        if (\n                            noReply >= noReplyThreshold\n                        ):  # only this noReply greater than 0 we can write it to cursor. LOGIC ELSEWHERE",
        "type": "code",
        "location": "/tasks/qq/qq_red_packet_collect/botoy_redpacket_collect.py:153-172"
    },
    "1515": {
        "file_id": 123,
        "content": "Code snippet handles sending messages to a group using the 'sendMessageStatus' response. If an error occurs (non-empty 'ErrMsg' or non-zero 'Ret'), the group is marked as banned. If no errors, the count of unsent messages is incremented for that group. If the count exceeds a threshold, it can be written to the cursor.",
        "type": "comment"
    },
    "1516": {
        "file_id": 123,
        "content": "                            groupNoReplyStack.update({group_id: -noReplyBuffer})\n                    # stderrPrint(\"sendMessageStatus:\", sendMessageStatus)\n                    return True\ndef sendRandomGroupMessage():\n    sendAtriGroupChatMessage = (\n        keywordDecorator(getChatApiReply, chatApiIndex=0),\n        [\"msg\", \"group_id\"],\n        \"SENDING ATRI API REPLY:\",\n        True,\n        True,\n    )  # last is enableRetryFlag\n    sendGPT2GroupChatMessage = (\n        keywordDecorator(getChatApiReply, chatApiIndex=1),\n        [\"msg\", \"group_id\"],\n        \"SENDING GPT2 API REPLY:\",\n        True,\n        True,\n    )  # last is enableRetryFlag\n    sendXiaoIceGroupChatMessage = (\n        keywordDecorator(getChatApiReply, chatApiIndex=2),\n        [\"msg\", \"group_id\"],\n        \"SENDING XIAOICE API REPLY:\",\n        True,\n        True,\n    )\n    sendChatLocalResponse = (\n        getChatLocalResponse,\n        [\"group_id\", \"msg\"],\n        \"SENDING CHATLOCAL REPLY:\",\n        False,\n        False,\n    )\n    sendRepeaterResponse = (",
        "type": "code",
        "location": "/tasks/qq/qq_red_packet_collect/botoy_redpacket_collect.py:173-209"
    },
    "1517": {
        "file_id": 123,
        "content": "This code defines four different functions for sending messages to a group chat: `sendAtriGroupChatMessage`, `sendGPT2GroupChatMessage`, `sendXiaoIceGroupChatMessage`, and `sendChatLocalResponse`. Each function takes a \"group_id\" and a \"msg\", and has a specific label indicating the source of the message. The last parameter in each tuple indicates whether to retry sending if an error occurs.",
        "type": "comment"
    },
    "1518": {
        "file_id": 123,
        "content": "        groupMsgRepeater,\n        [\"msg\"],\n        \"SENDING REPEATER REPLY:\",\n        False,\n        False,\n    )\n    replyGetterList = [\n        sendAtriGroupChatMessage,\n        sendGPT2GroupChatMessage,\n        sendChatLocalResponse,\n        sendRepeaterResponse,\n        sendXiaoIceGroupChatMessage,\n    ]\n    weightList = [2, 5, 1, 1, 5]\n    # weightList = [1, 3, 2, 2, 5] # said that is girish, because of xiaoice.\n    replyGetterYielder = weightedRandomYielder(replyGetterList, weightList)\n    sendBotGroupTextMsg(replyGetterYielder)\n# schedule.every(1).minute.do(sendApiGroupChatMessage)\n# schedule.every(30).seconds.do(sendChatLocalResponse) # will this shit work?\nschedule.every(1).minute.do(sendRandomGroupMessage)  # will this shit work?\ndef printGroupTextChatJson(group_id, sender_id, content):\n    message = {\"group_id\": group_id, \"sender_id\": sender_id, \"content\": content}\n    message = json.dumps(message, ensure_ascii=False)\n    stderrPrint(\n        \"[GROUP_TEXT_MESSAGE]\", message\n    )  # strange. who the fuck added this shit?",
        "type": "code",
        "location": "/tasks/qq/qq_red_packet_collect/botoy_redpacket_collect.py:210-240"
    },
    "1519": {
        "file_id": 123,
        "content": "This code is setting up a weighted random reply yielder for a group chat bot, with multiple reply options and weights. It also schedules a function to send a random group message periodically and prints group text chat messages in JSON format. The schedule functionality seems to have some uncertainty about its effectiveness.",
        "type": "comment"
    },
    "1520": {
        "file_id": 123,
        "content": "# convert to simplified chinese.\nimport opencc\nchinese_t2s = opencc.OpenCC()\nadBuffer = {}\n# hook up this thing, send cat video only if we receive that topic.\nfrom adtools import checkIsCatOrDogImage\n@asyncThread\ndef catOrDogAsyncThread(group_id:str, sender_id:str,Content:str,is_image:bool=False, is_user:bool=False):\n    if is_image:\n        try:\n            cat_or_dog = checkIsCatOrDogImage(Content)\n        except:\n            import traceback\n            traceback.print_exc()\n            print(\"Exception when detecting image if it is cat or dog\")\n            return\n    else:\n        cat_or_dog = checkCatOrDog(Content)\n    # we need to update neo4j database, using group_id, sender_id, cat_or_dog.\n    if cat_or_dog:\n        makeCatOrDogConnections(\n            str(group_id), str(sender_id), cat_or_dog\n        )\n        # act accordingly. decide to send ad or not.\n        if adBuffer.get(str(group_id), 0) <= 0:\n            penalty = 10\n            # send the ad.\n            success = sendCatOrDogAdToQQGroup(str(group_id), cat_or_dog, action)",
        "type": "code",
        "location": "/tasks/qq/qq_red_packet_collect/botoy_redpacket_collect.py:243-274"
    },
    "1521": {
        "file_id": 123,
        "content": "This code defines a function that checks whether the content sent in a QQ group is an image of a cat or dog. If it is, the function updates a Neo4j database and decides whether to send an ad based on a counter for each group. The code uses OpenCC library for text simplification and imports traceback module for error handling.",
        "type": "comment"
    },
    "1522": {
        "file_id": 123,
        "content": "            if success:\n                penalty += 40 # every 50 messages we have one ad.\n            adBuffer[str(group_id)] = penalty\n        # decrease that counter by standard group messages.\nfrom botoy.collection import MsgTypes\n@bot.on_group_msg\ndef group(ctx: GroupMsg, groupInitReplyDelayRange=(4, 15)):\n    # too broad for groupInitReplyDelayRange to be (2, 20)\n    # global groupChatCursor\n    #    stderrPrint('收到群消息，群号为', ctx.FromGroupId)\n    # recommed you to check the curret group only.\n    #    stderrPrint(\"checkGroupNoReply:\",groupNoReplyStack.get(ctx.FromGroupId,None))\n    data_dict = ctx.data  # recommend to use this json object. or not?\n    groupName = data_dict.get(\"FromGroupName\", None)\n    group_id = data_dict[\"FromGroupId\"]\n    # decrease that ad counter.\n    adCounter = adBuffer.get(str(group_id), AD_INIT_COUNTER)\n    if adCounter > 0:\n        adCounter -= 1\n    adBuffer[str(group_id)] = adCounter\n    if groupName is not None:\n        updateGroupNameDict(groupName, group_id)\n    sender_id = data_dict[\"FromUserId\"]",
        "type": "code",
        "location": "/tasks/qq/qq_red_packet_collect/botoy_redpacket_collect.py:275-301"
    },
    "1523": {
        "file_id": 123,
        "content": "This code fragment monitors a group chat and manages ad counters based on the number of messages sent. For each message, it checks if a penalty should be added and updates an ad counter for the specific group ID. The ad counter is decreased with every standard group message received. It also updates the group name dictionary and handles user input data from messages sent to the group chat.",
        "type": "comment"
    },
    "1524": {
        "file_id": 123,
        "content": "    RedBaginfoDict = data_dict[\"RedBaginfo\"]\n    RedBaginfo = ctx.RedBaginfo\n    MsgType = ctx.MsgType\n    # how to download these shits?\n    try:\n        from botoy.parser.group import Pic\n        if MsgType == MsgTypes.PicMsg:\n            pic_obj = Pic(**json.loads(ctx.Content))\n            pics = pic_obj.GroupPic\n            for pic in pics:\n                pic_url = pic.Url\n                catOrDogAsyncThread(str(group_id), str(sender_id),pic_url,is_image=True)\n        elif MsgType == MsgTypes.VideoMsg:\n            ...\n        elif MsgType == MsgTypes.VoiceMsg:\n            ...\n        elif MsgType == MsgTypes.JsonMsg:\n            ... # hope you can receive that? nope? you can only receive that by go-cqhttp.\n    except:\n        import traceback\n        traceback.print_exc()\n        print(\"ERROR WHEN PROCESSING MEDIA MESSAGES.\")\n        print(\"MSGTYPE:\",MsgType)\n    # first initialize random delay for every group in groupNoReplyStack\n    if group_id not in groupNoReplyStack.keys():\n        groupNoReplyStack.update({group_id: -random.randint(*groupInitReplyDelayRange)})",
        "type": "code",
        "location": "/tasks/qq/qq_red_packet_collect/botoy_redpacket_collect.py:302-329"
    },
    "1525": {
        "file_id": 123,
        "content": "This code is handling media messages (images, videos, voice) in a chat group. It tries to download images and may process videos or voice messages. If the message type is not recognized, it logs an error. The code also initializes random delay for a group's no-reply stack if the group ID isn't present.",
        "type": "comment"
    },
    "1526": {
        "file_id": 123,
        "content": "    def writeGroupChatCursor(Content, enable_t2s=True):\n        if enable_t2s:\n            Content = chinese_t2s.convert(Content)\n        # content need to converted into simplified chinese.\n        global groupChatCursor, chat_stack_lock\n        # maybe we should create the mapping table here.\n        content_length = len(Content)\n        content_min_length = 4\n        # maybe we should split sentence into shorter ones, or via summarization/title generation apis.\n        content_max_length = 15\n        recv_content_min_length, recv_content_max_length = 4, 20\n        if not (Content.startswith(\"[\") or Content.endswith(\"]\")):\n            if (\n                content_length <= recv_content_max_length\n                and content_length >= recv_content_min_length\n            ):\n                printGroupTextChatJson(\n                    group_id, sender_id, Content\n                )  # why the fuck you are not printing?\n            if (\n                content_length <= content_max_length\n                and content_length >= content_min_length",
        "type": "code",
        "location": "/tasks/qq/qq_red_packet_collect/botoy_redpacket_collect.py:331-352"
    },
    "1527": {
        "file_id": 123,
        "content": "Function writes group chat cursor data to a file, with support for Chinese simplified to traditional conversion. The content length is checked against minimum and maximum thresholds before printing or potentially splitting into shorter segments.",
        "type": "comment"
    },
    "1528": {
        "file_id": 123,
        "content": "            ):  # 新版qq之类的信息\n                # we log group chat text for gpt training here. shall we?\n                result = checkGroupMsgSendStatus(group_id)\n                if (\n                    result\n                ):  # will not write banned group to cursor since we will not reply it.\n                    noReply = groupNoReplyStack.get(group_id, 0)\n                    # stderrPrint(\"NOREPLYSTACK:\",groupNoReplyStack)\n                    if noReply >= 0:\n                        groupChatCursor = {\"group_id\": group_id, \"msg\": Content}\n                    else:\n                        groupNoReplyStack.update({group_id: noReply + 1})\n                # chat_stack update logic within the content length filter\n                if chat_stack_lock:\n                    # do not do anything about the chat_stack while locked.\n                    return\n                else:\n                    # check if we are hit by something interesting?\n                    catOrDogAsyncThread(group_id, sender_id, Content)",
        "type": "code",
        "location": "/tasks/qq/qq_red_packet_collect/botoy_redpacket_collect.py:353-373"
    },
    "1529": {
        "file_id": 123,
        "content": "This code block handles QQ group chat messages and logs them for GPT training. It also manages a \"noReply\" counter for groups where the bot should not reply, and updates a chat_stack based on certain conditions. The chat_stack lock is checked to ensure no simultaneous updates are made. If interesting content is detected, it triggers a catOrDogAsyncThread function.",
        "type": "comment"
    },
    "1530": {
        "file_id": 123,
        "content": "                    updateChatStack(group_id, Content)\n                    # or we could simply add the filter on the reply side.\n    if sender_id != my_qq:  # skip text content sent by itself.\n        # how to act like the TextMsg? it could include video/image contents.\n        if MsgType == \"AtMsg\":\n            Content = ctx.Content  # this is string.\n            Content_json = json.loads(Content)\n            content_text = Content_json[\"Content\"].strip()\n            # print(Content_json)\n            # breakpoint()\n            # 'SrcContent', 'UserID'(list)\n            content_at_target = (\"@\" + content_text.split(\"@\")[1]).strip()\n            content_text = content_text.replace(content_at_target, \"\")\n            content_text = replaceDuplicateChar(content_text, \" \", maxRepeat=1)\n            # UserExt = Content_json[\"UserExt\"]\n            # # shit revised. no more 'UserExt'\n            # for elem in UserExt:\n            #     QQNick = elem[\"QQNick\"]\n            #     at_QQNick = \"@{}\".format(QQNick)\n            #     content_text = content_text.replace(at_QQNick + \" \" + at_QQNick, \"\")",
        "type": "code",
        "location": "/tasks/qq/qq_red_packet_collect/botoy_redpacket_collect.py:374-394"
    },
    "1531": {
        "file_id": 123,
        "content": "This code checks if the message type is an \"AtMsg\" and extracts the content, removes the mentioned user, replaces duplicate characters with a maximum repeat of 1, and updates the chat stack. If the sender is not the bot itself, it performs these actions on the message content.",
        "type": "comment"
    },
    "1532": {
        "file_id": 123,
        "content": "            #     content_text = content_text.replace(at_QQNick, \"\")\n            # now the content is ready.\n            writeGroupChatCursor(content_text)\n        if MsgType == \"TextMsg\":\n            # is that group allowing sending messages?\n            content_text = ctx.Content  # must not be empty.\n            writeGroupChatCursor(content_text)\n    if RedBaginfoDict is not None:\n        prefix = \"[MREDBAG_LOG]\"\n        print(prefix, \"RECEIVED RED PACKET\", file=sys.stderr)\n        print(\n            prefix, \"_____________RedPacket Message Dump_____________\", file=sys.stderr\n        )\n        print(prefix, ctx, file=sys.stderr)\n        print(\n            prefix, \"_____________RedPacket Message Dump_____________\", file=sys.stderr\n        )\n        startThread(openRedBag, (RedBaginfoDict, group_id, RedBaginfo))\n    schedule.run_pending()  # this is async.\n    # breakpoint()\nif __name__ == \"__main__\":\n    bot.run()\n# do not send porn shits or you need to relogin.",
        "type": "code",
        "location": "/tasks/qq/qq_red_packet_collect/botoy_redpacket_collect.py:395-421"
    },
    "1533": {
        "file_id": 123,
        "content": "Code chunk handles messages and red packets in a QQ group chat. It filters the message type, prepares content for writing, writes to the group chat cursor, handles received red packets by starting a thread, and runs pending tasks asynchronously.",
        "type": "comment"
    },
    "1534": {
        "file_id": 124,
        "content": "/tasks/qq/qq_red_packet_collect/test_cat_dog_info_get.py",
        "type": "filepath"
    },
    "1535": {
        "file_id": 124,
        "content": "The code imports the getCatOrDogAd function from adtools, defines test_init and test_cats_and_dogs_get_video_names functions. test_init randomly selects a response from getCatOrDogAd (20 responses) for 'cat' category and prints the video information. test_cats_and_dogs_get_video_names iterates over 'cat' and 'dog' categories, retrieves videos using getCatOrDogAd with method 'bm25', and prints their titles. The code also checks if the __name__ is '__main__' to execute either test_init or test_cats_and_dogs_get_video_names function.",
        "type": "summary"
    },
    "1536": {
        "file_id": 124,
        "content": "import random\nfrom adtools import getCatOrDogAd\ndef test_init():\n    cat_or_dog='cat'\n    for _ in range(2):\n        responses = getCatOrDogAd(cat_or_dog)\n        videoInfo = random.choice(responses[:20])\n        print(videoInfo)\n        print()\ndef test_cats_and_dogs_get_video_names():\n    for category in ['cat','dog']:\n        print(\"_\"*20)\n        print(\"CATEGORY?\",category)\n        print()\n        responses = getCatOrDogAd(category,method='bm25') # it does not update that often. use online search instead? (fill keywords in description)\n        for info in responses:\n            title = info['title']\n            print(\"VIDEO?\",title)\nif __name__ == '__main__':\n    # test_init()\n    test_cats_and_dogs_get_video_names()",
        "type": "code",
        "location": "/tasks/qq/qq_red_packet_collect/test_cat_dog_info_get.py:1-25"
    },
    "1537": {
        "file_id": 124,
        "content": "The code imports the getCatOrDogAd function from adtools, defines test_init and test_cats_and_dogs_get_video_names functions. test_init randomly selects a response from getCatOrDogAd (20 responses) for 'cat' category and prints the video information. test_cats_and_dogs_get_video_names iterates over 'cat' and 'dog' categories, retrieves videos using getCatOrDogAd with method 'bm25', and prints their titles. The code also checks if the __name__ is '__main__' to execute either test_init or test_cats_and_dogs_get_video_names function.",
        "type": "comment"
    },
    "1538": {
        "file_id": 125,
        "content": "/tasks/qq/qq_red_packet_collect/fill_mask_model/test_macbert.sh",
        "type": "filepath"
    },
    "1539": {
        "file_id": 125,
        "content": "The code makes a POST request to the Hugging Face API's inference endpoint for the \"hfl/chinese-macbert-base\" model, sending input text with a masked token and an access token as authorization.",
        "type": "summary"
    },
    "1540": {
        "file_id": 125,
        "content": "curl https://api-inference.huggingface.co/models/hfl/chinese-macbert-base \\\n\t-X POST \\\n\t-d '{\"inputs\": \"我感冒了，今天天气[MASK]\"}' \\\n\t-H \"Authorization: Bearer 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\"",
        "type": "code",
        "location": "/tasks/qq/qq_red_packet_collect/fill_mask_model/test_macbert.sh:1-4"
    },
    "1541": {
        "file_id": 125,
        "content": "The code makes a POST request to the Hugging Face API's inference endpoint for the \"hfl/chinese-macbert-base\" model, sending input text with a masked token and an access token as authorization.",
        "type": "comment"
    },
    "1542": {
        "file_id": 126,
        "content": "/tasks/qq/qq_red_packet_collect/fill_mask_model/test.py",
        "type": "filepath"
    },
    "1543": {
        "file_id": 126,
        "content": "The code uses 'hfl/chinese-macbert-base' for masked language modeling, removes [] or [MASK] values, tokenizes input, gets logits, iterates over mask token indices, predicts next tokens, decodes to strings, and prints without gradients.",
        "type": "summary"
    },
    "1544": {
        "file_id": 126,
        "content": "import os\nimport torch\nos.environ[\"http_proxy\"] = \"\"\nos.environ[\"https_proxy\"] = \"\"\nfrom transformers import BertTokenizer, BertForMaskedLM\nmodel_name = \"hfl/chinese-macbert-base\"\ntokenizer = BertTokenizer.from_pretrained(model_name)\nmodel = BertForMaskedLM.from_pretrained(model_name) # where the heck is the model?\n#location:\n# resolved_archive_file = cached_path(...)\n# already outsourced this shit.\n# /root/.cache/huggingface/transformers/f350d12c99d2a8d00f4299b8e292c2248422676424702a2c45a8a3d65646f738.749c1a543002a65141e104ba5e040263fd8eabc9d2dcfb537bf681345565ef45\n# first ensure there is no [MASK] or [] surrounded values. otherwise, remove these shits.\n# split them using re.split and filter these shits out with re.match\ninputs = tokenizer(\"如果今天天气[MASK][MASK]\", return_tensors=\"pt\")\nwith torch.no_grad():\n    logits = model(**inputs).logits\n# retrieve index of [MASK]\nmask_token_indexs = (inputs.input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0] # (tensor([5, 6]),) without [0]\n# print(mask_token_indexs) #5 and 6.",
        "type": "code",
        "location": "/tasks/qq/qq_red_packet_collect/fill_mask_model/test.py:1-27"
    },
    "1545": {
        "file_id": 126,
        "content": "The code is using the 'hfl/chinese-macbert-base' model for masked language modeling. It first checks if there are any [MASK] or [] surrounded values and removes them. Then, it tokenizes the input text using the BertTokenizer from transformers library. After that, it passes the input to the BertForMaskedLM model and retrieves the logits corresponding to the mask tokens. The mask token indices are obtained from the input tensors.",
        "type": "comment"
    },
    "1546": {
        "file_id": 126,
        "content": "for mask_token_index in mask_token_indexs:\n    predicted_token_id = logits[0, mask_token_index].argmax(axis=-1)\n    result = tokenizer.decode(predicted_token_id)\n    print(mask_token_index,result)\n# with torch.no_grad():\n#     outputs = model(**inputs)\n# print(dir(outputs))",
        "type": "code",
        "location": "/tasks/qq/qq_red_packet_collect/fill_mask_model/test.py:28-35"
    },
    "1547": {
        "file_id": 126,
        "content": "Iterates over each mask token index in the list, predicts the next token, decodes the predicted token ID to string using tokenizer, and prints the mask token index and result. This process is done without gradients for computational efficiency.",
        "type": "comment"
    },
    "1548": {
        "file_id": 127,
        "content": "/tasks/qq/qq_red_packet_collect/fill_mask_model/init.sh",
        "type": "filepath"
    },
    "1549": {
        "file_id": 127,
        "content": "The code installs git LFS, runs a Python script named \"test.py\" with specified environment variables, and clones the \"chinese-macbert-base\" model from Hugging Face Co.",
        "type": "summary"
    },
    "1550": {
        "file_id": 127,
        "content": "# git lfs install\nenv http_proxy=\"\" https_proxy=\"\" python3 test.py\n# env http_proxy=\"\" https_proxy=\"\" git clone https://huggingface.co/hfl/chinese-macbert-base",
        "type": "code",
        "location": "/tasks/qq/qq_red_packet_collect/fill_mask_model/init.sh:1-3"
    },
    "1551": {
        "file_id": 127,
        "content": "The code installs git LFS, runs a Python script named \"test.py\" with specified environment variables, and clones the \"chinese-macbert-base\" model from Hugging Face Co.",
        "type": "comment"
    },
    "1552": {
        "file_id": 128,
        "content": "/tasks/qq/qq_red_packet_collect/textfilter/launch.sh",
        "type": "filepath"
    },
    "1553": {
        "file_id": 128,
        "content": "Launches the FastAPI application using uvicorn, listens on port 8932, and provides an option for auto-reloading with --reload. However, the comment suggests not to use the reload feature.",
        "type": "summary"
    },
    "1554": {
        "file_id": 128,
        "content": "python3 -m uvicorn filter_py3_fastapi:app --port 8932 \n# python3 -m uvicorn filter_py3_fastapi:app --reload --port 8932 \n# do not use reload!",
        "type": "code",
        "location": "/tasks/qq/qq_red_packet_collect/textfilter/launch.sh:1-3"
    },
    "1555": {
        "file_id": 128,
        "content": "Launches the FastAPI application using uvicorn, listens on port 8932, and provides an option for auto-reloading with --reload. However, the comment suggests not to use the reload feature.",
        "type": "comment"
    },
    "1556": {
        "file_id": 129,
        "content": "/tasks/qq/qq_red_packet_collect/textfilter/filter_py3_fastapi.py",
        "type": "filepath"
    },
    "1557": {
        "file_id": 129,
        "content": "The code offers text filtering through classes like NaiveFilter and BSFilter for keyword removal and regex processing, plus a DFAFilter for performance boost. It checks characters, translates Chinese to Pinyin, and returns moderated text via FastAPI endpoint.",
        "type": "summary"
    },
    "1558": {
        "file_id": 129,
        "content": "#!/usr/bin/env python\n# -*- coding:utf-8 -*-\nfrom collections import defaultdict\nimport re\n__all__ = ['NaiveFilter', 'BSFilter', 'DFAFilter']\n__author__ = 'observer'\n__date__ = '2012.01.05'\nclass NaiveFilter():\n    '''Filter Messages from keywords\n    very simple filter implementation\n    >>> f = NaiveFilter()\n    >>> f.add(\"sexy\")\n    >>> f.filter(\"hello sexy baby\")\n    hello **** baby\n    '''\n    def __init__(self):\n        self.keywords = set([])\n    def parse(self, path):\n        for keyword in open(path):\n            self.keywords.add(keyword.strip().decode('utf-8').lower())\n    def filter(self, message, repl=\"*\"):\n        message = str(message).lower()\n        for kw in self.keywords:\n            message = message.replace(kw, repl)\n        return message\nclass BSFilter:\n    '''Filter Messages from keywords\n    Use Back Sorted Mapping to reduce replacement times\n    >>> f = BSFilter()\n    >>> f.add(\"sexy\")\n    >>> f.filter(\"hello sexy baby\")\n    hello **** baby\n    '''\n    def __init__(self):\n        self.keywords = []",
        "type": "code",
        "location": "/tasks/qq/qq_red_packet_collect/textfilter/filter_py3_fastapi.py:1-48"
    },
    "1559": {
        "file_id": 129,
        "content": "This code is for creating a filter to remove specific keywords from a given message. It provides three classes: NaiveFilter, BSFilter, and DFAFilter. NaiveFilter is the simplest implementation using set data structure, while BSFilter uses Back Sorted Mapping to improve performance by reducing replacement times. The code also includes parsing functionality to add keywords from a file.",
        "type": "comment"
    },
    "1560": {
        "file_id": 129,
        "content": "        self.kwsets = set([])\n        self.bsdict = defaultdict(set)\n        self.pat_en = re.compile(r'^[0-9a-zA-Z]+$')  # english phrase or not\n    def add(self, keyword):\n        if not isinstance(keyword, str):\n            keyword = keyword.decode('utf-8')\n        keyword = keyword.lower()\n        if keyword not in self.kwsets:\n            self.keywords.append(keyword)\n            self.kwsets.add(keyword)\n            index = len(self.keywords) - 1\n            for word in keyword.split():\n                if self.pat_en.search(word):\n                    self.bsdict[word].add(index)\n                else:\n                    for char in word:\n                        self.bsdict[char].add(index)\n    def parse(self, path):\n        with open(path, \"r\") as f:\n            for keyword in f:\n                self.add(keyword.strip())\n    def filter(self, message, repl=\"*\"):\n        if not isinstance(message, str):\n            message = message.decode('utf-8')\n        message = message.lower()\n        for word in message.split():",
        "type": "code",
        "location": "/tasks/qq/qq_red_packet_collect/textfilter/filter_py3_fastapi.py:49-77"
    },
    "1561": {
        "file_id": 129,
        "content": "This code defines a class with methods for adding keywords, parsing from a file, and filtering text. It uses regular expressions to identify English words and stores them in dictionaries based on their characters or full words. The parse method reads a file of keywords and the filter method processes input messages by replacing non-keyword parts with \"*\".",
        "type": "comment"
    },
    "1562": {
        "file_id": 129,
        "content": "            if self.pat_en.search(word):\n                for index in self.bsdict[word]:\n                    message = message.replace(self.keywords[index], repl)\n            else:\n                for char in word:\n                    for index in self.bsdict[char]:\n                        message = message.replace(self.keywords[index], repl)\n        return message\nclass DFAFilter():\n    '''Filter Messages from keywords\n    Use DFA to keep algorithm perform constantly\n    >>> f = DFAFilter()\n    >>> f.add(\"sexy\")\n    >>> f.filter(\"hello sexy baby\")\n    hello **** baby\n    '''\n    def __init__(self):\n        self.keyword_chains = {}\n        self.delimit = '\\x00'\n    def add(self, keyword):\n        if not isinstance(keyword, str):\n            keyword = keyword.decode('utf-8')\n        keyword = keyword.lower()\n        chars = keyword.strip()\n        if not chars:\n            return\n        level = self.keyword_chains\n        for i in range(len(chars)):\n            if chars[i] in level:\n                level = level[chars[i]]",
        "type": "code",
        "location": "/tasks/qq/qq_red_packet_collect/textfilter/filter_py3_fastapi.py:78-113"
    },
    "1563": {
        "file_id": 129,
        "content": "This code defines a DFAFilter class to filter messages from keywords. It uses DFA (Deterministic Finite Automaton) to improve algorithm performance. The add method adds a keyword and the filter method replaces keywords with asterisks (*). If the keyword is already in the DFA, it updates the level of the DFA accordingly.",
        "type": "comment"
    },
    "1564": {
        "file_id": 129,
        "content": "            else:\n                if not isinstance(level, dict):\n                    break\n                for j in range(i, len(chars)):\n                    level[chars[j]] = {}\n                    last_level, last_char = level, chars[j]\n                    level = level[chars[j]]\n                last_level[last_char] = {self.delimit: 0}\n                break\n        if i == len(chars) - 1:\n            level[self.delimit] = 0\n    def parse(self, path):\n        with open(path) as f:\n            for keyword in f:\n                self.add(keyword.strip())\n    def filter(self, message, repl=\"*\"):  # what is this repl?\n        if not isinstance(message, str):\n            message = message.decode('utf-8')\n        message = message.lower()\n        ret = []\n        start = 0\n        while start < len(message):\n            level = self.keyword_chains\n            step_ins = 0\n            for char in message[start:]:\n                if char in level:\n                    step_ins += 1\n                    if self.delimit not in level[char]:",
        "type": "code",
        "location": "/tasks/qq/qq_red_packet_collect/textfilter/filter_py3_fastapi.py:114-143"
    },
    "1565": {
        "file_id": 129,
        "content": "This code is parsing a text file of keywords and their corresponding chains. It then filters a given message by replacing instances of the keyword chains with a placeholder (\"*\"). The \"repl\" argument in filter() function seems to be optional and represents the placeholder character used for replacement. If the input message is not a string, it's decoded from its original format (like bytes) to a string. The code maintains a nested dictionary structure representing keyword chains, and if a delimiter is found missing in a chain, it's set to 0.",
        "type": "comment"
    },
    "1566": {
        "file_id": 129,
        "content": "                        level = level[char]\n                    else:\n                        # print(\"STEPINS\", step_ins)\n                        # print(\"CHAR\", char)\n                        # print(level[char])\n                        ret.append(repl * step_ins)\n                        start += step_ins - 1\n                        break\n                else:\n                    ret.append(message[start])\n                    break\n            else:\n                ret.append(message[start])\n            start += 1\n        return ''.join(ret)\ndef test_first_character():\n    gfw = DFAFilter()\n    gfw.add(\"1989年\")\n    assert gfw.filter(\"1989\", \"*\") == \"1989\"\ngfw = DFAFilter()\ngfw.parse(\"keywords\")\nfrom typing import Union\nfrom fastapi import FastAPI\napp = FastAPI()\nfrom snownlp import SnowNLP\nfrom snownlp.normal import pin, re_zh\n# import re\ndef getPinyin(originalText,\n              filteredText,\n              whitelistChars=[\"的\"],\n              whitelistNonChinese=True):  # any repl will do.\n    blocks = [x for x in re_zh.split(originalText) if len(x) > 0]",
        "type": "code",
        "location": "/tasks/qq/qq_red_packet_collect/textfilter/filter_py3_fastapi.py:144-186"
    },
    "1567": {
        "file_id": 129,
        "content": "This function takes a message and filters it based on a DFA (Deterministic Finite Automaton) filter. It checks each character in the message, appending replacements if necessary or keeping the original character if not. If a keyword is found, it replaces it with '*'. The function returns the filtered text as a string.\n\nThe code also includes a test case for checking if the first character of the filter result matches the expected output for the given input. Additionally, there are import statements and function definitions for other functionalities like getting pinyin and handling Chinese text.",
        "type": "comment"
    },
    "1568": {
        "file_id": 129,
        "content": "    # words = result.words\n    translate_list = []\n    for block in blocks:\n        if re_zh.match(block):\n            block_pinyin = pin.get(block)\n            for index, pinyin in enumerate(block_pinyin):\n                character = block[index]\n                translate_list.append((character, pinyin[0]))\n        else:\n            for index, character in enumerate(block):\n                translate_list.append((character, character))\n    moderatedText = \"\"\n    for index, (originalCharacter, pinyin) in enumerate(translate_list):\n        filteredCharacter = filteredText[index]\n        if filteredCharacter == originalCharacter or originalCharacter in whitelistChars or (\n                whitelistNonChinese and (not re_zh.match(originalCharacter))): # changed the moderator logic.\n            moderatedText += originalCharacter\n        elif pinyin != originalCharacter:\n            moderatedText += pinyin\n        else:\n            moderatedText += filteredCharacter\n    return moderatedText\n@app.get(\"/\")\ndef read_root():",
        "type": "code",
        "location": "/tasks/qq/qq_red_packet_collect/textfilter/filter_py3_fastapi.py:187-213"
    },
    "1569": {
        "file_id": 129,
        "content": "This code filters text by translating Chinese characters into their corresponding Pinyin while preserving non-Chinese characters or whitelisted characters. It creates a list of translated characters and applies the filter logic to generate a moderated text output, which is returned as the result. Additionally, there's an API endpoint (\"/\") defined for accessing this functionality through a FastAPI server.",
        "type": "comment"
    },
    "1570": {
        "file_id": 129,
        "content": "    return {\"response\": \"DFAFilter based Chinese text filter(censor)\"}\n@app.get(\"/filter\")\ndef read_item(text: Union[str, None] = None, moderate: bool = True):\n    originalText = text\n    filteredText = gfw.filter(text, \"*\")\n    if moderate:\n        moderatedText = getPinyin(originalText, filteredText)\n        return {\"response\": moderatedText}\n    else:\n        return {\"response\": filteredText}",
        "type": "code",
        "location": "/tasks/qq/qq_red_packet_collect/textfilter/filter_py3_fastapi.py:214-225"
    },
    "1571": {
        "file_id": 129,
        "content": "This code defines a FastAPI route (\"/filter\") that takes in a text input and applies GFW filtering. If the \"moderate\" parameter is True, it generates a moderated text by replacing Chinese characters with their corresponding pinyin. Otherwise, it returns the filtered text.",
        "type": "comment"
    },
    "1572": {
        "file_id": 130,
        "content": "/tasks/qq/qq_red_packet_collect/paddletts/test.sh",
        "type": "filepath"
    },
    "1573": {
        "file_id": 130,
        "content": "The code uses the PaddleSpeech TTS (Text-to-Speech) model to convert text \"你好\" into speech and saves it as hello.wav using CPU device. The model is located at ~/.paddlespeech in a drive named Toshiba3000, with possible English-Chinese splitting tests. Pyjom is related to these tests and possibly offers other server interactions.",
        "type": "summary"
    },
    "1574": {
        "file_id": 130,
        "content": "paddlespeech tts --input \"你好\" --output hello.wav --voc hifigan_csmsc --device cpu\n# model location ~/.paddlespeech -> /media/root/Toshiba3000/paddlespeech_models\n# check out pyjom about english-chinese spliting tests. we have model server other than cli interactions.",
        "type": "code",
        "location": "/tasks/qq/qq_red_packet_collect/paddletts/test.sh:1-4"
    },
    "1575": {
        "file_id": 130,
        "content": "The code uses the PaddleSpeech TTS (Text-to-Speech) model to convert text \"你好\" into speech and saves it as hello.wav using CPU device. The model is located at ~/.paddlespeech in a drive named Toshiba3000, with possible English-Chinese splitting tests. Pyjom is related to these tests and possibly offers other server interactions.",
        "type": "comment"
    },
    "1576": {
        "file_id": 131,
        "content": "/symlinks/README.md",
        "type": "filepath"
    },
    "1577": {
        "file_id": 131,
        "content": "This folder holds essential symlinks to crucial resources, ensuring efficient organization and access.",
        "type": "summary"
    },
    "1578": {
        "file_id": 131,
        "content": "# this folder contains some important symlinks to my resources.",
        "type": "code",
        "location": "/symlinks/README.md:1-1"
    },
    "1579": {
        "file_id": 131,
        "content": "This folder holds essential symlinks to crucial resources, ensuring efficient organization and access.",
        "type": "comment"
    },
    "1580": {
        "file_id": 132,
        "content": "/externals/init_darknet.sh",
        "type": "filepath"
    },
    "1581": {
        "file_id": 132,
        "content": "This code clones the Darknet repository, changes directory to it, sets GPU and CUDNN variables, then builds the darknet executable using make.",
        "type": "summary"
    },
    "1582": {
        "file_id": 132,
        "content": "git clone --depth 1 https://github.com/AlexeyAB/darknet\ncd darknet\n# GPU=1\n# CUDNN=1\n# make y",
        "type": "code",
        "location": "/externals/init_darknet.sh:1-6"
    },
    "1583": {
        "file_id": 132,
        "content": "This code clones the Darknet repository, changes directory to it, sets GPU and CUDNN variables, then builds the darknet executable using make.",
        "type": "comment"
    },
    "1584": {
        "file_id": 133,
        "content": "/externals/get_the_heck_ffmpeg.py",
        "type": "filepath"
    },
    "1585": {
        "file_id": 133,
        "content": "This code splits a string into enabled FFmpeg library names, adds \"-dev\" if necessary, and installs them using \"apt install\".",
        "type": "summary"
    },
    "1586": {
        "file_id": 133,
        "content": "mystr = \"-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librabbitmq --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-pocketsphinx --enable-librsvg --enable-libmfx --enable-libdc1394 --enable-libdrm --enable-chromaprint --enable-frei0r --enable-libx264 \"\nmystr = mystr.split(\" \")",
        "type": "code",
        "location": "/externals/get_the_heck_ffmpeg.py:1-3"
    },
    "1587": {
        "file_id": 133,
        "content": "Code is splitting the string mystr into an array of individual enabled libraries for FFmpeg.",
        "type": "comment"
    },
    "1588": {
        "file_id": 133,
        "content": "mylibs = []\nfor elem in mystr:\n    a = elem.replace(\"-\",\"\").replace(\" \",\"\").replace(\"enable\",\"\")\n    if len(a) <=2:\n        continue\n    if a.startswith(\"lib\"):\n        a +=\"-dev\"\n        mylibs.append(a)\n    else:\n        a+= \"-dev\"\n        b = \"lib\"+a\n        mylibs.append(a)\n        mylibs.append(b)\nfor lib in mylibs:\n    print(\"yes | apt install {}\".format(lib))",
        "type": "code",
        "location": "/externals/get_the_heck_ffmpeg.py:5-20"
    },
    "1589": {
        "file_id": 133,
        "content": "This code filters out a list of library names from a given string, adds \"-dev\" if it doesn't start with \"lib\", and then installs each library using \"apt install\".",
        "type": "comment"
    },
    "1590": {
        "file_id": 134,
        "content": "/externals/ffmpeg_libspeex_tensorflow_support.sh",
        "type": "filepath"
    },
    "1591": {
        "file_id": 134,
        "content": "The code configures FFmpeg with various libraries, modules, and specific paths for CUDA include and library files, then builds and installs the custom FFmpeg.",
        "type": "summary"
    },
    "1592": {
        "file_id": 134,
        "content": "cd ffmpeg\n###LAST WORKING ONE\n./configure --toolchain=hardened  --arch=amd64  --enable-libspeex --enable-gpl --enable-nonfree --enable-pthreads --extra-libs=-lstdc++ --enable-cuda-nvcc --enable-cuvid --enable-nvenc --enable-shared --enable-libnpp --extra-cflags=-I/usr/local/cuda/include --enable-libnpp --extra-ldflags=-L/usr/local/cuda/lib64 --disable-static --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librabbitmq --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 ",
        "type": "code",
        "location": "/externals/ffmpeg_libspeex_tensorflow_support.sh:1-5"
    },
    "1593": {
        "file_id": 134,
        "content": "The code configures the ffmpeg build with various options such as libspeex, gpl, nonfree, pthreads, and more. It also enables cuda-nvcc, cuvid, nvenc, shared, gnutls, ladspa, aom, ass, bluray, bs2b, caca, dav1d, fontconfig, fribidi, gme, gsm, jack, mp3lame, mysofa, openjpeg, openmpt, opus, pulse, rabbitmq, rubberband, shine, snappy, soxr, ssh, theora, twolame, vidstab, vorbis, vpx, webp, and x265. It uses specific paths for include and library files of cuda. Disables static build.",
        "type": "comment"
    },
    "1594": {
        "file_id": 134,
        "content": "--enable-libsrt --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --disable-sndio --enable-pocketsphinx --enable-librsvg --enable-libmfx --enable-libdc1394 --enable-libdrm --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared --enable-libopenh264  --enable-libtensorflow # wtf?\nmake -j8\nmake install",
        "type": "code",
        "location": "/externals/ffmpeg_libspeex_tensorflow_support.sh:5-8"
    },
    "1595": {
        "file_id": 134,
        "content": "The code is enabling a variety of libraries and modules for FFmpeg, including audio and video codecs, user interfaces, and specialized libraries like libtensorflow. It then builds and installs the configured FFmpeg.",
        "type": "comment"
    },
    "1596": {
        "file_id": 135,
        "content": "/externals/dgl_init.sh",
        "type": "filepath"
    },
    "1597": {
        "file_id": 135,
        "content": "This code is cloning and installing DGL (Dynamic Graph Library) for Python. It uses git to clone the repository, sets up a build directory with CMake, compiles the code using make, and then installs it with pip3. This process ensures that the necessary dependencies and configurations are in place before installation.",
        "type": "summary"
    },
    "1598": {
        "file_id": 135,
        "content": "git clone --depth 1 --recurse-submodules https://github.com/dmlc/dgl.git\ncd dgl # set up my fucking fastgithub proxy!\n# git submodule update --init --recursive\nmkdir build\ncd build\ncmake -DUSE_CUDA=ON ..\nmake -j4\ncd ..\npip3 install ./python # the way to install this shit.\n# cd ../python\n# python3 setup.py install",
        "type": "code",
        "location": "/externals/dgl_init.sh:1-11"
    },
    "1599": {
        "file_id": 135,
        "content": "This code is cloning and installing DGL (Dynamic Graph Library) for Python. It uses git to clone the repository, sets up a build directory with CMake, compiles the code using make, and then installs it with pip3. This process ensures that the necessary dependencies and configurations are in place before installation.",
        "type": "comment"
    }
}