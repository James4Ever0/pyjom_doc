{
    "1500": {
        "file_id": 121,
        "content": "                    )\n                    frameDict[frame_num][\"end\"] = frameIndex\n                    # we mark the first and last time to display this frame.\n                # how to stablize this shit?\n            cv2.imshow(\"video\", frame)\n            # just video.\n            # cv2.imshow('img_output', img_output)\n            # cv2.imshow('img_bgmodel', img_bgmodel)\n        else:\n            # cv2.waitKey(1000) # what the heck?\n            break\n        # if 0xFF & cv2.waitKey(10) == 27:\n        #     break\n    # cv2.destroyAllWindows()\n    print(\"FINAL FRAME DETECTIONS:\")\n    print(frameDict)\n    return frameDict\n    # {1: {'coords': ((80, 199), (496, 825)), 'start': 13, 'end': 269}, 2: {'coords': ((80, 381), (483, 644)), 'start': 297, 'end': 601}}\ndef frameborder_default_configs(model=\"framedifference_talib\"):\n    assert model in [\"framedifference_talib\",\"huffline_horizontal_vertical\"]\n    if model == \"framedifference_talib\":\n        df_config = {\"past_frames\":19,\"timeperiod\":10, \"change_threshold\":0.2,\"perc\":0.03, \"max_reputation\": 3,\"framePeriod\":1,\"minVariance\" :10}",
        "type": "code",
        "location": "/pyjom/medialang/functions/detectors/frameborder_Detector.py:675-699"
    },
    "1501": {
        "file_id": 121,
        "content": "This code is a part of the frameborder_Detector function, which detects frames with high frame borders. It uses OpenCV's imshow function to display the video and frames. The frameDict variable stores the frame numbers and their respective coordinates and timeframes. If a specific key is pressed, it breaks out of the loop. After detecting final frame detections, it returns the frameDict. This code also includes a default config function for different models.",
        "type": "comment"
    },
    "1502": {
        "file_id": 121,
        "content": "    else:\n        df_config = {\"line_thresh\":150, # original 150\n        \"includeBoundaryLines\":True,\"blurKernel\":(5,5),\"angle_error\":0.00003,\"delta_thresh\":0.1,\"min_rect_life\":0,\"max_rect_life\":6,\"framePeriod\":1,\"min_rect_life_display_thresh\":3,\"minRectArea\":1}\n    return df_config\ndef frameborder_Detector(mediapaths, model=\"framedifference_talib\",config={}):\n    print(\"MODEL:\",model)\n    # breakpoint()\n    yconfig = copy.deepcopy(config)\n    # breakpoint()\n    # if config is None:\n    #     breakpoint()\n    #     config = {}\n    # any better detectors? deeplearning?\n    assert model in [\"framedifference_talib\",\"huffline_horizontal_vertical\"]\n    results = []\n    keyword = \"{}_detector\".format(model)\n    data_key = keyword # different than yolo.\n    for mediapath in mediapaths:\n        print(\"mediapath:\", mediapath)\n        # breakpoint()\n        mediatype = getFileType(mediapath)\n        print(\"subtitle of mediatype:\", mediatype)\n        assert mediatype in [\"video\", \"image\"]  # gif? anything like that?\n        if model == \"framedifference_talib\":",
        "type": "code",
        "location": "/pyjom/medialang/functions/detectors/frameborder_Detector.py:700-725"
    },
    "1503": {
        "file_id": 121,
        "content": "The code is defining a function called \"frameborder_Detector\" which takes in media paths and a model parameter. The function checks the model and applies different detection algorithms based on the input model. It also initializes a deepcopy of the configuration and checks if the model is either \"framedifference_talib\" or \"huffline_horizontal_vertical\". If not, it raises an assertion error. Finally, it iterates through the media paths and performs some actions specific to the given model.",
        "type": "comment"
    },
    "1504": {
        "file_id": 121,
        "content": "            assert mediatype == \"video\"\n            # advice you to check out only those areas with rectangles, not something else, if detected any rectangular area.\n            # but does that happen for normal videos? you might want huffline transforms.\n            # you can eliminate unwanted rectangles by time duration and huffline transforms.\n        result = {\"type\": mediatype, data_key: {}}\n        default_config = frameborder_default_configs(model)\n        xconfig = default_config\n        xconfig.update(yconfig) # override default configs.\n        # do not freaking assign directly after update.\n        config = xconfig\n        # print(\"YCONFIG:\", yconfig)\n        # breakpoint()\n        if mediatype == \"image\":\n            data = cv2.imread(mediapath)\n            data = keywordDecorator(huffline_stillImage_Identifier, **config)(data) # this is just oldfashioned function decorator\n            result[data_key].update({keyword: data})\n            result[data_key].update({\"config\": config})\n        else:",
        "type": "code",
        "location": "/pyjom/medialang/functions/detectors/frameborder_Detector.py:726-744"
    },
    "1505": {
        "file_id": 121,
        "content": "This code is performing frame detection on video or image media. It checks if the mediatype is \"video\" and provides advice for checking only rectangular areas, suggesting huffline transforms to eliminate unwanted rectangles by time duration. If the mediatype is \"image\", it reads the image using cv2.imread, applies keywordDecorator with huffline_stillImage_Identifier and config, updates the result dictionary with the data and config.",
        "type": "comment"
    },
    "1506": {
        "file_id": 121,
        "content": "            # you may not want videoFrameIterator.\n            if model == \"framedifference_talib\":\n                mdata= framedifference_talib_FrameIterator(\n                    mediapath,**config)\n            elif model == \"huffline_horizontal_vertical\":\n                mdata = huffline_horizontal_vertical_FrameIterator(mediapath,**config)\n            metadata = {\"config\": config}\n            result[data_key][keyword] = mdata\n            result[data_key].update(metadata)\n        results.append(result)\n    return results",
        "type": "code",
        "location": "/pyjom/medialang/functions/detectors/frameborder_Detector.py:745-755"
    },
    "1507": {
        "file_id": 121,
        "content": "This code selects a specific detector model based on the input \"model\" parameter. If the model is \"framedifference_talib\", it creates an instance of framedifference_talib_FrameIterator, and if the model is \"huffline_horizontal_vertical\", it creates an instance of huffline_horizontal_vertical_FrameIterator. It then adds the created iterator to the result dictionary along with a metadata dictionary containing the original config parameters. The final results list is appended with this result dictionary.",
        "type": "comment"
    },
    "1508": {
        "file_id": 122,
        "content": "/pyjom/medialang/functions/detectors/entityDetector.py",
        "type": "filepath"
    },
    "1509": {
        "file_id": 122,
        "content": "The code performs text processing, device movement detection, and entity identification using Levenshtein's algorithm. It calculates string similarities, generates test results, compares content and location changes to find or create entity structures, initializes variables, detects and tracks entities, updates text detection results, merges entries based on similarity and temporal proximity, and returns a formatted result dictionary.",
        "type": "summary"
    },
    "1510": {
        "file_id": 122,
        "content": "from .mediaDetector import *\nimport Levenshtein\nimport string\nimport zhon.hanzi\nimport wordninja\ndef resplitedEnglish(string2,skipSpecial=True):\n    if skipSpecial:\n        header = string2[0]\n        if header in string.punctuation or header in zhon.hanzi.punctuation:\n            return string2 # this could be buggy.\n    result = wordninja.split(string2)\n    if len(result)>1:\n        mlist = zip(result[:-1], result[1:])\n        for a,b in mlist:\n            combined = \"{} {}\".format(a,b)\n            error = a+b\n            string2 = string2.replace(error,combined)\n    return string2 # really? this is not good. maybe you should provide some version of continuality, for channel id watermarks.\n    # @MA SECO. -> @MASECO\n# maybe you can read it here?\n# you need double language check. both chinese and english. or really?\ndef ocrEntityDetector(mdata):\n    alteredData = [] # we should do a demo. \n    return alteredData # now we are on the same page, paddleocr is using cuda 11.2 which is compatible to 11.3\ndef getMinLenStr(a,b):",
        "type": "code",
        "location": "/pyjom/medialang/functions/detectors/entityDetector.py:1-29"
    },
    "1511": {
        "file_id": 122,
        "content": "The code includes a function \"resplitedEnglish\" that splits the given string into individual words, potentially skipping special characters. It uses wordninja for splitting and attempts to merge adjacent words if they have been mistakenly separated by special characters. Another function, \"ocrEntityDetector\", receives data as input and returns alteredData, but the code lacks implementation details. Lastly, there's a helper function \"getMinLenStr\" that takes two inputs 'a' and 'b', likely for comparison purposes.",
        "type": "comment"
    },
    "1512": {
        "file_id": 122,
        "content": "    la,lb = len(a),len(b)\n    if la < lb:return a\n    return b\ndef getBlockType(dlocation,dcontent):\n    if not dlocation:\n        if not dcontent: return \"stationary\"\n        else: return \"typing\"\n    else:\n        if not dcontent: return \"typing_moving\"\n        else: return \"moving\"\ndef getStringDistance(a,b):\n    return Levenshtein.distance(a,b)\ndef getStringSimilarity(a,b):\n    return Levenshtein.ratio(a,b)\ndef getChineseLen(string2):\n    counter  = 0\n    upperLimit, lowerLimit = 0x4e00, 0x9fff\n    for elem in string2:\n        ordNum = ord(elem)\n        if ordNum <= upperLimit and ordNum>=lowerLimit:\n            counter+=1\n    return counter\ndef getPunctualLen(string2):\n    counter = 0\n    chinesePunctuals = zhon.hanzi.punctuation\n    englishPunctuals = string.punctuation\n    standardString = chinesePunctuals+englishPunctuals\n    for elem in string2:\n        if elem in standardString:\n            counter+=2\n    return counter\ndef getEnglishLen(string2):\n    counter = 0\n    standardString = \"abcdefghijklmnopqrstuvwxyz\"",
        "type": "code",
        "location": "/pyjom/medialang/functions/detectors/entityDetector.py:30-70"
    },
    "1513": {
        "file_id": 122,
        "content": "This code defines several functions for text processing, including getting the length of Chinese characters, English punctuation marks, and English words. It also calculates string similarity and distance using Levenshtein's algorithm. The \"getBlockType\" function determines if the device is stationary or moving based on its location and content.",
        "type": "comment"
    },
    "1514": {
        "file_id": 122,
        "content": "    standardString += standardString.upper()\n    standardString += \"0123456789\"\n    # it won't split. you may need double check the thing.\n    # standardString += \" \"\n    for elem in string2:\n        if elem in standardString:\n            counter+=1\n    return counter\ndef getMinMaxText(a,b):\n    mlist = [a,b]\n    clens = [getChineseLen(x) for x in mlist]\n    elens = [getEnglishLen(x) for x in mlist]\n    slens = [getPunctualLen(x) for x in mlist]\n    mlens = [x[0]+x[1]+x[2] for x in zip(clens,elens,slens)]\n    if len(a) > len(b):\n        if mlens[0] > mlens[1]:\n            return a\n        return b\n    else:\n        if mlens[0] < mlens[1]:\n            return b\n        return a\ndef pointDifference(a,b):\n    return [a[0] - b[0], a[1] - b[1]]\ndef makeOCREntity(ocrData,minMaxThresh = 24 ,# max difference is ten pixel. or it is considered as moving.\nstrDisThreshold = 2 ,# or considered as changing?\ncertThreshold = 0.6,\nchangingMinMaxThresh = 45,\nchangingstrDisThreshold = 3,\ntimeThreshold = 0.3 ,# i intentially set it.\nblockTimeThreshold = 0.3, # at least last this long?",
        "type": "code",
        "location": "/pyjom/medialang/functions/detectors/entityDetector.py:71-104"
    },
    "1515": {
        "file_id": 122,
        "content": "The code defines a series of functions for detecting changes in textual data, such as entity detection and calculating point differences. It also involves calculating Chinese, English, and punctuation lengths and uses them to determine if the text is changing or moving based on certain thresholds.",
        "type": "comment"
    },
    "1516": {
        "file_id": 122,
        "content": "strSimThreshold = 0.8):\n    testElemIds = {} # just show processed items.\n    for line in ocrData:\n        mtime,mframe,mresult = line[\"time\"],line[\"frame\"],line[\"paddleocr\"]\n        # print(\"______________________\")\n        # print(\"time:\",mtime)\n        # newlyAddedIds = [] # will directly added if not in.\n        # initiate things here.\n        for mid in testElemIds.keys(): # must trt\n            testElemIds[mid][\"hasIdentical\"] =False  #initialization.\n        for presult in mresult:\n            location, mtext = presult\n            p1, p2, p3, p4 = location\n            text, certainty = mtext\n            print(\"RECOGNIZED TEXT:\",text)\n            mtimestamp = {\"frame\":mframe,\"time\":mtime}\n            # print(\"location:\",location)\n            # print(\"text:\",text)\n            # print(\"certainty:\",certainty)\n            foundIdentical = False\n            for mid in testElemIds.keys():\n                myid = testElemIds[mid]\n                myLastLocation = myid[\"locations\"][-1]\n                px1,px2,px3,px4 = myLastLocation",
        "type": "code",
        "location": "/pyjom/medialang/functions/detectors/entityDetector.py:105-128"
    },
    "1517": {
        "file_id": 122,
        "content": "This code initializes variables and iterates through OCR data, specifically focusing on recognized text from each result. It checks if any previously identified elements have identical locations to the current element, updating a flag accordingly. The purpose seems to be detecting entities based on overlapping locations in different frames or times.",
        "type": "comment"
    },
    "1518": {
        "file_id": 122,
        "content": "                myMinMax = max([max(pointDifference(a,b)) for a,b in zip(location,myLastLocation)])\n                myMinMaxs = [max([max(pointDifference(a,b)) for a,b in zip(location,myLL)]) for myLL in myid[\"locations\"]] # changing it. the max movement.\n                mLastTime = myid[\"timestamps\"][-1][\"time\"]\n                timeDelta = mLastTime - mtime\n                myLastContent = myid[\"contents\"][-1]\n                strDistance = getStringDistance(myLastContent,text)\n                strDistances = [getStringDistance(myLastContent,text) for myLC in myid[\"contents\"]]\n                strSim = getStringSimilarity(myLastContent,text)\n                strSims = [getStringSimilarity(myLC,text) for myLC in myid[\"contents\"]]\n                foundIdentical = False\n                movementMap = {\"location\":False,\"content\":False,\"continual\":False}\n                if timeDelta < timeThreshold:\n                    if myMinMax <= minMaxThresh and ((strDistance <= strDisThreshold) or (strSim >= strSimThreshold )) : # wrong logic.",
        "type": "code",
        "location": "/pyjom/medialang/functions/detectors/entityDetector.py:129-142"
    },
    "1519": {
        "file_id": 122,
        "content": "This code calculates the maximum movement and string similarity between consecutive entities, considering time deltas and thresholds. If the movement is within a threshold and the string distance or similarity meets specific criteria, it indicates an identical entity. However, the current logic for identifying identical entities may be incorrect.",
        "type": "comment"
    },
    "1520": {
        "file_id": 122,
        "content": "                        foundIdentical = True\n                        print(\"test result:\",myMinMax <= minMaxThresh ,(strDistance <= strDisThreshold) , (strSim >= strSimThreshold ))\n                        print(myMinMax,strDistance,strSim)\n                        print(minMaxThresh,strDisThreshold,strSimThreshold)\n                        print(\"line a\")\n                        pass\n                        # stricter limit, to know if really is movement?\n                    elif myMinMax <= changingMinMaxThresh:\n                        foundIdentical = True\n                        movementMap[\"location\"] = True\n                        if max(strDistances) <= strDisThreshold or max(strSims) >= strSimThreshold:\n                            # make sure it is globally the same.\n                            print(\"line b\")\n                            pass\n                        elif min(strDistances) <= changingstrDisThreshold:\n                            print(\"line c\")\n                            movementMap[\"content\"] = True",
        "type": "code",
        "location": "/pyjom/medialang/functions/detectors/entityDetector.py:143-161"
    },
    "1521": {
        "file_id": 122,
        "content": "This code block is checking for identical entities or potential movements in a video. It uses thresholds to determine if the entity is the same, and if it's a movement, it checks if it's the same content or location. The code also prints test results and specific lines for debugging purposes.",
        "type": "comment"
    },
    "1522": {
        "file_id": 122,
        "content": "                        else: # consider something else\n                            print(\"line d\")\n                            foundIdentical = False\n                    elif strDistance <= changingstrDisThreshold or strSim >= strSimThreshold:\n                        foundIdentical = True\n                        print(\"line e\")\n                        movementMap[\"content\"] = True\n                        if max(myMinMaxs) <= minMaxThresh:\n                            print(\"line f\")\n                            pass\n                        elif min(myMinMaxs) <= changingMinMaxThresh:\n                            print(\"line g\")\n                            movementMap[\"location\"] = True\n                        else:\n                            print(\"line h\")\n                            foundIdentical = False\n                else:\n                    foundIdentical = False\n                if foundIdentical:\n                    print(\"FOUND IDENTICAL\",text,myLastContent)\n                    print(\"REASON\",movementMap)",
        "type": "code",
        "location": "/pyjom/medialang/functions/detectors/entityDetector.py:162-183"
    },
    "1523": {
        "file_id": 122,
        "content": "This code checks if the text has a matching content or location change using string similarity and minimum-maximum thresholds. If a match is found, it prints \"FOUND IDENTICAL\" with the original text and reason for identification from the movementMap dictionary.",
        "type": "comment"
    },
    "1524": {
        "file_id": 122,
        "content": "                    print(\"ID\",mid)\n                    print()\n                    # care about continuality here.\n                    if timeDelta < timeThreshold:\n                        movementMap[\"continual\"] = True\n                    if myid[\"hasIdentical\"] or timeDelta == 0: # eliminate duplicates.\n                        continue # do not check this.\n                    # print(\"found Identical:\",mid)\n                    testElemIds[mid][\"hasIdentical\"]=True\n                    testElemIds[mid][\"locations\"].append(location)\n                    testElemIds[mid][\"contents\"].append(text)\n                    testElemIds[mid][\"timestamps\"].append(mtimestamp)\n                    testElemIds[mid][\"movements\"].append(movementMap)\n                    break\n            if not foundIdentical:\n                if certainty > certThreshold:\n                    minitStruct = {str(uuid.uuid4()):{\"locations\":[copy.deepcopy(location)],\"contents\":[copy.deepcopy(text)],\"movements\":[],\"hasIdentical\":False,\"timestamps\":[mtimestamp]}}",
        "type": "code",
        "location": "/pyjom/medialang/functions/detectors/entityDetector.py:184-200"
    },
    "1525": {
        "file_id": 122,
        "content": "This code is searching for identical entities based on location, text content, and timestamps. If a duplicate is found or time difference is small, it continues to the next entity. If no duplicates are found and certainty level is high, it creates a new entity structure with unique ID, locations, contents, movements, and timestamps.",
        "type": "comment"
    },
    "1526": {
        "file_id": 122,
        "content": "                    testElemIds.update(minitStruct) # do you really expect it? i mean it could have cracks.\n    # print(json.dumps(testElemIds,indent=4))\n    keys= list(testElemIds.keys())\n    print(\"keyNum:\",len(keys))\n    mfinal = {}\n    for key in keys:\n        mblocks = []\n        kElem = testElemIds[key]\n        # we try to compress this thing.\n        initBlock = {\"type\":\"stationary\",\"text\":None,\"location\":None,\"timespan\":{\"start\":None,\"end\":None}} # we will have shortest text.\n        for index, mtimestamp in enumerate(kElem[\"timestamps\"]):\n            thisText = kElem[\"contents\"][index]\n            thisLocation = kElem[\"locations\"][index]\n            if index == 0:\n                initBlock[\"timespan\"][\"start\"] = mtimestamp\n                initBlock[\"timespan\"][\"end\"] = mtimestamp\n                initBlock[\"text\"] = thisText\n                initBlock[\"location\"] = copy.deepcopy(thisLocation)\n            else:\n                movementMap = kElem[\"movements\"][index-1]\n                dlocation, dcontent, dtime = movementMap[\"location\"], movementMap[\"content\"], movementMap[\"continual\"]",
        "type": "code",
        "location": "/pyjom/medialang/functions/detectors/entityDetector.py:201-221"
    },
    "1527": {
        "file_id": 122,
        "content": "This code is initializing variables for block detection and compression. It creates an empty dictionary, loops through the timestamps and contents of elements, and assigns the first timestamp and content as the starting point of a stationary block. It also copies the location from the first content to the \"location\" field in the initBlock.",
        "type": "comment"
    },
    "1528": {
        "file_id": 122,
        "content": "                lastType = initBlock[\"type\"]\n                thisType = getBlockType(dlocation,dcontent)\n                if (not dtime) or (thisType != lastType):\n                    # will abandon all no matter what. cause it is not continual.\n                    mblocks.append(copy.deepcopy(initBlock))\n                    initBlock = {\"type\":thisType,\"timespan\":{\"start\":mtimestamp,\"end\":mtimestamp}} # get the content.\n                    if thisType in [\"stationary\", \"moving\"]:\n                        # lastText = initBlock[\"text\"]\n                        # mselectedText = getMinMaxText(lastText,thisText)\n                        initBlock.update({\"text\":thisText}) # not right. we select the best one.\n                    if thisType in [\"stationary\", \"typing\"]:\n                        initBlock.update({\"location\":copy.deepcopy(thisLocation)})\n                    if thisType in [\"typing_moving\",\"typing\"]:\n                        initBlock.update({\"texts\":[thisText]})\n                    if thisType in [\"moving\", \"typing_moving\"]:",
        "type": "code",
        "location": "/pyjom/medialang/functions/detectors/entityDetector.py:222-237"
    },
    "1529": {
        "file_id": 122,
        "content": "This code snippet initializes a new block for text detection whenever there is a change in type or no time stamp. The block's details are updated based on the current type, location, and texts. It also stores the previous and new types, with the option to select the best text if the type is \"stationary\" or \"moving\".",
        "type": "comment"
    },
    "1530": {
        "file_id": 122,
        "content": "                        initBlock.update({\"locations\":[copy.deepcopy(thisLocation)]})\n                else:\n                    initBlock[\"timespan\"][\"end\"] = mtimestamp\n                    if thisType in [\"moving\",\"typing_moving\"]:\n                        initBlock[\"locations\"].append(copy.deepcopy(thisLocation))\n                    if thisType in [\"typing_moving\",\"typing\"]:\n                        initBlock[\"texts\"].append(thisText)\n                    if thisType in [\"moving\",\"stationary\"]: # we don't change stationary/typing's location. or do we?\n                        mLastText = initBlock[\"text\"]\n                        initBlock[\"text\"] = getMinMaxText(mLastText,thisText)\n                        # initBlock[\"text\"] = getMinLenStr(mLastText,thisText)\n        mblocks.append(copy.deepcopy(initBlock))\n        mblocks2 = []\n        for block in mblocks:\n            start,end = block[\"timespan\"][\"start\"], block[\"timespan\"][\"end\"]\n            timedelta = end[\"time\"] - start[\"time\"]\n            if timedelta > blockTimeThreshold:",
        "type": "code",
        "location": "/pyjom/medialang/functions/detectors/entityDetector.py:238-254"
    },
    "1531": {
        "file_id": 122,
        "content": "This code appears to be part of a function that detects and tracks entities such as text or movement. It updates the initialization block based on the detected entity type, appends locations or texts, creates a copy of the block for later use, and filters out blocks with duration shorter than the given threshold.",
        "type": "comment"
    },
    "1532": {
        "file_id": 122,
        "content": "                mblocks2.append(block)\n        mfinal.update({key:mblocks2})\n    return mfinal\n    # print(json.dumps(mfinal,indent=4))\n    # print(\"___________\")\ndef staticOCRCombinator(myresult,simThreshold= 0.8):\n    # we use wordninja to do the english spliting.\n    # you can also get this working for non-static.\n    myNewResult = {}\n    lastWordResult = None\n    lastId = None\n    lastLocation = None\n    lastTimeStamp = {\"start\":{},\"end\":{}}\n    for key in myresult.keys():\n        melems = myresult[key]\n        for melem in melems:\n            mtype = melem[\"type\"]\n            if mtype in [\"stationary\", \"moving\"]:\n                mtext = melem[\"text\"] # maybe there are some moving things out there?\n            else:\n                mtext = melem[\"texts\"][0]\n            mtext = resplitedEnglish(mtext)\n            if lastWordResult is None:\n                lastWordResult = mtext\n                lastId = key\n                lastTimeStamp[\"start\"] = melem[\"timespan\"][\"start\"]\n                lastTimeStamp[\"end\"] = melem[\"timespan\"][\"end\"]",
        "type": "code",
        "location": "/pyjom/medialang/functions/detectors/entityDetector.py:255-283"
    },
    "1533": {
        "file_id": 122,
        "content": "This function takes a dictionary of results and applies static OCR combining by processing each element in the dictionary. It uses WordNinja for English splitting, and stores last word result, id, location, and timestamp.",
        "type": "comment"
    },
    "1534": {
        "file_id": 122,
        "content": "                if mtype in [\"stationary\", \"typing\"]:\n                    lastLocation = melem[\"location\"]\n                else:\n                    lastLocation = melem[\"locations\"][0]\n            else:\n                if getStringSimilarity(lastWordResult,mtext) > simThreshold:\n                    # merge the thing.\n                    lastTimeStamp[\"start\"] = list(sorted([melem[\"timespan\"][\"start\"],lastTimeStamp[\"start\"]],key=lambda x:x[\"time\"]))[0]\n                    lastTimeStamp[\"end\"] = list(sorted([melem[\"timespan\"][\"end\"],lastTimeStamp[\"end\"]],key=lambda x:-x[\"time\"]))[0]\n                else:\n                    myNewResult.update({lastId:{\"content\":lastWordResult,\"timespan\":lastTimeStamp,\"location\":lastLocation}})\n                    lastWordResult = mtext\n                    lastId = key\n                    if mtype in [\"stationary\", \"typing\"]:\n                        lastLocation = melem[\"location\"]\n                    else:\n                        lastLocation = melem[\"locations\"][0]\n                    lastTimeStamp[\"start\"] = melem[\"timespan\"][\"start\"]",
        "type": "code",
        "location": "/pyjom/medialang/functions/detectors/entityDetector.py:284-301"
    },
    "1535": {
        "file_id": 122,
        "content": "This code is merging and updating text detection results based on similarity and temporal proximity. It keeps track of the last detected word, its location, and timestamp, updating or creating new result entries accordingly.",
        "type": "comment"
    },
    "1536": {
        "file_id": 122,
        "content": "                    lastTimeStamp[\"end\"] = melem[\"timespan\"][\"end\"]\n            # else:\n            #     print(\"found different type:\",mtype)\n            #     print(\"text:\",mtext)\n            #     print(\"element:\",melem)\n    myNewResult.update({lastId:{\"content\":lastWordResult,\"timespan\":lastTimeStamp,\"location\":lastLocation}})\n    print(\"process complete\")\n    return myNewResult",
        "type": "code",
        "location": "/pyjom/medialang/functions/detectors/entityDetector.py:302-309"
    },
    "1537": {
        "file_id": 122,
        "content": "Checking if the element matches the expected format, if not print relevant details and proceed with updating the result dictionary. Finally, print \"process complete\" and return the result dictionary.",
        "type": "comment"
    },
    "1538": {
        "file_id": 123,
        "content": "/pyjom/medialang/functions/detectors/blackoutDetector.py",
        "type": "filepath"
    },
    "1539": {
        "file_id": 123,
        "content": "This code detects blackouts in media by calculating scores per frame block and storing results. It works for videos and images, using OpenCV or videoFrameIterator. The code updates metadata dictionaries and appends updated results to a list before returning the final list of results.",
        "type": "summary"
    },
    "1540": {
        "file_id": 123,
        "content": "from .mediaDetector import *\ndef blackoutIdentifier(frame_a, cut=3, threshold=30, method=\"average\"):\n    assert cut >= 1\n    methods = {\"average\": np.average, \"max\": np.max, \"min\": np.min}\n    mshape = frame_a.shape\n    width, height = mshape[:2]\n    mcut = int(min(width, height) / cut)\n    if len(mshape) == 3:\n        result = methods[method](result, axis=2)\n        shape0 = int(width / mcut)\n    shape1 = int(height / mcut)\n    diff = np.zeros((shape0, shape1)).tolist()\n    # mapping = {}\n    for x in range(shape0):\n        for y in range(shape1):\n            area = result[x * mcut : (x + 1) * mcut, y * mcut : (y + 1) * mcut]\n            score = (area < threshold).astype(int)\n            diff[x][y] = score.sum() / score.size\n    return {\n        \"blackout\": diff,\n        \"blocksize\": mcut,\n    }  # required for recovering center points.\ndef blackoutDetector(mediapaths, cut=3, threshold=30, method=\"average\", timestep=0.2):\n    # any better detectors? deeplearning?\n    results = []\n    data_key = \"blackout_score\"",
        "type": "code",
        "location": "/pyjom/medialang/functions/detectors/blackoutDetector.py:1-30"
    },
    "1541": {
        "file_id": 123,
        "content": "blackoutIdentifier function: Calculates blackout score per block of a frame using average, max, or min method; accepts cut, threshold, and method parameters.\n\nblackoutDetector function: Detects blackouts across multiple media paths by calling blackoutIdentifier; stores results in 'data_key' for further use.",
        "type": "comment"
    },
    "1542": {
        "file_id": 123,
        "content": "    for mediapath in mediapaths:\n        print(\"mediapath:\", mediapath)\n        mediatype = getFileType(mediapath)\n        print(\"subtitle of mediatype:\", mediatype)\n        assert mediatype in [\"video\", \"image\"]  # gif? anything like that?\n        result = {\"type\": mediatype, data_key: {}}\n        config = {\"cut\": cut, \"threshold\": threshold, \"method\": method}\n        if mediatype == \"image\":\n            data = cv2.imread(mediapath)\n            data = keywordDecorator(blackoutIdentifier, **config)(data)\n            result[data_key].update({\"blackout_detector\": data})\n            result[data_key].update({\"config\": config})\n        else:\n            keyword = \"blackout_detector\"\n            mdata, metadata = videoFrameIterator(\n                mediapath,\n                data_producer=keywordDecorator(blackoutIdentifier, **config),\n                framebatch=1,\n                timestep=timestep,\n                keyword=keyword,\n            )\n            metadata.update({\"config\": config})\n            result[data_key][keyword] = mdata",
        "type": "code",
        "location": "/pyjom/medialang/functions/detectors/blackoutDetector.py:31-54"
    },
    "1543": {
        "file_id": 123,
        "content": "This code iterates over mediapaths, detects media type (video or image), applies blackout detection configuration, and stores the result in a dictionary. For images, it uses OpenCV to read image data and apply keyword decorator for blackout identification. For videos, it uses videoFrameIterator with keyword decorator as data producer for frame-by-frame blackout detection.",
        "type": "comment"
    },
    "1544": {
        "file_id": 123,
        "content": "            result[data_key].update(metadata)\n        results.append(result)\n    return results",
        "type": "code",
        "location": "/pyjom/medialang/functions/detectors/blackoutDetector.py:55-57"
    },
    "1545": {
        "file_id": 123,
        "content": "This code block takes a metadata dictionary and updates it to an existing data key in the result dictionary. After updating, it appends the updated result dictionary to the results list and returns the final list of results.",
        "type": "comment"
    },
    "1546": {
        "file_id": 124,
        "content": "/pyjom/medialang/functions/detectors/__init__.py",
        "type": "filepath"
    },
    "1547": {
        "file_id": 124,
        "content": "This code imports various detector functions, defines a medialang input function, and creates a dictionary of detectors for processing media data. It handles potential problematic inputs.",
        "type": "summary"
    },
    "1548": {
        "file_id": 124,
        "content": "# from pyjom.medialang.functions.detectors.mediaDetector import *\nfrom .blackoutDetector import *\nfrom .subtitleDetector import *\nfrom .videoDiffDetector import *\nfrom .yolov5_Detector import *\nfrom .frameborder_Detector import *\n# maybe these shits are gonna ruin my life...\ndef getMedialangInputFixed(medialangPathsInput):\n    for fbase0 in medialangPathsInput:\n        if type(fbase0) == str:\n            yield fbase0\n        elif (\n            type(fbase0) == list\n            and len(fbase0) == 1\n            and type(fbase0[0] == dict)\n            and \"cache\" in fbase0[0].keys()\n        ):\n            yield fbase0[0][\"cache\"]\n        else:\n            print(\"weird medialang detector input\")\n            print(fbase0)\n        # then it must be the medialang shit.\ndef processInputWrapperFunction(function, wrapperFunction):\n    def mFunction(data, *args, **kwargs):\n        return function(wrapperFunction(data), *args, **kwargs)\n    return mFunction\nmedialangDetectors = {\n    \"subtitle_detector\": mediaSubtitleDetector,",
        "type": "code",
        "location": "/pyjom/medialang/functions/detectors/__init__.py:1-36"
    },
    "1549": {
        "file_id": 124,
        "content": "This code imports various detector functions, defines a function for getting medialang input, and creates a dictionary of detectors. It seems to be part of a larger program used for processing media data. The comment on line 30 suggests that the code is concerned with potentially difficult or problematic inputs.",
        "type": "comment"
    },
    "1550": {
        "file_id": 124,
        "content": "    \"framediff_detector\": videoDiffDetector,\n    \"blackout_detector\": blackoutDetector,\n    \"yolov5_detector\": yolov5_Detector,\n    \"frameborder_detector\": frameborder_Detector,\n}\nmedialangDetectors = { # strange. i don't feel it.\n    key: processInputWrapperFunction(medialangDetectors[key], getMedialangInputFixed)\n    for key in medialangDetectors.keys()\n}",
        "type": "code",
        "location": "/pyjom/medialang/functions/detectors/__init__.py:37-46"
    },
    "1551": {
        "file_id": 124,
        "content": "This code initializes and processes media language detectors with input wrappers. It maps detector names to corresponding functions and applies a processing function to each detector for handling inputs.",
        "type": "comment"
    },
    "1552": {
        "file_id": 125,
        "content": "/pyjom/templates/medialang/autoCensor/yolov5_detector.mdl.j2",
        "type": "filepath"
    },
    "1553": {
        "file_id": 125,
        "content": "This code is creating a tuple containing a JSON file with optional parameters for timestep, threshold and model name, along with the media file path.",
        "type": "summary"
    },
    "1554": {
        "file_id": 125,
        "content": "(\".json\", processor=\"yolov5_detector\"{% if timestep %}, timestep={{timestep}}{% endif %} {% if threshold %}, threshold={{threshold}}{% endif %} {% if model %}, model=\"{{model}}\"{% endif %}\n)\n(\"{{ mediafile }}\")",
        "type": "code",
        "location": "/pyjom/templates/medialang/autoCensor/yolov5_detector.mdl.j2:1-4"
    },
    "1555": {
        "file_id": 125,
        "content": "This code is creating a tuple containing a JSON file with optional parameters for timestep, threshold and model name, along with the media file path.",
        "type": "comment"
    },
    "1556": {
        "file_id": 126,
        "content": "/pyjom/templates/medialang/autoCensor/subtitle_detector.mdl.j2",
        "type": "filepath"
    },
    "1557": {
        "file_id": 126,
        "content": "The code specifies a template for the \"subtitle_detector\" processor. If timestep is provided, it includes it in the output. It combines a JSON string with the media file path.",
        "type": "summary"
    },
    "1558": {
        "file_id": 126,
        "content": "(\".json\", processor=\"subtitle_detector\"{% if timestep %}, timestep={{timestep}}{% endif %}\n)\n(\"{{ mediafile }}\")",
        "type": "code",
        "location": "/pyjom/templates/medialang/autoCensor/subtitle_detector.mdl.j2:1-4"
    },
    "1559": {
        "file_id": 126,
        "content": "The code specifies a template for the \"subtitle_detector\" processor. If timestep is provided, it includes it in the output. It combines a JSON string with the media file path.",
        "type": "comment"
    },
    "1560": {
        "file_id": 127,
        "content": "/pyjom/templates/medialang/autoCensor/framediff_detector.mdl.j2",
        "type": "filepath"
    },
    "1561": {
        "file_id": 127,
        "content": "The code defines a template for the \"framediff_detector\" processor, which takes an optional \"timestep\" parameter and a media file as input. It may check moving areas, total movements, and local movements, differentiating it from frame border detector.",
        "type": "summary"
    },
    "1562": {
        "file_id": 127,
        "content": "(\".json\", processor=\"framediff_detector\"{% if timestep %}, timestep={{timestep}}{% endif %} \n) # different from frame border detector. may check moving areas, check total movements, local movements.\n(\"{{ mediafile }}\")",
        "type": "code",
        "location": "/pyjom/templates/medialang/autoCensor/framediff_detector.mdl.j2:1-4"
    },
    "1563": {
        "file_id": 127,
        "content": "The code defines a template for the \"framediff_detector\" processor, which takes an optional \"timestep\" parameter and a media file as input. It may check moving areas, total movements, and local movements, differentiating it from frame border detector.",
        "type": "comment"
    },
    "1564": {
        "file_id": 128,
        "content": "/pyjom/templates/medialang/autoCensor/frameborder_detector.mdl.j2",
        "type": "filepath"
    },
    "1565": {
        "file_id": 128,
        "content": "This code defines a template for frameborder_detector, which takes in model and config as parameters. By default, the model is framedifference_talib, but it can be configured to be huffline_horizontal_vertical. The output includes main frame time and location, possibly from a moving object.",
        "type": "summary"
    },
    "1566": {
        "file_id": 128,
        "content": "(\".json\", processor=\"frameborder_detector\" {% if model %}, model=\"{{model}}\"{% endif %} {% if config %}, config={{config}}{% endif %}\n) # model default to be framedifference_talib, or config it to be huffline_horizontal_vertical, only output main frame time and location. might be moving. i don't know. maybe we need to eliminate all moving things.\n(\"{{ mediafile }}\")",
        "type": "code",
        "location": "/pyjom/templates/medialang/autoCensor/frameborder_detector.mdl.j2:1-4"
    },
    "1567": {
        "file_id": 128,
        "content": "This code defines a template for frameborder_detector, which takes in model and config as parameters. By default, the model is framedifference_talib, but it can be configured to be huffline_horizontal_vertical. The output includes main frame time and location, possibly from a moving object.",
        "type": "comment"
    },
    "1568": {
        "file_id": 129,
        "content": "/pyjom/templates/medialang/autoCensor/file_format_detector.mdl.j2",
        "type": "filepath"
    },
    "1569": {
        "file_id": 129,
        "content": "This code snippet defines a tuple containing the file format \"json\" and specifies the processor \"file_format_detector\". The mediafile variable is used within double curly braces to represent the file's content.",
        "type": "summary"
    },
    "1570": {
        "file_id": 129,
        "content": "(\".json\", processor=\"file_format_detector\"\n)\n(\"{{ mediafile }}\")",
        "type": "code",
        "location": "/pyjom/templates/medialang/autoCensor/file_format_detector.mdl.j2:1-4"
    },
    "1571": {
        "file_id": 129,
        "content": "This code snippet defines a tuple containing the file format \"json\" and specifies the processor \"file_format_detector\". The mediafile variable is used within double curly braces to represent the file's content.",
        "type": "comment"
    },
    "1572": {
        "file_id": 130,
        "content": "/pyjom/templates/medialang/autoCensor/blackout_detector.mdl.j2",
        "type": "filepath"
    },
    "1573": {
        "file_id": 130,
        "content": "The code represents a template for the blackout_detector.mdl.j2 file in the pyjom project. It defines a processor called \"blackout_detector\" and includes optional timestep variable, as well as the mediafile path.",
        "type": "summary"
    },
    "1574": {
        "file_id": 130,
        "content": "(\".json\", processor=\"blackout_detector\"{% if timestep %}, timestep={{timestep}}{% endif %}\n)\n(\"{{ mediafile }}\")",
        "type": "code",
        "location": "/pyjom/templates/medialang/autoCensor/blackout_detector.mdl.j2:1-4"
    },
    "1575": {
        "file_id": 130,
        "content": "The code represents a template for the blackout_detector.mdl.j2 file in the pyjom project. It defines a processor called \"blackout_detector\" and includes optional timestep variable, as well as the mediafile path.",
        "type": "comment"
    },
    "1576": {
        "file_id": 131,
        "content": "/symlinks/README.md",
        "type": "filepath"
    },
    "1577": {
        "file_id": 131,
        "content": "This folder holds essential symlinks to crucial resources, ensuring efficient organization and access.",
        "type": "summary"
    },
    "1578": {
        "file_id": 131,
        "content": "# this folder contains some important symlinks to my resources.",
        "type": "code",
        "location": "/symlinks/README.md:1-1"
    },
    "1579": {
        "file_id": 131,
        "content": "This folder holds essential symlinks to crucial resources, ensuring efficient organization and access.",
        "type": "comment"
    },
    "1580": {
        "file_id": 132,
        "content": "/externals/torchrec_init.sh",
        "type": "filepath"
    },
    "1581": {
        "file_id": 132,
        "content": "This script clones torchrec, modifies setup.py, installs torchrec, and fixes paths for installation in third_party/fbgemm/fbgemm_gpu directory. It also exports CUB_DIR, CUDA_BIN_PATH, and CUDACXX environment variables before the final installation.",
        "type": "summary"
    },
    "1582": {
        "file_id": 132,
        "content": "# git clone --depth 1 --recurse-submodules https://github.com/pytorch/torchrec\ncd torchrec\n# modify the freaking setup.py first. don't want no trouble.\n# python3 setup.py install\nexport CUB_DIR=/usr/include/cub\nexport CUDA_BIN_PATH=/usr/lib/nvidia-cuda-toolkit\nexport CUDACXX=/usr/bin/nvcc\ncp -R /usr/local/lib/python3.9/dist-packages/torch/include/* third_party/fbgemm/fbgemm_gpu/include # great shit.\npython3 setup.py install \n# the freaking fix.\n# cd third_party/fbgemm/fbgemm_gpu\n# cp -R /usr/local/lib/python3.9/dist-packages/torch/include/* ./include # great shit.\n# export CUB_DIR=/usr/include/cub\n# export CUDA_BIN_PATH=/usr/lib/nvidia-cuda-toolkit\n# export CUDACXX=/usr/bin/nvcc\n# python3 setup.py install ",
        "type": "code",
        "location": "/externals/torchrec_init.sh:1-17"
    },
    "1583": {
        "file_id": 132,
        "content": "This script clones torchrec, modifies setup.py, installs torchrec, and fixes paths for installation in third_party/fbgemm/fbgemm_gpu directory. It also exports CUB_DIR, CUDA_BIN_PATH, and CUDACXX environment variables before the final installation.",
        "type": "comment"
    },
    "1584": {
        "file_id": 133,
        "content": "/externals/three_init.sh",
        "type": "filepath"
    },
    "1585": {
        "file_id": 133,
        "content": "This command clones the Three.js library from GitHub, ensuring only the latest commit is downloaded for potentially faster download times.",
        "type": "summary"
    },
    "1586": {
        "file_id": 133,
        "content": "git clone --depth 1 https://github.com/mrdoob/three.js # this might be faster than shit.",
        "type": "code",
        "location": "/externals/three_init.sh:1-1"
    },
    "1587": {
        "file_id": 133,
        "content": "This command clones the Three.js library from GitHub, ensuring only the latest commit is downloaded for potentially faster download times.",
        "type": "comment"
    },
    "1588": {
        "file_id": 134,
        "content": "/externals/test_import_opencv_site.py",
        "type": "filepath"
    },
    "1589": {
        "file_id": 134,
        "content": "This code imports necessary libraries and checks for the location of OpenCV libraries. It then inserts that location into sys.path if there is only one library found, and finally imports the OpenCV library (cv2). The code then prints out the available functions and methods within cv2 using dir(cv2).",
        "type": "summary"
    },
    "1590": {
        "file_id": 134,
        "content": "import pathlib\nimport site\nimport sys\n# this is root. this is not site-packages.\n# site_path = pathlib.Path([x for x in site.getsitepackages() if \"site-packages\" in x][0])\nsite_path = pathlib.Path(\"/usr/local/lib/python3.9/site-packages\")\ncv2_libs_dir = site_path / 'cv2' / f'python-{sys.version_info.major}.{sys.version_info.minor}'\nprint(cv2_libs_dir)\ncv2_libs = sorted(cv2_libs_dir.glob(\"*.so\"))\nif len(cv2_libs) == 1:\n    print(\"INSERTING:\",cv2_libs[0].parent)\n    sys.path.insert(1, str(cv2_libs[0].parent))\nimport cv2\nprint(dir(cv2)) # shit?",
        "type": "code",
        "location": "/externals/test_import_opencv_site.py:1-18"
    },
    "1591": {
        "file_id": 134,
        "content": "This code imports necessary libraries and checks for the location of OpenCV libraries. It then inserts that location into sys.path if there is only one library found, and finally imports the OpenCV library (cv2). The code then prints out the available functions and methods within cv2 using dir(cv2).",
        "type": "comment"
    },
    "1592": {
        "file_id": 135,
        "content": "/externals/pybgs_init.sh",
        "type": "filepath"
    },
    "1593": {
        "file_id": 135,
        "content": "This code is installing the \"pybgs\" package using pip, after setting the OpenCV_DIR environment variable. It then performs a build and installation of the package in the \"bgslibrary\" directory. The PATH variable is updated to include relevant directories for proper execution.",
        "type": "summary"
    },
    "1594": {
        "file_id": 135,
        "content": "# opencvdir=\"\"\n# env OpenCV_DIR= pip3 install pybgs\nexport PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/local/games:/usr/games:/usr/local/cuda-10.2/bin:/snap/bin\ncd bgslibrary\npython3 setup.py build\npython3 setup.py install",
        "type": "code",
        "location": "/externals/pybgs_init.sh:1-6"
    },
    "1595": {
        "file_id": 135,
        "content": "This code is installing the \"pybgs\" package using pip, after setting the OpenCV_DIR environment variable. It then performs a build and installation of the package in the \"bgslibrary\" directory. The PATH variable is updated to include relevant directories for proper execution.",
        "type": "comment"
    },
    "1596": {
        "file_id": 136,
        "content": "/externals/opencv_py_reset.sh",
        "type": "filepath"
    },
    "1597": {
        "file_id": 136,
        "content": "The code uninstalls opencv packages, installs opencv-python, and adds a .so file for CUDA function. It updates installation options with directories, compilation settings, and OpenCV Python reset script choices.",
        "type": "summary"
    },
    "1598": {
        "file_id": 136,
        "content": "cd opencv/build/python_loader\nyes | pip3 uninstall opencv\nyes | pip3 uninstall opencv-contrib\nyes | pip3 uninstall opencv-python\nyes | pip3 uninstall opencv-contrib-python\nyes | pip3 install opencv-python # will that break shit?\npython3 setup.py install\n# it is all about that .so file.\n# you would like to say that you have installed the thing.\n# place a link of .so or something, to use the cuda function.\ncp /usr/local/lib/python3.9/site-packages/cv2/python-3.9/cv2.cpython-39-x86_64-linux-gnu.so /usr/local/lib/python3.9/dist-packages/\n# pip3 install .\n# python3 setup.py install --install-lib /usr/lib/python3/dist-packages/\n# # python3 setup.py install --root /usr/lib/python3/dist-packages/\n# Options for 'install' command:\n#   --prefix                             installation prefix\n#   --exec-prefix                        (Unix only) prefix for platform-\n#                                        specific files\n#   --home                               (Unix only) home directory to install\n#                                        under",
        "type": "code",
        "location": "/externals/opencv_py_reset.sh:1-24"
    },
    "1599": {
        "file_id": 136,
        "content": "Code uninstalls opencv packages, installs opencv-python, and places a .so file to use CUDA function. It updates installation prefix, exec-prefix, and home directory options for 'install' command.",
        "type": "comment"
    }
}