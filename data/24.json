{
    "2400": {
        "file_id": 253,
        "content": "/tests/bilibili_practices/bilibili_video_translate/functional_redraw_chinese_text_offline.py",
        "type": "filepath"
    },
    "2401": {
        "file_id": 253,
        "content": "The code uses PaddleOCR for OCR, focusing on English text detection and supporting multiple languages. It downloads the language model once at runtime and utilizes the `redraw_english_to_chinese` function to translate and display results over images. The developer's custom CUDA libraries may cause potential CUDA errors with OpenCV.",
        "type": "summary"
    },
    "2402": {
        "file_id": 253,
        "content": "from paddleocr import PaddleOCR\nimport wordninja\n# from m2m100_1b_translator import zh_to_en_translator as translator\n# cannot translate everything... not frame by frame...\n# can summarize things. can block texts on location.\n# Paddleocr supports Chinese, English, French, German, Korean and Japanese.\n# You can set the parameter `lang` as `ch`, `en`, `french`, `german`, `korean`, `japan`\n# to switch the language model in order.\n# img_path = 'target.png' # only detect english. or not?\nocr = PaddleOCR(use_angle_cls=True, lang='en') # need to run only once to download and load model into memory\n# image = cv2.imread(img_path)\n# we will give it to you...\n# internalFrameCounter = 0\n# resultChineseInternal = []\ndef redraw_english_to_chinese(image): # whatever. it is dumb anyway. we need to be prudent. really?\n    # global resultChineseInternal, internalFrameCounter # we need to look ahead.\n    resultChineseInternal2 = ocr.ocr(image, cls=True) # you will be fucked if skip frames.\n    prob_thresh = 0.6 # found watermark somewhere. scorpa",
        "type": "code",
        "location": "/tests/bilibili_practices/bilibili_video_translate/functional_redraw_chinese_text_offline.py:1-19"
    },
    "2403": {
        "file_id": 253,
        "content": "This code uses PaddleOCR to perform optical character recognition (OCR) on an image, specifically detecting and translating English text within it. It downloads and loads the language model only once during runtime. The code is designed to handle Chinese and other languages as well, but currently focuses on English detection. The function `redraw_english_to_chinese` takes an image as input, uses OCR to identify text, and returns the translated text results.",
        "type": "comment"
    },
    "2404": {
        "file_id": 253,
        "content": "    resultChineseInternal = []\n    for index, line in enumerate(resultChineseInternal2):\n        # print(line)\n        # breakpoint()\n        coords, (text, prob) = line\n        prob = float(prob)\n        if prob > prob_thresh:\n            rectified_text = \" \".join(wordninja.split(text))\n            line[1] = (rectified_text, prob)\n            print(line)\n            resultChineseInternal.append(line)\n    return resultChineseInternal\n# cv2.imshow('dst',dst2)\n# cv2.waitKey(0)\n# cv2.destroyAllWindows()\n# expand the area somehow.\n# draw resultChineseInternal\n# simhei_path = \"/root/Desktop/works/bilibili_tarot/SimHei.ttf\"\n# from PIL import Image\n# image = Image.open(img_path).convert('RGB')\n# boxes = [line[0] for line in resultChineseInternal]\n# txts = [line[1][0] for line in resultChineseInternal]\n# scores = [line[1][1] for line in resultChineseInternal]\n# im_show = draw_ocr(image, boxes, txts, scores, font_path=simhei_path)\n# im_show = Image.fromarray(im_show)\n# im_show.save('resultChineseInternal.jpg')\n# we will be testing one image only. not the whole goddamn video.",
        "type": "code",
        "location": "/tests/bilibili_practices/bilibili_video_translate/functional_redraw_chinese_text_offline.py:20-50"
    },
    "2405": {
        "file_id": 253,
        "content": "Iterates over resultChineseInternal2, rectifies text using wordninja if prob > prob_thresh. Stores modified lines in resultChineseInternal. Displays image with OCR results using draw_ocr function and saves it as \"resultChineseInternal.jpg\".",
        "type": "comment"
    },
    "2406": {
        "file_id": 253,
        "content": "# may have cuda error when using my cv2 cuda libs.",
        "type": "code",
        "location": "/tests/bilibili_practices/bilibili_video_translate/functional_redraw_chinese_text_offline.py:51-51"
    },
    "2407": {
        "file_id": 253,
        "content": "This code seems to be a comment expressing a potential issue when using the developer's custom CUDA libraries with OpenCV. It mentions that it might throw a CUDA error in those specific cases, possibly requiring attention or alternative solutions.",
        "type": "comment"
    },
    "2408": {
        "file_id": 254,
        "content": "/tests/bilibili_practices/bilibili_video_translate/functional_dl_translator_1b_deepspeed.py",
        "type": "filepath"
    },
    "2409": {
        "file_id": 254,
        "content": "This code utilizes Deepspeed ZeRO for model inference, translation using M2M100, and offers GPU/CPU options. It sets up a distributed environment with DeepSpeed for training, initializes the model, provides mixed precision training options on Ampere or higher GPUs, prepares a machine translation environment using Deepspeed ZeRO, partitions the model, creates an engine object for parallel processing, and measures time cost per iteration in translating Chinese to English using experimentation with varying parameters.",
        "type": "summary"
    },
    "2410": {
        "file_id": 254,
        "content": "# loadable or not?\n# OOM ready?\n# maybe you want to load this shit over kaggle.\n# 3521MB on inference. does that mean you can do the big fucker now?\n# 1.9G model size.\nimport os\n# mt = dlt.TranslationModel(modelpath, model_family=\"m2m100\",device=\"gpu\") # OOM?\nfrom transformers import M2M100Tokenizer, M2M100ForConditionalGeneration\n# model = M2M100ForConditionalGeneration.from_pretrained(modelpath)\n# model_inputs = tokenizer(text_to_translate, return_tensors=\"pt\")\n# translate to French\n# gen_tokens = model.generate(**model_inputs, forced_bos_token_id=tokenizer.get_lang_id(\"en\"))\n# print(tokenizer.batch_decode(gen_tokens, skip_special_tokens=True))\n# either load model with trainer or just use some other stuffs.\n#!/usr/bin/env python\n# This script demonstrates how to use Deepspeed ZeRO in an inference mode when one can't fit a model\n# into a single GPU\n#\n# 1. Use 1 GPU with CPU offload\n# 2. Or use multiple GPUs instead\n#\n# First you need to install deepspeed: pip install deepspeed\n#\n# Here we use a 3B \"bigscience/T0_3B\" model which needs about 15GB GPU RAM - so 1 largish or 2",
        "type": "code",
        "location": "/tests/bilibili_practices/bilibili_video_translate/functional_dl_translator_1b_deepspeed.py:1-33"
    },
    "2411": {
        "file_id": 254,
        "content": "This code demonstrates how to use Deepspeed ZeRO for inference when the model size exceeds available GPU RAM. It loads a 1.9GB model and translates text from English to French using the M2M100 tokenizer and model. The code provides two options: using one GPU with CPU offload or multiple GPUs, and requires installing Deepspeed before running.",
        "type": "comment"
    },
    "2412": {
        "file_id": 254,
        "content": "# small GPUs can handle it. or 1 small GPU and a lot of CPU memory.\n#\n# To use a larger model like \"bigscience/T0\" which needs about 50GB, unless you have an 80GB GPU -\n# you will need 2-4 gpus. And then you can adapt the script to handle more gpus if you want to\n# process multiple inputs at once.\n#\n# The provided deepspeed config also activates CPU memory offloading, so chances are that if you\n# have a lot of available CPU memory and you don't mind a slowdown you should be able to load a\n# model that doesn't normally fit into a single GPU. If you have enough GPU memory the program will\n# run faster if you don't want offload to CPU - so disable that section then.\n#\n# To deploy on 1 gpu:\n#\n# deepspeed --num_gpus 1 t0.py\n# or:\n# python -m torch.distributed.run --nproc_per_node=1 t0.py\n#\n# To deploy on 2 gpus:\n#\n# deepspeed --num_gpus 2 t0.py\n# or:\n# python -m torch.distributed.run --nproc_per_node=2 t0.py\nfrom transformers import AutoConfig\nfrom transformers.deepspeed import HfDeepSpeedConfig\nimport deepspeed\nimport os",
        "type": "code",
        "location": "/tests/bilibili_practices/bilibili_video_translate/functional_dl_translator_1b_deepspeed.py:34-61"
    },
    "2413": {
        "file_id": 254,
        "content": "The code snippet describes how to deploy a larger transformer model like \"bigscience/T0\" on a GPU or multiple GPUs using the DeepSpeed library. The code provides instructions for running the program on 1 or 2 GPUs, and mentions the benefits of CPU memory offloading if sufficient CPU memory is available. The code also imports necessary libraries and configurations.",
        "type": "comment"
    },
    "2414": {
        "file_id": 254,
        "content": "import torch\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # To avoid warnings about parallelism in tokenizers\n# distributed setup\nlocal_rank = int(os.getenv(\"LOCAL_RANK\", \"0\"))\nworld_size = int(os.getenv(\"WORLD_SIZE\", \"1\"))\ntorch.cuda.set_device(local_rank)\ndeepspeed.init_distributed()\n# model_name = \"bigscience/T0_3B\"\nmodelpath = \"/media/root/Jumpcut/person_segmentation/paraphraser/m2m100_1.2B\"\nconfig = AutoConfig.from_pretrained(modelpath)\nmodel_hidden_size = config.d_model\n# batch size has to be divisible by world_size, but can be bigger than world_size\ntrain_batch_size = 1 * world_size\n# ds_config notes\n#\n# - enable bf16 if you use Ampere or higher GPU - this will run in mixed precision and will be\n# faster.\n#\n# - for older GPUs you can enable fp16, but it'll only work for non-bf16 pretrained models - e.g.\n# all official t5 models are bf16-pretrained\n#\n# - set offload_param.device to \"none\" or completely remove the `offload_param` section if you don't\n# - want CPU offload\n#\n# - if using `offload_param` you can manually finetune stage3_param_persistence_threshold to control",
        "type": "code",
        "location": "/tests/bilibili_practices/bilibili_video_translate/functional_dl_translator_1b_deepspeed.py:62-92"
    },
    "2415": {
        "file_id": 254,
        "content": "This code sets up a distributed environment with DeepSpeed, initializes the model using the specified path, defines the batch size divisible by world_size, and provides configuration options for mixed precision training on Ampere or higher GPUs.",
        "type": "comment"
    },
    "2416": {
        "file_id": 254,
        "content": "# - which params should remain on gpus - the larger the value the smaller the offload size\n#\n# For indepth info on Deepspeed config see\n# https://huggingface.co/docs/transformers/main/main_classes/deepspeed\n# keeping the same format as json for consistency, except it uses lower case for true/false\n# fmt: off\nds_config = {\n    \"fp16\": {\n        \"enabled\": True # to half the model precision.\n    },\n    \"zero_optimization\": {\n        \"stage\": 3,\n        \"offload_param\": {\n            \"device\": \"cpu\",\n            \"pin_memory\": True\n        },\n        \"overlap_comm\": True,\n        \"contiguous_gradients\": True,\n        \"reduce_bucket_size\": model_hidden_size * model_hidden_size,\n        \"stage3_prefetch_bucket_size\": 0.9 * model_hidden_size * model_hidden_size,\n        \"stage3_param_persistence_threshold\": 10 * model_hidden_size\n    },\n    \"steps_per_print\": 2000,\n    \"train_batch_size\": train_batch_size,\n    \"train_micro_batch_size_per_gpu\": 1,\n    \"wall_clock_breakdown\": False\n}\n# fmt: on\n# next line instructs transformers to partition the model directly over multiple gpus using",
        "type": "code",
        "location": "/tests/bilibili_practices/bilibili_video_translate/functional_dl_translator_1b_deepspeed.py:93-123"
    },
    "2417": {
        "file_id": 254,
        "content": "This code snippet is initializing a Deepspeed configuration for model training with specific settings. The configuration includes enabling FP16 (half precision) for the model, setting zero optimization stage to 3, and configuring offload parameters such as device and pin memory. Additionally, it specifies steps per print, train batch size, train micro-batch size per GPU, and whether to display wall clock breakdown. This configuration aims to optimize model training on multiple GPUs efficiently.",
        "type": "comment"
    },
    "2418": {
        "file_id": 254,
        "content": "# deepspeed.zero.Init when model's `from_pretrained` method is called.\n#\n# **it has to be run before loading the model AutoModelForSeq2SeqLM.from_pretrained(model_name)**\n#\n# otherwise the model will first be loaded normally and only partitioned at forward time which is\n# less efficient and when there is little CPU RAM may fail\ndschf = HfDeepSpeedConfig(ds_config)  # keep this object alive\nos.environ[\"http_proxy\"] = \"\"\nos.environ[\"https_proxy\"] = \"\"\n# now a model can be loaded.\n# model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\nmodel = M2M100ForConditionalGeneration.from_pretrained(modelpath)\n# this will not fuck shit up.\n# initialise Deepspeed ZeRO and store only the engine object\nds_engine = deepspeed.initialize(model=model, config_params=ds_config)[0]\nds_engine.module.eval()  # inference\n# Deepspeed ZeRO can process unrelated inputs on each GPU. So for 2 gpus you process 2 inputs at once.\n# If you use more GPUs adjust for more.\n# And of course if you have just one input to process you then need to pass the same string to both gpus",
        "type": "code",
        "location": "/tests/bilibili_practices/bilibili_video_translate/functional_dl_translator_1b_deepspeed.py:124-146"
    },
    "2419": {
        "file_id": 254,
        "content": "The code initializes Deepspeed ZeRO before loading the model and sets the engine object for parallel processing. This ensures efficient usage of resources by partitioning the model at initialization rather than during forward pass, and allows handling multiple inputs on each GPU if available.",
        "type": "comment"
    },
    "2420": {
        "file_id": 254,
        "content": "# # If you use only one GPU, then you will have only rank 0.\n# rank = torch.distributed.get_rank()\n# if rank == 0:\n#     text_in = \"Is this review positive or negative? Review: this is the best cast iron skillet you will ever buy\"\n# elif rank == 1:\n#     text_in = \"Is this review positive or negative? Review: this is the worst restaurant ever\"\n# tokenizer = AutoTokenizer.from_pretrained(model_name)\n# sentence = \"你吃饭了没有\" # You have eaten. from m2m100 418M\ntokenizer = M2M100Tokenizer.from_pretrained(modelpath,src_lang=\"en\",tgt_lang=\"zh\")\n# source = tokenizer.get_lang_id(\"zh\")\n# tokenizer.src_lang = source\nmdevice = torch.device(\"cuda\")\n# tokenizer.to(mdevice)\n# inputs = tokenizer.encode(text_in, return_tensors=\"pt\").to(device=local_rank)\ndef get_response(sentence):\n    text_to_translate =sentence\n    model_inputs = tokenizer(text_to_translate, return_tensors=\"pt\")\n    # inputs = model.generate(**model_inputs, forced_bos_token_id=tokenizer.get_lang_id(\"en\"))\n    model_inputs = {k:model_inputs[k].to(mdevice) for k in model_inputs.keys()}",
        "type": "code",
        "location": "/tests/bilibili_practices/bilibili_video_translate/functional_dl_translator_1b_deepspeed.py:147-174"
    },
    "2421": {
        "file_id": 254,
        "content": "This code is setting up the environment for a machine translation task using the M2M100 model. It assigns GPU ranks to different tasks, initializes the tokenizer, and defines a function called \"get_response\" that takes a sentence as input, tokenizes it, prepares inputs for the model, and performs translation. The code is specifically tailored for a CUDA device, meaning it will utilize GPUs for processing.",
        "type": "comment"
    },
    "2422": {
        "file_id": 254,
        "content": "    with torch.no_grad():\n        # outputs = ds_engine.module.generate(inputs, synced_gpus=True)\n        while True:\n            try:\n                gen_tokens = ds_engine.module.generate(**model_inputs, forced_bos_token_id=tokenizer.get_lang_id(\"zh\"),synced_gpus=True,do_sample=True).cpu() # whatever. no too heavy lifting.\n                # gen_tokens = ds_engine.module.generate(**model_inputs, forced_bos_token_id=tokenizer.get_lang_id(\"zh\"),synced_gpus=True,do_sample=True,top_k=0,num_beams=8,num_return_sequences=1,no_repeat_ngram_size=2,temperature=1.4).cpu() # whatever.\n                # gen_tokens = ds_engine.module.generate(**model_inputs, forced_bos_token_id=tokenizer.get_lang_id(\"zh\"),synced_gpus=True,do_sample=True,top_k=0,top_p=0.92,num_beams=5,num_return_sequences=5,no_repeat_ngram_size=2,temperature=0.7).cpu() # whatever.\n                break\n            except:\n                import traceback\n                traceback.print_exc()\n                breakpoint() # translate speed is slow as hell. must do some summarization. or you cover them all.",
        "type": "code",
        "location": "/tests/bilibili_practices/bilibili_video_translate/functional_dl_translator_1b_deepspeed.py:176-187"
    },
    "2423": {
        "file_id": 254,
        "content": "This code uses deepspeed engine to generate translated text using a model. It employs different generate() calls with varying parameters (top_k, top_p, num_beams) in a while loop, likely for experimentation purposes. The loop continues until a successful generation is achieved without any exceptions or until a breakpoint is hit, and it handles exceptions by printing the traceback and breaking out of the loop.",
        "type": "comment"
    },
    "2424": {
        "file_id": 254,
        "content": "                # you may do this for pictures.\n    # text_out = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    print(\"TRANSLATED:\")\n    return tokenizer.batch_decode(gen_tokens, skip_special_tokens=True)\n# print(get_response(\"你吃饭了没有\"))\n# print(\"PROMPT READY.\")\n# print(\"type exit to exit.\")\n# while True:\n#     targetSentence = input(\"\\nprompt>\")\n#     if \"exit\" not in targetSentence:\n#         result = get_response(targetSentence)\n#         print(result) # this is goddamly working. fuck!\n#     else:\n#         break\n# import time\n# values = []\n# for _ in range(3):\n#     a = time.time()\n#     translate_once()\n#     b = time.time()\n#     value = b-a\n#     # value = timeit.timeit(stmt=\"translate_once()\")\n#     print(\"TIME COST: {}\".format(value))\n#     values.append(value)\n# print(\"TOTAL COST:\",values)\n# print(\"AVERAGE COST:\",sum(values)/len(values))\n# stuck at the end.\n# TOTAL COST: [6.2853310108184814, 4.705244541168213, 4.688654661178589]\n# AVERAGE COST: 5.226410071055095\n# better not to use swap.",
        "type": "code",
        "location": "/tests/bilibili_practices/bilibili_video_translate/functional_dl_translator_1b_deepspeed.py:188-220"
    },
    "2425": {
        "file_id": 254,
        "content": "This code is a functional implementation of a DL translator, likely for Chinese to English translation. It takes user input in the form of text and returns the translated output. The code includes batch decoding of tokenizer outputs and handles user input within a while loop. It also measures the time cost and average cost per iteration of the function.",
        "type": "comment"
    },
    "2426": {
        "file_id": 255,
        "content": "/tests/bilibili_practices/bilibili_video_translate/frame_translate_processor3.py",
        "type": "filepath"
    },
    "2427": {
        "file_id": 255,
        "content": "The code uses video processing libraries to read a source video, apply Chinese translation and color reduction to frames, save the processed frames in a dictionary, write JSON result to file, and finally saves the final video with h.264 codec.",
        "type": "summary"
    },
    "2428": {
        "file_id": 255,
        "content": "from functional_redraw_chinese_text_offline2 import redraw_english_to_chinese2\nimport cv2\nimport progressbar as pb\nsource_video = \"japan_day.webm\"\noutput_json = \"japan_day.json\"\noutput_video = \"japan_day_change_color3.mp4\"\nimport os\nif os.path.exists(output_video): os.remove(output_video)\n# OOM for local translation!\n# this will not work. fucking shit. though ocr is speedy.\n# in this we will get no audio.\n# use ffmpeg and time strencher.\n# this is ideal for frame by frame processing.\n# oh shit!\n# the task is very long to run, i believe.\nvideo_cap = cv2.VideoCapture(source_video)\nfps = video_cap.get(cv2.CAP_PROP_FPS) # 60.\nframe_width = int(video_cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(video_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nframe_size = (frame_width, frame_height)\nframe_count = int(video_cap.get(cv2.CAP_PROP_FRAME_COUNT))\nfourcc = cv2.VideoWriter_fourcc(*'H264') # h.264 # this will fail.\n# fourcc = cv2.VideoWriter_fourcc('X', 'V', 'I', 'D') # h.264\nvideo_writer = cv2.VideoWriter(output_video,fourcc,fps,frame_size)",
        "type": "code",
        "location": "/tests/bilibili_practices/bilibili_video_translate/frame_translate_processor3.py:1-32"
    },
    "2429": {
        "file_id": 255,
        "content": "This code imports the necessary modules and sets up variables for video processing. It removes any existing output file, initializes a VideoCapture object to read the source video, determines the video's FPS, frame size, and total frames count, and creates a VideoWriter object with h.264 codec for the output video file.",
        "type": "comment"
    },
    "2430": {
        "file_id": 255,
        "content": "# frame_index_counter = 0\n# this is determinism.\n# or you could use framedifference? come on...\n# while True:\nimport json\nmjson_result = open(output_json, 'r',encoding='utf8').read()\nmjson_result = json.loads(mjson_result)\nimport copy\n# use some tweening? pytweening?\n# from test_curve_converter import curve_converter\n    # for index, (orig, target) in enumerate(curve_function):\n    #     if value <= orig:\n    #         forig,ftarget = curve_function[index+1]\n    #         if value == orig: return target\n    #         elif value <=forig:\n    #             if value ==forig: return ftarget\n    #             else:\n    #                 loc = (value-orig)/(forig-orig)\n    #                 new_diff = loc*(ftarget-target)\n    #                 new_value = target+new_diff\n    #                 return new_value\n    # return curve_function[-1][1]\n# def remove_much_red(image,curve_function):\n#     target = copy.copy(image[:,:,2])\n#     target = curve_converter(target,curve_function)\n#     image[:,:,2] = target\n#     return image",
        "type": "code",
        "location": "/tests/bilibili_practices/bilibili_video_translate/frame_translate_processor3.py:34-62"
    },
    "2431": {
        "file_id": 255,
        "content": "Code imports json and copy libraries, reads a JSON file and converts its contents. It defines a function for curve conversion and a potential function for removing excessive red from an image.",
        "type": "comment"
    },
    "2432": {
        "file_id": 255,
        "content": "def remove_much_red_with_rate(image,reduce_rate = 0.8):\n    target = copy.copy(image[:,:,2])\n    target = target*(1-reduce_rate)\n    image[:,:,2] = target\n    return image\n# curve_function = [[0,0],[40,30],[100,50],[150,100],[255,130]]\nfor frame_index_counter in pb.progressbar(range(frame_count)): # are you sure?\n    success, frame = video_cap.read() # let's just use 1, no frame skip.\n    if not success: break\n    # print(\"processing frame\",frame_index_counter)\n    # write the frame to the output file\n    string_frame_index_counter = str(frame_index_counter)  #inpainting is still slow somehow. freaking shit. though i have the freaking shit.\n    # maybe you can improvise.\n    # this is done purely in CPU.\n    processed_frame_data = mjson_result[string_frame_index_counter]# fucking string key.\n    processed_frame = redraw_english_to_chinese2(frame,processed_frame_data) # step 1\n    processed_frame = remove_much_red_with_rate(processed_frame)\n    # mjson_result.update({frame_index_counter:processed_frame_data})",
        "type": "code",
        "location": "/tests/bilibili_practices/bilibili_video_translate/frame_translate_processor3.py:64-83"
    },
    "2433": {
        "file_id": 255,
        "content": "The code reads a video frame by frame and applies two transformations: redrawing English to Chinese (step 1) and reducing the intensity of red color with a certain rate. The progress bar indicates the processing progress, and the processed frames are stored in a dictionary using frame index as the key.",
        "type": "comment"
    },
    "2434": {
        "file_id": 255,
        "content": "    video_writer.write(processed_frame) # what frame?\n    # frame_index_counter+=1\n    # cv2.imshow(\"image\",processed_frame) #\n    # # cv2.waitKey(1) # not wait infinitely.\n    # if cv2.waitKey(20) == ord('q'):\n    #     break\n# with open(output_json,\"w+\",encoding=\"utf-8\") as f:\n#     data = json.dumps(mjson_result,indent=4)\n#     f.write(data)\n# cv2.close\nprint(\"VIDEO DONE. SAVED AT:\",output_video)",
        "type": "code",
        "location": "/tests/bilibili_practices/bilibili_video_translate/frame_translate_processor3.py:84-95"
    },
    "2435": {
        "file_id": 255,
        "content": "This code writes the processed frame to the video, increments the frame index counter, displays the processed frame using OpenCV's imshow function, waits for a 'q' key press to break the loop, and finally writes the JSON result data to the file. The video is then saved at the specified output_video location.",
        "type": "comment"
    },
    "2436": {
        "file_id": 256,
        "content": "/tests/bilibili_practices/bilibili_video_translate/frame_translate_processor2.py",
        "type": "filepath"
    },
    "2437": {
        "file_id": 256,
        "content": "The code translates text, applies color correction, and processes frames from a video using ffmpeg and OpenCV for display and waiting for user input.",
        "type": "summary"
    },
    "2438": {
        "file_id": 256,
        "content": "from functional_redraw_chinese_text_offline2 import redraw_english_to_chinese2\nimport cv2\nimport progressbar as pb\nsource_video = \"japan_day.webm\"\noutput_json = \"japan_day.json\"\noutput_video = \"japan_day_change_color2.mp4\"\nimport os\nif os.path.exists(output_video): os.remove(output_video)\n# OOM for local translation!\n# this will not work. fucking shit. though ocr is speedy.\n# in this we will get no audio.\n# use ffmpeg and time strencher.\n# this is ideal for frame by frame processing.\n# oh shit!\n# the task is very long to run, i believe.\nvideo_cap = cv2.VideoCapture(source_video)\nfps = video_cap.get(cv2.CAP_PROP_FPS) # 60.\nframe_width = int(video_cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(video_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nframe_size = (frame_width, frame_height)\nframe_count = int(video_cap.get(cv2.CAP_PROP_FRAME_COUNT))\n# fourcc = cv2.VideoWriter_fourcc(*'H264') # h.264\n# fourcc = cv2.VideoWriter_fourcc('X', 'V', 'I', 'D') # h.264 \n# this is builtin ffmpeg. not external shits.\n# video_writer = cv2.VideoWriter(output_video,fourcc,fps,frame_size)",
        "type": "code",
        "location": "/tests/bilibili_practices/bilibili_video_translate/frame_translate_processor2.py:1-33"
    },
    "2439": {
        "file_id": 256,
        "content": "The code is preparing to process a video file frame by frame for translation. It imports necessary libraries, checks if the output video exists and deletes it, obtains video properties, and sets up variables for further processing. The code also comments on possible issues and suggests using ffmpeg for audio and video processing.",
        "type": "comment"
    },
    "2440": {
        "file_id": 256,
        "content": "# frame_index_counter = 0\n# this is determinism.\n# or you could use framedifference? come on...\n# while True:\nimport json\nmjson_result = open(output_json, 'r',encoding='utf8').read()\nmjson_result = json.loads(mjson_result)\nimport copy\n# use some tweening? pytweening?\nfrom test_curve_converter import curve_converter\n    # for index, (orig, target) in enumerate(curve_function):\n    #     if value <= orig:\n    #         forig,ftarget = curve_function[index+1]\n    #         if value == orig: return target\n    #         elif value <=forig:\n    #             if value ==forig: return ftarget\n    #             else:\n    #                 loc = (value-orig)/(forig-orig)\n    #                 new_diff = loc*(ftarget-target)\n    #                 new_value = target+new_diff\n    #                 return new_value\n    # return curve_function[-1][1]\ndef remove_much_red(image,curve_function):\n    target = copy.copy(image[:,:,2])\n    target = curve_converter(target,curve_function)\n    image[:,:,2] = target\n    return image\ndef remove_much_red_with_rate(image,reduce_rate = 0.8):",
        "type": "code",
        "location": "/tests/bilibili_practices/bilibili_video_translate/frame_translate_processor2.py:35-65"
    },
    "2441": {
        "file_id": 256,
        "content": "The code defines a function, `remove_much_red`, which takes an image and curve function as parameters. It applies the given curve function to the red channel of the image, effectively reducing the intensity of red colors. The code also includes another function, `remove_much_red_with_rate`, that allows for adjusting the reduction rate of red colors in images. Both functions modify the image by changing the values of its red channel based on a given curve function or reduction rate.",
        "type": "comment"
    },
    "2442": {
        "file_id": 256,
        "content": "    target = copy.copy(image[:,:,2])\n    target = target*(1-reduce_rate)\n    image[:,:,2] = target\n    return image\ncurve_function = [[0,0],[40,30],[100,50],[150,100],[255,130]]\nfor frame_index_counter in pb.progressbar(range(frame_count)): # are you sure?\n    success, frame = video_cap.read() # let's just use 1, no frame skip.\n    if not success: break\n    # print(\"processing frame\",frame_index_counter)\n    # write the frame to the output file\n    string_frame_index_counter = str(frame_index_counter)  #inpainting is still slow somehow. freaking shit. though i have the freaking shit.\n    # maybe you can improvise.\n    # this is done purely in CPU.\n    processed_frame_data = mjson_result[string_frame_index_counter]# fucking string key.\n    processed_frame = redraw_english_to_chinese2(frame,processed_frame_data) # step 1\n    processed_frame = remove_much_red(processed_frame,curve_function)\n    # mjson_result.update({frame_index_counter:processed_frame_data})\n    # video_writer.write(processed_frame) # what frame?",
        "type": "code",
        "location": "/tests/bilibili_practices/bilibili_video_translate/frame_translate_processor2.py:66-85"
    },
    "2443": {
        "file_id": 256,
        "content": "The code reads frames from a video and applies color correction using a curve function. It also translates text on the frames and processes them. The progress bar shows the current frame being processed, and if a frame can't be read, the loop breaks. Finally, the processed frames are saved to an output file.",
        "type": "comment"
    },
    "2444": {
        "file_id": 256,
        "content": "    # frame_index_counter+=1\n    cv2.imshow(\"image\",processed_frame) #\n    # # cv2.waitKey(1) # not wait infinitely.\n    if cv2.waitKey(20) == ord('q'):\n        break\n# with open(output_json,\"w+\",encoding=\"utf-8\") as f:\n#     data = json.dumps(mjson_result,indent=4)\n#     f.write(data)\n# cv2.close\nprint(\"VIDEO DONE. SAVED AT:\",output_video)",
        "type": "code",
        "location": "/tests/bilibili_practices/bilibili_video_translate/frame_translate_processor2.py:87-96"
    },
    "2445": {
        "file_id": 256,
        "content": "This code displays a processed frame on the screen, waits for 20 milliseconds for input to break the loop, and saves the final video. It uses OpenCV to display frames and wait for user input to stop the process.",
        "type": "comment"
    },
    "2446": {
        "file_id": 257,
        "content": "/tests/bilibili_practices/bilibili_video_translate/frame_translate_processor.py",
        "type": "filepath"
    },
    "2447": {
        "file_id": 257,
        "content": "This code reads frames from a video, translates each frame using the redraw_english_to_chinese function, updates a dictionary with frame data, and saves the processed frames to an output JSON file. The process stops when no more frames can be read or if 'q' is pressed.",
        "type": "summary"
    },
    "2448": {
        "file_id": 257,
        "content": "import cv2\nimport progressbar as pb\nsource_video = \"japan_day.webm\"\noutput_json = \"japan_day.json\"\n# output_video = \"japan_day_translated.mp4\"\n# OOM for local translation!\n# in this we will get no audio.\n# use ffmpeg and time strencher.\nfrom functional_redraw_chinese_text_offline import redraw_english_to_chinese\n# this is ideal for frame by frame processing.\n# oh shit!\n# the task is very long to run, i believe.\nvideo_cap = cv2.VideoCapture(source_video)\nfps = video_cap.get(cv2.CAP_PROP_FPS) # 60.\nframe_width = int(video_cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(video_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nframe_size = (frame_width, frame_height)\nframe_count = int(video_cap.get(cv2.CAP_PROP_FRAME_COUNT))\nfourcc = cv2.VideoWriter_fourcc('X', 'V', 'I', 'D') # h.264\n# video_writer =cv2.VideoWriter(output_video,fourcc,fps,frame_size)\n# frame_index_counter = 0\n# this is determinism.\n# or you could use framedifference? come on...\n# while True:\nimport json\nmjson_result = {}\nfor frame_index_counter in pb.progressbar(range(frame_count)): # are you sure?",
        "type": "code",
        "location": "/tests/bilibili_practices/bilibili_video_translate/frame_translate_processor.py:1-34"
    },
    "2449": {
        "file_id": 257,
        "content": "The code is reading a video file, extracting frame-by-frame information including FPS, frame width, height, and the total number of frames. It initializes variables for output JSON result and a potential output video (currently commented out). The code uses progress bar to iterate through each frame, but the actual translation process is missing from the given code snippet. The original task seems to be long and might require a lot of computation resources.",
        "type": "comment"
    },
    "2450": {
        "file_id": 257,
        "content": "    success, frame = video_cap.read() # let's just use 1, no frame skip.\n    if not success: break\n    print(\"processing frame\",frame_index_counter)\n    # write the frame to the output file\n    processed_frame_data= redraw_english_to_chinese(frame) # step 1\n    mjson_result.update({frame_index_counter:processed_frame_data})\n    # video_writer.write(processed_frame) # what frame?\n    # frame_index_counter+=1\n    # if cv2.waitKey(20) == ord('q'):\n        # break\nwith open(output_json,\"w+\",encoding=\"utf-8\") as f:\n    data = json.dumps(mjson_result,indent=4)\n    f.write(data)\nprint(\"VIDEO DONE ANALYSING. SAVED AT:\",output_json)",
        "type": "code",
        "location": "/tests/bilibili_practices/bilibili_video_translate/frame_translate_processor.py:35-48"
    },
    "2451": {
        "file_id": 257,
        "content": "This code reads frames from a video, processes one frame at a time by translating it using the redraw_english_to_chinese function, updates a dictionary with frame data, and saves the processed frames to an output JSON file. The process stops when no more frames can be read or if 'q' is pressed.",
        "type": "comment"
    },
    "2452": {
        "file_id": 258,
        "content": "/tests/bilibili_practices/bilibili_video_translate/frame_iterator_copy.py",
        "type": "filepath"
    },
    "2453": {
        "file_id": 258,
        "content": "This code reads video frames, prints their indices, and writes them to an output file using video_writer. It tracks progress with a progress bar and checks for keypresses to exit. Upon completion, it displays the saved location as VIDEO DONE message.",
        "type": "summary"
    },
    "2454": {
        "file_id": 258,
        "content": "import cv2\nimport progressbar as pb\nsource_video = \"japan_day.webm\"\noutput_video = \"japan_day_copy.mp4\"\n# in this we will get no audio.\n# use ffmpeg and time strencher.\n# from functional_redraw_chinese_text_offline import \n# this is ideal for frame by frame processing.\n# oh shit!\n# the task is very long to run, i believe.\nvideo_cap = cv2.VideoCapture(source_video)\nfps = video_cap.get(cv2.CAP_PROP_FPS) # 60.\nframe_width = int(video_cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(video_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nframe_size = (frame_width, frame_height)\nframe_count = int(video_cap.get(cv2.CAP_PROP_FRAME_COUNT))\nfourcc = cv2.VideoWriter_fourcc('X', 'V', 'I', 'D') # h.264\nvideo_writer =cv2.VideoWriter(output_video,fourcc,fps,frame_size)\n# frame_index_counter = 0\n# while True:\nfor frame_index_counter in pb.progressbar(range(frame_count)): # are you sure?\n    success, frame = video_cap.read()\n    if not success: break\n    print(\"processing frame\",frame_index_counter)\n    # write the frame to the output file",
        "type": "code",
        "location": "/tests/bilibili_practices/bilibili_video_translate/frame_iterator_copy.py:1-31"
    },
    "2455": {
        "file_id": 258,
        "content": "This code reads a video file, gets its frame properties like FPS and dimensions, creates an output video file with the same format, then iterates through each frame of the input video, printing its index while processing it. It uses a progress bar for tracking frame index in a loop, but there's a potential issue with the usage of the range function (the video might not have that many frames). Finally, it writes each processed frame to the output file.",
        "type": "comment"
    },
    "2456": {
        "file_id": 258,
        "content": "    video_writer.write(frame) # what frame?\n    # frame_index_counter+=1\n    # if cv2.waitKey(20) == ord('q'):\n        # break\nprint(\"VIDEO DONE. SAVED AT:\",output_video)",
        "type": "code",
        "location": "/tests/bilibili_practices/bilibili_video_translate/frame_iterator_copy.py:32-36"
    },
    "2457": {
        "file_id": 258,
        "content": "Writes frames to video file using video_writer, counts frame index, checks for keypress to exit. VIDEO DONE message with saved location output_video.",
        "type": "comment"
    },
    "2458": {
        "file_id": 259,
        "content": "/tests/bilibili_practices/bilibili_science_subtitle_with_cn_voice/web_translator.py",
        "type": "filepath"
    },
    "2459": {
        "file_id": 259,
        "content": "The code defines a translator function that randomly selects from multiple translation services to convert English text to either \"zh\" or \"zh-CHS\". It uses the \"translators\" module and imports random for selecting the translation service and language. The code also allows for different combinations of translation services to be tested by uncommenting specific lines in the mtranslators list. If an error occurs during translation, it prints the exception stack trace using traceback.",
        "type": "summary"
    },
    "2460": {
        "file_id": 259,
        "content": "import translators as ts\n# translator = \n# mtranslators = [ts.sogou] #this is pure shit.\n# mtranslators = [ts.baidu,ts.sogou]\n# mtranslators = [ts.baidu,ts.sogou,ts.iciba]\nmtranslators = [ts.youdao,ts.baidu,ts.alibaba] # no yandex, tencent, sogou.\n# mtranslators = [ts.baidu,ts.iciba]\nimport random\ndef translator(text):\n    randomLang = [\"zh\",\"zh-CHS\"]\n    from_language = \"en\"\n    # lang = random.choice(randomLang)\n    while True:\n        t = random.choice(mtranslators)\n        # print(type(translator))\n        for rl in randomLang:\n            try:\n                result = t(text,from_language=from_language,to_language=rl)\n                # if len(result) < 3:\n                #     print(t)\n                #     breakpoint()\n                return result\n            except:\n                import traceback\n                traceback.print_exc()",
        "type": "code",
        "location": "/tests/bilibili_practices/bilibili_science_subtitle_with_cn_voice/web_translator.py:1-27"
    },
    "2461": {
        "file_id": 259,
        "content": "The code defines a translator function that randomly selects from multiple translation services to convert English text to either \"zh\" or \"zh-CHS\". It uses the \"translators\" module and imports random for selecting the translation service and language. The code also allows for different combinations of translation services to be tested by uncommenting specific lines in the mtranslators list. If an error occurs during translation, it prints the exception stack trace using traceback.",
        "type": "comment"
    },
    "2462": {
        "file_id": 260,
        "content": "/tests/bilibili_practices/bilibili_science_subtitle_with_cn_voice/translate_srt.py",
        "type": "filepath"
    },
    "2463": {
        "file_id": 260,
        "content": "Code reads an SRT file, translates each line of the content using a web translator, wraps the translated lines to meet a certain character limit, and then saves the results in a new SRT file. The process involves parsing the input SRT file using the \"srt\" library, translating text with \"web_translator\", and modifying the line wrapping for better readability.",
        "type": "summary"
    },
    "2464": {
        "file_id": 260,
        "content": "src = \"en_.srt\"\nfinal_srt = \"zh_translated.srt\"\nimport srt\nwrap_limit = 20\nsource_srt = open(src, \"r\",encoding=\"utf-8\").read()\nssrt = srt.parse(source_srt)\nfrom web_translator import translator\nimport math\ndef wrapLine(line):\n    lines = [line[x*wrap_limit:(x+1)*wrap_limit] for x in range(math.ceil(len(line)/wrap_limit))]\n    return \"\\n\".join(lines)\ndef fixline(line):\n    notEndings = [\"。\",\"，\"]\n    for x in notEndings:\n        if line.endswith(x): return line[:-1]\n    return line\nnew_ssrt = []\nfor line in ssrt:\n    # print(line)\n    start = line.start\n    end = line.end # timedelta.\n    content = line.content\n    index = line.index\n    unwrapped_content = content.replace(\"\\n\",\" \")\n    result = translator(unwrapped_content)\n    result = fixline(result)\n    print(result)\n    line.content = result\n    new_ssrt.append(line)\n    # wrapped = wrapLine(result)\n    # print(wrapped)\n    # print(start, end, content, index)\nfinal_content = srt.compose(new_ssrt)\nwith open(final_srt,\"w+\",encoding=\"utf-8\") as f:\n    f.write(final_content)",
        "type": "code",
        "location": "/tests/bilibili_practices/bilibili_video_translate/translate_srt.py:1-45"
    },
    "2465": {
        "file_id": 260,
        "content": "Code reads an SRT file, translates each line of the content using a web translator, wraps the translated lines to meet a certain character limit, and then saves the results in a new SRT file. The process involves parsing the input SRT file using the \"srt\" library, translating text with \"web_translator\", and modifying the line wrapping for better readability.",
        "type": "comment"
    },
    "2466": {
        "file_id": 261,
        "content": "/tests/bilibili_practices/bilibili_science_subtitle_with_cn_voice/test2.py",
        "type": "filepath"
    },
    "2467": {
        "file_id": 261,
        "content": "The code imports the os module and defines two commands - one to install yt-dlp using pip3, another to download subtitles from a YouTube video with yt-dlp. It then iterates over each command in the list and runs them using os.system(). This will result in yt-dlp being installed and the subtitles being downloaded for the specified YouTube video.",
        "type": "summary"
    },
    "2468": {
        "file_id": 261,
        "content": "import os\ncommands = [\"pip3 install yt-dlp\",'yt-dlp --write-subs --convert-subtitles srt \"https://m.youtube.com/watch?v=At7ORzmAaT4\"'] # get recommendation this time.\n# we will still get many videoId from curl.\nfor c in commands:\n    os.system(c)",
        "type": "code",
        "location": "/tests/bilibili_practices/bilibili_science_subtitle_with_cn_voice/test2.py:1-8"
    },
    "2469": {
        "file_id": 261,
        "content": "The code imports the os module and defines two commands - one to install yt-dlp using pip3, another to download subtitles from a YouTube video with yt-dlp. It then iterates over each command in the list and runs them using os.system(). This will result in yt-dlp being installed and the subtitles being downloaded for the specified YouTube video.",
        "type": "comment"
    },
    "2470": {
        "file_id": 262,
        "content": "/tests/bilibili_practices/bilibili_science_subtitle_with_cn_voice/test.py",
        "type": "filepath"
    },
    "2471": {
        "file_id": 262,
        "content": "Code installs yt-dlp and downloads subtitles from a YouTube video, then converts them to SRT format. Optionally, it also enables sponsorblock-mark for highlighting ad breaks in the output.",
        "type": "summary"
    },
    "2472": {
        "file_id": 262,
        "content": "import os\ncommands = [\"pip3 install yt-dlp\",'yt-dlp --write-subs --convert-subtitles srt  \"https://m.youtube.com/watch?v=At7ORzmAaT4\"']\n# commands = [\"pip3 install yt-dlp\",'yt-dlp --write-subs --convert-subtitles srt --sponsorblock-mark poi_highlight \"https://m.youtube.com/watch?v=At7ORzmAaT4\"']\n# this will mark the highlights.\nfor c in commands:\n    os.system(c)",
        "type": "code",
        "location": "/tests/bilibili_practices/bilibili_science_subtitle_with_cn_voice/test.py:1-8"
    },
    "2473": {
        "file_id": 262,
        "content": "Code installs yt-dlp and downloads subtitles from a YouTube video, then converts them to SRT format. Optionally, it also enables sponsorblock-mark for highlighting ad breaks in the output.",
        "type": "comment"
    },
    "2474": {
        "file_id": 263,
        "content": "/tests/bilibili_practices/bilibili_science_subtitle_with_cn_voice/init.sh",
        "type": "filepath"
    },
    "2475": {
        "file_id": 263,
        "content": "This script initializes a Kaggle kernel, checks its status, and sets proxy environment variables to download at maximum speed.",
        "type": "summary"
    },
    "2476": {
        "file_id": 263,
        "content": "# kaggle kernels init # we have it do not fuck up again\n# code/jessysisca/some-yt-stuff \n# kaggle kernels push\nkaggle kernels status jessysisca/test-of-yt-dlp2\n# jessysisca/some-yt-stuff has status \"complete\"\n# root@alpharetta ~/android_connect_scrcpy_patch# \n# kaggle kernels status jessysisca/test-of-yt-dlp\n# jessysisca/test-of-yt-dlp has status \"running\"\n# after it is done, we pull back all shit.\n# skip all proxies.\n# export http_proxy=\"\"\n# export https_proxy=\"\"\n# kaggle kernels output jessysisca/test-of-yt-dlp2 # what is the freaking speed?\n# not too slow.",
        "type": "code",
        "location": "/tests/bilibili_practices/bilibili_science_subtitle_with_cn_voice/init.sh:1-14"
    },
    "2477": {
        "file_id": 263,
        "content": "This script initializes a Kaggle kernel, checks its status, and sets proxy environment variables to download at maximum speed.",
        "type": "comment"
    },
    "2478": {
        "file_id": 264,
        "content": "/tests/bilibili_practices/bilibili_science_subtitle_with_cn_voice/get_ytInitialData.py",
        "type": "filepath"
    },
    "2479": {
        "file_id": 264,
        "content": "This code uses BeautifulSoup and JavaScript libraries to extract HTML data, processes it into a JSON object with view count and video length updates.",
        "type": "summary"
    },
    "2480": {
        "file_id": 264,
        "content": "target = \"curl_dump_youtube.html\"\nfrom bs4 import BeautifulSoup\n# this is m.youtube.com/watch?v={videoId}\n# import esprima\nimport js2py\nsoup = open(target,\"r\",encoding=\"utf-8\").read()\nsoup = BeautifulSoup(soup,features=\"lxml\")\nscripts = soup.find_all(\"script\")\njsfunc = lambda x: \"function f9x() { \"+x+ \"  \\n return ytInitialData;}\"\njsfunc2 = lambda x: \"function f9x() { \"+x+ \"  \\n return ytInitialPlayerResponse;}\"\n# breakpoint()\nfrom commons import *\ndata = None\ndata2 = None\nfor script in scripts:\n    content = script.string\n    if content is not None:\n        if \"var ytInitialPlayerResponse = {\" in content:\n            print(\"HAS DATA\") # only one.\n            # script_obj = esprima.parse(content)\n            script_obj = jsfunc2(content)\n            # print(script_obj)\n            obj = js2py.eval_js(script_obj)\n            # print(obj)\n            data2 = obj() # need a json walker, from pyjom.\n            # breakpoint()\n    # print(content)\nfor script in scripts:\n    content = script.string\n    if content is not None:",
        "type": "code",
        "location": "/tests/bilibili_practices/bilibili_science_subtitle_with_cn_voice/get_ytInitialData.py:1-40"
    },
    "2481": {
        "file_id": 264,
        "content": "This code retrieves HTML from a specific target file, parses it using BeautifulSoup, and searches for scripts containing \"ytInitialData\" or \"ytInitialPlayerResponse\". It then uses JavaScript conversion libraries to extract the data from these scripts as Python objects. The data is stored in variables 'data' and 'data2', respectively.",
        "type": "comment"
    },
    "2482": {
        "file_id": 264,
        "content": "        if \"var ytInitialData = {\" in content:\n            print(\"HAS DATA\") # only one.\n            # script_obj = esprima.parse(content)\n            script_obj = jsfunc(content)\n            # print(script_obj)\n            obj = js2py.eval_js(script_obj)\n            # print(obj)\n            data = obj() # need a json walker, from pyjom.\n            # breakpoint()\n    # print(content)\n    # print(\"================================\")\n#     # breakpoint()\ndata_dict =  data.to_dict()\ndata2_dict =  data2.to_dict()\n# print(type(data))\n# breakpoint()\ntarget1 = [\"viewCountText\",\"lengthText\",\"publishedTimeText\"]\ntargets = [\"videoId\", \"simpleText\"]\ninits = ['contents', 'twoColumnWatchNextResults', 'secondaryResults', 'secondaryResults', 'results']\n# inits2 = ['contents', 'twoColumnWatchNextResults', 'secondaryResults', 'secondaryResults', 'results']\nends2 = {\"title\":['compactVideoRenderer', 'title', 'simpleText'],\"viewCountText\": ['compactVideoRenderer', 'viewCountText', 'simpleText'],\"publishTime\":['compactVideoRe",
        "type": "code",
        "location": "/tests/bilibili_practices/bilibili_science_subtitle_with_cn_voice/get_ytInitialData.py:41-67"
    },
    "2483": {
        "file_id": 264,
        "content": "This code checks if the content contains \"var ytInitialData = {\" and then parses it using jsfunc, converts to Python object with js2py, extracts data, converts it to dictionaries, and defines some target variables.",
        "type": "comment"
    },
    "2484": {
        "file_id": 264,
        "content": "nderer', 'publishedTimeText', 'simpleText'],\"lengthText\":['compactVideoRenderer', 'lengthText', 'simpleText'],\"videoId\":['compactVideoRenderer', 'videoId']}\nvideoDetails = data2_dict[\"videoDetails\"]\nvideoDetails = {k:videoDetails[k] for k in [\"viewCount\",\"author\",\"keywords\",\"channelId\",\"shortDescription\",\"lengthSeconds\",\"videoId\",\"title\"]}\n# \"https://i.ytimg.com/vi_webp/{videoId}/maxresdefault.webp # default cover.\nvideoDicts = {}\nfor key, content in json.walk(data_dict):\n    # print(key)\n    final_key = key[-1]\n    if final_key in targets:\n        if list_startswith(key,inits):\n            for k in ends2.keys():\n                v = ends2[k]\n                if list_endswith(key,v):\n                    valueType = k\n                    value = content\n                    valueIndex = key[len(inits)]\n                    if valueIndex not in videoDicts.keys():\n                        videoDicts[valueIndex] = {}\n                    # print(valueIndex,valueType,value)\n                    videoDicts[valueIndex].update({valueType:value})",
        "type": "code",
        "location": "/tests/bilibili_practices/bilibili_science_subtitle_with_cn_voice/get_ytInitialData.py:67-88"
    },
    "2485": {
        "file_id": 264,
        "content": "This code appears to extract specific data from a JSON object, specifically looking for keys that match certain endings and initials. The extracted data is then stored in a dictionary called \"videoDicts\" with the index as the key and the type and value of the data as the values. The purpose seems to be extracting specific information from the given JSON data, potentially for further use or processing.",
        "type": "comment"
    },
    "2486": {
        "file_id": 264,
        "content": "                    break\n        # print(key)  # i want to know the views of these.\n    # breakpoint()\ndef getViewCount(vc): return vc.replace(\",\",\"\").split(\" \")[0]\ndef getLengthSeconds(lt):\n    lt0 = lt.split(\":\")\n    assert len(lt0) <=5 # no more than week please?\n    dicIndex = {0:1,1:60,2:60*60,3:60*60*24,4:60*60*24*7}\n    seconds = 0\n    for i,v in enumerate(reversed(lt0)):\n        vn = int(v)\n        vs = vn*dicIndex[i]\n        seconds += vs\n    return str(seconds)\nfor k in videoDicts.keys():\n    v = videoDicts[k]\n    viewCount = getViewCount(v[\"viewCountText\"])\n    v.update({\"viewCount\":viewCount})\n    lengthSeconds = getLengthSeconds(v[\"lengthText\"])\n    v.update({\"lengthSeconds\":lengthSeconds})\n    print(v)\n    # for k0 in ends2.keys():\n    #     assert k0 in v.keys()\nprint(videoDetails)",
        "type": "code",
        "location": "/tests/bilibili_practices/bilibili_science_subtitle_with_cn_voice/get_ytInitialData.py:89-116"
    },
    "2487": {
        "file_id": 264,
        "content": "This code iterates over the 'videoDicts' dictionary, extracting and updating the view count and video length (in seconds) for each video. The extracted information is stored as values in the dictionary with keys \"viewCount\" and \"lengthSeconds\". Finally, it prints the updated 'videoDicts' dictionary and the 'videoDetails'.",
        "type": "comment"
    },
    "2488": {
        "file_id": 265,
        "content": "/tests/bilibili_practices/bilibili_science_subtitle_with_cn_voice/download_when_complete.py",
        "type": "filepath"
    },
    "2489": {
        "file_id": 265,
        "content": "The code is using the subprocess module to periodically check the status of a Kaggle kernel. If the status is \"running\" or \"complete\", it executes a final command and sets a lock file. It continues checking until the status is one of the valid ones or an unknown status occurs, in which case it breaks the loop.",
        "type": "summary"
    },
    "2490": {
        "file_id": 265,
        "content": "import parse\nimport subprocess\nimport time\nimport os\n# import pathlib\ndownload_lock = \".kaggle_downloaded\"\nif os.path.exists(download_lock):\n    print(\"already fetched content.\")\nwait_duration = 60\nformatx = '{a} has status \"{b}\"'\nvalid_status = [\"running\",\"complete\"]\nfinal_command = \"kaggle kernels output jessysisca/test-of-yt-dlp2\"\ncmd = \"kaggle kernels status jessysisca/test-of-yt-dlp2\"\nwhile True:\n    output = subprocess.check_output(cmd.split(\" \"))\n    output = output.decode('utf-8')\n    output = output.replace('\\n',\"\").strip()\n    result = parse.parse(formatx,output)\n    rb = result['b']\n    print(\"STATUS:\",rb)\n    if rb in valid_status:\n        if rb == \"complete\":\n            print(\"DOWNLOADING OUTPUT\")\n            os.system(final_command)\n            os.system(\"touch {}\".format(download_lock))\n            break\n        else:\n            time.sleep(wait_duration)\n    else:\n        print(\"UNKNOWN STATUS. ERROR.\")\n        break",
        "type": "code",
        "location": "/tests/bilibili_practices/bilibili_science_subtitle_with_cn_voice/download_when_complete.py:1-40"
    },
    "2491": {
        "file_id": 265,
        "content": "The code is using the subprocess module to periodically check the status of a Kaggle kernel. If the status is \"running\" or \"complete\", it executes a final command and sets a lock file. It continues checking until the status is one of the valid ones or an unknown status occurs, in which case it breaks the loop.",
        "type": "comment"
    },
    "2492": {
        "file_id": 266,
        "content": "/tests/bilibili_practices/bilibili_science_subtitle_with_cn_voice/create_output.sh",
        "type": "filepath"
    },
    "2493": {
        "file_id": 266,
        "content": "This code utilizes FFmpeg to merge webm video with subtitle files, trimming and styling as needed. It provides links for similar tasks, and seeks PlayResX, PlayResY, and ssa subtitle coordinates.",
        "type": "summary"
    },
    "2494": {
        "file_id": 266,
        "content": "ffmpeg -y -vsync 0 -hwaccel_output_format cuda -i \"Scientists Discovered a Bubble Around Our Solar System! [At7ORzmAaT4].webm\"  -vf \"subtitles=zh_translated.srt:force_style='MarginV=60',subtitles=en_.srt:force_style='Fontsize=10,PrimaryColour=&H00FFFF00,Alignment=6,MarginV=228'\" scientists_bubbles.mp4\n# ffmpeg -y -vsync 0 -hwaccel_output_format cuda -i \"Scientists Discovered a Bubble Around Our Solar System! [At7ORzmAaT4].webm\" -ss 00:00:07 -to 00:01:00  -vf \"subtitles=zh_translated.srt:force_style='MarginV=60',subtitles=en_.srt:force_style='Fontsize=10,PrimaryColour=&H00FFFF00,Alignment=6,MarginV=228'\" scientists_bubbles.mp4\n# https://www.zhihu.com/question/20779091\n# https://www.jianshu.com/p/cfdbfdc6d3a7\n# https://fileformats.fandom.com/wiki/SubStation_Alpha#Style_overrides\n# PlayResX: 384\n# PlayResY: 288\n# 384×288是标准的4：3画面分辨率之一。ssa字幕里的坐标（字幕的位置）即根据这2个数值的范围来定义。\n# ffmpeg -y -vsync 0 -hwaccel_output_format cuda -i \"Scientists Discovered a Bubble Around Our Solar System! [At7ORzmAaT4].webm\" -ss",
        "type": "code",
        "location": "/tests/bilibili_practices/bilibili_science_subtitle_with_cn_voice/create_output.sh:1-9"
    },
    "2495": {
        "file_id": 266,
        "content": "The code uses FFmpeg to combine a webm video with two subtitle files, creating an mp4 output. It also trims the video for a specific duration and applies style overrides for subtitles. The provided links are for reference material on similar tasks. The final part of the code seeks information about PlayResX and PlayResY, along with ssa subtitle coordinates.",
        "type": "comment"
    },
    "2496": {
        "file_id": 266,
        "content": " 00:00:07 -to 00:01:00  -vf \"subtitles=zh_translated.srt:force_style='MarginV=0',subtitles=en_.srt\" scientists_bubbles.mp4",
        "type": "code",
        "location": "/tests/bilibili_practices/bilibili_science_subtitle_with_cn_voice/create_output.sh:9-9"
    },
    "2497": {
        "file_id": 266,
        "content": "Applying subtitles to a video.",
        "type": "comment"
    },
    "2498": {
        "file_id": 267,
        "content": "/tests/bilibili_practices/bilibili_science_subtitle_with_cn_voice/commons.py",
        "type": "filepath"
    },
    "2499": {
        "file_id": 267,
        "content": "This code defines `jsonWalk` and `jsonLocate` functions that recursively traverse JSON objects, handling dictionaries, lists, tuples, and raising exceptions for non-JSON types. It also updates json's dictionary with new \"walk\" and \"locate\" functions.",
        "type": "summary"
    }
}