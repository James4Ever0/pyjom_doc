{
    "2700": {
        "file_id": 300,
        "content": "    # import requests\n    data = requests.get(myUrl)\n    data = data.json()\n    print(data)\n    result = data['result']\n    assert result == 0  # 202 -> busy\n    content = data['content']\n    return content\n    # breakpoint()\ndef xiaobing(msg):\n    # 其实是新浪微博群发器 微博群发的逻辑类似于b站群发\n    # 刚关注的只能发一条消息\n    uid = '5175429989'\n    source = '209678993'\n    SUB = '_2A25PyitTDeRhGeBG7VAS8y_MwjmIHXVsvhubrDV8PUNbmtANLRfTkW9NRhxXNiVv6Qwut5wwnc8rys3cbJFAxVdX'\n    url_send = 'https://api.weibo.com/webim/2/direct_messages/new.json'\n    data = {\n        'text': msg,\n        'uid': uid,\n        'source': source\n    }\n    headers = {\n        'cookie': 'SUB='+SUB,\n        'Content-Type': 'application/x-www-form-urlencoded',\n        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.130 Safari/537.36',\n        'Referer': 'https://api.weibo.com/chat/'\n    }\n    response = requests.post(url_send, data=data, headers=headers).json()\n    sendMsg = response['text']\n    time.sleep(1)\n    while True:",
        "type": "code",
        "location": "/tests/conversation_talk_apis/api_tests.py:38-69"
    },
    "2701": {
        "file_id": 300,
        "content": "This code is using the requests library to send a POST request to the Weibo API's direct messaging endpoint. It creates a new message with the provided text, sends it to a specified user (uid), and retrieves the response from the API. The script includes necessary headers and uses JSON format for the data payload.",
        "type": "comment"
    },
    "2702": {
        "file_id": 300,
        "content": "        print(\"RETRYING\")\n        url_get = 'https://api.weibo.com/webim/2/direct_messages/conversation.json?uid={}&source={}'.format(uid, source)\n        response = requests.get(url_get, headers=headers).json()\n        getMsg = response['direct_messages'][0]['text']\n        if sendMsg == getMsg:\n            time.sleep(1)\n        else:\n            return getMsg\ndef chatOwnThink(msg:str):\n    url = \"https://api.ownthink.com/bot?appid=xiaosi&userid=user&spoken=\"\n    msg = urllib.parse.quote(msg)\n    myUrl = url+msg\n    data = requests.get(myUrl)\n    data = data.json()\n    # output = subprocess.check_output([\"curl\", myUrl])\n    # data = json.loads(output.decode(\"utf-8\"))\n    if data[\"message\"] == \"success\":\n        if data[\"data\"][\"type\"] == 5000:\n            return data[\"data\"][\"info\"][\"text\"]\n    # print(data)\n    # breakpoint()\n    # result = data['result']\n    # assert result == 0  # 202 -> busy\n    # content = data['content']\n    # return content\nif __name__ == '__main__':\n    # execute my tests.\n    message = \"你好\"",
        "type": "code",
        "location": "/tests/conversation_talk_apis/api_tests.py:70-99"
    },
    "2703": {
        "file_id": 300,
        "content": "The code is attempting to interact with two APIs - Weibo and OwnThink. It first checks if the message from the user matches the response received from the Weibo API conversation. If it's a match, the code waits for a second before rechecking. If there's no match, it sends the message to the OwnThink API to get a response. The response is then checked for success and if the type of response is 5000, the text information from the response is returned.",
        "type": "comment"
    },
    "2704": {
        "file_id": 300,
        "content": "    # checkApi(chatAtri, message, \"ATRI\")\n    # checkApi(xiaobing, message, \"XIAOBING\")\n    # checkApi(chatOwnThink, message, \"OWNTHINK\")\n    checkApi(chatQingKeYun, message, \"QINGYUNKE\")",
        "type": "code",
        "location": "/tests/conversation_talk_apis/api_tests.py:100-103"
    },
    "2705": {
        "file_id": 300,
        "content": "This code is calling the checkApi function with different parameters for each chatbot instance: chatAtri, xiaobing, chatOwnThink, and chatQingKeYun. The function call passes a message and specific identifier to perform an API test on each chatbot.",
        "type": "comment"
    },
    "2706": {
        "file_id": 301,
        "content": "/tests/viral_video_experiments/init.sh",
        "type": "filepath"
    },
    "2707": {
        "file_id": 301,
        "content": "This code initiates the setup for viral video data analysis and prediction. It clones two repositories, ViralCaster for analysis and prediction tasks, and 360ImageSearch and BaiduSerchImgApi for image recognition purposes.",
        "type": "summary"
    },
    "2708": {
        "file_id": 301,
        "content": "# viral video data analysis, prediction\n# git clone https://github.com/jjbreen/ViralCaster\n# image recognition\ngit clone https://github.com/chenguanyou/BaiduSerchImgApi\ngit clone https://github.com/chenguanyou/360ImageSearch",
        "type": "code",
        "location": "/tests/viral_video_experiments/init.sh:1-6"
    },
    "2709": {
        "file_id": 301,
        "content": "This code initiates the setup for viral video data analysis and prediction. It clones two repositories, ViralCaster for analysis and prediction tasks, and 360ImageSearch and BaiduSerchImgApi for image recognition purposes.",
        "type": "comment"
    },
    "2710": {
        "file_id": 302,
        "content": "/tests/dapp_ethereum_python_crypto/test.py",
        "type": "filepath"
    },
    "2711": {
        "file_id": 302,
        "content": "The code uses Web3 to connect to a local Ethereum node, imports necessary libraries, checks connection status and account balance, unlocks accounts, sends transactions, and verifies received funds.",
        "type": "summary"
    },
    "2712": {
        "file_id": 302,
        "content": "from web3 import Web3\n# testnet, bitcoind, regtest\n# https://bitcoin.stackexchange.com/questions/42026/is-it-possible-to-use-bitcoind-as-a-private-blockchain\n# mine only when pending transaction happens:\n# https://ethereum.stackexchange.com/questions/3151/how-to-make-miner-to-mine-only-when-there-are-pending-transactions\n# maybe you want money even if without transaction, or low in cash.\n# https://hackernoon.com/hands-on-creating-your-own-local-private-geth-node-beginner-friendly-3d45902cc612\nlink = \"/root/.ethereum/geth.ipc\"\nweb3 = Web3(Web3.IPCProvider(link))\nprint(web3.isConnected())\n# account_genesis = \"0xde478bde26d711414fae26133e759d8a82a202ab\"  # aka: eth.coinbase\n# account_genesis = \"0x6fe20a7157fdb705278fffda4ea0ebf4694f31ea\"\naccount_genesis = \"0xd6e79c8d5b7d41cc1a3b98373c98618ea267852f\"\naccount_genesis = Web3.toChecksumAddress(account_genesis)\npassword_genesis = \"abcdefg\"\n# let's see!\n# target_account = \"0x033799af9b29e1d7dbf3c8dd64647df345f67bf1\"\ntarget_account = \"0x463f061d2add7987e2a7d14920e18194107ea991\"",
        "type": "code",
        "location": "/tests/dapp_ethereum_python_crypto/test.py:1-26"
    },
    "2713": {
        "file_id": 302,
        "content": "The code imports Web3, sets the IPC link to connect to a local Ethereum node, checks the connection status, assigns an account address and password, and specifies a target account.",
        "type": "comment"
    },
    "2714": {
        "file_id": 302,
        "content": "target_account = Web3.toChecksumAddress(target_account)\n# you was connected ethereum to mainnet! not good.\n# anyway, we need money!\nb = web3.eth.get_balance(web3.eth.coinbase)\nprint(b)\n# proof of authority, puppeth\n## need password!\nweb3.geth.personal.unlock_account(web3.eth.coinbase, password_genesis)\nweb3.eth.send_transaction(\n    {\n        \"to\": target_account,\n        \"from\": web3.eth.coinbase,\n        \"value\": 1,\n    }\n)\nweb3.geth.personal.lock_account(web3.eth.coinbase)\n# you can choose to use 'with' statement.\nb = web3.eth.get_balance(target_account)\nprint(b)\n# still no money! fuck.",
        "type": "code",
        "location": "/tests/dapp_ethereum_python_crypto/test.py:27-52"
    },
    "2715": {
        "file_id": 302,
        "content": "Code connects to Ethereum mainnet, checks balance of the coinbase account, unlocks account using a password, sends transaction to target_account, and verifies if funds have been received.",
        "type": "comment"
    },
    "2716": {
        "file_id": 303,
        "content": "/tests/dapp_ethereum_python_crypto/README.md",
        "type": "filepath"
    },
    "2717": {
        "file_id": 303,
        "content": "The code is expressing the difficulty in validating a 'hacker' program within AGI and the need to create dummy crypto elements for testing purposes.",
        "type": "summary"
    },
    "2718": {
        "file_id": 303,
        "content": "not sure how to validate my 'hacker' program in AGI. just create some dummy crypto things.",
        "type": "code",
        "location": "/tests/dapp_ethereum_python_crypto/README.md:1-1"
    },
    "2719": {
        "file_id": 303,
        "content": "The code is expressing the difficulty in validating a 'hacker' program within AGI and the need to create dummy crypto elements for testing purposes.",
        "type": "comment"
    },
    "2720": {
        "file_id": 304,
        "content": "/tests/video_script_generation_reconstruction/spp_module.py",
        "type": "filepath"
    },
    "2721": {
        "file_id": 304,
        "content": "The `spatial_pyramid_pool` function performs spatial pyramid pooling on convolutional output using max pooling, and the code includes a main section for testing. The code also performs 1D convolutions followed by LSTM layers to process sequential data.",
        "type": "summary"
    },
    "2722": {
        "file_id": 304,
        "content": "import math\nfrom torch import nn\nimport torch\ndef spatial_pyramid_pool(previous_conv, num_sample, previous_conv_size, out_pool_size):\n    '''\n    previous_conv: a tensor vector of previous convolution layer\n    num_sample: an int number of image in the batch\n    previous_conv_size: an int vector [height, width] of the matrix features size of previous convolution layer\n    out_pool_size: a int vector of expected output size of max pooling layer\n    returns: a tensor vector with shape [1 x n] is the concentration of multi-level pooling\n    '''    \n    # print(previous_conv.size())\n    for i in range(len(out_pool_size)):\n        # print(previous_conv_size)\n        h_wid = int(math.ceil(previous_conv_size[0] / out_pool_size[i]))\n        w_wid = int(math.ceil(previous_conv_size[1] / out_pool_size[i]))\n        h_pad = (h_wid*out_pool_size[i] - previous_conv_size[0] + 1)/2 # float man.\n        h_pad = math.ceil(h_pad)\n        w_pad = (w_wid*out_pool_size[i] - previous_conv_size[1] + 1)/2\n        w_pad = math.ceil(w_pad)",
        "type": "code",
        "location": "/tests/video_script_generation_reconstruction/spp_module.py:1-22"
    },
    "2723": {
        "file_id": 304,
        "content": "This function, `spatial_pyramid_pool`, takes in a tensor vector from the previous convolution layer, the number of samples in the batch, the size of the matrix features in the previous layer, and the expected output size for max pooling. It returns a tensor vector with shape [1 x n], which represents the concentration of multi-level pooling. The function calculates the height and width padding required for each level of the max pooling based on the input sizes and desired output sizes.",
        "type": "comment"
    },
    "2724": {
        "file_id": 304,
        "content": "        maxpool = nn.MaxPool2d((h_wid, w_wid), stride=(h_wid, w_wid), padding=(h_pad, w_pad)) # this has no trainable parameter.\n        x = maxpool(previous_conv)\n        # print(x.size())\n        torch.Size([20, 16, 20, 20])\n        # this is it.\n        if(i == 0):\n            spp = x.view(num_sample,-1)\n            # print(\"spp size:\", spp.size())\n        else:\n            # print(\"size:\",spp.size())\n            spp = torch.cat((spp,x.view(num_sample,-1)), 1)\n    return spp\nif __name__ == \"__main__\":\n    # to test the freaking video.\n    for i in [200,1000]:\n        w0 = h0 = i\n        x = torch.rand(20,3,w0,h0) # 20 frames, 20 width, 20 height8\n        # three channels? where is the optical flow layer?\n        c2layer_1 = nn.Conv2d(3,4,4)\n        c2_output_1 = c2layer_1(x)\n        print(c2_output_1.shape)\n        c2layer_2 = nn.Conv2d(4,16,20)\n        c2_output_2 = c2layer_2(c2_output_1)\n        print(c2_output_2.shape)\n        output_num = [20]\n        spp = spatial_pyramid_pool(c2_output_2,20,[int(c2_output_2.size(2)),int(c2_output_2.size(3))],output_num) # great now you have the batch size.",
        "type": "code",
        "location": "/tests/video_script_generation_reconstruction/spp_module.py:23-49"
    },
    "2725": {
        "file_id": 304,
        "content": "This code defines a function that performs spatial pyramid pooling on convolutional output. It uses max pooling with fixed window sizes and combines the results into a single tensor. The code also includes a main section for testing purposes, where it applies convolutions to random input data and then calls the spatial_pyramid_pool function.",
        "type": "comment"
    },
    "2726": {
        "file_id": 304,
        "content": "        print(x.shape,spp.shape) # 1,5120\n        spp_lstm = spp[None,:]\n        print(spp_lstm.shape) # 1,1,5120\n        cnn_1 = nn.Conv1d(20,20,16,stride=2)\n        cout_1 = cnn_1(spp_lstm)\n        print(cout_1.shape)\n        cnn_2 = nn.Conv1d(20,20,16,stride=2)\n        cout_2 = cnn_2(cout_1)\n        print(cout_2.shape)\n        lstm_1 = nn.LSTM(1589,400)\n        out_1,hid_1 = lstm_1(cout_2)\n        print(out_1.shape)\n        lstm_2 = nn.LSTM(400,20)\n        out_2,hid_2 = lstm_2(out_1)\n        print(out_2.shape)\n        lstm_3 = nn.LSTM(20,2)\n        out_3,hid_3 = lstm_3(out_2)\n        print(out_3.shape)",
        "type": "code",
        "location": "/tests/video_script_generation_reconstruction/spp_module.py:50-67"
    },
    "2727": {
        "file_id": 304,
        "content": "The code performs 1D convolutions followed by LSTM layers to process a sequence of data. It reshapes the input and applies two Conv1d operations, reducing the dimensionality of the data. Then, it applies three LSTM layers with decreasing hidden dimensions for further processing. The output shapes are printed at each step.",
        "type": "comment"
    },
    "2728": {
        "file_id": 305,
        "content": "/tests/video_script_generation_reconstruction/spp_any_video.py",
        "type": "filepath"
    },
    "2729": {
        "file_id": 305,
        "content": "The code initializes video and audio data arrays, defines a VideoCutNet model with CNN layers, performs spatial pyramid pooling on video frames, uses LSTM for audio processing, iterates over training loop for gradient descent, and lacks batch size specification.",
        "type": "summary"
    },
    "2730": {
        "file_id": 305,
        "content": "# 2d understanding or 3d?\n# what about the freaking audio?\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nimport torch.nn as nn\nvideo_shape = (20,3,100,100) # thirty frames extracted.\naudio_shape = (1,40000)\nvideo2_shape = (70,3,200,200) # thirty frames extracted. # change it!\naudio2_shape = (2,120000) # no freaking padding game.\ntarget_sentence_shape = (20,2) # full charset. you may choose not to speak. when should you freaking speak?\ntarget_sentence2_shape = (70,2) # full charset. you may choose not to speak. when should you freaking speak?\n# one hot encoding.\nimport random\ntarget_sentence = np.array([random.randint(0,1) for _ in range(20)]) # do one-hot encoding please.\ntarget_sentence2 = np.array([random.randint(0,1) for _ in range(70)])\ntarget_sentence = np.eye(2)[target_sentence]\ntarget_sentence2 = np.eye(2)[target_sentence2]\npad_video_shape_2 = np.zeros((20,3,200,200))\npad_sentence_2 = np.zeros((20,2))\ntarget_sentence2 = np.concatenate([target_sentence2,pad_sentence_2])\n# print(target_sentence2.shape,pad_sentence_2.shape)",
        "type": "code",
        "location": "/tests/video_script_generation_reconstruction/spp_any_video.py:1-30"
    },
    "2731": {
        "file_id": 305,
        "content": "This code defines video and audio shapes for various inputs, creates random target sentences, and pads the data with zeros to maintain consistent shape. It uses numpy array manipulation and one-hot encoding to represent categorical data.",
        "type": "comment"
    },
    "2732": {
        "file_id": 305,
        "content": "# breakpoint()\n# i really don't care how. freaking do it!\nvideo_data = np.random.random(video_shape)\naudio_data = np.random.random(audio_shape)\nvideo2_data = np.random.random(video2_shape)\naudio2_data = np.random.random(audio2_shape)\n# print(data)\nvideo2_data = np.concatenate([video2_data,pad_video_shape_2])\n# I really not caring the freaking data range.\nprint(video_data.shape)\nprint(audio_data.shape)\nfrom spp_module import spatial_pyramid_pool\nclass VideoCutNet(torch.nn.Module):\n    def __init__(self,debug=True):\n        super().__init__()\n        self.debug = debug\n        self.hidden_states=[None]\n        self.audio_hidden_states = [None]\n        self.va_hidden_states = [None,None]\n        self.c2layer_1 = nn.Conv2d(3,4,4)\n        self.c2layer_2 = nn.Conv2d(4,16,20)\n        self.output_num = [20]\n        # print(x.shape,spp.shape) # 1,5120\n        self.cnn_1 = nn.Conv1d(2,20,16,stride=2,padding=8) # you could use this on the audio.\n        self.cnn_2 = nn.Conv1d(20,16,16,stride=2,padding=8)\n        self.cnn_3 = nn.Conv1d(16,30,16,stride=4,padding=8)",
        "type": "code",
        "location": "/tests/video_script_generation_reconstruction/spp_any_video.py:31-63"
    },
    "2733": {
        "file_id": 305,
        "content": "This code snippet initializes random video and audio data arrays with specified shapes, concatenates the second video data array with padding, defines a class for the VideoCutNet model, and sets up various layers such as convolutional layers (CNNs) for processing audio and videos. The debug parameter allows controlling whether or not to print the shapes of the initialized arrays.",
        "type": "comment"
    },
    "2734": {
        "file_id": 305,
        "content": "        self.lstm_1 = nn.LSTM(6400,1200,batch_first=True) # huge?\n        # self.lstm_2 = nn.LSTM(400,20)\n        # self.lstm_3 = nn.LSTM(20,2)\n        self.audio_lstm_1 = nn.LSTM(2501,500,batch_first=True)\n        self.video_audio_merger = nn.Linear(1700,300)\n        # self.audio_lstm_2 = nn.LSTM()\n        # self.audio_lstm_3 = nn.LSTM()\n        self.va_lstm_2 = nn.LSTM(300,50,batch_first=True)\n        self.va_lstm_3 = nn.LSTM(50,20,batch_first=True)\n        self.va_linear = nn.Linear(20,2)\n    def clear_hidden_state(self):\n        self.hidden_states=[None] # no tuple.\n        self.audio_hidden_states=[None] # no tuple.\n        self.va_hidden_states=[None,None] # no tuple.\n    def forward(self,x,audio_x):\n        # with torch.autograd.set_detect_anomaly(False):\n        c2_output_1 = self.c2layer_1(x)\n        if self.debug:\n            print(c2_output_1.shape)\n        c2_output_1 = F.relu(c2_output_1)\n        c2_output_2 = self.c2layer_2(c2_output_1)\n        if self.debug:\n            print(c2_output_2.shape)",
        "type": "code",
        "location": "/tests/video_script_generation_reconstruction/spp_any_video.py:65-91"
    },
    "2735": {
        "file_id": 305,
        "content": "This code defines a neural network model with LSTM layers for processing video and audio data. The forward function takes input x (video) and audio_x, and passes them through convolutional layers followed by LSTM layers to extract features. The clear\\_hidden\\_state method initializes hidden states for each LSTM layer.",
        "type": "comment"
    },
    "2736": {
        "file_id": 305,
        "content": "        c2_output_2 = F.relu(c2_output_2)\n        msize = int(c2_output_2.size(0))\n        # print(msize)\n        # breakpoint()\n        spp = spatial_pyramid_pool(c2_output_2,msize,[int(c2_output_2.size(2)),int(c2_output_2.size(3))],self.output_num) # great now you have the batch size.\n        spp_lstm = spp[None,:]\n        spp_lstm = F.relu(spp_lstm)\n        if self.debug:\n            print(spp_lstm.shape) # 1,1,5120\n###AUDIO\n        cout_1 = self.cnn_1(audio_x)\n        if self.debug:\n            print(\"AUDIO\",cout_1.shape)\n        cout_1 = F.relu(cout_1)\n        cout_2 = self.cnn_2(cout_1)\n        if self.debug:\n            print(\"AUDIO\",cout_2.shape)\n        cout_2 = F.relu(cout_2)\n        cout_3 = self.cnn_3(cout_2)\n        if self.debug:\n            print(\"AUDIO\",cout_3.shape)\n        cout_3 = F.relu(cout_3)\n        aout_1, ahid_1 = self.audio_lstm_1(cout_3,self.audio_hidden_states[0])\n        self.audio_hidden_states[0] =(ahid_1[0].detach(),ahid_1[1].detach())\n        if self.debug:\n            print(\"AUDIO LSTM\",aout_1.shape)",
        "type": "code",
        "location": "/tests/video_script_generation_reconstruction/spp_any_video.py:92-123"
    },
    "2737": {
        "file_id": 305,
        "content": "This code snippet performs feature extraction and pooling on video frames using CNNs and a spatial pyramid pooling layer, followed by LSTM processing for audio. It then prints the shapes of intermediate tensors for debugging purposes.",
        "type": "comment"
    },
    "2738": {
        "file_id": 305,
        "content": "        aout_1 = F.relu(aout_1) # for audio only this time we apply this.\n###AUDIO\n        out_1, hid_1 = self.lstm_1(spp_lstm,self.hidden_states[0]) # passing no hidden state at all.\n        self.hidden_states[0] =(hid_1[0].detach(),hid_1[1].detach())\n        if self.debug:\n            print(out_1.shape)\n        out_1 = F.relu(out_1)\n        # breakpoint()\n##VIDEO AUDIO MERGE\n        merged = torch.cat([aout_1,out_1],dim=2)\n        if self.debug:\n            print(merged.shape)\n        mout_1 = self.video_audio_merger(merged)\n        if self.debug:\n            print(mout_1.shape)\n        # breakpoint()\n        mout_2,mhid_2 = self.va_lstm_2(mout_1,self.va_hidden_states[0])\n        self.va_hidden_states[0] =(mhid_2[0].detach(),mhid_2[1].detach())\n        if self.debug:\n            print(mout_2.shape)\n        mout_3,mhid_3 = self.va_lstm_3(mout_2,self.va_hidden_states[1])\n        self.va_hidden_states[1] =(mhid_3[0].detach(),mhid_3[1].detach())\n        if self.debug:\n            print(mout_3.shape)\n        # breakpoint()",
        "type": "code",
        "location": "/tests/video_script_generation_reconstruction/spp_any_video.py:124-153"
    },
    "2739": {
        "file_id": 305,
        "content": "Applying ReLU activation to audio output, passing no hidden state to LSTM, merging audio and video outputs through concatenation, feeding merged output to two additional LSTMs for further processing.",
        "type": "comment"
    },
    "2740": {
        "file_id": 305,
        "content": "        mout_4 = self.va_linear(mout_3)\n        if self.debug:\n            print(mout_4.shape)\n        return mout_4\nvideo_cut_net = VideoCutNet(debug=True).cuda()\nvideo_data = torch.Tensor(video_data).cuda()\nvideo_data2 = torch.Tensor(video2_data).cuda()\naudio_data2 = torch.Tensor(audio2_data).cuda()\naudio_data2 = audio_data2[None,:]\n# must equal to 20 frames.\ntarget_sentence = torch.Tensor(target_sentence).cuda()\ntarget_sentence2 = torch.Tensor(target_sentence2).cuda()\ncriterion= nn.CrossEntropyLoss()\noptim = torch.optim.Adam(video_cut_net.parameters(),lr=0.0001)\ntarget = target_sentence\ntarget = target_sentence[None,:]\ntarget2 = target_sentence2\ntarget2 = target_sentence2[None,:]\n# for _ in range(240):# we pass 5 identical segments to our network, require to produce different labels.\nvideo_cut_net.clear_hidden_state() # to make sure we can train this shit.\ndivisor = 30\naudio_divisor = 40000\nprint(video_data2.shape) # ([60, 3, 100, 100])\n# breakpoint()\nframes2 = video_data2.shape[0]\nimport math\nbest_index = math.ceil(frames2/divisor)",
        "type": "code",
        "location": "/tests/video_script_generation_reconstruction/spp_any_video.py:154-186"
    },
    "2741": {
        "file_id": 305,
        "content": "This code initializes a VideoCutNet model, prepares input data, defines a loss function and optimizer, and sets up the training loop to pass 5 identical segments of video data for the network to produce different labels. The model's hidden state is cleared before each iteration. The code also calculates the number of frames in the second video segment and finds the best index for dividing it into sections.",
        "type": "comment"
    },
    "2742": {
        "file_id": 305,
        "content": "for index in range(best_index):\n    optim.zero_grad()\n    video_data_slice = video_data2[index*divisor:(index+1)*divisor,:]\n    audio_data_slice = audio_data2[:,:,index*audio_divisor:(index+1)*audio_divisor]\n    print(\"AUDIO_DATA_SLICE\",audio_data_slice.shape)\n    # breakpoint()\n    # use some padding for our video and label processes. make sure it is divisible by 20\n    # data_input = video_data_slice\n    target_slice = target2[:,index*divisor:(index+1)*divisor,:] # must be the right freaking target.\n    print(video_data_slice.shape,target_slice.shape)\n    # breakpoint()\n    with torch.nn.utils.parametrize.cached():\n        output = video_cut_net(video_data_slice,audio_data_slice)\n        # print(output.shape,target_slice.shape) # 1,20,2\n        # breakpoint()\n        loss = criterion(output, target_slice)\n        # print(loss)\n        val_loss = loss.detach().cpu().numpy()\n        print('CURRENT LOSS:',val_loss) # taking longer for long videos. may kill your freaking ram.\n        loss.backward()\n    optim.step()",
        "type": "code",
        "location": "/tests/video_script_generation_reconstruction/spp_any_video.py:188-209"
    },
    "2743": {
        "file_id": 305,
        "content": "This code is iterating over a range of indices, performing gradient descent on a loss function using audio and video data slices. The slicing ensures the right target is used for each iteration. It also prints the shape of the audio data slice, video data slice, and checks the shapes before calculating the loss. The current loss is printed at each iteration to monitor progress.",
        "type": "comment"
    },
    "2744": {
        "file_id": 305,
        "content": "    # where is the batch size? reduce it?\n    # there is no batch size. this is recurrent network. must process sequentially.",
        "type": "code",
        "location": "/tests/video_script_generation_reconstruction/spp_any_video.py:210-211"
    },
    "2745": {
        "file_id": 305,
        "content": "These lines indicate that there is no batch size specified in the code and it's a recurrent network which needs to be processed sequentially.",
        "type": "comment"
    },
    "2746": {
        "file_id": 306,
        "content": "/tests/video_script_generation_reconstruction/README.md",
        "type": "filepath"
    },
    "2747": {
        "file_id": 306,
        "content": "This code refers to a project containing various AI models for feature extraction, video summarization, audio classification, image labeling, text extraction, and keyword suggestion. The user needs to write a script before using these tools from PaddleVideo.",
        "type": "summary"
    },
    "2748": {
        "file_id": 306,
        "content": "contains multiple feature extractor, video summarizer, audio classifier, image labeler, text extractor, keyword suggestor. but you need to write one freaking script first. that is your freaking style. you can write that shit in markdown anyway, in reference of existing excellent(?) videos.\nfrom https://github.com/PaddlePaddle/PaddleVideo.\nvideo understanding.",
        "type": "code",
        "location": "/tests/video_script_generation_reconstruction/README.md:1-5"
    },
    "2749": {
        "file_id": 306,
        "content": "This code refers to a project containing various AI models for feature extraction, video summarization, audio classification, image labeling, text extraction, and keyword suggestion. The user needs to write a script before using these tools from PaddleVideo.",
        "type": "comment"
    },
    "2750": {
        "file_id": 307,
        "content": "/tests/video_script_generation_reconstruction/raw_data_understanding.py",
        "type": "filepath"
    },
    "2751": {
        "file_id": 307,
        "content": "The code defines the shapes of video and audio data, generates random one-hot encoded target sentences, and creates random data for both videos. It then prints the shapes of the generated video and audio data.",
        "type": "summary"
    },
    "2752": {
        "file_id": 307,
        "content": "# 2d understanding or 3d?\n# what about the freaking audio?\nimport numpy as np\nimport torch\nvideo_shape = (30,100,100) # thirty frames extracted.\naudio_shape = (1,40000)\nvideo2_shape = (60,200,200) # thirty frames extracted.\naudio2_shape = (1,80000)\ntarget_sentence_shape = (10,40000) # full charset. you may choose not to speak. when should you freaking speak?\ntarget_sentence2_shape = (15,40000) # full charset. you may choose not to speak. when should you freaking speak?\n# one hot encoding.\nimport random\ntarget_sentence = np.array([random.randint(0,39999) for _ in range(10)])\ntarget_sentence2 = np.array([random.randint(0,39999) for _ in range(15)])\n# i really don't care how. freaking do it!\nvideo_data = np.random.random(video_shape)\naudio_data = np.random.random(audio_shape)\nvideo2_data = np.random.random(video2_shape)\naudio2_data = np.random.random(audio2_shape)\n# print(data)\n# I really not caring the freaking data range.\nprint(video_data.shape)\nprint(audio_data.shape)",
        "type": "code",
        "location": "/tests/video_script_generation_reconstruction/raw_data_understanding.py:1-31"
    },
    "2753": {
        "file_id": 307,
        "content": "The code defines the shapes of video and audio data, generates random one-hot encoded target sentences, and creates random data for both videos. It then prints the shapes of the generated video and audio data.",
        "type": "comment"
    },
    "2754": {
        "file_id": 308,
        "content": "/tests/video_script_generation_reconstruction/raw_data_cut.py",
        "type": "filepath"
    },
    "2755": {
        "file_id": 308,
        "content": "This code initializes shapes, generates cut targets, and ensures correct tensor dimensions. It applies convolution, pooling, activation functions, performs two RNN operations, prints output/hidden state shapes, defines a final linear layer, transposes data, and suggests MaxPool1d for character extraction.",
        "type": "summary"
    },
    "2756": {
        "file_id": 308,
        "content": "# 2d understanding or 3d?\n# what about the freaking audio?\nimport numpy as np\nimport torch\nvideo_shape = (30,100,100) # thirty frames extracted.\naudio_shape = (1,40000) # so batch size is included.\nvideo2_shape = (60,200,200) # thirty frames extracted.\naudio2_shape = (1,80000)\ntarget_cut_shape = (30,2) # choose either beginning or to cut?\ntarget_cut2_shape = (60,2) # choose either beginning or to cut?\nimport random\ntarget_cut = np.array([random.randint(0,1) for _ in range(30)])\ntarget_cut2 = np.array([random.randint(0,1) for _ in range(60)])\nvideo_data = np.array(np.random.random(video_shape))\naudio_data = np.array(np.random.random(audio_shape))\nvideo2_data = np.array(np.random.random(video2_shape))\naudio2_data = np.array(np.random.random(audio2_shape))\n# print(data)\n# I really not caring the freaking data range.\nprint(video_data.shape)\nprint(audio_data.shape)\nprint(target_cut2.shape)\ndevice = torch.device(\"cuda\")\nvideo_data = torch.Tensor([video_data]) # to make sure the first dimension is batchsize\ntarget_cut = torch.Tensor([target_cut])",
        "type": "code",
        "location": "/tests/video_script_generation_reconstruction/raw_data_cut.py:1-33"
    },
    "2757": {
        "file_id": 308,
        "content": "Code initializes various shapes for video and audio data, randomly generates cut targets, and ensures tensor dimensions are correct for GPU usage.",
        "type": "comment"
    },
    "2758": {
        "file_id": 308,
        "content": "audio_data = torch.Tensor(audio_data)\nlayer_1 = torch.nn.Conv2d(30,3,10) # original shape: (30,100,100)\noutput_1 = layer_1(video_data)\nprint(output_1.shape) #(1,3,91,91)\nlayer_2 = torch.nn.Conv2d(3,1,10)\noutput_2 = layer_2(output_1)\nprint(output_2.shape) #([1, 2, 82, 82])\nlayer_3 = torch.nn.MaxPool1d(4)\noutput_3 = layer_3(audio_data)\nprint(output_3.shape) # torch.Size([1, 10000]) # what is this fuck?\nlayer_4 = torch.nn.MaxPool2d(2)\noutput_4 = layer_4(output_2)\nprint(output_4.shape) # 1,2,41,41 freaking bad.\nlayer_5 = torch.nn.Sigmoid()\noutput_5 = layer_5(output_4)\nprint(output_5.shape) # 1,2,41,41\noutput_5 = output_5.reshape(1,41,41)\n# get this reshaped.\noutput_5 = output_5.reshape(1,1,41*41)\nrnn_layer_1 = torch.nn.RNN(41*41,41*41,3) # must have three dimensions.\nrnn_output_1, rnn_hidd_1 = rnn_layer_1(output_5)\nprint(rnn_output_1.shape,rnn_hidd_1.shape) #tuple torch.Size([1, 41, 20]) torch.Size([3, 41, 20])\nrnn_output_2, rnn_hidd_2 = rnn_layer_1(output_5,rnn_hidd_1)\nprint(\"RNN 2:\",rnn_output_2.shape,rnn_hidd_2.shape)",
        "type": "code",
        "location": "/tests/video_script_generation_reconstruction/raw_data_cut.py:34-67"
    },
    "2759": {
        "file_id": 308,
        "content": "This code applies convolution, pooling, and activation functions to the input data. It reshapes the output for RNN processing with a specific structure, and performs two RNN operations.",
        "type": "comment"
    },
    "2760": {
        "file_id": 308,
        "content": "rnn_output_3, rnn_hidd_3 = rnn_layer_1(rnn_output_1,rnn_hidd_1)\nprint(\"RNN 3:\",rnn_output_3.shape,rnn_hidd_3.shape)\n# final_data = \nfinal_layer = torch.nn.Linear(41*41,2) # the final swap.\nfinal_data = final_layer(rnn_output_1)\nprint(final_data.shape)\n# find the max one.\nfinal_data = final_data.transpose(2,1)\nprint(final_data.shape)\n# output_final_layer = torch.nn.MaxPool1d(41) \n# final_data2 = output_final_layer(final_data)\n# print(final_data2.shape) # 40000,1 this is a single character. is it?",
        "type": "code",
        "location": "/tests/video_script_generation_reconstruction/raw_data_cut.py:69-81"
    },
    "2761": {
        "file_id": 308,
        "content": "This code applies an RNN layer, prints the shapes of output and hidden states, defines a final linear layer with 41x41 input size and 2 output sizes, passes RNN output through it, transposes the data, and suggests using MaxPool1d for possible character extraction.",
        "type": "comment"
    },
    "2762": {
        "file_id": 309,
        "content": "/tests/video_script_generation_reconstruction/lstm_trial.py",
        "type": "filepath"
    },
    "2763": {
        "file_id": 309,
        "content": "This code initializes a LSTM layer, passes a 3D input tensor through it, and prints the shapes of the input, output, and hidden states.",
        "type": "summary"
    },
    "2764": {
        "file_id": 309,
        "content": "from torch.nn import LSTM\nimport numpy as np\ndata = [[[1,2,3],[2,3,4],[3,5,6]]]\nfrom torch import Tensor\ndata = Tensor(data)\nlayer_lstm = LSTM(3,1)\noutput_1, (hid_1_a,hid_1_b) = layer_lstm(data)\n# print(len(hidden_1))\nprint(data.shape)\nprint(output_1.shape) # [1,3,10]\nprint(hid_1_a.shape,hid_1_b.shape)",
        "type": "code",
        "location": "/tests/video_script_generation_reconstruction/lstm_trial.py:1-17"
    },
    "2765": {
        "file_id": 309,
        "content": "This code initializes a LSTM layer, passes a 3D input tensor through it, and prints the shapes of the input, output, and hidden states.",
        "type": "comment"
    },
    "2766": {
        "file_id": 310,
        "content": "/tests/video_phash_deduplication/test_video_hash.py",
        "type": "filepath"
    },
    "2767": {
        "file_id": 310,
        "content": "The code defines `getVideoPHash` to calculate a video's phash using the `videohashes` tool, testing it by comparing pairwise differences between hash values for different videos and considering duplicates based on a threshold.",
        "type": "summary"
    },
    "2768": {
        "file_id": 310,
        "content": "# use some delogo stuff.\nfrom lazero.program.subprocess import runCommandGetJson\n# these two are similar. can be used as threshold.\n# aaaa3d8a2eaa1f8a delogo\n# aaaa398a2faa5d8a not delogoed.\n# aaaa3c8a2faa5e8a mp4 (very similar to delogoed version)\ndef getVideoPHash(filepath,debug=False, timeout=100):\n    import os\n    import imagehash\n    assert os.path.exists(filepath)\n    assert os.path.isfile(filepath)\n    if not os.path.isabs(filepath):\n        filepath = os.path.abspath(filepath)\n    commandLine = [\n        \"videohashes\", # installed in path.\n        # \"/root/Desktop/works/pyjom/tests/video_phash_deduplication/videohashes/videohashes-linux\",\n        \"-json\",\n        filepath,\n    ]\n    success, myJson = runCommandGetJson(commandLine, debug=debug, timeout=timeout)\n    if debug:\n        print(\"SUCCESS?\", success)\n        print(myJson, type(myJson))\n    if not success:\n        return\n    # breakpoint()\n    phashString = myJson[\"phash\"]\n    phash = imagehash.hex_to_hash(phashString)\n    if debug:\n        print(\"FILEPATH: %s\" % filepath)",
        "type": "code",
        "location": "/tests/video_phash_deduplication/test_video_hash.py:1-32"
    },
    "2769": {
        "file_id": 310,
        "content": "The code defines a function `getVideoPHash` that calculates a video's phash (a unique identifier for an image or video) using the `videohashes` command-line tool. It takes a filepath as input, checks if it exists and is a file, then runs the command to generate the JSON output. The function also converts the returned phash string to a binary hash and optionally prints debug information.",
        "type": "comment"
    },
    "2770": {
        "file_id": 310,
        "content": "        print(myJson)\n        print(\"PHASH:\", phash)\n    # if withDuration:\n    #     duration = myJson[\"duration\"]\n    #     return duration, phash\n    # duration is inaccurate\n    return phash\nif __name__ == \"__main__\":\n    videoPaths = [\n        \"cute_cat_gif.mp4\",\n        \"cute_cat_gif.gif\",\n        \"cat_delogo.gif\",\n        \"/root/Desktop/works/pyjom/samples/video/dog_with_large_text.gif\",\n    ]\n    hashs = [getVideoPHash(filepath,debug=True) for filepath in videoPaths]\n    dis0 = hashs[0] - hashs[1]  # small\n    dis1 = hashs[1] - hashs[2]  # big\n    dis2 = hashs[0] - hashs[2]  # big\n    dis3 = hashs[0] - hashs[3]  # big\n    print(dis0, dis1, dis2, dis3)\n    # 4 4 4\n    # strange. why?\n    # 4 4 4 42\n    # huge difference.\n    # what value do you decide to be duplicate?\n    # phash < 7 (really?)\n    # so how do we run this test?",
        "type": "code",
        "location": "/tests/video_phash_deduplication/test_video_hash.py:33-65"
    },
    "2771": {
        "file_id": 310,
        "content": "This code tests the video hashing function by calculating pairwise differences between hash values for different video files. It then compares the differences to determine potential duplicates and prints the results. The hash difference threshold for considering duplicates is set to 7, but this seems low and may need adjustment based on further testing.",
        "type": "comment"
    },
    "2772": {
        "file_id": 311,
        "content": "/tests/video_phash_deduplication/test_milvus_library.py",
        "type": "filepath"
    },
    "2773": {
        "file_id": 311,
        "content": "This code defines a Milvus function for connecting, managing collections, and caching. It creates Collections with specified data types, searches duplicated videos, retrieves video duration/hash, indexes videos, and reloads collection if necessary.",
        "type": "summary"
    },
    "2774": {
        "file_id": 311,
        "content": "# # duplicate -> remove, do not insert\n# # not duplicate -> get the data, insert\n# # you want to clear the collection after this run?\n# from functools import lru_cache\n# from pymilvus import connections\n# @lru_cache(maxsize=1)\n# def connectMilvusDatabase(alias=\"default\", host=\"localhost\", port=\"19530\"):\n#     connection = connections.connect(\n#         alias=alias, host=host, port=port\n#     )  # can we reconnect?\n#     print(\"milvus connected\")\n# # connectMilvusDatabase()\n# # connectMilvusDatabase() # will not connect again.\n# from pymilvus import Collection\n# from pymilvus import utility\n# from pymilvus import CollectionSchema, FieldSchema, DataType\n# import traceback\n# def getMilvusVideoDeduplicationCollection(\n#     get_existing: bool = False,\n# ):  # most of the time we just use the same\n#     collection_name = \"video_deduplication\"\n#     try:\n#         if utility.has_collection(collection_name):  # be prudent.\n#             if get_existing:\n#                 return Collection(collection_name)\n#             utility.drop_collection(collection_name)",
        "type": "code",
        "location": "/tests/video_phash_deduplication/test_milvus_library.py:1-35"
    },
    "2775": {
        "file_id": 311,
        "content": "The code defines a function to connect to a Milvus database, get or remove an existing collection named \"video_deduplication\", and returns the collection if it already exists. The function uses caching and checks if the collection already exists before performing any actions.",
        "type": "comment"
    },
    "2776": {
        "file_id": 311,
        "content": "#     except:\n#         traceback.print_exc()\n#         print(\"maybe the collection does not exist\")\n#     video_semantic_id = FieldSchema(  # how to insert this shit without prior knowledge?\n#         name=\"video_semantic_id\",\n#         dtype=DataType.INT64,\n#         is_primary=True,  # if is primary, will do check for 'not duplicate' or something.\n#         auto_id=True,  # no need for id generation.\n#     )\n#     video_length = FieldSchema(\n#         name=\"video_length\",\n#         dtype=DataType.FLOAT,\n#     )\n#     video_phash = FieldSchema(\n#         name=\"video_phash\", dtype=DataType.BINARY_VECTOR, dim=64\n#     )  # 64\n#     # single dimension? no multi dimension support?\n#     schema = CollectionSchema(\n#         fields=[video_semantic_id, video_length, video_phash],\n#         description=\"Test video deduplication\",\n#     )\n#     collection = Collection(\n#         name=collection_name,\n#         schema=schema,\n#         using=\"default\",\n#         shards_num=2,\n#     )\n#     # is this demo collection?\n#     return collection",
        "type": "code",
        "location": "/tests/video_phash_deduplication/test_milvus_library.py:36-65"
    },
    "2777": {
        "file_id": 311,
        "content": "This code defines a CollectionSchema and Collection for Milvus library. The schema contains fields for video_semantic_id, video_length, and video_phash, with their respective data types and properties. The Collection is created with a name, schema, database usage, and number of shards.",
        "type": "comment"
    },
    "2778": {
        "file_id": 311,
        "content": "# # seems hard to setup.\n# # not started!\n# # https://milvus.io/docs/v2.0.0/metric.md#binary\n# # the metric is important to us.\n# import numpy as np\n# import bitarray\n# @lru_cache(maxsize=1)\n# def transformVideoPhash(videoPhash):\n#     # we need the raw phash.\n#     queryData = videoPhash.hash  # videoPhashTruthTable8x8 or something\n#     queryData = queryData.reshape(-1).tolist()\n#     queryData = [\"1\" if x else \"0\" for x in queryData]\n#     queryData = bitarray.bitarray(\"\".join(queryData), endian=\"little\")\n#     queryData = queryData.tobytes()\n#     return queryData\n# # dimension: 8*8=64\n# def indexVideoWithVideoDurationAndPhash(collection, videoDuration, videoPhash):\n#     queryData = transformVideoPhash(videoPhash)\n#     collection.insert([[np.float32(videoDuration)], [queryData]])\n# # can release even if not loaded.\n# from test_video_hash import getVideoPHash\n# import caer\n# @lru_cache(maxsize=1)\n# def getVideoDurationAndPhashFromFile(videoFilePath):\n#     videoDuration = caer.video.frames_and_fps.get_duration(videoFilePath)",
        "type": "code",
        "location": "/tests/video_phash_deduplication/test_milvus_library.py:68-103"
    },
    "2779": {
        "file_id": 311,
        "content": "Function `transformVideoPhash` takes a video phash and converts it into a binary format for Milvus library indexing. Function `indexVideoWithVideoDurationAndPhash` inserts the video duration and transformed phash into the specified collection. The `getVideoDurationAndPhashFromFile` function retrieves the video duration and corresponding phash of a given video file using caer's video module. All functions are cached to avoid redundant computations.",
        "type": "comment"
    },
    "2780": {
        "file_id": 311,
        "content": "#     videoPhash = getVideoPHash(videoFilePath)\n#     return videoDuration, videoPhash\n# def indexVideoWithVideoDurationAndPhashFromFile(collection, videoFilePath):\n#     videoDuration, videoPhash = getVideoDurationAndPhashFromFile(videoFilePath)\n#     indexVideoWithVideoDurationAndPhash(collection, videoDuration, videoPhash)\n# def reloadMilvusCollection(collection):\n#     collection.release()  # unload.\n#     collection.load()\n# # make it into some library!\n# # insert after load?\n# # # 1,64\n# # what is wrong? wtf?\n# # queryData = queryData.tolist()\n# def getDistancesBySearchingDuplicatedVideoInMilvusByFile(\n#     collection,\n#     videoFilePath,\n#     search_params={\"metric_type\": \"Jaccard\", \"params\": {\"nprobe\": 10}},\n#     autoreload: bool = True,\n#     span: float = 2,\n#     debug: bool = False,\n#     limit: int = 10,\n# ):\n#     if autoreload:\n#         reloadMilvusCollection(collection)\n#     videoDuration, videoPhash = getVideoDurationAndPhashFromFile(videoFilePath)\n#     queryData = transformVideoPhash(videoPhash)",
        "type": "code",
        "location": "/tests/video_phash_deduplication/test_milvus_library.py:104-136"
    },
    "2781": {
        "file_id": 311,
        "content": "This code snippet defines a function for searching duplicated videos in Milvus by their file path, using Jaccard metric. It also includes functions to get video duration and hash from the file, index videos with duration and hash, and reload Milvus collection if necessary. The search parameters include metric type, probe count, span, limit, and whether to enable debug mode.",
        "type": "comment"
    },
    "2782": {
        "file_id": 311,
        "content": "#     minVideoLength = max(0, videoDuration - span)\n#     maxVideoLength = videoDuration + span\n#     results = collection.search(\n#         data=[queryData],  # this is the float dimension.\n#         anns_field=\"video_phash\",\n#         param=search_params,\n#         output_fields=[\"video_length\"],\n#         limit=limit,\n#         expr=\"video_length > {minVideoLength} and video_length < {maxVideoLength}\".format(\n#             minVideoLength=minVideoLength, maxVideoLength=maxVideoLength\n#         ),\n#     )\n#     theHit = results[0]\n#     # print(theHit)\n#     # so we can perform search without filtering afterwards.\n#     # results[0][0].entity.get('video_length')\n#     # print(results[0].ids)\n#     # now, we want to have the 'distance' parameter.\n#     # print(results[0])\n#     # print(theHit)\n#     distances = list(theHit.distances)\n#     if debug:\n#         print(\"distances: %s\" % distances)\n#     return distances\n#     # what is the distance? we need to try.\n#     # returh the closest distance?\n#     # results = [x for x in theHit]",
        "type": "code",
        "location": "/tests/video_phash_deduplication/test_milvus_library.py:137-164"
    },
    "2783": {
        "file_id": 311,
        "content": "This code searches for videos within a specified range of video length in the Milvus library. It sets minimum and maximum lengths based on the query's duration and span, and uses these values to filter results from the search. The closest distance between the query and each result is then returned.",
        "type": "comment"
    },
    "2784": {
        "file_id": 311,
        "content": "#     # hits = len(theHit)\n#     # breakpoint()\n#     # how to get document by id? wtf\n# def checkDuplicatedVideoAndInsertVector(\n#     collection,\n#     videoPath,\n#     threshold: float = 0.15,  # are you sure?\n#     insertDuplicatedVector: bool = True,\n#     debug: bool = True,\n# ):\n#     reloadMilvusCollection(collection)\n#     distances = getDistancesBySearchingDuplicatedVideoInMilvusByFile(\n#         collection, videoPath, debug=debug\n#     )\n#     minDistance = min(distances + [1])  # empty!\n#     duplicated = minDistance < threshold\n#     if insertDuplicatedVector or (not duplicated):\n#         indexVideoWithVideoDurationAndPhashFromFile(\n#             collection, videoPath\n#         )  # anyway let's do this.\n#     return duplicated\n# shall we insert that vector or not, even if we have detected the duplicated media?\n# you choose.\nimport sys\nimport os\n# os.chdir(\"../../\")\nsys.path.append(\"../../\")\n# ignore the global proxy now, we are not going to use that.\nos.environ[\"http_proxy\"] = \"\"\nos.environ[\"https_proxy\"] = \"\"",
        "type": "code",
        "location": "/tests/video_phash_deduplication/test_milvus_library.py:165-200"
    },
    "2785": {
        "file_id": 311,
        "content": "Function checkDuplicatedVideoAndInsertVector checks if a video file exists in Milvus collection and returns whether the video is duplicated or not. If insertDuplicatedVector is True, it indexes the video regardless of duplication status. The function uses getDistancesBySearchingDuplicatedVideoInMilvusByFile to find distances between the new video and existing videos in Milvus.",
        "type": "comment"
    },
    "2786": {
        "file_id": 311,
        "content": "from pyjom.videotoolbox import getMilvusVideoDeduplicationCollection,checkDuplicatedVideoAndInsertVector\nif __name__ == \"__main__\":\n    # connectMilvusDatabase()\n    collection = (\n        getMilvusVideoDeduplicationCollection()\n    )  # will not get existing collections\n    videoPaths = [\n        \"cute_cat_gif.mp4\",\n        \"cute_cat_gif.gif\",\n        \"cat_delogo.gif\",\n        \"/root/Desktop/works/pyjom/samples/video/dog_with_large_text.gif\",\n    ]\n    # for videoPath in videoPaths:\n    from lazero.utils.logger import sprint\n    for videoPath in videoPaths:\n        print(\"filepath: %s\" % videoPath)\n        duplicated = checkDuplicatedVideoAndInsertVector(collection, videoPath)\n        sprint(\"duplicated?\", duplicated)\n\"\"\"\nfilepath: cute_cat_gif.mp4\ndistances: [0.0, 0.11764705926179886, 0.11764705926179886, 0.7200000286102295, 0.7200000286102295, 0.7346938848495483, 0.7659574747085571, 0.7924528121948242]\n______________________________\nfilepath: cute_cat_gif.gif\ndistances: [0.0, 0.11764705926179886, 0.117647",
        "type": "code",
        "location": "/tests/video_phash_deduplication/test_milvus_library.py:201-226"
    },
    "2787": {
        "file_id": 311,
        "content": "The code connects to a Milvus database, retrieves the video deduplication collection, and checks if each given video path is already in the collection. It prints the file paths of the videos and whether they are duplicated or not using `checkDuplicatedVideoAndInsertVector` function from `lazero.utils.logger` module. The distances between the new video and existing ones in the database are also printed.",
        "type": "comment"
    },
    "2788": {
        "file_id": 311,
        "content": "05926179886, 0.7200000286102295, 0.7200000286102295, 0.7346938848495483, 0.7659574747085571, 0.7692307829856873]\n______________________________\nfilepath: cat_delogo.gif\ndistances: [0.0, 0.11764705926179886, 0.11764705926179886, 0.7200000286102295, 0.7200000286102295, 0.7346938848495483, 0.7659574747085571, 0.7924528121948242]\n______________________________\nfilepath: /root/Desktop/works/pyjom/samples/video/dog_with_large_text.gif\ndistances: [0.0, 0.6808510422706604, 0.6938775777816772, 0.6938775777816772, 0.739130437374115, 0.7692307829856873, 0.7924528121948242, 0.7924528121948242]\n______________________________\n\"\"\"",
        "type": "code",
        "location": "/tests/video_phash_deduplication/test_milvus_library.py:226-234"
    },
    "2789": {
        "file_id": 311,
        "content": "The code appears to be storing and comparing distances between video phashes for different files. Each line contains a file path followed by an array of distances, indicating the similarity of that video phash to other video phashes in the system. The lower the distance value, the more similar the videos are.",
        "type": "comment"
    },
    "2790": {
        "file_id": 312,
        "content": "/tests/video_phash_deduplication/test_milvus.py",
        "type": "filepath"
    },
    "2791": {
        "file_id": 312,
        "content": "The code demonstrates Milvus database operations, including creating a \"video\" collection, inserting data and performing searches. It is part of debugging process to retrieve documents by ID. The programmer is stuck and requires further investigation.",
        "type": "summary"
    },
    "2792": {
        "file_id": 312,
        "content": "# duplicate -> remove, do not insert\n# not duplicate -> get the data, insert\n# you want to clear the collection after this run?\n# import pymilvus\nfrom pymilvus import connections\nfrom functools import lru_cache\n@lru_cache(maxsize=1)\ndef connectMilvusDatabase(alias=\"default\", host=\"localhost\", port=\"19530\"):\n\tconnection = connections.connect(alias=alias, host=host, port=port)# can we reconnect?\n\tprint('milvus connected')\nconnectMilvusDatabase()\nconnectMilvusDatabase() # will not connect again.\ncollection_name = \"video_deduplication\"\nfrom pymilvus import Collection\n# Collection(collection_name)\n# remote this thing.\nfrom pymilvus import utility\ntry:\n    if utility.has_collection(collection_name):  # be prudent.\n        utility.drop_collection(collection_name)\nexcept:\n    import traceback\n    traceback.print_exc()\n    print(\"maybe the collection does not exist\")\nfrom pymilvus import CollectionSchema, FieldSchema, DataType\nvideo_semantic_id = FieldSchema(  # how to insert this shit without prior knowledge?\n    name=\"video_semantic_id\",",
        "type": "code",
        "location": "/tests/video_phash_deduplication/test_milvus.py:1-39"
    },
    "2793": {
        "file_id": 312,
        "content": "This code establishes a connection to a Milvus database, checks if the \"video_deduplication\" collection exists, and if so, removes it before creating a new one. The `connectMilvusDatabase` function sets up a connection with specified alias, host, and port (default values used in this code). The `utility.has_collection` and `utility.drop_collection` functions from the `pymilvus` utility module are used to check for and remove an existing collection named \"video_deduplication\". A `CollectionSchema` is defined for the new collection, specifying a field schema named \"video_semantic_id\".",
        "type": "comment"
    },
    "2794": {
        "file_id": 312,
        "content": "    dtype=DataType.INT64,\n    is_primary=True,  # if is primary, will do check for 'not duplicate' or something.\n    auto_id=True,  # no need for id generation.\n)\nvideo_length = FieldSchema(\n    name=\"video_length\",\n    dtype=DataType.FLOAT,\n)\nvideo_phash = FieldSchema(\n    name=\"video_phash\", dtype=DataType.BINARY_VECTOR, dim=64\n)  # 64\n# single dimension? no multi dimension support?\nschema = CollectionSchema(\n    fields=[video_semantic_id, video_length, video_phash],\n    description=\"Test video deduplication\",\n)\n# collection = Collection(\"video\")      # Get an existing collection.\ncollection = Collection(\n    name=collection_name,\n    schema=schema,\n    using=\"default\",\n    shards_num=2,\n)\n# is this demo collection?\n# seems hard to setup.\n# not started!\n# https://milvus.io/docs/v2.0.0/metric.md#binary\n# the metric is important to us.\nsearch_params = {\"metric_type\": \"Jaccard\", \"params\": {\"nprobe\": 10}}\nimport numpy as np\nqueryData = np.array(\n    [\n        [True, True, True, False, False, True, False, True],\n        [True, False, False, True, False, True, True, False],",
        "type": "code",
        "location": "/tests/video_phash_deduplication/test_milvus.py:40-76"
    },
    "2795": {
        "file_id": 312,
        "content": "This code is defining a schema for a Milvus collection, specifying the data types and field names. The schema includes video_semantic_id, video_length, and video_phash fields, which are used in video deduplication. The code creates a collection named \"video\" with 2 shards using the specified schema and sets the metric type for searching as Jaccard with nprobe parameter set to 10. It also imports numpy and creates queryData, which seems to be a binary vector.",
        "type": "comment"
    },
    "2796": {
        "file_id": 312,
        "content": "        [True, False, False, True, True, False, False, True],\n        [True, True, True, True, True, False, False, True],\n        [True, False, False, True, False, True, True, False],\n        [False, True, True, False, False, False, False, True],\n        [True, True, False, False, False, True, True, False],\n        [False, False, True, False, False, True, False, False],\n    ]\n)\nqueryData = queryData.reshape(-1).tolist()\nqueryData = [\"1\" if x else \"0\" for x in queryData]\nimport bitarray\nqueryData = bitarray.bitarray(\"\".join(queryData), endian=\"little\")\nqueryData2 = queryData.copy()\nqueryData2[1:4] = 0\nqueryData3 = queryData2.copy()\nqueryData2 = queryData2.tobytes()\nqueryData3[8:15] = 0\nqueryData3 = queryData3.tobytes()\nqueryData = queryData.tobytes()\n# dimension: 8*8=64\n# collection.insert([[1], [np.float32(3.5)], [queryData]])\n# collection.insert([[np.float32(3.5)], [queryData]])\n# for _ in range(8):\ncollection.insert([[np.float32(3.5)], [queryData]])\ncollection.insert([[np.float32(3.5)], [queryData2]])  # slight difference.",
        "type": "code",
        "location": "/tests/video_phash_deduplication/test_milvus.py:77-102"
    },
    "2797": {
        "file_id": 312,
        "content": "The code is preparing and inserting data into a Milvus collection. It creates binary representations of queryData, queryData2, and queryData3 (slightly different from queryData2), then inserts them along with a float value (np.float32(3.5)) into the collection, representing a 64-dimensional vector.",
        "type": "comment"
    },
    "2798": {
        "file_id": 312,
        "content": "collection.insert([[np.float32(3.5)], [queryData3]])  # more difference.\n# print(len(queryData), len(queryData)*8)\n# # print(queryData.shape)\n# breakpoint()\n# collection.load()\ncollection.insert([[np.float32(3.5)], [queryData]]) # still three.\n# can release even if not loaded.\ncollection.release() # unload.\ncollection.load()\n# make it into some library!\n# insert after load?\n# # 1,64\n# what is wrong? wtf?\n# queryData = queryData.tolist()\nresults = collection.search(\n    data=[queryData],  # this is the float dimension.\n    anns_field=\"video_phash\",\n    param=search_params,\n    output_fields=[\"video_length\"],\n    limit=10,\n    expr=\"video_length > 1.2 and video_length < 4\",\n    # expr='video_length < 1.2',\n)\ntheHit = results[0]\nprint(theHit)\n# so we can perform search without filtering afterwards.\n# results[0][0].entity.get('video_length')\n# print(results[0].ids)\n# now, we want to have the 'distance' parameter.\n# print(results[0])\n# print(theHit)\n# distances = theHit.distances\n# results = [x for x in theHit]\n# hits = len(theHit)",
        "type": "code",
        "location": "/tests/video_phash_deduplication/test_milvus.py:103-139"
    },
    "2799": {
        "file_id": 312,
        "content": "The code is inserting data into a collection, releasing and reloading it, performing a search based on specific parameters, and accessing the results. The purpose seems to be searching for video data within a database based on certain criteria, such as length, and extracting relevant information from the resulting hits.",
        "type": "comment"
    }
}