{
    "600": {
        "file_id": 42,
        "content": "                            import parse\n                            commandParams = parse.parse(\n                                keyword + \"_{x:d}_{y:d}_{w:d}_{h:d}\", renderCommand\n                            )\n                            # print(defaultWidth, defaultHeight)\n                            mX, mY, mW, mH = (\n                                commandParams[\"x\"],\n                                commandParams[\"y\"],\n                                commandParams[\"w\"],\n                                commandParams[\"h\"],\n                            )\n                            status, XYWH = checkXYWH(\n                                (mX, mY, mW, mH), (defaultWidth, defaultHeight)\n                            )\n                            if not status:\n                                # cannot process this delogo filter since its parameters are outraged.\n                                # shall we warn you?\n                                # print(\"SOMEHOW DELOGO IS NOT WORKING PROPERLY\")\n                                # breakpoint()",
        "type": "code",
        "location": "/pyjom/medialang/processors/dotProcessor/videoProcessor.py:299-318"
    },
    "601": {
        "file_id": 42,
        "content": "The code imports the \"parse\" module and parses a command string using a specific format. It then extracts x, y, w, and h values from the parsed command and checks if they match the default width and height. If not, it cannot process this delogo filter and may warn or raise an issue.",
        "type": "comment"
    },
    "602": {
        "file_id": 42,
        "content": "                                # maybe it's not because of out of bounds error\n                                print(\"_\" * 30)\n                                print(\n                                    \"ABNORMAL {} FILTER PARAMS:\".format(\n                                        keyword.upper()\n                                    ),\n                                    commandParams,\n                                )\n                                print(\n                                    \"maxX: {} maxY: {}\".format(\n                                        commandParams[\"x\"] + commandParams[\"w\"],\n                                        commandParams[\"y\"] + commandParams[\"h\"],\n                                    )\n                                )\n                                print(\"VALID BOUNDARIES:\", defaultWidth, defaultHeight)\n                                print(\"_\" * 30)\n                                continue\n                            else:\n                                (mX, mY, mW, mH) = XYWH",
        "type": "code",
        "location": "/pyjom/medialang/processors/dotProcessor/videoProcessor.py:319-337"
    },
    "603": {
        "file_id": 42,
        "content": "If out of bounds error occurs, it prints abnormal filter params and boundary information before continuing execution.",
        "type": "comment"
    },
    "604": {
        "file_id": 42,
        "content": "                                commandParams = {\"x\": mX, \"y\": mY, \"w\": mW, \"h\": mH}\n                            # mX1, mY1 = mX+mW, mY+mH\n                            # if mX1>defaultWidth or mY1>defaultHeight: # opecv to be blamed?\n                            #     print(\"DELOGO ERROR:\")\n                            #     print(mX1,defaultWidth,mY1,defaultHeight)\n                            #     breakpoint()\n                            # we also need to consider if this is necessary.\n                            if keyword == \"delogo\":\n                                stream = delogoFilter(stream, commandParams)\n                            elif keyword == \"crop\":\n                                stream = cropFilter(stream, commandParams)\n                                # TODO: the main shit happens here is that if pip region is detected, it (the crop region) will not maintain the width to height ratio. you might need padding, and that's what we about to do here. you may also extract that clip as standalone material.",
        "type": "code",
        "location": "/pyjom/medialang/processors/dotProcessor/videoProcessor.py:338-349"
    },
    "605": {
        "file_id": 42,
        "content": "This code checks if the keyword is \"delogo\" or \"crop\". If it's \"crop\", it applies a crop filter to the stream and checks if the pipeline region is detected. If so, it might need padding or could be extracted as standalone material.",
        "type": "comment"
    },
    "606": {
        "file_id": 42,
        "content": "                                # more inspection is needed for comprehensive reasoning.\n        if paddingBlur:\n            stream = paddingBlurFilter(\n                stream, mWidth=output_width, mHeight=output_height\n            )\n        else:\n            stream = paddingFilter(stream, mWidth=output_width, mHeight=output_height)\n        if preview:  # final filter? need us to crop this?\n            stream = previewFilter(\n                stream\n            )  # just preview, no need to set output width/height!\n            # do nothing here! (no fx.)\n        # and?\n        # we need to concat these shit!\n        # print(stream)\n        # print(dir(stream))\n        # breakpoint()\n        # import copy\n        # print(stream)\n        renderVideoStreamList.append(stream)\n    # for x in renderVideoStreamList:\n    #     print(x)\n    # print(len(renderVideoStreamList))\n    # breakpoint()\n    # breakpoint()\n    renderVideoStream = ffmpeg.concat(*renderVideoStreamList)\n    # detect if there is really anything audio related!",
        "type": "code",
        "location": "/pyjom/medialang/processors/dotProcessor/videoProcessor.py:350-376"
    },
    "607": {
        "file_id": 42,
        "content": "The code is processing video streams with optional padding, blur, and preview filters. The resulting streams are concatenated into a single video using the ffmpeg library.",
        "type": "comment"
    },
    "608": {
        "file_id": 42,
        "content": "    if audio:\n        renderStream = ffmpeg.output(renderVideoStream, renderAudioStream, cachePath)\n    else:\n        renderStream = ffmpeg.output(renderVideoStream, cachePath)\n    # DEBUG #\n    # args = renderStream.get_args()\n    # print(args)\n    # breakpoint()\n    # DEBUG #\n    renderStream.run(overwrite_output=True)\n    return cachePath\ndef dotVideoProcessor(\n    item, previous, format=None, verbose=True, medialangTmpDir=\"/dev/shm/medialang/\"\n):\n    # print(\"DOTVIDEO ARGS:\", item, previous, format)\n    # this item is the video output config, medialang item.\n    itemArgs = item.args\n    if format is None:\n        format = item.path.split(\".\")[-1]\n    backend = itemArgs.get(\n        \"backend\", \"editly\"  # this is mere assumption!\n    )  # so all things will be assumed to put directly into editly render json, unless explicitly specified under other medialang or other backend and need to be resolved into media file format before rendering. sure?\n    fast = itemArgs.get(\"fast\", True)\n    bgm = itemArgs.get(\"bgm\", None)",
        "type": "code",
        "location": "/pyjom/medialang/processors/dotProcessor/videoProcessor.py:377-402"
    },
    "609": {
        "file_id": 42,
        "content": "The code is checking if there's an audio stream present. If so, it combines the video and audio streams using ffmpeg. Otherwise, it only processes the video stream. The DEBUG section is for debugging purposes and logs arguments of the rendered stream. Finally, the code runs the rendering process and returns the cache path. This function aims to generate a video output configuration from medialang item, assuming the backend as \"editly\" if not specified.",
        "type": "comment"
    },
    "610": {
        "file_id": 42,
        "content": "    # outputPath = itemArgs.get(\"\",None)\n    randomUUID = str(uuid.uuid4())\n    outputPath = os.path.join(\n        medialangTmpDir, randomUUID + \".\" + format\n    )  # this is temporary!\n    # usually we choose to use something under medialang tempdir as the storage place.\n    print(\"medialang config:\", format, backend, fast, bgm)\n    # the \"previous\" is the clips, was fucked, filled with non-existant intermediate mpegts files, but no source was out there.\n    # this is initially decided to output mp4, however you might want to decorate it.\n    if verbose:\n        print(\"_________INSIDE DOT VIDEO PROCESSOR_________\")\n        print(\"ITEM:\", item)\n        print(\"PREVIOUS:\", previous)\n        print(\"_________INSIDE DOT VIDEO PROCESSOR_________\")\n    with tempfile.TemporaryDirectory(\n        dir=medialangTmpDir\n    ) as tmpdirname:  # maybe you should take care of the directory prefix?\n        # wtf are you doing over here?\n        # find out where our cache leads to!\n        # maybe the final product is one move away.",
        "type": "code",
        "location": "/pyjom/medialang/processors/dotProcessor/videoProcessor.py:403-423"
    },
    "611": {
        "file_id": 42,
        "content": "This code snippet generates a random UUID and stores the video temporarily in a specified directory. It ensures that the output is under medialang's temporary directory, and the verbose parameter allows printing details about the item, previous clip, and other relevant information. The code also utilizes tempfile.TemporaryDirectory to manage the temporary directory.",
        "type": "comment"
    },
    "612": {
        "file_id": 42,
        "content": "        tmpdirname = os.path.abspath(tmpdirname)\n        print(\"created temporary directory\", tmpdirname)\n        output_path = os.path.join(\n            tmpdirname, randomUUID + \".\" + format\n        )  # this is temporary!\n        # that is the tweak. we have successfully changed the directory!\n        if backend == \"editly\":\n            # iterate through all items.\n            template = {\n                \"width\": 1920,\n                \"height\": 1080,\n                \"fast\": fast,\n                \"fps\": 60,\n                \"outPath\": output_path,\n                \"defaults\": {\"transition\": None},\n                \"clips\": [],\n            }\n            if bgm is not None:\n                template.update({\"audioFilePath\": bgm})\n            for elem in previous:\n                duration = 3  # default duration\n                clip = {\n                    \"duration\": duration,\n                    \"layers\": [],\n                }\n                layer_durations = []\n                for layerElem in elem:\n                    layer = None",
        "type": "code",
        "location": "/pyjom/medialang/processors/dotProcessor/videoProcessor.py:424-452"
    },
    "613": {
        "file_id": 42,
        "content": "The code creates a temporary directory, generates a unique output path for the edited video file, and sets up a template for processing using Editly backend. The template includes parameters such as width, height, fast mode, fps, output path, audio file path (if provided), duration of each clip, and layers in each clip. A default transition is also included.",
        "type": "comment"
    },
    "614": {
        "file_id": 42,
        "content": "                    # print(layerElem) # {\"item\":<item>, \"cache\": <cache_path>}\n                    cachePath = layerElem[\"cache\"]\n                    # breakpoint()\n                    layerElemItem = layerElem[\"item\"]\n                    filepath = layerElemItem.path\n                    # what type is this damn media?\n                    filetype = getFileType(filepath)\n                    if layerElemItem.args.get(\"backend\", \"editly\") == \"editly\":\n                        if filetype == \"video\":\n                            videoInfo = get_media_info(filepath)\n                            endOfVideo = videoInfo[\"duration\"]\n                            cutFrom = layerElemItem.args.get(\"cutFrom\", 0)\n                            cutTo = layerElemItem.args.get(\"cutTo\", endOfVideo)\n                            layerOriginalDuration = cutTo - cutFrom\n                            mute = layerElemItem.args.get(\"slient\", False)\n                            processedFilePath = ffmpegVideoPreProductionFilter(\n                                filepath,",
        "type": "code",
        "location": "/pyjom/medialang/processors/dotProcessor/videoProcessor.py:453-471"
    },
    "615": {
        "file_id": 42,
        "content": "This code segment is responsible for processing video layers in a media project. It first retrieves the cache path and filepath from the input layer element, then determines the file type using getFileType function. If the backend is set to \"editly\" and the file type is a video, it extracts information about the video's duration and any specified cut range within the layer element. Finally, it applies ffmpegVideoPreProductionFilter to process the video according to the specified parameters.",
        "type": "comment"
    },
    "616": {
        "file_id": 42,
        "content": "                                start=cutFrom,\n                                end=cutTo,\n                                cachePath=cachePath,\n                                preview=fast,\n                                audio=not mute,\n                            )\n                            # what is this filepath? man how do i handle this?\n                            videoFilePath = processedFilePath\n                            # get video information!\n                            # if processed:\n                            # this must be true now.\n                            cutFrom = 0\n                            cutTo = layerOriginalDuration\n                            speed = layerElemItem.args.get(\"speed\", 1)\n                            # was wrong.\n                            layerDuration = (cutTo - cutFrom) / speed\n                            layer_durations.append(layerDuration)\n                            layer = {\n                                \"type\": \"video\",\n                                \"path\": videoFilePath,",
        "type": "code",
        "location": "/pyjom/medialang/processors/dotProcessor/videoProcessor.py:472-492"
    },
    "617": {
        "file_id": 42,
        "content": "This code is processing a video file and saving it to a specified path. It sets the start and end time of the clip, determines the speed of playback, and appends the duration of the layer to a list. The processed video file's path is stored in \"videoFilePath\".",
        "type": "comment"
    },
    "618": {
        "file_id": 42,
        "content": "                                \"resizeMode\": \"contain\",\n                                \"cutFrom\": cutFrom,\n                                \"cutTo\": cutTo,\n                                # that's how we mute it.\n                                \"mixVolume\": 1 - int(mute),\n                            }\n                            removeKeys = []\n                            for key, elem in layer.items():\n                                if elem is None:\n                                    removeKeys.append(key)\n                            for key in removeKeys:\n                                del layer[key]\n                    if layer is not None:\n                        clip[\"layers\"].append(layer)\n                    else:\n                        raise Exception(\"NOT IMPLEMENTED LAYER FORMAT:\", layerElem)\n                maxDuration = max(layer_durations)\n                clip[\"duration\"] = maxDuration\n                template[\"clips\"].append(clip)\n                # then just execute this template, or let's just view it.",
        "type": "code",
        "location": "/pyjom/medialang/processors/dotProcessor/videoProcessor.py:493-512"
    },
    "619": {
        "file_id": 42,
        "content": "The code is modifying a clip's properties, such as mute and resize mode, based on input parameters like cutFrom and cutTo. It also handles None values in the layer dictionary by removing them. The code then sets the maximum duration of all layers, adds the modified clip to the template, and finally executes or views the template.",
        "type": "comment"
    },
    "620": {
        "file_id": 42,
        "content": "            if verbose:\n                print(\"________________editly template________________\")\n                print(\n                    json.dumps(template, ensure_ascii=False, indent=4)\n                )  # let's view it elsewhere? or in `less`?\n                print(\"________________editly template________________\")\n            # breakpoint()\n            # return template\n            executeEditlyScript(medialangTmpDir, template)\n            os.rename(output_path, outputPath)\n            return outputPath",
        "type": "code",
        "location": "/pyjom/medialang/processors/dotProcessor/videoProcessor.py:513-523"
    },
    "621": {
        "file_id": 42,
        "content": "This code snippet checks if verbose is true and prints the editly template in a formatted way. It then executes the Editly script, renames the output file from `output_path` to `outputPath`, and finally returns the output path.",
        "type": "comment"
    },
    "622": {
        "file_id": 43,
        "content": "/pyjom/medialang/processors/dotProcessor/jsonProcessor.py",
        "type": "filepath"
    },
    "623": {
        "file_id": 43,
        "content": "This function dotJsonProcessor takes an item, a previous item, and optional verbose and medialangTmpDir parameters. It extracts the processor name from the item's arguments, gets the corresponding Medialang function using getMedialangFunction, checks if it exists or not, and then applies the processor on the previous item using keywordDecorator. The output of this processing is returned.",
        "type": "summary"
    },
    "624": {
        "file_id": 43,
        "content": "from pyjom.medialang.functions import *\nfrom pyjom.medialang.commons import *\ndef dotJsonProcessor(item, previous, verbose=True, medialangTmpDir=\"/dev/shm/medialang/\"):\n    # must contain something.\n    args = item.args\n    processorName = args[\"processor\"]\n    processor = getMedialangFunction(processorName)\n    if processor is None:\n        medialangFatalError(\"processor {} not found.\".format(processorName), __file__)\n    print(\"Using JSON processor:\", processorName)\n    args.pop(\"processor\")\n    # breakpoint()\n    output = keywordDecorator(processor, **args)(previous)  # what is this shit?\n    return output",
        "type": "code",
        "location": "/pyjom/medialang/processors/dotProcessor/jsonProcessor.py:1-16"
    },
    "625": {
        "file_id": 43,
        "content": "This function dotJsonProcessor takes an item, a previous item, and optional verbose and medialangTmpDir parameters. It extracts the processor name from the item's arguments, gets the corresponding Medialang function using getMedialangFunction, checks if it exists or not, and then applies the processor on the previous item using keywordDecorator. The output of this processing is returned.",
        "type": "comment"
    },
    "626": {
        "file_id": 44,
        "content": "/pyjom/medialang/processors/dotProcessor/__init__.py",
        "type": "filepath"
    },
    "627": {
        "file_id": 44,
        "content": "The code imports necessary classes from jsonProcessor and videoProcessor modules, then creates a dictionary called dotProcessors, which maps file extensions to their respective processors. The .json extension is associated with the dotJsonProcessor, while .mp4 is associated with the keywordDecorator-decorated dotVideoProcessor.",
        "type": "summary"
    },
    "628": {
        "file_id": 44,
        "content": "from pyjom.medialang.processors.dotProcessor.jsonProcessor import *\nfrom pyjom.medialang.processors.dotProcessor.videoProcessor import *\nfrom pyjom.commons import keywordDecorator\ndotProcessors = {\".json\":dotJsonProcessor, \".mp4\": keywordDecorator(dotVideoProcessor,format=\"mp4\")}",
        "type": "code",
        "location": "/pyjom/medialang/processors/dotProcessor/__init__.py:1-5"
    },
    "629": {
        "file_id": 44,
        "content": "The code imports necessary classes from jsonProcessor and videoProcessor modules, then creates a dictionary called dotProcessors, which maps file extensions to their respective processors. The .json extension is associated with the dotJsonProcessor, while .mp4 is associated with the keywordDecorator-decorated dotVideoProcessor.",
        "type": "comment"
    },
    "630": {
        "file_id": 45,
        "content": "/pyjom/medialang/functions/__init__.py",
        "type": "filepath"
    },
    "631": {
        "file_id": 45,
        "content": "The code imports detectors from the medialangFunctions module and creates a dictionary of available media language functions. The getMedialangFunction function searches the dictionary for the specified function name, returns it if found, and prints its type and name. If not found, it returns None.",
        "type": "summary"
    },
    "632": {
        "file_id": 45,
        "content": "from pyjom.medialang.functions.detectors import *\nmedialangFunctions = {\"detector\": medialangDetectors}\ndef getMedialangFunction(function):\n    for key in medialangFunctions:\n        mgroup = medialangFunctions[key]\n        for key2 in mgroup:\n            if key2 == function:\n                function = mgroup[key2]\n                print(\"function type:\", key)\n                print(\"function name:\", key2)\n                return function\n    return None",
        "type": "code",
        "location": "/pyjom/medialang/functions/__init__.py:1-15"
    },
    "633": {
        "file_id": 45,
        "content": "The code imports detectors from the medialangFunctions module and creates a dictionary of available media language functions. The getMedialangFunction function searches the dictionary for the specified function name, returns it if found, and prints its type and name. If not found, it returns None.",
        "type": "comment"
    },
    "634": {
        "file_id": 46,
        "content": "/pyjom/medialang/functions/detectors/yolov5_Detector.py",
        "type": "filepath"
    },
    "635": {
        "file_id": 46,
        "content": "The code introduces two functions, `yolov5_Identifier` and `yolov5_Detector`, which utilize the YOLOv5 model for object detection and identification. The functions apply the model to input frames and media paths at set intervals, storing and returning results with specified configurations.",
        "type": "summary"
    },
    "636": {
        "file_id": 46,
        "content": "from .mediaDetector import *\n# assume you not to run many instances at once?\n# how to identify same video in a sequence?\ndef yolov5_Identifier(frame, threshold=0.4,model = \"yolov5s\"):\n    model = configYolov5(model=model)\n    # assert to be read from opencv2\n    img = cv2_HWC2CHW(frame)\n    results = model(img) # pass the image through our model\n    df = results.pandas().xyxy[0]\n    # print(df)\n    data = []\n    for _, line in df.iterrows():\n        left = (line[\"xmin\"],line[\"ymin\"])\n        right = (line[\"xmax\"],line[\"ymax\"])\n        confidence = line[\"confidence\"]\n        if confidence < threshold:\n            continue # skipping threshold too low.\n        class_ = line[\"class\"]\n        name = line[\"name\"]\n        data.append({\"location\":[left,right],\"confidence\":confidence,\"identity\":{\"class\":class_,\"name\":name}})\n    return data\ndef yolov5_Detector(mediapaths, model=\"yolov5s\", threshold=0.4, timestep=0.2):\n    # any better detectors? deeplearning?\n    results = []\n    data_key = \"yolov5\"\n    assert model i",
        "type": "code",
        "location": "/pyjom/medialang/functions/detectors/yolov5_Detector.py:1-31"
    },
    "637": {
        "file_id": 46,
        "content": "The code defines two functions, `yolov5_Identifier` and `yolov5_Detector`, which are used for object detection and identification using the YOLOv5 model. The `yolov5_Identifier` function takes a frame as input, applies the YOLOv5 model to it, and returns the identified objects with their locations, confidences, and classes. The `yolov5_Detector` function applies the `yolov5_Identifier` function to multiple media paths at specified time intervals, storing and returning the detection results for each path.",
        "type": "comment"
    },
    "638": {
        "file_id": 46,
        "content": "n [\"yolov5n\",\"yolov5s\",\"yolov5m\",\"yolov5l\",\"yolov5x\",\"yolov5n6\",\"yolov5s6\",\"yolov5m6\",\"yolov5l6\",\"yolov5x6\"] # increase the parameters does not sufficiently improve accuracy.\n    keyword = \"{}_detector\".format(data_key)\n    for mediapath in mediapaths:\n        print(\"mediapath:\", mediapath)\n        mediatype = getFileType(mediapath)\n        print(\"subtitle of mediatype:\", mediatype)\n        assert mediatype in [\"video\", \"image\"]  # gif? anything like that?\n        result = {\"type\": mediatype, data_key: {}}\n        config = {\"threshold\": threshold,\"model\":model}\n        if mediatype == \"image\":\n            data = cv2.imread(mediapath)\n            data = keywordDecorator(yolov5_Identifier, **config)(data)\n            result[data_key].update({keyword: data})\n            result[data_key].update({\"config\": config})\n            # results.append(result)\n        else:\n            mdata, metadata = videoFrameIterator(\n                mediapath,\n                data_producer=keywordDecorator(yolov5_Identifier, **config),",
        "type": "code",
        "location": "/pyjom/medialang/functions/detectors/yolov5_Detector.py:31-50"
    },
    "639": {
        "file_id": 46,
        "content": "Looping through each media path, the code checks the media type (video or image) and applies YOLOv5 detection using keywordDecorator on the specified data_key. It stores the result in a dictionary and also includes the configuration used for detection.",
        "type": "comment"
    },
    "640": {
        "file_id": 46,
        "content": "                framebatch=1,\n                timestep=timestep,\n                keyword=keyword,\n            )\n            metadata.update({\"config\": config})\n            result[data_key][keyword] = mdata\n            result[data_key].update(metadata)\n        results.append(result)\n    return results",
        "type": "code",
        "location": "/pyjom/medialang/functions/detectors/yolov5_Detector.py:51-59"
    },
    "641": {
        "file_id": 46,
        "content": "Function is creating a data structure for YOLOv5 detector output, updating metadata with the configuration and storing it in a list.",
        "type": "comment"
    },
    "642": {
        "file_id": 47,
        "content": "/pyjom/medialang/functions/detectors/videoDiffDetector.py",
        "type": "filepath"
    },
    "643": {
        "file_id": 47,
        "content": "The code includes a `frameDifferential` function that computes the average or maximum difference between frames, and a `videoDiffDetector` function which calculates pixel values for each block in a video. The results are stored in a dictionary and appended to a list of results after updating metadata.",
        "type": "summary"
    },
    "644": {
        "file_id": 47,
        "content": "from .mediaDetector import *\ndef frameDifferential(frame_a, frame_b, cut=3, absolute=True, method=\"average\"):\n    assert cut >= 1\n    # calculate average difference.\n    # you can select ROI instead.\n    # the cut is generated by the smallest side. neglect the boundary.\n    mshape = frame_a.shape\n    width, height = mshape[:2]\n    mcut = int(min(width, height) / cut)\n    result = frame_a - frame_b\n    methods = {\"average\": np.average, \"max\": np.max, \"min\": np.min}\n    # it is hard to tell where the heck does the target go. since the color difference means nothing precisely.\n    # maybe you should mark the target for us? for our training model?\n    # and again use our superduper unet? you know sometimes we get static.\n    # so use both inputs. one for static and one for motion.\n    if absolute:\n        result = np.abs(result)\n    if len(mshape) == 3:\n        result = methods[method](result, axis=2)  # just np.max\n        # i guess it is about the max value not the unified.\n    shape0 = int(width / mcut)\n    shape1 = int(height / mcut)",
        "type": "code",
        "location": "/pyjom/medialang/functions/detectors/videoDiffDetector.py:1-24"
    },
    "645": {
        "file_id": 47,
        "content": "This code defines a function called `frameDifferential` that calculates the average difference between two frames. It takes in two frame images, a cut value to determine the size of each block, and an optional absolute parameter. The code calculates the average or maximum difference within each block, with the option to take the absolute value if necessary. The result is returned as a new image with the same shape as the input frames.",
        "type": "comment"
    },
    "646": {
        "file_id": 47,
        "content": "    diff = np.zeros((shape0, shape1)).tolist()\n    # mapping = {}\n    for x in range(shape0):\n        for y in range(shape1):\n            diff[x][y] = float(\n                np.average(result[x * mcut : (x + 1) * mcut, y * mcut : (y + 1) * mcut])\n            )\n            # this mapping is bad.\n            # mapping.update({str((x,y)):((x*mcut,(x+1)*mcut),(y*mcut,(y+1)*mcut))})\n    return {\"diff\": diff, \"blocksize\": mcut}  # required for recovering center points.\n    # transform the frames into smaller matricies.\n    # not required all the time though.\ndef videoDiffDetector(mediapaths, cut=3, absolute=True, method=\"average\", timestep=0.2):\n    # any better detectors? deeplearning?\n    results = []\n    data_key = \"diff_result\"\n    for mediapath in mediapaths:\n        print(\"mediapath:\", mediapath)\n        mediatype = getFileType(mediapath)\n        print(\"subtitle of mediatype:\", mediatype)\n        assert mediatype in [\"video\"]  # gif? anything like that?\n        result = {\"type\": mediatype, data_key: {}}\n        config = {\"cut\": cut, \"absolute\": absolute, \"method\": method}",
        "type": "code",
        "location": "/pyjom/medialang/functions/detectors/videoDiffDetector.py:25-49"
    },
    "647": {
        "file_id": 47,
        "content": "Code snippet defines a videoDiffDetector function that takes mediapaths as input, iterates through each path, and calculates the average pixel values of frames within each block of size mcut. The results are stored in a dictionary and returned along with the block size for recovering center points later on if needed.",
        "type": "comment"
    },
    "648": {
        "file_id": 47,
        "content": "        keyword = \"frame_differential\"\n        mdata, metadata = videoFrameIterator(\n            mediapath,\n            data_producer=keywordDecorator(frameDifferential, **config),\n            framebatch=2,\n            timestep=timestep,\n            keyword=keyword,\n        )\n        metadata.update({\"config\": config})\n        result[data_key][keyword] = mdata\n        result[data_key].update(metadata)\n        results.append(result)\n    return results",
        "type": "code",
        "location": "/pyjom/medialang/functions/detectors/videoDiffDetector.py:50-62"
    },
    "649": {
        "file_id": 47,
        "content": "This code calls a function to iterate over frames in a video using frame differential as the data producer, then updates metadata and appends the result to a list of results.",
        "type": "comment"
    },
    "650": {
        "file_id": 48,
        "content": "/pyjom/medialang/functions/detectors/subtitleDetector.py",
        "type": "filepath"
    },
    "651": {
        "file_id": 48,
        "content": "The `mediaSubtitleDetector` function uses PaddleOCR to detect subtitles in videos or images, processing media paths and storing results under 'subtitle_result'. It alters and updates metadata before appending the altered results to a list and returning them.",
        "type": "summary"
    },
    "652": {
        "file_id": 48,
        "content": "from .mediaDetector import *\nfrom .entityDetector import ocrEntityDetector\ndef getPaddleOCR(mediapath, lang=\"ch\",use_angle_cls=True,cls=True,rec=True):\n    ocr = configOCR(use_angle_cls=use_angle_cls,cls=cls,rec=rec, lang=lang)\n    # print(mediapath)\n    # breakpoint()\n    result = ocr.ocr(mediapath,cls=cls,rec=rec)\n    # print(result)\n    # breakpoint()\n    return result\ndef stablePaddleOCR(mediapath, lang=\"ch\"):\n    data = getPaddleOCR(mediapath, lang=lang)\n    for ind, element in enumerate(data):\n        certainty = element[1][1]\n        # print(\"certainty:\",certainty)\n        data[ind][1] = (element[1][0], float(certainty))  # fix the float32 error.\n        # what is the fetched shit anyway?\n    return data\ndef mediaSubtitleDetector(\n    mediapaths,\n    videocr=False,\n    timestep=0.5,\n    videocr_config={\"lang\": \"chi_sim+eng\", \"sim_threshold\": 70, \"conf_threshold\": 65},\n):\n    # it must be video/image.\n    # we detect shit not to remove shit.\n    # this is separated.\n    results = []\n    data_key = \"subtitle_result\"",
        "type": "code",
        "location": "/pyjom/medialang/functions/detectors/subtitleDetector.py:1-34"
    },
    "653": {
        "file_id": 48,
        "content": "Code snippet defines a `mediaSubtitleDetector` function that detects subtitles in videos or images. It utilizes the `getPaddleOCR` and `stablePaddleOCR` functions for OCR operations. The `getPaddleOCR` function takes media path, language, use_angle_cls, cls, and rec as input to configure OCR, perform OCR on the image, and return the result. The `stablePaddleOCR` function improves certainty values in the results and returns them. The `mediaSubtitleDetector` function processes a list of media paths, performs subtitle detection, and stores the results in the 'subtitle_result' key.",
        "type": "comment"
    },
    "654": {
        "file_id": 48,
        "content": "    for mediapath in mediapaths:\n        print(\"mediapath:\", mediapath)\n        mediatype = getFileType(mediapath)\n        print(\"subtitle of mediatype:\", mediatype)\n        assert mediatype in [\"video\", \"image\"]\n        result = {\"type\": mediatype, data_key: {}}\n        if mediatype == \"image\":\n            data = getPaddleOCR(mediapath)\n            result[data_key].update({\"paddleocr\": data})\n            # each line per sentence, coordinates.\n        else:\n            if videocr:\n                config = videocr_config\n                data = get_subtitles(\n                    mediapath, **config\n                )  # what is the speed of this? also the quality?\n                data = srt.parse(data)\n                data = [serializeSRT(x) for x in data]\n                result[data_key].update({\"videocr\": data})\n                result[data_key][\"config\"] = config\n            else:\n                keyword = \"paddleocr\" # we will try to merge alike ones.\n                mdata, metadata = videoFrameIterator(\n                    mediapath,",
        "type": "code",
        "location": "/pyjom/medialang/functions/detectors/subtitleDetector.py:35-58"
    },
    "655": {
        "file_id": 48,
        "content": "The code iterates through a list of media paths, checks the file type, and then fetches subtitles or performs OCR based on the file type. It uses PaddleOCR for image files and either the provided videocr or a default config for video files. The speed and quality of the subtitle detection are not mentioned in the code.",
        "type": "comment"
    },
    "656": {
        "file_id": 48,
        "content": "                    data_producer=stablePaddleOCR, \n                    timestep=timestep,\n                    keyword=keyword,\n                )\n                # we should process the mdata. alter it and change it.\n                # mdata = ocrEntityDetector(mdata) # we enable this step later.\n                result[data_key][keyword] = mdata\n                result[data_key].update(metadata)\n                # what is this frame?\n            # use traditional things.\n        results.append(result)\n    return results",
        "type": "code",
        "location": "/pyjom/medialang/functions/detectors/subtitleDetector.py:59-70"
    },
    "657": {
        "file_id": 48,
        "content": "The code processes media data using stablePaddleOCR, potentially alters and changes mdata, and updates metadata for the result. The altered results are then appended to a list before being returned.",
        "type": "comment"
    },
    "658": {
        "file_id": 49,
        "content": "/pyjom/medialang/functions/detectors/mediaDetector.py",
        "type": "filepath"
    },
    "659": {
        "file_id": 49,
        "content": "The videoFrameIterator function initializes a video capture object, iterates through frames, checks arguments, raises exceptions, and reads frames using paddleocr. It stores metadata per batch, stops at no detection or end of video, and returns results in `mdata` and `metadata`.",
        "type": "summary"
    },
    "660": {
        "file_id": 49,
        "content": "from pyjom.medialang.commons import *\nimport cv2\n# import numpy.core.multiarray # caused by numpy version errors. upgrade to resolve.\nfrom videocr import get_subtitles  # are you sure?\nimport srt\nimport progressbar\ndef videoFrameIterator(\n    mediapath, timestep=0.5, framebatch=1, data_producer=None, keyword=None\n):\n    assert data_producer is not None\n    assert type(keyword) == str\n    assert type(framebatch) == int\n    assert framebatch >= 1\n    # obviously not for motion detection, if i was saying.\n    cap = cv2.VideoCapture(mediapath)\n    mdata = []\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    frames = cap.get(cv2.CAP_PROP_FRAME_COUNT)  # no need of this to get the timecode.\n    frames = int(frames)\n    # timestep = timestep # unit in seconds.\n    if timestep != None:\n        frameStep = int(fps * timestep)\n    else:\n        frameStep = 1 # for None we do frame by frame analysis.\n        timestep = 1/fps # generate fake timestep. nothing special.\n    assert frameStep > 0\n    frameIndex = 0\n    # while(cap.isOpened()):",
        "type": "code",
        "location": "/pyjom/medialang/functions/detectors/mediaDetector.py:1-31"
    },
    "661": {
        "file_id": 49,
        "content": "This function, videoFrameIterator, initializes a video capture object and iterates through the frames of a video file while maintaining a timestep. The timestep determines how many frames to skip between captures and can be adjusted based on user needs. It also includes checks for proper arguments and raises exceptions if any are not met.",
        "type": "comment"
    },
    "662": {
        "file_id": 49,
        "content": "    stepframes = []\n    for _ in progressbar.progressbar(range(frames)):\n        ret, frame = cap.read()\n        if type(frame) != np.ndarray:\n            # most likely no thing shown.\n            break\n        timecode = float(frameIndex / fps)\n        if (frameIndex % frameStep) == 0:\n            # what is this shit?\n            # print(\"frame:\",type(frame))\n            # will be replaced!\n            stepframes.append(copy.deepcopy(frame))\n            if len(stepframes) == framebatch:\n                data = data_producer(\n                    *stepframes\n                )  # usually we treat frames differently?\n                stepframes.pop(0)\n                # this is part of paddleocr.\n                mdata.append(\n                    {\n                        \"time\": timecode,\n                        \"frame\": frameIndex,\n                        keyword: copy.deepcopy(data),\n                    }\n                )\n        frameIndex += 1\n    # result[\"subtitle_result\"][\"paddleocr\"] = mdata\n    cap.release()\n    metadata = {\"fps\": float(fps), \"timestep\": timestep, \"framebatch\": framebatch}",
        "type": "code",
        "location": "/pyjom/medialang/functions/detectors/mediaDetector.py:32-60"
    },
    "663": {
        "file_id": 49,
        "content": "This code reads frames from a video and processes them in batches using the paddleocr library. It stores metadata such as frame index, timecode, and resulting data for each processed batch. The loop stops when no object is detected or it reaches the end of the video. Finally, the captured metadata and the result are returned.",
        "type": "comment"
    },
    "664": {
        "file_id": 49,
        "content": "    return mdata, metadata",
        "type": "code",
        "location": "/pyjom/medialang/functions/detectors/mediaDetector.py:61-61"
    },
    "665": {
        "file_id": 49,
        "content": "This line of code is returning two variables, `mdata` and `metadata`, after the function's processing.",
        "type": "comment"
    },
    "666": {
        "file_id": 50,
        "content": "/pyjom/medialang/functions/detectors/frameborder_Detector.py",
        "type": "filepath"
    },
    "667": {
        "file_id": 50,
        "content": "The code detects squares in images using blurring, edge detection, and HoughLines for lines. It processes video frames for object detection, OCR adjustments, and removes premature rectangles while applying theta filtering. It performs calculations, checks rectangles, and stores data using background models and contours to detect frame changes.",
        "type": "summary"
    },
    "668": {
        "file_id": 50,
        "content": "from .mediaDetector import *\nimport numpy as np\nimport cv2\nimport pybgs as bgs\nimport talib\nimport uuid\nimport itertools\nimport copy\n# consider merging this project with autoup, or just borrow some of its content.\n# assume you not to run many instances at once?\n# how to identify same video in a sequence?\n# maybe you can paint translated words with paddleocr?\n# framedifference can only be applied to videos, not freaking images.\ndef huffline_stillImage_Identifier(mediapath,**config): # wtf?\n    img = cv2.imread(mediapath)\n    line_thresh =  config[\"line_thresh\"]\n    includeBoundaryLines = config[\"includeBoundaryLines\"] # applied to those cornered crops.\\\n    blurKernel = config[\"blurKernel\"]\n    blurred = cv2.GaussianBlur(img,blurKernel, 0)\n    edges = cv2.Canny(blurred,20,210,apertureSize = 3) # great.\n    lines = cv2.HoughLines(edges,1,np.pi/180,line_thresh)\n    angle_error = config[\"angle_error\"]   # this can only detect square things, absolute square.\n    # we need to know horizontal and vertical lines, when they cross we get points.",
        "type": "code",
        "location": "/pyjom/medialang/functions/detectors/frameborder_Detector.py:1-26"
    },
    "669": {
        "file_id": 50,
        "content": "This code reads an image and applies Gaussian blur and Canny edge detection. It then uses the HoughLines algorithm to identify straight lines in the image. The identified horizontal and vertical lines can be used to detect square objects, but this implementation may have limitations as it assumes squares only.",
        "type": "comment"
    },
    "670": {
        "file_id": 50,
        "content": "    frameHeight, frameWidth = img.shape[:2]\n    # print(\"height: \", frameHeight)\n    # print(\"width: \", frameWidth)\n    mlines = {\"horizontal\":[], \"vertical\":[]}\n    if includeBoundaryLines:\n        originPoint = (0,0)\n        cornerPoint = (frameWidth,frameHeight)\n        mlines[\"horizontal\"].append(originPoint)\n        mlines[\"horizontal\"].append(cornerPoint)\n        mlines[\"vertical\"].append(originPoint)\n        mlines[\"vertical\"].append(cornerPoint)\n    if lines is None: lines = []\n    for line in lines:\n        for r_theta in line:\n            # breakpoint()\n            r,theta = r_theta.tolist()\n            # Stores the value of cos(theta) in a\n            # filter detected lines?\n            # theta filter:\n            if not abs(theta % (np.pi/2) )< angle_error:\n                continue # this is filtering.\n            # print(\"line parameter:\",r,theta)\n            a = np.cos(theta)\n            # Stores the value of sin(theta) in b\n            b = np.sin(theta)\n            # x0 stores the value rcos(theta)",
        "type": "code",
        "location": "/pyjom/medialang/functions/detectors/frameborder_Detector.py:27-54"
    },
    "671": {
        "file_id": 50,
        "content": "This code calculates the image height and width, creates horizontal and vertical line lists, appends origin and corner points to each list if including boundary lines is enabled. It also checks if lines are provided as input, filters out lines with angle error, and computes cos(theta) and sin(theta) for line parameter calculations.",
        "type": "comment"
    },
    "672": {
        "file_id": 50,
        "content": "            x0 = a*r\n            # y0 stores the value rsin(theta)\n            y0 = b*r\n            # x1 stores the rounded off value of (rcos(theta)-1000sin(theta))\n            x1 = int(x0 + 1000*(-b))\n            # y1 stores the rounded off value of (rsin(theta)+1000cos(theta))\n            y1 = int(y0 + 1000*(a))\n            # x2 stores the rounded off value of (rcos(theta)+1000sin(theta))\n            x2 = int(x0 - 1000*(-b))\n            # y2 stores the rounded off value of (rsin(theta)-1000cos(theta))\n            y2 = int(y0 - 1000*(a))\n            # cv2.line draws a line in img from the point(x1,y1) to (x2,y2).\n            # (0,0,255) denotes the colour of the line to be\n            #drawn. In this case, it is red.\n            df_x = abs(x1-x2)\n            df_y = abs(y1-y2)\n            lineType = \"vertical\"\n            if df_x > df_y:\n                lineType = \"horizontal\"\n            # we just need one single point and lineType.\n            linePoint = (x1,y1)\n            mlines[lineType].append(linePoint)",
        "type": "code",
        "location": "/pyjom/medialang/functions/detectors/frameborder_Detector.py:55-83"
    },
    "673": {
        "file_id": 50,
        "content": "This code calculates the coordinates of a line using trigonometry (a*r and b*r for x0 and y0) with an additional 1000 multipliers in the calculations. It then determines the rounded values for four points based on these coordinates, creating lines for vertical or horizontal segments as needed. The calculated line data is stored in a list called mlines.",
        "type": "comment"
    },
    "674": {
        "file_id": 50,
        "content": "            # cv2.line(img,(x1,y1), (x2,y2), (0,0,255),2)\n            # would not draw lines this time. draw found rects instead.\n    # get rectangle points. or just all possible rectangles?\n    # enumerate all possible lines.\n    rects =[] # list of rectangles\n    if len(mlines[\"horizontal\"]) < 2 or len(mlines[\"vertical\"]) < 2:\n        # print(\"unable to form rectangles.\")\n        # return [] # no rect.\n        pass\n    else:\n        for line_h1, line_h2 in itertools.combinations(mlines[\"horizontal\"],2):\n            ymin, ymax = list(sorted([line_h1[1],line_h2[1]]))\n            for line_v1, line_v2 in itertools.combinations(mlines[\"vertical\"], 2):\n                xmin, xmax = list(sorted([line_v1[0],line_v2[0]]))\n                rect = ((xmin,ymin),(xmax,ymax))\n                rects.append(rect)\n        # print(\"RECT DICT MAIN LIST:\")\n        # print(rect_dict_main_list) # maybe i want this shit?\n    return rects\ndef huffline_horizontal_vertical_FrameIterator(mediapath,**config):\n    video_file = mediapath # this one with cropped boundaries.",
        "type": "code",
        "location": "/pyjom/medialang/functions/detectors/frameborder_Detector.py:84-105"
    },
    "675": {
        "file_id": 50,
        "content": "This function generates rectangles based on horizontal and vertical lines detected in the image. It requires a minimum of two horizontal and two vertical lines for rectangle formation. The function combines these lines to find the bounding box coordinates, then stores them as rectangles in a list before returning the list of rectangles.",
        "type": "comment"
    },
    "676": {
        "file_id": 50,
        "content": "    video = cv2.VideoCapture(video_file)\n    def rectMerge(oldRect, newRect,delta_thresh = config[\"delta_thresh\"]):\n        # if very much alike, we merge these rects.\n        # what about those rect that overlaps? we check exactly those who overlaps.\n        # 1. check all new rects against all old rects. if they overlap, highly alike (or not) then mark it as having_alike_rect (or not) and append to new old rect list. <- after those old rects have been marked with alike sign, one cannot revoke the sign. still remaining new rects will be checked against them.\n        # 2. while checking, if not very alike then append newRect to new rect list.\n        # 3. if one old rect has not yet been checked as having_alike_rect then cut its life. otherwise extend its life, though not extend above max_rect_life.\n        (old_x1,old_y1), (old_x2, old_y2) = oldRect\n        (new_x1,new_y1), (new_x2, new_y2) = newRect\n        # too many rects?\n        old_w = old_x2-old_x1\n        old_h = old_y2-old_y1\n        det_x1 = abs(new_x1 - old_x1)/ old_w",
        "type": "code",
        "location": "/pyjom/medialang/functions/detectors/frameborder_Detector.py:107-122"
    },
    "677": {
        "file_id": 50,
        "content": "The code defines a function `rectMerge` that merges similar rectangles based on their overlap and likeness. It first checks all new rectangles against all old rectangles for overlap, marks them as having_alike_rect if highly alike or not, appends new rectangles to a new rectangle list, and extends the life of old rectangles without exceeding max_rect_life.",
        "type": "comment"
    },
    "678": {
        "file_id": 50,
        "content": "        det_x2 = abs(new_x2 - old_x2)/ old_w\n        det_y1 = abs(new_y1 - old_y1)/ old_h\n        det_y2 = abs(new_y2 - old_y2)/ old_h\n        # print(\"deltas:\",det_x1, det_x2, det_y1, det_y2)\n        having_alike_rect =  (det_x1 < delta_thresh) and (det_y1 < delta_thresh) and (det_x2 < delta_thresh ) and (det_y2 < delta_thresh)\n        myRect = newRect\n        if having_alike_rect:\n            myRect = oldRect\n        return myRect, having_alike_rect\n    def rectSurge(oldRectList, newRectList,diff_img_output,delta_thresh = config[\"delta_thresh\"], min_rect_life = config[\"min_rect_life\"], max_rect_life = config[\"max_rect_life\"],max_rect_list_length = 30, rect_area_threshold = 0.05):\n        yd,xd = diff_img_output.shape\n        aread = yd*xd\n        min_area_thresh = rect_area_threshold * aread\n        def getRectArea(rect):\n            (x0,y0),(x1,y1) = rect\n            return (x1-x0)*(y1-y0)\n        def getDiff(rect,diff_img_output):\n            (x0,y0),(x1,y1) = rect\n            diff_area = diff_img_output[y0:y1,x0:x1]",
        "type": "code",
        "location": "/pyjom/medialang/functions/detectors/frameborder_Detector.py:123-146"
    },
    "679": {
        "file_id": 50,
        "content": "Function detects changes in rectangles by comparing their dimensions and returns the updated rectangle and a boolean value if they are similar or different. It also calculates the area threshold based on image size, and provides functions for getting rectangle area and difference with diff_img_output.",
        "type": "comment"
    },
    "680": {
        "file_id": 50,
        "content": "            return np.sum(diff_area)\n        def getScore(rect,totalArea,r0=2,r1=5,key=\"rect\"):\n            # if key:\n            #     rect = x[\"rect\"]\n            # else: rect = x\n            area = getRectArea(rect)\n            diff = getDiff(rect,diff_img_output)\n            val1 = diff/area\n            val2 = diff/totalArea\n            return r0*val1+r1*val2\n        newToOldDictList = []\n        oldRectDictList = [{\"rect\":x[\"rect\"], \"alike\":False, \"life\":x[\"life\"],\"uuid\":x[\"uuid\"]} for x in oldRectList if getRectArea(x[\"rect\"]) > min_area_thresh] # actually they are all dict lists. you can pass an empty list as oldRectList anyway.\n        newRectList = [x for x in newRectList if getRectArea(x) > min_area_thresh] # get something else.\n        oldRectDictList = list(reversed(sorted(oldRectDictList,key=lambda x: getScore(x[\"rect\"],aread))))[:max_rect_list_length]\n        newRectList = list(reversed(sorted(newRectList,key=lambda x: getScore(x,aread))))[:max_rect_list_length]\n        # oldRectDictLis",
        "type": "code",
        "location": "/pyjom/medialang/functions/detectors/frameborder_Detector.py:147-164"
    },
    "681": {
        "file_id": 50,
        "content": "The code defines a function to detect and process rectangles based on their area and difference from an image. It creates two lists of rectangle dictionaries, one for new and old rectangles, and sorts them based on a score calculated using the getScore() function. The sorted lists are then truncated to the max_rect_list_length value.",
        "type": "comment"
    },
    "682": {
        "file_id": 50,
        "content": "t = sorted(oldRectDictList,key=lambda x:getRectArea(x[\"rect\"]), reverse=True) # not good since we got other freaking shits.\n        # print(\"OLDRECTDICTLIST:\",oldRectDictList)\n        # print(\"NEW RECT LENGTH:\",len(newRectList))\n        for newRect in newRectList:\n            needAppend = True\n            for index, oldRectDict in enumerate(oldRectDictList):\n                # print(\"ENUMERATING OLD INDEX:\",index)\n                oldRect = oldRectDict[\"rect\"]\n                _, having_alike_rect = rectMerge(oldRect,newRect,delta_thresh=delta_thresh)\n                if having_alike_rect:\n                    needAppend = False\n                    if not oldRectDict[\"alike\"]:\n                        # print(\"SET ALIKE:\",index,oldRect)\n                        oldRectDictList[index][\"alike\"] = True\n                    # ignore myRect.\n            if needAppend:\n                newToOldDictList.append({\"rect\":newRect,\"life\":1,\"uuid\":str(uuid.uuid4())}) # make sure it is not duplicated?\n                # if appended we shall break this loop. but when shall we append?",
        "type": "code",
        "location": "/pyjom/medialang/functions/detectors/frameborder_Detector.py:164-182"
    },
    "683": {
        "file_id": 50,
        "content": "Code snippet is sorting a list of oldRectDictList based on the area of rectangles in reverse order. It then compares each new rectangle with old rectangles and updates their alike status accordingly. If no match is found, it adds a new entry to newToOldDictList.",
        "type": "comment"
    },
    "684": {
        "file_id": 50,
        "content": "        oldToOldDictList = []\n        # print(\"OLD RECT LENGTH:\",len(oldRectDictList))\n        for oldRectDict in oldRectDictList:\n            alike = oldRectDict[\"alike\"]\n            life = oldRectDict[\"life\"]\n            oldRect = oldRectDict[\"rect\"]\n            myUUID = oldRectDict[\"uuid\"]\n            if not alike:\n                life -=1\n            else:\n                life +=1\n                life = min(max_rect_life, life)\n            if life <= min_rect_life:\n                continue\n            oldToOldDictList.append({\"rect\":oldRect,\"life\":life,\"uuid\":myUUID})\n        return oldToOldDictList + newToOldDictList # a combination.\n    def updateTotalRects(oldTotalRectDict,rectList,currentFrameIndex,diffFrame,minRectArea = 1):\n        for elem in rectList:\n            uuid = elem[\"uuid\"]\n            rect = elem[\"rect\"]\n            (x0,y0),(x1,y1) = rect\n            rectArea = (x1-x0)*(y1-y0)\n            if rectArea <minRectArea:\n                continue\n            if uuid not in oldTotalRectDict.keys():",
        "type": "code",
        "location": "/pyjom/medialang/functions/detectors/frameborder_Detector.py:183-209"
    },
    "685": {
        "file_id": 50,
        "content": "The code is iterating over old and new rectangles, updating their life count and filtering out rectangles with areas below a certain threshold. It then combines the old and new rectangle lists to form a single list.",
        "type": "comment"
    },
    "686": {
        "file_id": 50,
        "content": "                oldTotalRectDict.update({uuid:{\"rect\":rect,\"startFrame\":currentFrameIndex,\"endFrame\":None,\"meanDifference\":None}}) # finally,remove those without endFrame.\n            else:\n                duration = currentFrameIndex - oldTotalRectDict[uuid][\"startFrame\"]\n                # filter rect areas.\n                diff = diffFrame[y0:y1,x0:x1] # this is shit. we need to crop this shit.\n                # grayscale.\n                # std = np.abs(std)\n                # get the total delta over time?\n                # std = np.mean(std,axis=2)\n                diff_x = np.mean(diff.flatten())\n                # std_x = np.std(std,axis=2)\n                # std_x = np.std(std_x,axis=1)\n                # std_x = np.std(std_x,axis=0)\n                std_total = diff_x # later we need to convert this float64.\n                # breakpoint()\n                if std_total is None:\n                    print(\"RECT:\",rect)\n                    breakpoint()\n                prev_std = oldTotalRectDict[uuid][\"meanDifference\"]",
        "type": "code",
        "location": "/pyjom/medialang/functions/detectors/frameborder_Detector.py:210-228"
    },
    "687": {
        "file_id": 50,
        "content": "This code updates the oldTotalRectDict with a new rectangle, calculates duration of rectangles, crops and grayscales the difference frame, computes mean difference over time, and checks if std_total is not None to prevent potential errors.",
        "type": "comment"
    },
    "688": {
        "file_id": 50,
        "content": "                if duration == 1:\n                    oldTotalRectDict[uuid][\"meanDifference\"] = std_total\n                elif prev_std is None:\n                    oldTotalRectDict[uuid][\"meanDifference\"] = std_total\n                else:\n                    dur2 = duration - 1\n                    try:\n                        new_std = (dur2*prev_std + std_total)/duration # may freaking exceed limit.\n                    except:\n                        print(\"dur2\",dur2)\n                        print(\"prev_std\",prev_std)\n                        print(\"std_total\",std_total)\n                        print(\"duration\",duration)\n                        breakpoint()\n                    oldTotalRectDict[uuid][\"meanDifference\"] = new_std\n                oldTotalRectDict[uuid][\"endFrame\"] = currentFrameIndex\n        return oldTotalRectDict\n    total_rect_dict ={}\n    rect_dict_main_list = []\n    min_rect_life_display_thresh = config[\"min_rect_life_display_thresh\"] # a filter.\n    # mode = 1\n    line_thresh = config[\"line_thresh\"]",
        "type": "code",
        "location": "/pyjom/medialang/functions/detectors/frameborder_Detector.py:229-252"
    },
    "689": {
        "file_id": 50,
        "content": "This code calculates the mean difference of a rectangular object's characteristics (like area or aspect ratio) over time, taking into account previous calculations and the duration of frames. It filters out any rectangles that have not met the minimum threshold for display duration.",
        "type": "comment"
    },
    "690": {
        "file_id": 50,
        "content": "    includeBoundaryLines = config[\"includeBoundaryLines\"] # applied to those cornered crops.\n    # this will slow down the process. or maybe?\n    frameIndex = -1\n    prevFrame = None\n    # if mode == 1:\n    # import pybgs as bgs\n    algorithm = (\n    bgs.FrameDifference()\n    )  # this\n    framePeriod = config[\"framePeriod\"]\n    config_minRectArea = config[\"minRectArea\"]\n    frame_total_count = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n    # print(\"FRAME_TOTAL_COUNT:\",frame_total_count)\n    # breakpoint()\n    ocr_period = 10\n    ocr_result = []\n    # from .subtitleDetector import getPaddleOCR\n    from shapely.geometry import Point, Polygon\n    def checkPointInOcrRect(ocr_result,point,span=0):\n        xp,yp = point\n        p = Point(xp,yp)\n        for ocr_rect in ocr_result:\n            # print(\"OCR_RECT\", ocr_rect)\n            # breakpoint()\n            # if certainty < certainty_thresh: continue\n            p0,p1,p2,p3 = ocr_rect\n            p0 = (p0[0]-span,p0[1]-span)\n            p1 = (p1[0]+span,p1[1]-span)\n            p2 = (p2[0]+span,p2[1]+span)",
        "type": "code",
        "location": "/pyjom/medialang/functions/detectors/frameborder_Detector.py:253-281"
    },
    "691": {
        "file_id": 50,
        "content": "This code appears to be part of a video processing pipeline. It initializes an object detection algorithm and uses Shapely's Polygon for point-in-polygon testing with PaddleOCR. The function checkPointInOcrRect checks if a given point lies within the polygons defined by the OCR results, potentially using a span or offset for more accurate detection. Frame border detection is performed through frame differencing with a configurable frame period and minimum rectangle area threshold.",
        "type": "comment"
    },
    "692": {
        "file_id": 50,
        "content": "            p3 = (p3[0]-span,p3[1]+span) # it is float.\n            plist = [(x[0],x[1]) for x in [p0,p1,p2,p3]]\n            poly = Polygon(plist)\n            if poly.contains(p):\n                return True\n        return False\n    def checkLineIntersectOcrRect(ocr_result,linepoint,linetype,span=0):\n        xp,yp = linepoint\n        # p = Point(xp,yp)\n        for ocr_rect in ocr_result:\n            # print(\"OCR_RECT\", ocr_rect)\n            # breakpoint()\n            # if certainty < certainty_thresh: continue\n            p0,p1,p2,p3 = ocr_rect\n            p0 = (p0[0]-span,p0[1]-span)\n            p1 = (p1[0]+span,p1[1]-span)\n            p2 = (p2[0]+span,p2[1]+span)\n            p3 = (p3[0]-span,p3[1]+span) # it is float.\n            plist = [(x[0],x[1]) for x in [p0,p1,p2,p3]]\n            if linetype == \"vertical\":\n                xlist = [x[0] for x in plist]\n                xmin,xmax = min(xlist),max(xlist)\n                if xp >=xmin and xp <= xmax:\n                    return True\n            else:\n                ylist = [x[1] for x in plist]",
        "type": "code",
        "location": "/pyjom/medialang/functions/detectors/frameborder_Detector.py:282-308"
    },
    "693": {
        "file_id": 50,
        "content": "The function `frameborder_Detector()` checks if a given point is inside a polygon. The `checkLineIntersectOcrRect()` function detects whether a line intersects an OCR rectangle, considering both horizontal and vertical lines. It adjusts the coordinates of the OCR rectangle's points by subtracting or adding a span, then checks if the line intersects the adjusted rectangle.",
        "type": "comment"
    },
    "694": {
        "file_id": 50,
        "content": "                ymin,ymax = min(ylist),max(ylist)\n                if yp >= ymin and yp <= ymax:\n                    return True\n            # poly = Polygon(plist)\n            # if poly.contains(p):\n            #     return True\n        return False\n    for _ in progressbar.progressbar(range(frame_total_count)):\n        ret, img = video.read()\n        # if frameIndex% ocr_period == 0:\n        #     ocr_result = getPaddleOCR(img,cls=True,rec=False)\n        if img is None:\n            # if mode == 1:\n            popKeys = []\n            for key in total_rect_dict.keys():\n                elem = total_rect_dict[key]\n                if elem[\"endFrame\"] is None:\n                    popKeys.append(key)\n            for key in popKeys:\n                total_rect_dict.pop(key) # remove premature rectangles.\n            break\n        else: frameIndex+=1\n        if not frameIndex % framePeriod == 0:\n            continue# do shit.\n        # if mode == 1:\n        diff_img_output = algorithm.apply(img)\n        # what about the freaking still image?",
        "type": "code",
        "location": "/pyjom/medialang/functions/detectors/frameborder_Detector.py:309-336"
    },
    "695": {
        "file_id": 50,
        "content": "The code is performing image detection and processing, using a video source to read frames. It checks if the frameIndex meets certain conditions and performs OCR (Optical Character Recognition) on specific frames. If a premature rectangle is detected in the total_rect_dict dictionary, it removes it. The code also applies an algorithm to the still image frames.",
        "type": "comment"
    },
    "696": {
        "file_id": 50,
        "content": "        # Convert the img to grayscale\n        # gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n        # no need to use gray image.\n        # Apply edge detection method on the image\n        blurred = cv2.GaussianBlur(img, config[\"blurKernel\"], 0)\n        edges = cv2.Canny(blurred,20,210,apertureSize = 3) # great.\n        # why not applying edges directly to rectangles?\n        # This returns an array of r and theta values\n        # line_thresh =  200\n        # maintain a rectangle list. merge the alikes?\n        # if mode == 1:\n        lines = cv2.HoughLines(edges,1,np.pi/180,line_thresh)\n        if lines is None:\n            lines = []\n        angle_error = 0.00003   # this can only detect square things, absolute square.\n        # we need to know horizontal and vertical lines, when they cross we get points.\n        frameHeight, frameWidth = img.shape[:2]\n        # print(\"height: \", frameHeight)\n        # print(\"width: \", frameWidth)\n        mlines = {\"horizontal\":[], \"vertical\":[]}\n        if includeBoundaryLines:",
        "type": "code",
        "location": "/pyjom/medialang/functions/detectors/frameborder_Detector.py:337-362"
    },
    "697": {
        "file_id": 50,
        "content": "This code snippet is performing edge detection on an image and applying the HoughLines transform to identify horizontal and vertical lines. It also includes an option for including boundary lines. The code converts the image to grayscale, applies Gaussian blur, and performs Canny edge detection. The resulting edges are then passed to the HoughLines transform to find lines, which are categorized as either \"horizontal\" or \"vertical\".",
        "type": "comment"
    },
    "698": {
        "file_id": 50,
        "content": "            originPoint = (0,0)\n            cornerPoint = (frameWidth,frameHeight)\n            mlines[\"horizontal\"].append(originPoint)\n            mlines[\"horizontal\"].append(cornerPoint)\n            mlines[\"vertical\"].append(originPoint)\n            mlines[\"vertical\"].append(cornerPoint)\n        # lineTrans = {}\n        for line in lines:\n            for r_theta in line:\n                # breakpoint()\n                r,theta = r_theta.tolist()\n                # Stores the value of cos(theta) in a\n                # filter detected lines?\n                # theta filter:\n                if not abs(theta % (np.pi/2) )< angle_error:\n                    continue # this is filtering.\n                # print(\"line parameter:\",r,theta)\n                a = np.cos(theta)\n                # Stores the value of sin(theta) in b\n                b = np.sin(theta)\n                # x0 stores the value rcos(theta)\n                x0 = a*r\n                # y0 stores the value rsin(theta)\n                y0 = b*r\n                # x1 stores the rounded off value of (rcos(theta)-1000sin(theta))",
        "type": "code",
        "location": "/pyjom/medialang/functions/detectors/frameborder_Detector.py:363-391"
    },
    "699": {
        "file_id": 50,
        "content": "This code calculates the line parameters and applies a theta filter to eliminate lines that are close to being vertical or horizontal. It stores cos(theta) in 'a', sin(theta) in 'b', r*cos(theta) in 'x0', and r*sin(theta) in 'y0'. The theta filter ensures only appropriate angles are included, preventing false detections of vertical/horizontal lines.",
        "type": "comment"
    }
}