{
    "2300": {
        "file_id": 242,
        "content": "/tests/anime1_me_video_download/api_curl.sh",
        "type": "filepath"
    },
    "2301": {
        "file_id": 242,
        "content": "The code sends a POST request to anime1.me API using cURL, containing video ID, episode number, timestamp, and secret key, likely for interacting with anime videos. It also includes the \"--compressed\" flag for file compression during download, saving storage space and time.",
        "type": "summary"
    },
    "2302": {
        "file_id": 242,
        "content": "curl 'https://v.anime1.me/api' \\\n  -H 'authority: v.anime1.me' \\\n  -H 'accept: */*' \\\n  -H 'accept-language: en-US,en;q=0.9' \\\n  -H 'content-type: application/x-www-form-urlencoded' \\\n  -H 'origin: https://anime1.me' \\\n  -H 'referer: https://anime1.me/' \\\n  --data-raw 'd=%7B%22c%22%3A%221019%22%2C%22e%22%3A%226b%22%2C%22t%22%3A1652431596%2C%22p%22%3A0%2C%22s%22%3A%221bc65800b44a935eb4e5287655c64cb2%22%7D' \\\n  --compressed\n# curl 'https://v.anime1.me/api' \\\n#   -H 'authority: v.anime1.me' \\\n#   -H 'accept: */*' \\\n#   -H 'accept-language: en-US,en;q=0.9' \\\n#   -H 'content-type: application/x-www-form-urlencoded' \\\n#   -H 'origin: https://anime1.me' \\\n#   -H 'referer: https://anime1.me/' \\\n#   -H 'sec-ch-ua: \" Not A;Brand\";v=\"99\", \"Chromium\";v=\"101\"' \\\n#   -H 'sec-ch-ua-mobile: ?1' \\\n#   -H 'sec-ch-ua-platform: \"Android\"' \\\n#   -H 'sec-fetch-dest: empty' \\\n#   -H 'sec-fetch-mode: cors' \\\n#   -H 'sec-fetch-site: same-site' \\\n#   -H 'user-agent: Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.4951.41 Mobile Safari/537.36' \\",
        "type": "code",
        "location": "/tests/anime1_me_video_download/api_curl.sh:1-24"
    },
    "2303": {
        "file_id": 242,
        "content": "This code sends a POST request to 'https://v.anime1.me/api' using cURL, with specified headers and data in the request body. The request includes an API key for authentication and retrieves data from the anime1.me website.",
        "type": "comment"
    },
    "2304": {
        "file_id": 242,
        "content": "#   --data-raw 'd=%7B%22c%22%3A%221019%22%2C%22e%22%3A%226b%22%2C%22t%22%3A1652431596%2C%22p%22%3A0%2C%22s%22%3A%221bc65800b44a935eb4e5287655c64cb2%22%7D' \\\n#   --compressed\n# curl 'https://v.anime1.me/api' \\\n#   -H 'authority: v.anime1.me' \\\n#   -H 'accept: */*' \\\n#   -H 'accept-language: en-US,en;q=0.9' \\\n#   -H 'content-type: application/x-www-form-urlencoded' \\\n#   -H 'cookie: _ga=GA1.2.354375679.1652431604; _gid=GA1.2.1847563412.1652431604; _gat=1' \\\n#   -H 'origin: https://anime1.me' \\\n#   -H 'referer: https://anime1.me/' \\\n#   -H 'sec-ch-ua: \" Not A;Brand\";v=\"99\", \"Chromium\";v=\"101\"' \\\n#   -H 'sec-ch-ua-mobile: ?1' \\\n#   -H 'sec-ch-ua-platform: \"Android\"' \\\n#   -H 'sec-fetch-dest: empty' \\\n#   -H 'sec-fetch-mode: cors' \\\n#   -H 'sec-fetch-site: same-site' \\\n#   -H 'user-agent: Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.4951.41 Mobile Safari/537.36' \\\n#   --data-raw 'd=%7B%22c%22%3A%221019%22%2C%22e%22%3A%226b%22%2C%22t%22%3A1652431596%2C%22p%22%3A0%2C%22s%22%3A%221bc65800b44a935eb4e5287655c64cb2%22%7D' \\",
        "type": "code",
        "location": "/tests/anime1_me_video_download/api_curl.sh:25-43"
    },
    "2305": {
        "file_id": 242,
        "content": "This code is making an API request to 'https://v.anime1.me/api' using curl command with various headers and a data payload in JSON format. The payload contains information such as video ID, episode number, timestamp, and secret key. It seems to be fetching information or performing an action related to an anime video from the anime1.me website.",
        "type": "comment"
    },
    "2306": {
        "file_id": 242,
        "content": "#   --compressed",
        "type": "code",
        "location": "/tests/anime1_me_video_download/api_curl.sh:44-44"
    },
    "2307": {
        "file_id": 242,
        "content": "The code snippet \"--compressed\" is used to compress the file during download, which can save storage space and reduce transfer time.",
        "type": "comment"
    },
    "2308": {
        "file_id": 243,
        "content": "/tests/bilibili_tag_recommend_activities/README.md",
        "type": "filepath"
    },
    "2309": {
        "file_id": 243,
        "content": "Code snippet provides a link to another file, \"bilibili_up.py\", which contains information on how to get the 'upload_id' in bilibili API.",
        "type": "summary"
    },
    "2310": {
        "file_id": 243,
        "content": "[how to get upload_id](https://github.com/xunsword/bilibil/blob/2abf66a9771daebc12c181f88d8af82613975548/bilibili_up.py)",
        "type": "code",
        "location": "/tests/bilibili_tag_recommend_activities/README.md:1-1"
    },
    "2311": {
        "file_id": 243,
        "content": "Code snippet provides a link to another file, \"bilibili_up.py\", which contains information on how to get the 'upload_id' in bilibili API.",
        "type": "comment"
    },
    "2312": {
        "file_id": 244,
        "content": "/tests/bilibili_practices/bilibili_video_translate/web_translator.py",
        "type": "filepath"
    },
    "2313": {
        "file_id": 244,
        "content": "This code imports a translators module and defines a function for translating Chinese to English. It randomly selects a translator, fixes line endings if needed, and returns the result. The code also prints exception tracebacks in case of errors.",
        "type": "summary"
    },
    "2314": {
        "file_id": 244,
        "content": "import translators as ts\n# translator = \n# mtranslators = [ts.sogou] #this is pure shit.\n# mtranslators = [ts.baidu,ts.sogou]\n# mtranslators = [ts.baidu,ts.sogou,ts.iciba]\nmtranslators = [ts.youdao,ts.baidu,ts.alibaba] # no yandex, tencent, sogou.\n# mtranslators = [ts.baidu,ts.iciba]\nimport random\ndef fixline(line):\n    notEndings = [\"。\",\"，\"]\n    for x in notEndings:\n        if line.endswith(x): return line[:-1]\n    return line\ndef zh_to_en_translator(text,needFixLine=True):\n    randomLang = [\"zh\",\"zh-CHS\"]\n    from_language = \"en\"\n    # lang = random.choice(randomLang)\n    while True:\n        t = random.choice(mtranslators)\n        # print(type(translator))\n        for rl in randomLang:\n            try:\n                result = t(text,from_language=from_language,to_language=rl)\n                # if len(result) < 3:\n                #     print(t)\n                #     breakpoint()\n                if needFixLine:\n                    result = fixline(result)\n                return result\n            except:\n                import traceback",
        "type": "code",
        "location": "/tests/bilibili_practices/bilibili_video_translate/web_translator.py:1-34"
    },
    "2315": {
        "file_id": 244,
        "content": "This code imports a translators module and defines a function for translating Chinese (zh) to English (en). It randomly selects a translator from a list of options and attempts to translate the text. If needed, it fixes the line endings before returning the result.",
        "type": "comment"
    },
    "2316": {
        "file_id": 244,
        "content": "                traceback.print_exc()",
        "type": "code",
        "location": "/tests/bilibili_practices/bilibili_video_translate/web_translator.py:35-35"
    },
    "2317": {
        "file_id": 244,
        "content": "This line of code is used for printing the exception traceback in case of an error or unhandled exception.",
        "type": "comment"
    },
    "2318": {
        "file_id": 245,
        "content": "/tests/bilibili_practices/bilibili_video_translate/translate_srt.py",
        "type": "filepath"
    },
    "2319": {
        "file_id": 245,
        "content": "Code reads an SRT file, translates each line of the content using a web translator, wraps the translated lines to meet a certain character limit, and then saves the results in a new SRT file. The process involves parsing the input SRT file using the \"srt\" library, translating text with \"web_translator\", and modifying the line wrapping for better readability.",
        "type": "summary"
    },
    "2320": {
        "file_id": 245,
        "content": "src = \"en_.srt\"\nfinal_srt = \"zh_translated.srt\"\nimport srt\nwrap_limit = 20\nsource_srt = open(src, \"r\",encoding=\"utf-8\").read()\nssrt = srt.parse(source_srt)\nfrom web_translator import translator\nimport math\ndef wrapLine(line):\n    lines = [line[x*wrap_limit:(x+1)*wrap_limit] for x in range(math.ceil(len(line)/wrap_limit))]\n    return \"\\n\".join(lines)\ndef fixline(line):\n    notEndings = [\"。\",\"，\"]\n    for x in notEndings:\n        if line.endswith(x): return line[:-1]\n    return line\nnew_ssrt = []\nfor line in ssrt:\n    # print(line)\n    start = line.start\n    end = line.end # timedelta.\n    content = line.content\n    index = line.index\n    unwrapped_content = content.replace(\"\\n\",\" \")\n    result = translator(unwrapped_content)\n    result = fixline(result)\n    print(result)\n    line.content = result\n    new_ssrt.append(line)\n    # wrapped = wrapLine(result)\n    # print(wrapped)\n    # print(start, end, content, index)\nfinal_content = srt.compose(new_ssrt)\nwith open(final_srt,\"w+\",encoding=\"utf-8\") as f:\n    f.write(final_content)",
        "type": "code",
        "location": "/tests/bilibili_practices/bilibili_video_translate/translate_srt.py:1-45"
    },
    "2321": {
        "file_id": 245,
        "content": "Code reads an SRT file, translates each line of the content using a web translator, wraps the translated lines to meet a certain character limit, and then saves the results in a new SRT file. The process involves parsing the input SRT file using the \"srt\" library, translating text with \"web_translator\", and modifying the line wrapping for better readability.",
        "type": "comment"
    },
    "2322": {
        "file_id": 246,
        "content": "/tests/bilibili_practices/bilibili_video_translate/test_curve_converter.py",
        "type": "filepath"
    },
    "2323": {
        "file_id": 246,
        "content": "The code defines a function `curve_converter` that applies bitwise operations and conditional logic on an input value using a provided curve function, transforming it into an output array after iterating through pairs of points from the curve and maintaining data types.",
        "type": "summary"
    },
    "2324": {
        "file_id": 246,
        "content": "import numpy as np\ndef curve_converter(value,curve_function):\n    # do bitwise operation.\n    marray = None\n    curve2 = zip(curve_function[:-1],curve_function[1:])\n    dtype = value.dtype\n    for (orig, target0), (forig,ftarget) in curve2:\n        locs1 = value>orig # forget about zero.\n        locs1 = locs1.astype(dtype)\n        locs2 = value<=forig\n        locs2 = locs2.astype(dtype)\n        # new_value = locs1 and locs2\n        new_value = locs1 * locs2\n        mask_backup = new_value.copy()\n        # print(\"LOCMAP:\",new_value)\n        # new_value = new_value.astype(dtype)\n        new_value = value * new_value\n        # print(\"MASKED VALUES:\", new_value)\n        new_value = new_value.astype(np.float32)\n        new_value2 = (new_value-orig)/(forig-orig)\n        new_diff = new_value2*(ftarget-target0)\n        new_value = target0+new_diff\n        new_value = new_value*mask_backup\n        new_value = new_value.astype(dtype)\n        if marray is None:\n            marray = new_value.copy()\n        else:\n            # assert np.all(marray< value) # NOT RIGHT. WHERE?",
        "type": "code",
        "location": "/tests/bilibili_practices/bilibili_video_translate/test_curve_converter.py:1-33"
    },
    "2325": {
        "file_id": 246,
        "content": "This code defines a function `curve_converter` that performs bitwise operations on an input `value` using a provided curve function. The function iterates through pairs of points from the curve and applies conditional logic based on the value's relationship to these points, resulting in a transformed output array. The function also checks for duplicate values and ensures the proper data types are maintained throughout the process.",
        "type": "comment"
    },
    "2326": {
        "file_id": 246,
        "content": "            marray += new_value\n        # print(\"INTERMEDIATES:\",marray)\n    return marray\nif __name__ == '__main__':\n    data = np.array([1,40,100,245])\n    curve_function = [[0,0],[40,30],[100,50],[150,100],[255,130]]\n    out = curve_converter(data,curve_function)\n    print(data)\n    print(out)",
        "type": "code",
        "location": "/tests/bilibili_practices/bilibili_video_translate/test_curve_converter.py:34-44"
    },
    "2327": {
        "file_id": 246,
        "content": "The code defines a function called curve_converter that takes two inputs - data and curve_function. It then adds the new values to marray, which seems to be an array of intermediate values. The code returns the marray after performing the calculations. In the main block, it tests the function by creating arrays for data and curve_function and prints out both the original data and the output of the curve_converter function.",
        "type": "comment"
    },
    "2328": {
        "file_id": 247,
        "content": "/tests/bilibili_practices/bilibili_video_translate/main_redraw_english_text.py",
        "type": "filepath"
    },
    "2329": {
        "file_id": 247,
        "content": "This code utilizes PaddleOCR to translate and rectify text from images, applies Levenshtein distance filtering, calculates text size and positioning, and saves results.",
        "type": "summary"
    },
    "2330": {
        "file_id": 247,
        "content": "from paddleocr import PaddleOCR\n# cannot translate everything... not frame by frame...\n# can summarize things. can block texts on location.\n# Paddleocr supports Chinese, English, French, German, Korean and Japanese.\n# You can set the parameter `lang` as `ch`, `en`, `french`, `german`, `korean`, `japan`\n# to switch the language model in order.\nocr = PaddleOCR(use_angle_cls=True, lang='en') # need to run only once to download and load model into memory\nimg_path = 'target.png' # only detect english. or not?\nimport cv2\nimage = cv2.imread(img_path)\nresult2 = ocr.ocr(image, cls=True)\nprob_thresh = 0.6 # found watermark somewhere. scorpa\nresult = []\nimport wordninja\nfor index, line in enumerate(result2):\n    # print(line)\n    # breakpoint()\n    coords, (text, prob) = line\n    prob = float(prob)\n    if prob > prob_thresh:\n        rectified_text = \" \".join(wordninja.split(text))\n        line[1] = (rectified_text, prob)\n        print(line)\n        result.append(line)\nimport numpy as np\na,b,c = image.shape\nblank_image = np.zeros(shape=[a,b], dtype=np.uint8) # the exact order",
        "type": "code",
        "location": "/tests/bilibili_practices/bilibili_video_translate/main_redraw_english_text.py:1-38"
    },
    "2331": {
        "file_id": 247,
        "content": "The code utilizes the PaddleOCR library to translate text from an image. It supports multiple languages and requires model downloading upon initialization. The code reads an image, detects English text using OCR, and applies a rectification process to improve readability. It then filters out results below a probability threshold before saving the final results.",
        "type": "comment"
    },
    "2332": {
        "file_id": 247,
        "content": "for coords, (text,prob) in result:\n    polyArray = np.array(coords).astype(np.int64) # fuck.\n    # print(polyArray)\n    # print(polyArray.shape)\n    # breakpoint()\n    # points = np.array([[160, 130], [350, 130], [250, 300]])\n    # print(points.dtype)\n    # points = np.array([[454.0, 22.0], [464.0, 26.0], [464.0, 85.0]]).astype(np.int64)\n    color= 255\n    cv2.fillPoly(blank_image,[polyArray],color)\n    isClosed = True\n    thickness = 30\n    cv2.polylines(blank_image, [polyArray], isClosed, color, thickness) # much better.\n#     # cv2.fillPoly(blank_image,pts=[points],color=(255, 255,255))\n# cv2.imshow(\"mask\",blank_image)\n# cv2.waitKey(0)\n# use wordninja.\n# before translation we need to lowercase these shits.\ndst = cv2.inpaint(image,blank_image,3,cv2.INPAINT_TELEA)\n# from PIL import Image\nfrom PIL import Image, ImageFont, ImageDraw  \ndef np2pillow(opencv_image):\n    color_coverted = cv2.cvtColor(opencv_image, cv2.COLOR_BGR2RGB)\n    pil_image = Image.fromarray(color_coverted)\n    return pil_image\n    # pil_image.show()",
        "type": "code",
        "location": "/tests/bilibili_practices/bilibili_video_translate/main_redraw_english_text.py:40-67"
    },
    "2333": {
        "file_id": 247,
        "content": "Iterating through coordinates and text-probability pairs, converting coordinates to numpy array for drawing on image. Using cv2.fillPoly() and cv2.polylines() for shape fills and outlines. Inpainting image with cv2.inpaint(), then converting opencv image to pillow image using np2pillow() function.",
        "type": "comment"
    },
    "2334": {
        "file_id": 247,
        "content": "def pillow2np(pil_image):\n    # pil_image=Image.open(\"demo2.jpg\") # open image using PIL\n    # use numpy to convert the pil_image into a numpy array\n    numpy_image=np.array(pil_image)  \n    # convert to a openCV2 image, notice the COLOR_RGB2BGR which means that \n    # the color is converted from RGB to BGR format\n    opencv_image=cv2.cvtColor(numpy_image, cv2.COLOR_RGB2BGR) \n    return opencv_image\n# draw text now!\nmpil_image = np2pillow(dst)\ndraw = ImageDraw.Draw(mpil_image)\nfont_location = \"/root/Desktop/works/bilibili_tarot/SimHei.ttf\" # not usual english shit.\ndef get_coord_orientation_font_size_and_center(coords):\n    xlist, ylist = [x[0] for x in coords], [x[1] for x in coords]\n    min_x, max_x = min(xlist), max(xlist)\n    min_y, max_y = min(ylist), max(ylist)\n    width,height = max_x-min_x, max_y-min_y\n    center = (int((max_x+min_x)/2),int((max_y+min_y)/2))\n    # what about rotation? forget about it...\n    if (width / height) < 0.8:\n        orientation = \"vertical\"\n        font_size = int(width)\n    else:",
        "type": "code",
        "location": "/tests/bilibili_practices/bilibili_video_translate/main_redraw_english_text.py:69-93"
    },
    "2335": {
        "file_id": 247,
        "content": "Function `pillow2np` converts a PIL image to a numpy array, then converts it to an OpenCV BGR format. Draws text on the image using PIL's ImageDraw module and specifies font location. Function `get_coord_orientation_font_size_and_center` calculates image dimensions, center, and determines orientation based on aspect ratio for possible vertical text.",
        "type": "comment"
    },
    "2336": {
        "file_id": 247,
        "content": "        orientation = \"horizontal\"\n        font_size = int(height)\n    return orientation, font_size, center,(width,height)\nreadjust_size=True\ncomparedWaterMarkString = \"scorpa\".lower() # the freaking name \ncomparedWaterMarkStringLength = len(comparedWaterMarkString)\nimport Levenshtein\nfor coords, (text,prob) in result:\n    # remove watermarks? how to filter?\n    editDistanceThreshold = 4\n    probThreshold = 0.8\n    textCompareCandidate = text.replace(\" \",\"\").lower()\n    distance = Levenshtein.distance(textCompareCandidate,comparedWaterMarkString)\n    string_length = len(text)\n    string_length_difference = abs(string_length-comparedWaterMarkStringLength)\n    length_difference_threshold = 3\n    if (distance < editDistanceThreshold and string_length_difference < length_difference_threshold) or prob < probThreshold:\n        continue # skip all shits.\n    # specified font size \n    orientation, font_size, center ,(width,height) = get_coord_orientation_font_size_and_center(coords)\n    if orientation == \"horizontal\":",
        "type": "code",
        "location": "/tests/bilibili_practices/bilibili_video_translate/main_redraw_english_text.py:94-116"
    },
    "2337": {
        "file_id": 247,
        "content": "This code is filtering and processing text from the results. It checks the distance between the text and a given string (comparedWaterMarkString) using Levenshtein distance algorithm. If the difference in length is less than a threshold, or the probability of the text being correct is below a certain threshold, the code skips that particular text. The orientation, font size, center, and dimensions are obtained from the coordinates and returned.",
        "type": "comment"
    },
    "2338": {
        "file_id": 247,
        "content": "        font = ImageFont.truetype(font_location, font_size)\n        # text = original_text\n        # drawing text size \n        stroke_width = int(0.1*font_size)\n        (string_width,string_height) = draw.textsize(text,font=font,stroke_width=stroke_width)\n        # print(string_width)\n        # breakpoint()\n        if readjust_size:\n            change_ratio = width/string_width\n            new_fontsize = font_size*change_ratio\n            font = ImageFont.truetype(font_location, new_fontsize)\n            start_x = int(center[0]-width/2)\n            start_y = int(center[1]-height/2)\n        else:\n            start_x = int(center[0]-string_width/2)\n            start_y = int(center[1]-font_size/2)\n        draw.text((start_x, start_y), text, font = font, fill=(255,255,255),stroke_fill=(0,0,0),stroke_width = stroke_width,align =\"left\") # what is the freaking align?\n# mpil_image.show() \nmpil_image.save(\"redraw_english.png\")\n# cv2.imshow('dst',dst2)\n# cv2.waitKey(0)\n# cv2.destroyAllWindows()\n# expand the area somehow.",
        "type": "code",
        "location": "/tests/bilibili_practices/bilibili_video_translate/main_redraw_english_text.py:117-141"
    },
    "2339": {
        "file_id": 247,
        "content": "This code calculates the size of text using a given font, adjusts it based on image size, and draws the text centered or aligned to the left. It then saves the resulting image.",
        "type": "comment"
    },
    "2340": {
        "file_id": 247,
        "content": "# draw result\n# simhei_path = \"/root/Desktop/works/bilibili_tarot/SimHei.ttf\"\n# from PIL import Image\n# image = Image.open(img_path).convert('RGB')\n# boxes = [line[0] for line in result]\n# txts = [line[1][0] for line in result]\n# scores = [line[1][1] for line in result]\n# im_show = draw_ocr(image, boxes, txts, scores, font_path=simhei_path)\n# im_show = Image.fromarray(im_show)\n# im_show.save('result.jpg')\n# we will be testing one image only. not the whole goddamn video.\n# may have cuda error when using my cv2 cuda libs.",
        "type": "code",
        "location": "/tests/bilibili_practices/bilibili_video_translate/main_redraw_english_text.py:142-154"
    },
    "2341": {
        "file_id": 247,
        "content": "This code snippet is responsible for drawing OCR results on an image, saving the result as 'result.jpg'. It uses a specific font path and processes one image only to avoid potential CUDA errors with cv2 CUDA libraries.",
        "type": "comment"
    },
    "2342": {
        "file_id": 248,
        "content": "/tests/bilibili_practices/bilibili_video_translate/main_redraw_chinese_text.py",
        "type": "filepath"
    },
    "2343": {
        "file_id": 248,
        "content": "This code utilizes PaddleOCR for OCR, applies text rectification with probability threshold and WordNinja, performs color inpainting, filters coordinates, removes watermarks, adjusts font size/position, and outputs 'result.jpg'. Issues may arise with CUDA-based OpenCV libraries.",
        "type": "summary"
    },
    "2344": {
        "file_id": 248,
        "content": "from paddleocr import PaddleOCR\n# cannot translate everything... not frame by frame...\n# can summarize things. can block texts on location.\n# Paddleocr supports Chinese, English, French, German, Korean and Japanese.\n# You can set the parameter `lang` as `ch`, `en`, `french`, `german`, `korean`, `japan`\n# to switch the language model in order.\nocr = PaddleOCR(use_angle_cls=True, lang='en') # need to run only once to download and load model into memory\nimg_path = 'target.png' # only detect english. or not?\nimport cv2\nimage = cv2.imread(img_path)\nresult2 = ocr.ocr(image, cls=True)\nprob_thresh = 0.6 # found watermark somewhere. scorpa\nresult = []\nimport wordninja\nfor index, line in enumerate(result2):\n    # print(line)\n    # breakpoint()\n    coords, (text, prob) = line\n    prob = float(prob)\n    if prob > prob_thresh:\n        rectified_text = \" \".join(wordninja.split(text))\n        line[1] = (rectified_text, prob)\n        print(line)\n        result.append(line)\nimport numpy as np\na,b,c = image.shape\nblank_image = np.zeros(shape=[a,b], dtype=np.uint8) # the exact order",
        "type": "code",
        "location": "/tests/bilibili_practices/bilibili_video_translate/main_redraw_chinese_text.py:1-38"
    },
    "2345": {
        "file_id": 248,
        "content": "This code uses PaddleOCR to perform optical character recognition (OCR) on an image file. It detects English text in the image and applies a probability threshold for accuracy. The code rectifies the detected text by splitting it into words using WordNinja, then stores the result in a list. The script also creates a blank image using NumPy.",
        "type": "comment"
    },
    "2346": {
        "file_id": 248,
        "content": "for coords, (text,prob) in result:\n    polyArray = np.array(coords).astype(np.int64) # fuck.\n    # print(polyArray)\n    # print(polyArray.shape)\n    # breakpoint()\n    # points = np.array([[160, 130], [350, 130], [250, 300]])\n    # print(points.dtype)\n    # points = np.array([[454.0, 22.0], [464.0, 26.0], [464.0, 85.0]]).astype(np.int64)\n    color= 255\n    cv2.fillPoly(blank_image,[polyArray],color)\n    isClosed = True\n    thickness = 30\n    cv2.polylines(blank_image, [polyArray], isClosed, color, thickness) # much better.\n#     # cv2.fillPoly(blank_image,pts=[points],color=(255, 255,255))\n# cv2.imshow(\"mask\",blank_image)\n# cv2.waitKey(0)\n# use wordninja.\n# before translation we need to lowercase these shits.\ndst = cv2.inpaint(image,blank_image,3,cv2.INPAINT_TELEA)\n# from PIL import Image\nfrom PIL import Image, ImageFont, ImageDraw  \ndef np2pillow(opencv_image):\n    color_coverted = cv2.cvtColor(opencv_image, cv2.COLOR_BGR2RGB)\n    pil_image = Image.fromarray(color_coverted)\n    return pil_image\n    # pil_image.show()",
        "type": "code",
        "location": "/tests/bilibili_practices/bilibili_video_translate/main_redraw_chinese_text.py:40-67"
    },
    "2347": {
        "file_id": 248,
        "content": "Iterates through result coordinates and text probabilities, converts coordinates to numpy array, fills polygon on the image, draws polyline, performs color inpainting on the image, and converts the final image from OpenCV to PIL format.",
        "type": "comment"
    },
    "2348": {
        "file_id": 248,
        "content": "def pillow2np(pil_image):\n    # pil_image=Image.open(\"demo2.jpg\") # open image using PIL\n    # use numpy to convert the pil_image into a numpy array\n    numpy_image=np.array(pil_image)  \n    # convert to a openCV2 image, notice the COLOR_RGB2BGR which means that \n    # the color is converted from RGB to BGR format\n    opencv_image=cv2.cvtColor(numpy_image, cv2.COLOR_RGB2BGR) \n    return opencv_image\n# draw text now!\nmpil_image = np2pillow(dst)\ndraw = ImageDraw.Draw(mpil_image)\nfont_location = \"/root/Desktop/works/bilibili_tarot/SimHei.ttf\" # not usual english shit.\ndef get_coord_orientation_font_size_and_center(coords):\n    xlist, ylist = [x[0] for x in coords], [x[1] for x in coords]\n    min_x, max_x = min(xlist), max(xlist)\n    min_y, max_y = min(ylist), max(ylist)\n    width,height = max_x-min_x, max_y-min_y\n    center = (int((max_x+min_x)/2),int((max_y+min_y)/2))\n    # what about rotation? forget about it...\n    if (width / height) < 0.8:\n        orientation = \"vertical\"\n        font_size = int(width)\n    else:",
        "type": "code",
        "location": "/tests/bilibili_practices/bilibili_video_translate/main_redraw_chinese_text.py:69-93"
    },
    "2349": {
        "file_id": 248,
        "content": "Function `pillow2np` converts a PIL image to a numpy array and then to an OpenCV image, changing the color format from RGB to BGR.\nIn the `get_coord_orientation_font_size_and_center` function, it calculates width, height, center coordinates of the bounding box based on given coordinates, and determines the font size and orientation (vertical or horizontal) based on aspect ratio of the image.",
        "type": "comment"
    },
    "2350": {
        "file_id": 248,
        "content": "        orientation = \"horizontal\"\n        font_size = int(height)\n    return orientation, font_size, center,(width,height)\nreadjust_size=False # just center.\ncomparedWaterMarkString = \"scorpa\".lower() # the freaking name \ncomparedWaterMarkStringLength = len(comparedWaterMarkString)\nimport Levenshtein\nfrom web_translator import zh_to_en_translator as translator\nfor coords, (text,prob) in result:\n    # remove watermarks? how to filter?\n    editDistanceThreshold = 4\n    probThreshold = 0.8\n    textCompareCandidate = text.replace(\" \",\"\").lower() # original text, no translation.\n    distance = Levenshtein.distance(textCompareCandidate,comparedWaterMarkString)\n    string_length = len(text)\n    string_length_difference = abs(string_length-comparedWaterMarkStringLength)\n    length_difference_threshold = 3\n    if (distance < editDistanceThreshold and string_length_difference < length_difference_threshold) or prob < probThreshold:\n        continue # skip all shits.\n    # specified font size \n    text = translator(text) # now translate.",
        "type": "code",
        "location": "/tests/bilibili_practices/bilibili_video_translate/main_redraw_chinese_text.py:94-117"
    },
    "2351": {
        "file_id": 248,
        "content": "Code snippet is filtering and translating text from a given list of coordinates and text-probability pairs. It removes watermarks by comparing the original text to \"scorpa\" (lowercase) and skips texts with high edit distance, length difference or low probability. The font size is set, and the translated text is returned.",
        "type": "comment"
    },
    "2352": {
        "file_id": 248,
        "content": "    orientation, font_size, center ,(width,height) = get_coord_orientation_font_size_and_center(coords)\n    if orientation == \"horizontal\":\n        font = ImageFont.truetype(font_location, font_size)\n        # text = original_text\n        # drawing text size \n        stroke_width = int(0.1*font_size)\n        (string_width,string_height) = draw.textsize(text,font=font,stroke_width=stroke_width)\n        # print(string_width)\n        # breakpoint()\n        if readjust_size:\n            change_ratio = width/string_width\n            new_fontsize = font_size*change_ratio\n            font = ImageFont.truetype(font_location, new_fontsize)\n            start_x = int(center[0]-width/2)\n            start_y = int(center[1]-height/2)\n        else:\n            start_x = int(center[0]-string_width/2)\n            start_y = int(center[1]-font_size/2)\n        draw.text((start_x, start_y), text, font = font, fill=(255,255,255),stroke_fill=(0,0,0),stroke_width = stroke_width,align =\"left\") # what is the freaking align?\n# mpil_image.show() ",
        "type": "code",
        "location": "/tests/bilibili_practices/bilibili_video_translate/main_redraw_chinese_text.py:118-138"
    },
    "2353": {
        "file_id": 248,
        "content": "The code calculates the text's dimensions and adjusts the font size and position based on the provided coordinates. If 'readjust_size' is True, it resizes the font to fit within the given width. It then draws the text with specified alignment using the ImageDraw module.",
        "type": "comment"
    },
    "2354": {
        "file_id": 248,
        "content": "mpil_image.save(\"redraw_eng_to_chinese.png\")\n# cv2.imshow('dst',dst2)\n# cv2.waitKey(0)\n# cv2.destroyAllWindows()\n# expand the area somehow.\n# draw result\n# simhei_path = \"/root/Desktop/works/bilibili_tarot/SimHei.ttf\"\n# from PIL import Image\n# image = Image.open(img_path).convert('RGB')\n# boxes = [line[0] for line in result]\n# txts = [line[1][0] for line in result]\n# scores = [line[1][1] for line in result]\n# im_show = draw_ocr(image, boxes, txts, scores, font_path=simhei_path)\n# im_show = Image.fromarray(im_show)\n# im_show.save('result.jpg')\n# we will be testing one image only. not the whole goddamn video.\n# may have cuda error when using my cv2 cuda libs.",
        "type": "code",
        "location": "/tests/bilibili_practices/bilibili_video_translate/main_redraw_chinese_text.py:139-157"
    },
    "2355": {
        "file_id": 248,
        "content": "This code saves an image, displays it using OpenCV, expands the area of the image and draws the result using a specific font, and finally saves the final output as 'result.jpg'. It is specifically testing one image from a video, and may encounter issues when using CUDA-based OpenCV libraries due to potential errors.",
        "type": "comment"
    },
    "2356": {
        "file_id": 249,
        "content": "/tests/bilibili_practices/bilibili_video_translate/main_mask_english_text.py",
        "type": "filepath"
    },
    "2357": {
        "file_id": 249,
        "content": "This code uses PaddleOCR to detect English text in images, applies a specific font for OCR, and visualizes the results. Testing is limited to one image, and CUDA errors may occur with certain CV2 libraries.",
        "type": "summary"
    },
    "2358": {
        "file_id": 249,
        "content": "from paddleocr import PaddleOCR,draw_ocr\n# cannot translate everything... not frame by frame...\n# can summarize things. can block texts on location.\n# Paddleocr supports Chinese, English, French, German, Korean and Japanese.\n# You can set the parameter `lang` as `ch`, `en`, `french`, `german`, `korean`, `japan`\n# to switch the language model in order.\nocr = PaddleOCR(use_angle_cls=True, lang='en') # need to run only once to download and load model into memory\nimg_path = 'target.png' # only detect english. or not?\nimport cv2\nimage = cv2.imread(img_path)\nresult2 = ocr.ocr(image, cls=True)\nprob_thresh = 0.8\nresult = []\nimport wordninja\nfor index, line in enumerate(result2):\n    # print(line)\n    # breakpoint()\n    coords, (text, prob) = line\n    prob = float(prob)\n    if prob > prob_thresh:\n        rectified_text = \" \".join(wordninja.split(text))\n        line[1] = (rectified_text, prob)\n        print(line)\n        result.append(line)\nimport numpy as np\na,b,c = image.shape\nblank_image = np.zeros(shape=[a,b], dtype=np.uint8) # the exact order",
        "type": "code",
        "location": "/tests/bilibili_practices/bilibili_video_translate/main_mask_english_text.py:1-38"
    },
    "2359": {
        "file_id": 249,
        "content": "The code uses PaddleOCR to detect English text in an image. It loads the model once and then reads the image file 'target.png'. The OCR function is used to recognize the text in the image, and a probability threshold is set. The detected lines with probabilities above the threshold are processed further using wordninja to rectify the text. These lines are stored in the result list. Finally, a blank image is created for an unknown purpose.",
        "type": "comment"
    },
    "2360": {
        "file_id": 249,
        "content": "for coords, (text,prob) in result:\n    polyArray = np.array(coords).astype(np.int64) # fuck.\n    # print(polyArray)\n    # print(polyArray.shape)\n    # breakpoint()\n    # points = np.array([[160, 130], [350, 130], [250, 300]])\n    # print(points.dtype)\n    # points = np.array([[454.0, 22.0], [464.0, 26.0], [464.0, 85.0]]).astype(np.int64)\n    color= 255\n    cv2.fillPoly(blank_image,[polyArray],color)\n    isClosed = True\n    thickness = 30\n    cv2.polylines(blank_image, [polyArray], isClosed, color, thickness) # much better.\n#     # cv2.fillPoly(blank_image,pts=[points],color=(255, 255,255))\n# cv2.imshow(\"mask\",blank_image)\n# cv2.waitKey(0)\n# use wordninja.\n# before translation we need to lowercase these shits.\ndst = cv2.inpaint(image,blank_image,3,cv2.INPAINT_TELEA)\ncv2.imshow('dst',dst)\ncv2.waitKey(0)\ncv2.destroyAllWindows()\n# expand the area somehow.\n# draw result\n# simhei_path = \"/root/Desktop/works/bilibili_tarot/SimHei.ttf\"\n# from PIL import Image\n# image = Image.open(img_path).convert('RGB')\n# boxes = [line[0] for line in result]",
        "type": "code",
        "location": "/tests/bilibili_practices/bilibili_video_translate/main_mask_english_text.py:40-68"
    },
    "2361": {
        "file_id": 249,
        "content": "Iterating through result coordinates, creating a numpy array for each set of coordinates, filling the poly with color and drawing polylines on blank image. Displaying and destroying windows after inpainting, expanding area, and converting to RGB using PIL Image.",
        "type": "comment"
    },
    "2362": {
        "file_id": 249,
        "content": "# txts = [line[1][0] for line in result]\n# scores = [line[1][1] for line in result]\n# im_show = draw_ocr(image, boxes, txts, scores, font_path=simhei_path)\n# im_show = Image.fromarray(im_show)\n# im_show.save('result.jpg')\n# we will be testing one image only. not the whole goddamn video.\n# may have cuda error when using my cv2 cuda libs.",
        "type": "code",
        "location": "/tests/bilibili_practices/bilibili_video_translate/main_mask_english_text.py:69-76"
    },
    "2363": {
        "file_id": 249,
        "content": "This code segment retrieves text and scores from the result, applies OCR to an image using a specific font, and saves the resulting image as 'result.jpg'. It mentions that testing is focused on one image only, and CUDA errors may occur when using certain CV2 libraries.",
        "type": "comment"
    },
    "2364": {
        "file_id": 250,
        "content": "/tests/bilibili_practices/bilibili_video_translate/m2m100_1b_translator.py",
        "type": "filepath"
    },
    "2365": {
        "file_id": 250,
        "content": "This code imports the `m2m100_zte_translator` function from the `functional_dl_translator_1b_deepspeed` module, and defines two helper functions: `fixline` to remove specific Chinese ending characters, and `zh_to_en_translator` which uses `m2m100_zte_translator` to translate Chinese text to English and applies the line-fixing function if needed.",
        "type": "summary"
    },
    "2366": {
        "file_id": 250,
        "content": "from functional_dl_translator_1b_deepspeed import get_response as m2m100_zte_translator # this shit could consume much computational resource.\n# advice you to do it with json.\ndef fixline(line):\n    notEndings = [\"。\",\"，\"]\n    for x in notEndings:\n        if line.endswith(x): return line[:-1]\n    return line\ndef zh_to_en_translator(text,needFixLine=True):\n    result = m2m100_zte_translator(text)[0] # shit.\n    if needFixLine: result = fixline(result)\n    return result",
        "type": "code",
        "location": "/tests/bilibili_practices/bilibili_video_translate/m2m100_1b_translator.py:1-13"
    },
    "2367": {
        "file_id": 250,
        "content": "This code imports the `m2m100_zte_translator` function from the `functional_dl_translator_1b_deepspeed` module, and defines two helper functions: `fixline` to remove specific Chinese ending characters, and `zh_to_en_translator` which uses `m2m100_zte_translator` to translate Chinese text to English and applies the line-fixing function if needed.",
        "type": "comment"
    },
    "2368": {
        "file_id": 251,
        "content": "/tests/bilibili_practices/bilibili_video_translate/launch_frame_translate.sh",
        "type": "filepath"
    },
    "2369": {
        "file_id": 251,
        "content": "This code launches three different Python scripts for video frame translation: 'frame_translate_processor.py', 'frame_translate_processor2.py', and 'frame_translate_processor3.py'. Each script is likely used for a specific video frame translation task, potentially with improvements or updates from the previous versions.",
        "type": "summary"
    },
    "2370": {
        "file_id": 251,
        "content": "# python3 frame_translate_processor.py\npython3 frame_translate_processor2.py\n# python3 frame_translate_processor3.py",
        "type": "code",
        "location": "/tests/bilibili_practices/bilibili_video_translate/launch_frame_translate.sh:1-3"
    },
    "2371": {
        "file_id": 251,
        "content": "This code launches three different Python scripts for video frame translation: 'frame_translate_processor.py', 'frame_translate_processor2.py', and 'frame_translate_processor3.py'. Each script is likely used for a specific video frame translation task, potentially with improvements or updates from the previous versions.",
        "type": "comment"
    },
    "2372": {
        "file_id": 252,
        "content": "/tests/bilibili_practices/bilibili_video_translate/functional_redraw_chinese_text_offline2.py",
        "type": "filepath"
    },
    "2373": {
        "file_id": 252,
        "content": "This code uses CUDA with OpenCV for image processing, inpainting and masked blurring. It converts images to Pillow for text rotation, stroke width handling, and transparency blending. The function estimates text orientation, size, and center, then applies image processing tasks before saving and returning the result.",
        "type": "summary"
    },
    "2374": {
        "file_id": 252,
        "content": "use_cuda_cv2 = True # after we compile shit\nif use_cuda_cv2: # the freaking speed is awful.\n    import pathlib\n    import site\n    import sys\n    # this is root. this is not site-packages.\n    # site_path = pathlib.Path([x for x in site.getsitepackages() if \"site-packages\" in x][0])\n    site_path = pathlib.Path(\"/usr/local/lib/python3.9/site-packages\") # maybe it is done after you make install the whole cv2 shit.\n    cv2_libs_dir = site_path / 'cv2' / f'python-{sys.version_info.major}.{sys.version_info.minor}'\n    print(cv2_libs_dir)\n    cv2_libs = sorted(cv2_libs_dir.glob(\"*.so\"))\n    if len(cv2_libs) == 1:\n        print(\"INSERTING:\",cv2_libs[0].parent)\n        sys.path.insert(1, str(cv2_libs[0].parent))\nimport cv2\nimport numpy as np\nfrom PIL import Image, ImageFont, ImageDraw  \nimport Levenshtein\nimport math\n# from m2m100_1b_translator import zh_to_en_translator as translator\n# i just want to do the freaking inpainting.\n# import statistics\ndef redraw_english_to_chinese2(image,resultChineseInternal): \n    a,b,c = image.shape",
        "type": "code",
        "location": "/tests/bilibili_practices/bilibili_video_translate/functional_redraw_chinese_text_offline2.py:1-30"
    },
    "2375": {
        "file_id": 252,
        "content": "The code is setting up the environment for using CUDA with OpenCV and importing necessary libraries. It checks if a specific OpenCV library file exists, and if so, it adds its parent directory to the system path. The function redraw_english_to_chinese2 is defined at the end of the code snippet, but its implementation is not visible in this chunk.",
        "type": "comment"
    },
    "2376": {
        "file_id": 252,
        "content": "    total_area = a*b\n    total_center = (a/2,b/2)\n    total_corners = [(a,0),(0,0),(0,b),(a,b)]\n    total_strings = [\"scorpa\"]\n    area_threshold = 1/15 # don't know.\n    area_threshold = total_area*area_threshold\n    mask3_threshold = area_threshold*0.6\n    blank_image = np.zeros(shape=[a,b], dtype=np.uint8) # the exact order\n    blank_image2 = np.zeros(shape=[a,b], dtype=np.uint8) # the exact order\n    blank_image3 = np.zeros(shape=[a,b], dtype=np.uint8) # the exact order\n    def compareStringSimilarity(text,targetCompareString=\"scorpa\"):\n        # what is this?\n        comparedWaterMarkString = targetCompareString.lower() # the freaking name \n        comparedWaterMarkStringLength = len(comparedWaterMarkString)\n            # remove watermarks? how to filter?\n            # no fucking translation at all.\n        editDistanceThreshold = 4\n        textCompareCandidate = text.replace(\" \",\"\").lower() # original text, no translation.\n        distance = Levenshtein.distance(textCompareCandidate,comparedWaterMarkString)",
        "type": "code",
        "location": "/tests/bilibili_practices/bilibili_video_translate/functional_redraw_chinese_text_offline2.py:32-52"
    },
    "2377": {
        "file_id": 252,
        "content": "This code calculates the total area and center of an image, sets the corners and initializes blank images. It defines a function to compare string similarity between two strings with an edit distance threshold, likely for text recognition or watermark detection.",
        "type": "comment"
    },
    "2378": {
        "file_id": 252,
        "content": "        string_length = len(text)\n        string_length_difference = abs(string_length-comparedWaterMarkStringLength)\n        length_difference_threshold = 3\n        if (distance < editDistanceThreshold and string_length_difference < length_difference_threshold):\n            return True\n        return False\n    def get_center(rectangle_coords):\n        x0,y0 = rectangle_coords[0]\n        x1,y1 = rectangle_coords[2]\n        return ((x0+x1)/2,(y0+y1)/2)\n    def get_distance(a,b): x = a[0]-b[0]; x2 = x**2; y = a[1]-b[1];y2 = y**2; return(math.sqrt(x2+y2))\n    resultChineseInternal2 = list(sorted(resultChineseInternal,key=lambda x:min([get_distance(corner,get_center(x[0])) for corner in total_corners]))) # sort by centrality. but not by corner. use corner instead.\n    resultChineseInternal2 = sorted(resultChineseInternal2,key=lambda x:1-max([int(compareStringSimilarity(x[1][0],tstring)) for tstring in total_strings])) # sort by centrality. but not by corner. use corner instead.\n    for coords, (text,prob) in resultChineseInternal2: # get boundary coords first.",
        "type": "code",
        "location": "/tests/bilibili_practices/bilibili_video_translate/functional_redraw_chinese_text_offline2.py:53-66"
    },
    "2379": {
        "file_id": 252,
        "content": "The code compares the length of a text string with a predefined length, and if the difference is within a certain threshold, it returns True. The code also calculates distances between points using coordinates, sorts a list based on these distances and centrality, and retrieves boundary coordinates for each text string.",
        "type": "comment"
    },
    "2380": {
        "file_id": 252,
        "content": "        polyArray = np.array(coords).astype(np.int64) # fuck.\n        # print(polyArray)\n        # print(polyArray.shape)\n        # breakpoint()\n        # points = np.array([[160, 130], [350, 130], [250, 300]])\n        # print(points.dtype)\n        # points = np.array([[454.0, 22.0], [464.0, 26.0], [464.0, 85.0]]).astype(np.int64)\n        # this is rectangular. simple shit. not simple for other shits.\n        color= 255\n        coord0, coord1, coord2 = coords[0],coords[1],coords[2]\n        sid1, sid2 = get_distance(coord0,coord1), get_distance(coord1,coord2)\n        polyArea = sid1*sid2\n        mask3_area = np.sum(blank_image3)\n        if polyArea >= area_threshold or mask3_area >= mask3_threshold:\n            cv2.fillPoly(blank_image,[polyArray],color)\n            isClosed = True\n            thickness = 20 # oh shit.\n            thickness2 = 40 # oh shit.\n            cv2.polylines(blank_image, [polyArray], isClosed, color, thickness) # much better.\n            cv2.polylines(blank_image2, [polyArray], isClosed, color, thickness2) # much better.",
        "type": "code",
        "location": "/tests/bilibili_practices/bilibili_video_translate/functional_redraw_chinese_text_offline2.py:67-86"
    },
    "2381": {
        "file_id": 252,
        "content": "Code creates a numpy array from given coordinates, checks if the area is above certain thresholds, and then uses cv2.fillPoly and cv2.polylines to draw on two images with different thicknesses.",
        "type": "comment"
    },
    "2382": {
        "file_id": 252,
        "content": "        else:\n            cv2.fillPoly(blank_image3,[polyArray],color)\n            isClosed = True\n            thickness = 30 # oh shit.\n            thickness2 = 50 # oh shit.\n            # cv2.polylines(blank_image, [polyArray], isClosed, color, thickness) # much better.\n            cv2.polylines(blank_image3, [polyArray], isClosed, color, thickness) # much better.\n            cv2.polylines(blank_image2, [polyArray], isClosed, color, thickness2) # much better.\n    #     # cv2.fillPoly(blank_image,pts=[points],color=(255, 255,255))\n    # cv2.imshow(\"mask\",blank_image)\n    # cv2.waitKey(0)\n    # use wordninja.\n    # before translation we need to lowercase these shits.\n    # inpaint_alternative = cv2.INPAINT_NS\n    # dst = cv2.inpaint(image,blank_image,3,inpaint_alternative)\n    def partial_blur(image0,mask,kernel=(200,200)):\n        # need improvement. malnly the boundary.\n        mask_total = mask\n        inv_mask_total = 255-mask_total\n        # mask0 = mask\n        # mask0 = mask/255\n        # inv_mask0 = inv_mask/255",
        "type": "code",
        "location": "/tests/bilibili_practices/bilibili_video_translate/functional_redraw_chinese_text_offline2.py:87-108"
    },
    "2383": {
        "file_id": 252,
        "content": "This code uses OpenCV for image processing tasks. It fills polygons and draws lines on an image, then applies inpainting to replace the filled area with surrounding pixels. The function `partial_blur` is defined to blur a specific region of an image using a mask.",
        "type": "comment"
    },
    "2384": {
        "file_id": 252,
        "content": "        non_blur_image = cv2.bitwise_and(image0, image0, mask=inv_mask_total)\n        blur_image0 = cv2.blur(image0,kernel) # half quicklier.\n        blur_image0 = cv2.bitwise_and(blur_image0, blur_image0, mask=mask_total)\n        dst0 = blur_image0 + non_blur_image\n        return dst0\n    def partial_blur_deprecated(image0,mask,mask2):\n        # need improvement. malnly the boundary.\n        mask_total = mask + mask2 # not good.\n        dtype = mask.dtype\n        mask_total = mask_total>0\n        mask_total=mask_total.astype(dtype)\n        mask_total = mask_total*255\n        inv_mask_total = 255-mask_total\n        mask0 = mask_total - mask2\n        # mask0 = mask\n        # mask0 = mask/255\n        # inv_mask0 = inv_mask/255\n        non_blur_image = cv2.bitwise_and(image0, image0, mask=inv_mask_total)\n        blur_image0 = cv2.blur(image0,(50,50)) # half quicklier.\n        blur_image2 = cv2.blur(image0,(30,30)) # half quicklier.\n        # not enough baby\n        blur_image0 = cv2.bitwise_and(blur_image0, blur_image0, mask=mask0)",
        "type": "code",
        "location": "/tests/bilibili_practices/bilibili_video_translate/functional_redraw_chinese_text_offline2.py:109-130"
    },
    "2385": {
        "file_id": 252,
        "content": "This code performs partial image blurring using OpenCV functions. It creates a total mask by adding two input masks, applies bitwise operations to separate areas of the image for non-blurred and blurred processing, and combines the results for output. The \"partial_blur_deprecated\" function needs improvement as it currently has hardcoded blur values and may not handle boundary areas well.",
        "type": "comment"
    },
    "2386": {
        "file_id": 252,
        "content": "        blur_image2 = cv2.bitwise_and(blur_image2, blur_image2, mask=mask2)\n        dst0 = blur_image0 +blur_image2 + non_blur_image\n        return dst0\n    dst = partial_blur(image,blank_image)\n    dst = cv2.inpaint(dst,blank_image3,3,cv2.INPAINT_TELEA) # this shit. only do for small areas\n    dst = partial_blur(dst,blank_image2,kernel=(30,30))\n    # to compensate all sharp boundaries.\n    # from PIL import Image\n    def np2pillow(opencv_image):\n        color_coverted = cv2.cvtColor(opencv_image, cv2.COLOR_BGR2RGB)\n        pil_image = Image.fromarray(color_coverted)\n        return pil_image\n        # pil_image.show()\n    def pillow2np(pil_image):\n        # pil_image=Image.open(\"demo2.jpg\") # open image using PIL\n        # use numpy to convert the pil_image into a numpy array\n        numpy_image=np.array(pil_image)  \n        # convert to a openCV2 image, notice the COLOR_RGB2BGR which means that \n        # the color is converted from RGB to BGR format\n        opencv_image=cv2.cvtColor(numpy_image, cv2.COLOR_RGB2BGR) ",
        "type": "code",
        "location": "/tests/bilibili_practices/bilibili_video_translate/functional_redraw_chinese_text_offline2.py:131-153"
    },
    "2387": {
        "file_id": 252,
        "content": "Performs bitwise AND operation on blur_image2 using mask2, combines images, applies inpainting with CV2, compensates sharp boundaries, converts OpenCV image to PIL and back.",
        "type": "comment"
    },
    "2388": {
        "file_id": 252,
        "content": "        return opencv_image\n    # draw text now!\n    mpil_image = np2pillow(dst)\n    draw = ImageDraw.Draw(mpil_image)\n    font_location = \"/root/Desktop/works/bilibili_tarot/SimHei.ttf\" # not usual english shit.\n    def draw_rotated_text(image, text, position, angle, font, fill=(255,255,255),stroke_width=1,stroke_fill=(0,0,0),align=\"left\"):\n    # Get rendered font width and height.\n        draw = ImageDraw.Draw(image)\n        width, height = draw.textsize(text, font=font,stroke_width=stroke_width)\n        # Create a new image with transparent background to store the text.\n        textimage = Image.new('RGBA', (width, height), (0,0,0,0))\n        # Render the text.\n        textdraw = ImageDraw.Draw(textimage)\n        textdraw.text((0,0), text, font=font, fill=fill,stroke_width=stroke_width,stroke_fill=stroke_fill,align=align)\n        # Rotate the text image.\n        rotated = textimage.rotate(angle, expand=1) # do you rotate shit?\n        # Paste the text into the image, using it as a mask for transparency.",
        "type": "code",
        "location": "/tests/bilibili_practices/bilibili_video_translate/functional_redraw_chinese_text_offline2.py:154-170"
    },
    "2389": {
        "file_id": 252,
        "content": "This code snippet is responsible for drawing rotated text on an OpenCV image using Pillow library. It first converts the OpenCV image to a Pillow image, then defines a function `draw_rotated_text()` to handle the text drawing process. The function calculates the text dimensions and creates a new image with a transparent background. It then renders the text onto this new image while accounting for stroke width and alignment. Finally, it rotates the text image by a specified angle and pastes it onto the original image using transparency.",
        "type": "comment"
    },
    "2390": {
        "file_id": 252,
        "content": "        image.paste(rotated, position, rotated)\n        return image\n    def average(mlist):return sum(mlist) / len(mlist)\n    def get_coord_orientation_font_size_and_center(coords):\n        xlist, ylist = [x[0] for x in coords], [x[1] for x in coords]\n        min_x, max_x = min(xlist), max(xlist)\n        min_y, max_y = min(ylist), max(ylist)\n        width,height = max_x-min_x, max_y-min_y\n        position = (min_x,min_y)\n        c0,c1,c2,c3 = coords\n        real_width = average([get_distance(c0,c1) ,get_distance(c2,c3)])\n        real_height = average([get_distance(c1,c2) ,get_distance(c3,c0)])\n        # c0-------------c1\n        # |              |\n        # c3-------------c2\n        rotate_vectors = (c1[0]-c0[0],c1[1]-c0[1]),(c2[0]-c3[0],c2[1]-c3[1])\n        rotate_vector = (average([rotate_vectors[0][0],rotate_vectors[1][0]]),average([rotate_vectors[0][1],rotate_vectors[1][1]]))\n        rotate_angle = math.atan2(rotate_vector[1],rotate_vector[0]) # problem with angle.\n        # print(\"ROTATE_VECTORS:\",rotate_vectors)",
        "type": "code",
        "location": "/tests/bilibili_practices/bilibili_video_translate/functional_redraw_chinese_text_offline2.py:171-193"
    },
    "2391": {
        "file_id": 252,
        "content": "This code defines a function `get_coord_orientation_font_size_and_center` that takes in coordinates and returns the orientation, font size, and center based on the given points. It calculates the dimensions of the bounding rectangle, determines the real width and height by averaging the distances between corresponding points, and estimates the rotation angle using `math.atan2`. The function may be useful for image processing tasks involving text or shapes with specific orientations.",
        "type": "comment"
    },
    "2392": {
        "file_id": 252,
        "content": "        # print(\"ROTATE VECTOR:\",rotate_vector)\n        # print(\"ROTATE ANGLE:\", rotate_angle)\n        center = (int((max_x+min_x)/2),int((max_y+min_y)/2))\n        # what about rotation? forget about it...\n        if (width / height) < 0.8:\n            orientation = \"vertical\"\n            font_size = int(real_width) # shit.\n        else:\n            orientation = \"horizontal\"\n            font_size = int(real_height)\n        return orientation, font_size, center,(real_width,real_height),rotate_angle,position \n    readjust_size=False # just center.\n    for coords, (text,prob) in resultChineseInternal:\n        probThreshold = 0.8\n        if compareStringSimilarity(text) or prob < probThreshold: # this is somehow not right. i don't know.\n            # mask all with low probabilities?\n            continue # skip all shits.\n        # specified font size \n        # text = translator(text) # now translate.\n        # too freaking slow. i need to freaking change this shit.\n        orientation, font_size, center ,(width,height) ,rotate_angle,position = get_coord_orientation_font_size_and_center(coords)",
        "type": "code",
        "location": "/tests/bilibili_practices/bilibili_video_translate/functional_redraw_chinese_text_offline2.py:194-217"
    },
    "2393": {
        "file_id": 252,
        "content": "This code is determining the orientation, font size, and center coordinates for a Chinese text. It checks if the aspect ratio of the image is vertical or horizontal and adjusts the font size accordingly. The function get_coord_orientation_font_size_and_center is called to get these values. If the probability threshold is met or the text is not similar enough, it will skip that particular text. It also considers rotation angle and position in its calculations.",
        "type": "comment"
    },
    "2394": {
        "file_id": 252,
        "content": "        if orientation == \"horizontal\":\n            font = ImageFont.truetype(font_location, font_size)\n            # text = original_text\n            # drawing text size \n            stroke_width = max((1,int(0.1*font_size)))\n            (string_width,string_height) = draw.textsize(text,font=font,stroke_width=stroke_width)\n            # print(string_width)\n            # breakpoint()\n            if readjust_size:\n                change_ratio = width/string_width\n                new_fontsize = font_size*change_ratio\n                font = ImageFont.truetype(font_location, new_fontsize)\n                (string_width,string_height) = draw.textsize(text,font=font,stroke_width=stroke_width)\n            #     start_x = int(center[0]-width2/2)\n            #     start_y = int(center[1]-height2/2)\n            # else:\n            theta  =rotate_angle\n            rot = np.array([[math.cos(theta), -math.sin(theta)], [math.sin(theta), math.cos(theta)]])\n            v1 = np.array([string_width,string_height])\n            v2 = np.array([string_width,-string_height])",
        "type": "code",
        "location": "/tests/bilibili_practices/bilibili_video_translate/functional_redraw_chinese_text_offline2.py:218-239"
    },
    "2395": {
        "file_id": 252,
        "content": "This code is calculating the size of a text string and adjusting its font size to fit within a specified width. If `readjust_size` is True, it recalculates the font size based on the new width. The text's rotation angle is calculated using trigonometry functions.",
        "type": "comment"
    },
    "2396": {
        "file_id": 252,
        "content": "            v3 = np.array([-string_width,-string_height])\n            v4 = np.array([-string_width,string_height])\n            # w = np.array([3, 4])\n            vc1 = np.dot(rot, v1)\n            vc2 = np.dot(rot, v2)\n            vc3 = np.dot(rot, v3)\n            vc4 = np.dot(rot, v4)\n            # sw2 = abs(float(vc2[0])) # no abs.\n            # sh2 = abs(float(vc2[1]))\n            start_x_arr = [int(center[0]-sw/2) for sw in [(float(x[0])) for x in [vc1,vc2,vc3,vc4]]]\n            start_y_arr = [int(center[1]-sh/2) for sh in [(float(x[1])) for x in [vc1,vc2,vc3,vc4]]]\n            # start_y = int(center[1]-string_height2/2)\n            start_x = int(min(start_x_arr))\n            start_y = int(min(start_y_arr))\n            # draw.text((start_x, start_y), text, font = font, fill=(255,255,255),stroke_fill=(0,0,0),stroke_width = stroke_width,align =\"left\") # what is the freaking align?\n            position2 = (start_x, start_y)\n            rotate_angle2 = -np.rad2deg(rotate_angle) # strange.\n            # debug_text = \"angle: {}\".format(rotate_angle2)",
        "type": "code",
        "location": "/tests/bilibili_practices/bilibili_video_translate/functional_redraw_chinese_text_offline2.py:240-258"
    },
    "2397": {
        "file_id": 252,
        "content": "This code performs rotations on given points using a rotation matrix, then calculates the starting coordinates for drawing text based on the minimum x and y values among these transformed points. It also adjusts the angle to its equivalent in degrees before potentially performing further operations with it.",
        "type": "comment"
    },
    "2398": {
        "file_id": 252,
        "content": "            mpil_image = draw_rotated_text(mpil_image,text,position2,rotate_angle2,font,stroke_width=stroke_width)\n    # mpil_image.show()\n    # mpil_image.save(\"redraw_eng_to_chinese.png\")\n    output_final_image = pillow2np(mpil_image)\n    return output_final_image",
        "type": "code",
        "location": "/tests/bilibili_practices/bilibili_video_translate/functional_redraw_chinese_text_offline2.py:259-264"
    },
    "2399": {
        "file_id": 252,
        "content": "This code snippet draws rotated text on an image, saves it as a PNG file, and then converts the image to a numpy array before returning it.",
        "type": "comment"
    }
}