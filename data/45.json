{
    "4500": {
        "file_id": 575,
        "content": "# ffmpeg -y -i edm_super_summit.m4a -ss 00:00:50 -to 00:01:05 best_edm_split.mp3\nffmpeg -y -i edm_super_summit.m4a -ss 00:01:38 -to 00:01:49 best_edm_split2.mp3",
        "type": "code",
        "location": "/tests/anime1_me_video_download/get_best_edm.sh:1-2"
    },
    "4501": {
        "file_id": 575,
        "content": "The code uses FFmpeg to extract segments from the \"edm_super_summit.m4a\" audio file, saving them as \"best_edm_split.mp3\" and \"best_edm_split2.mp3\". The -ss option specifies the start time and -to the end time for each segment.",
        "type": "comment"
    },
    "4502": {
        "file_id": 576,
        "content": "/tests/anime1_me_video_download/parse_static.py",
        "type": "filepath"
    },
    "4503": {
        "file_id": 576,
        "content": "Code is parsing HTML data from 'sample.html' using BeautifulSoup and finding video tags. It then extracts channel and episode information from JSON data, formats download link, and prints the episode ID and download link. The code likely automates the process of downloading videos from a specific website.",
        "type": "summary"
    },
    "4504": {
        "file_id": 576,
        "content": "source = \"sample.html\"\n# curl -L -o sample.html \"https://anime1.me/category/2022%e5%b9%b4%e6%98%a5%e5%ad%a3/%e5%8b%87%e8%80%85%e8%be%ad%e8%81%b7%e4%b8%8d%e5%b9%b9%e4%ba%86\"\nfrom bs4 import BeautifulSoup\ndata = open(source,\"r\",encoding=\"utf-8\").read()\ndom = BeautifulSoup(data)\n# dom = BeautifulSoup(data,features='lxml')\nimport urllib.parse as up\nimport json\nimport re\nvideos = dom.find_all(\"video\")\nformat_download_link = lambda c,e: \"https://shiro.v.anime1.me/{}/{}.mp4\".format(c,e)\nfor video in videos:\n    # print(dir(video))\n    data_src = \"data-apireq\"\n    json_obj = video[data_src]\n    json_obj = up.unquote(json_obj)\n    json_obj = json.loads(json_obj)\n    channel, episode = json_obj[\"c\"], json_obj[\"e\"]\n    link = format_download_link(channel, episode)\n    episode_id = re.findall(r\"\\d+\",episode)[0]\n    print(\"EPISODE:\",episode_id)\n    print(\"DOWNLOAD LINK:\",link)\n    # breakpoint()",
        "type": "code",
        "location": "/tests/anime1_me_video_download/parse_static.py:1-28"
    },
    "4505": {
        "file_id": 576,
        "content": "Code is parsing HTML data from 'sample.html' using BeautifulSoup and finding video tags. It then extracts channel and episode information from JSON data, formats download link, and prints the episode ID and download link. The code likely automates the process of downloading videos from a specific website.",
        "type": "comment"
    },
    "4506": {
        "file_id": 577,
        "content": "/tests/anime1_me_video_download/get_cookie_sample.py",
        "type": "filepath"
    },
    "4507": {
        "file_id": 577,
        "content": "This code downloads a file, displays progress in real-time, uses chunked data for memory efficiency, and sets cookies from response headers.",
        "type": "summary"
    },
    "4508": {
        "file_id": 577,
        "content": "import requests\nimport json\nimport urllib.parse as up\nimport sys\n# import multithread\nfrom fake_useragent import UserAgent\nua = UserAgent()\nuser_agent =ua.random\nurl = \"https://v.anime1.me/api\"\n# data = '{\"c\":\"1019\",\"e\":\"6b\",\"t\":1652428857,\"p\":0,\"s\":\"ec9042ac177510fd67dd508f4d974074\"}'\n# data = '%7B%22c%22%3A%221019%22%2C%22e%22%3A%222b%22%2C%22t%22%3A1652429744%2C%22p%22%3A0%2C%22s%22%3A%225a78c05bd07077f05278ed6b44897878%22%7D'\ndata = \"%7B%22c%22%3A%221019%22%2C%22e%22%3A%225b%22%2C%22t%22%3A1652429744%2C%22p%22%3A0%2C%22s%22%3A%222d424b87559a56d7f761c436bca72502%22%7D\"\ndata_unquote = up.unquote(data)\ndata_json = json.loads(data_unquote)\n# url0 = \"https://anime1.me/category/2022%e5%b9%b4%e6%98%a5%e5%ad%a3/%e5%8b%87%e8%80%85%e8%be%ad%e8%81%b7%e4%b8%8d%e5%b9%b9%e4%ba%86\"\ns = requests.Session()\ns.headers.update({\"User-Agent\":user_agent}) # no freaking drama.\n# s.get(url0)\n# r = requests.post(url,body=data)\nmdata = \"d={}\".format(data)\nmheaders = {'authority': 'v.anime1.me'\n  ,'accept': '*/*' \n  ,'accept-language': 'en-US,en;q=0.9' ",
        "type": "code",
        "location": "/tests/anime1_me_video_download/get_cookie_sample.py:1-28"
    },
    "4509": {
        "file_id": 577,
        "content": "Code imports necessary libraries, sets a random user agent, defines the URL and data for API request, creates a session with the user agent as header, and formats the data for the API call.",
        "type": "comment"
    },
    "4510": {
        "file_id": 577,
        "content": "  ,'content-type': 'application/x-www-form-urlencoded' \n  ,'origin': 'https://anime1.me' \n  ,'referer': 'https://anime1.me/'}\nrpost = s.post(url,data=mdata,headers=mheaders)\n# print(dir(rpost))\nmjson2 = rpost.json()\ndownload_url = mjson2['s']['src']\ndownload_url = \"https:\"+download_url\ndownload_name = \"sample\"\ndownload_name = \"{}.{}\".format(download_name,download_url.split(\".\")[-1])\n# '{\"success\":false,\"errors\":[\"Signature invalid.\"]}' <- shit.\n# breakpoint()\n# print(rpost.text) # good. then where is the cookie?\n# print(s.cookies)\nfilename = download_name\n# print(\"downloading target file:\",filename)\n# download_object = multithread.Downloader(download_url, filename,aiohttp_args= {\"headers\":mheaders_session}) # ther e is no 'Content-Length'\n# download_object.start()\nwith open(filename, 'wb') as f:\n    # response = requests.get(url, stream=True)\n    response = s.get(download_url,stream = True)\n    total = response.headers.get('content-length')\n    if total is None:\n        f.write(response.content)\n    else:\n        downloaded = 0",
        "type": "code",
        "location": "/tests/anime1_me_video_download/get_cookie_sample.py:29-60"
    },
    "4511": {
        "file_id": 577,
        "content": "This code downloads a video from anime1.me and saves it in the specified format. It uses requests library to handle HTTP requests, extracts download URL from JSON response, sets headers for post and get requests, opens file in write mode for downloading the video, checks content length of the video, and downloads it if content length is available.",
        "type": "comment"
    },
    "4512": {
        "file_id": 577,
        "content": "        total = int(total)\n        for data in response.iter_content(chunk_size=max(int(total/1000), 1024*1024)):\n            downloaded += len(data)\n            f.write(data)\n            done = int(50*downloaded/total)\n            sys.stdout.write('\\r[{}{}]'.format('█' * done, '.' * (50-done)))\n            sys.stdout.flush()\nsys.stdout.write('\\n')\n# print(download_content.headers)\n# now you have the freaking cookie.\n# <RequestsCookieJar[<Cookie e=1652444144 for .v.anime1.me/1019/2b.mp4>, <Cookie h=oRLPqsTE0KXMFmVWJD669g for .v.anime1.me/1019/2b.mp4>, <Cookie p=eyJpc3MiOiJhbmltZTEubWUiLCJleHAiOjE2NTI0NDQxNDQwMDAsImlhdCI6MTY1MjQzNDEzNzAwMCwic3ViIjoiLzEwMTkvMmIubXA0In0 for .v.anime1.me/1019/2b.mp4>]>\n# get set-cookie header.",
        "type": "code",
        "location": "/tests/anime1_me_video_download/get_cookie_sample.py:61-72"
    },
    "4513": {
        "file_id": 577,
        "content": "This code is downloading a file and displaying the progress in real-time. It uses chunked data to manage memory efficiently and sets cookies from the response headers.",
        "type": "comment"
    },
    "4514": {
        "file_id": 578,
        "content": "/tests/anime1_me_video_download/api_curl.sh",
        "type": "filepath"
    },
    "4515": {
        "file_id": 578,
        "content": "The code sends a POST request to anime1.me API using cURL, containing video ID, episode number, timestamp, and secret key, likely for interacting with anime videos. It also includes the \"--compressed\" flag for file compression during download, saving storage space and time.",
        "type": "summary"
    },
    "4516": {
        "file_id": 578,
        "content": "curl 'https://v.anime1.me/api' \\\n  -H 'authority: v.anime1.me' \\\n  -H 'accept: */*' \\\n  -H 'accept-language: en-US,en;q=0.9' \\\n  -H 'content-type: application/x-www-form-urlencoded' \\\n  -H 'origin: https://anime1.me' \\\n  -H 'referer: https://anime1.me/' \\\n  --data-raw 'd=%7B%22c%22%3A%221019%22%2C%22e%22%3A%226b%22%2C%22t%22%3A1652431596%2C%22p%22%3A0%2C%22s%22%3A%221bc65800b44a935eb4e5287655c64cb2%22%7D' \\\n  --compressed\n# curl 'https://v.anime1.me/api' \\\n#   -H 'authority: v.anime1.me' \\\n#   -H 'accept: */*' \\\n#   -H 'accept-language: en-US,en;q=0.9' \\\n#   -H 'content-type: application/x-www-form-urlencoded' \\\n#   -H 'origin: https://anime1.me' \\\n#   -H 'referer: https://anime1.me/' \\\n#   -H 'sec-ch-ua: \" Not A;Brand\";v=\"99\", \"Chromium\";v=\"101\"' \\\n#   -H 'sec-ch-ua-mobile: ?1' \\\n#   -H 'sec-ch-ua-platform: \"Android\"' \\\n#   -H 'sec-fetch-dest: empty' \\\n#   -H 'sec-fetch-mode: cors' \\\n#   -H 'sec-fetch-site: same-site' \\\n#   -H 'user-agent: Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.4951.41 Mobile Safari/537.36' \\",
        "type": "code",
        "location": "/tests/anime1_me_video_download/api_curl.sh:1-24"
    },
    "4517": {
        "file_id": 578,
        "content": "This code sends a POST request to 'https://v.anime1.me/api' using cURL, with specified headers and data in the request body. The request includes an API key for authentication and retrieves data from the anime1.me website.",
        "type": "comment"
    },
    "4518": {
        "file_id": 578,
        "content": "#   --data-raw 'd=%7B%22c%22%3A%221019%22%2C%22e%22%3A%226b%22%2C%22t%22%3A1652431596%2C%22p%22%3A0%2C%22s%22%3A%221bc65800b44a935eb4e5287655c64cb2%22%7D' \\\n#   --compressed\n# curl 'https://v.anime1.me/api' \\\n#   -H 'authority: v.anime1.me' \\\n#   -H 'accept: */*' \\\n#   -H 'accept-language: en-US,en;q=0.9' \\\n#   -H 'content-type: application/x-www-form-urlencoded' \\\n#   -H 'cookie: _ga=GA1.2.354375679.1652431604; _gid=GA1.2.1847563412.1652431604; _gat=1' \\\n#   -H 'origin: https://anime1.me' \\\n#   -H 'referer: https://anime1.me/' \\\n#   -H 'sec-ch-ua: \" Not A;Brand\";v=\"99\", \"Chromium\";v=\"101\"' \\\n#   -H 'sec-ch-ua-mobile: ?1' \\\n#   -H 'sec-ch-ua-platform: \"Android\"' \\\n#   -H 'sec-fetch-dest: empty' \\\n#   -H 'sec-fetch-mode: cors' \\\n#   -H 'sec-fetch-site: same-site' \\\n#   -H 'user-agent: Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.4951.41 Mobile Safari/537.36' \\\n#   --data-raw 'd=%7B%22c%22%3A%221019%22%2C%22e%22%3A%226b%22%2C%22t%22%3A1652431596%2C%22p%22%3A0%2C%22s%22%3A%221bc65800b44a935eb4e5287655c64cb2%22%7D' \\",
        "type": "code",
        "location": "/tests/anime1_me_video_download/api_curl.sh:25-43"
    },
    "4519": {
        "file_id": 578,
        "content": "This code is making an API request to 'https://v.anime1.me/api' using curl command with various headers and a data payload in JSON format. The payload contains information such as video ID, episode number, timestamp, and secret key. It seems to be fetching information or performing an action related to an anime video from the anime1.me website.",
        "type": "comment"
    },
    "4520": {
        "file_id": 578,
        "content": "#   --compressed",
        "type": "code",
        "location": "/tests/anime1_me_video_download/api_curl.sh:44-44"
    },
    "4521": {
        "file_id": 578,
        "content": "The code snippet \"--compressed\" is used to compress the file during download, which can save storage space and reduce transfer time.",
        "type": "comment"
    },
    "4522": {
        "file_id": 579,
        "content": "/tests/viral_video_experiments/init.sh",
        "type": "filepath"
    },
    "4523": {
        "file_id": 579,
        "content": "This code initiates the setup for viral video data analysis and prediction. It clones two repositories, ViralCaster for analysis and prediction tasks, and 360ImageSearch and BaiduSerchImgApi for image recognition purposes.",
        "type": "summary"
    },
    "4524": {
        "file_id": 579,
        "content": "# viral video data analysis, prediction\n# git clone https://github.com/jjbreen/ViralCaster\n# image recognition\ngit clone https://github.com/chenguanyou/BaiduSerchImgApi\ngit clone https://github.com/chenguanyou/360ImageSearch",
        "type": "code",
        "location": "/tests/viral_video_experiments/init.sh:1-6"
    },
    "4525": {
        "file_id": 579,
        "content": "This code initiates the setup for viral video data analysis and prediction. It clones two repositories, ViralCaster for analysis and prediction tasks, and 360ImageSearch and BaiduSerchImgApi for image recognition purposes.",
        "type": "comment"
    },
    "4526": {
        "file_id": 580,
        "content": "/tests/basic_pitch_multi_midi_conversion/test.sh",
        "type": "filepath"
    },
    "4527": {
        "file_id": 580,
        "content": "Creates a new folder called \"output_path\", then uses the 'basic-pitch' command to convert MIDI files from \"/media/root/help/pyjom/tests/bilibili_practices/bilibili_tarot/some_bgm.mp3\" into audio format, saving them in the newly created folder.",
        "type": "summary"
    },
    "4528": {
        "file_id": 580,
        "content": "mkdir output_path\nbasic-pitch --sonify-midi output_path /media/root/help/pyjom/tests/bilibili_practices/bilibili_tarot/some_bgm.mp3",
        "type": "code",
        "location": "/tests/basic_pitch_multi_midi_conversion/test.sh:1-2"
    },
    "4529": {
        "file_id": 580,
        "content": "Creates a new folder called \"output_path\", then uses the 'basic-pitch' command to convert MIDI files from \"/media/root/help/pyjom/tests/bilibili_practices/bilibili_tarot/some_bgm.mp3\" into audio format, saving them in the newly created folder.",
        "type": "comment"
    },
    "4530": {
        "file_id": 581,
        "content": "/tests/bezier_paddlehub_dogcat_detector_serving/server.py",
        "type": "filepath"
    },
    "4531": {
        "file_id": 581,
        "content": "This code changes the directory, appends current path to sys.path, and imports specific configurations and classes for a Bezier PaddleHub ResNet50 Image DogCatDetectorServer in pyjom project.",
        "type": "summary"
    },
    "4532": {
        "file_id": 581,
        "content": "import sys\nimport os\ndef changeDirForImport():\n    os.chdir(\"/root/Desktop/works/pyjom\")\n    sys.path.append(\".\")\nif __name__ == '__main__':\n    changeDirForImport()\n    from pyjom.config.shared import pyjom_config\n    pyjom_config['BEZIER_PADDLE_RESNET50_IMAGE_DOG_CAT_DETECTOR_SERVER_INSTANCE']=True\n    from pyjom.imagetoolbox import bezierPaddleHubResnet50ImageDogCatDetectorServer\n    bezierPaddleHubResnet50ImageDogCatDetectorServer()",
        "type": "code",
        "location": "/tests/bezier_paddlehub_dogcat_detector_serving/server.py:1-13"
    },
    "4533": {
        "file_id": 581,
        "content": "This code changes the directory, appends current path to sys.path, and imports specific configurations and classes for a Bezier PaddleHub ResNet50 Image DogCatDetectorServer in pyjom project.",
        "type": "comment"
    },
    "4534": {
        "file_id": 582,
        "content": "/tests/bezier_paddlehub_dogcat_detector_serving/client.py",
        "type": "filepath"
    },
    "4535": {
        "file_id": 582,
        "content": "The code reads an image file from a specific location, changes the working directory for importing necessary libraries, initializes a client and server object for detecting dog/cat images using PaddleHub's ResNet50 model, reads the test image using OpenCV, performs detection on the image with the client object, and finally prints the result.",
        "type": "summary"
    },
    "4536": {
        "file_id": 582,
        "content": "test_image = \"/root/Desktop/works/pyjom/samples/image/dog_with_text.jpg\"\nfrom server import changeDirForImport\nchangeDirForImport()\nfrom pyjom.imagetoolbox import bezierPaddleHubResnet50ImageDogCatDetectorClient,bezierPaddleHubResnet50ImageDogCatDetectorServerChecker\nimport cv2\ntest_image = cv2.imread(test_image)\nbezierPaddleHubResnet50ImageDogCatDetectorServerChecker()\nresult = bezierPaddleHubResnet50ImageDogCatDetectorClient(test_image)\nprint(\"RESULT?\",result)",
        "type": "code",
        "location": "/tests/bezier_paddlehub_dogcat_detector_serving/client.py:1-10"
    },
    "4537": {
        "file_id": 582,
        "content": "The code reads an image file from a specific location, changes the working directory for importing necessary libraries, initializes a client and server object for detecting dog/cat images using PaddleHub's ResNet50 model, reads the test image using OpenCV, performs detection on the image with the client object, and finally prints the result.",
        "type": "comment"
    },
    "4538": {
        "file_id": 583,
        "content": "/tests/voice_detect_extract_split/paddlespeech/test.sh",
        "type": "filepath"
    },
    "4539": {
        "file_id": 583,
        "content": "The code exports HTTP and HTTPS proxy variables, runs TTS (Text-to-Speech) to convert text into audio (output.wav), and then performs ASR (Automatic Speech Recognition) in Chinese language using the output audio file. The code is intended for testing purposes with PaddleSpeech deep learning framework.",
        "type": "summary"
    },
    "4540": {
        "file_id": 583,
        "content": "export http_proxy=\"\"\nexport https_proxy=\"\"\n# this voice is great. excellent for my shit.\npaddlespeech tts --input \"你好，欢迎使用飞桨深度学习框架！\" --output output.wav # must download models on the fly.\npaddlespeech asr --lang zh --input output.wav\n# 你好欢迎使用非讲深度学习框架\n# how does it feel to have errors?\n# left and right variables are not the same. what is that?",
        "type": "code",
        "location": "/tests/voice_detect_extract_split/paddlespeech/test.sh:1-12"
    },
    "4541": {
        "file_id": 583,
        "content": "The code exports HTTP and HTTPS proxy variables, runs TTS (Text-to-Speech) to convert text into audio (output.wav), and then performs ASR (Automatic Speech Recognition) in Chinese language using the output audio file. The code is intended for testing purposes with PaddleSpeech deep learning framework.",
        "type": "comment"
    },
    "4542": {
        "file_id": 584,
        "content": "/tests/voice_detect_extract_split/spleeter/test2.sh",
        "type": "filepath"
    },
    "4543": {
        "file_id": 584,
        "content": "This code downloads an audio example, separates it into two components using Spleeter's 2-stems model, and saves the results in separate files. However, there seems to be an issue with the second separation process.",
        "type": "summary"
    },
    "4544": {
        "file_id": 584,
        "content": "# wget https://github.com/deezer/spleeter/raw/master/audio_example.mp3\n# separate the example audio into two components\npython3 -m spleeter separate -p spleeter:2stems -o output you_got_me.mp3\npython3 -m spleeter separate -p spleeter:2stems -o output tarot_desc.mp3\n# seems not working at all",
        "type": "code",
        "location": "/tests/voice_detect_extract_split/spleeter/test2.sh:1-5"
    },
    "4545": {
        "file_id": 584,
        "content": "This code downloads an audio example, separates it into two components using Spleeter's 2-stems model, and saves the results in separate files. However, there seems to be an issue with the second separation process.",
        "type": "comment"
    },
    "4546": {
        "file_id": 585,
        "content": "/tests/voice_detect_extract_split/spleeter/test.sh",
        "type": "filepath"
    },
    "4547": {
        "file_id": 585,
        "content": "This code retrieves an example audio file, separates it into two components using the Spleeter library, and saves the result in the \"output\" directory. However, it seems to be facing some issues with separation.",
        "type": "summary"
    },
    "4548": {
        "file_id": 585,
        "content": "# wget https://github.com/deezer/spleeter/raw/master/audio_example.mp3\n# separate the example audio into two components\npython3 -m spleeter separate -p spleeter:2stems -o output audio_example.mp3\n# seems not working at all",
        "type": "code",
        "location": "/tests/voice_detect_extract_split/spleeter/test.sh:1-4"
    },
    "4549": {
        "file_id": 585,
        "content": "This code retrieves an example audio file, separates it into two components using the Spleeter library, and saves the result in the \"output\" directory. However, it seems to be facing some issues with separation.",
        "type": "comment"
    },
    "4550": {
        "file_id": 586,
        "content": "/tests/voice_detect_extract_split/spleeter/spleeter_init.sh",
        "type": "filepath"
    },
    "4551": {
        "file_id": 586,
        "content": "Installs spleeter version 2.1.0, downloads spleeter-2.2.2-py3-none-any.whl and installs it, and imports pretrained_models/4stems.",
        "type": "summary"
    },
    "4552": {
        "file_id": 586,
        "content": "# pip3n install spleeter==2.1.0\nwget https://files.pythonhosted.org/packages/fb/2e/5d2cd3d0179d3f749d03eddf0172f1dbababbc371c1b5cbd7fc27d741070/spleeter-2.2.2-py3-none-any.whl\npip3n install spleeter-2.2.2-py3-none-any.whl # why you require specific tensorflow version?\n# https://github.com/deezer/spleeter/releases/download/v1.4.0/4stems.tar.gz\n# at pretrained_models/4stems",
        "type": "code",
        "location": "/tests/voice_detect_extract_split/spleeter/spleeter_init.sh:1-6"
    },
    "4553": {
        "file_id": 586,
        "content": "Installs spleeter version 2.1.0, downloads spleeter-2.2.2-py3-none-any.whl and installs it, and imports pretrained_models/4stems.",
        "type": "comment"
    },
    "4554": {
        "file_id": 587,
        "content": "/tests/voice_detect_extract_split/spleeter/README.md",
        "type": "filepath"
    },
    "4555": {
        "file_id": 587,
        "content": "This code describes using Spleeter, an open-sourced tool, and mentions two model hosts, Hugging Face and Wolfram Neural Network Library. These libraries provide paraphrasing models and can be utilized with the Wolfram Developer Engine respectively.",
        "type": "summary"
    },
    "4556": {
        "file_id": 587,
        "content": "use spleeter which is open-sourced.\nmany model hoster interests me. the most gigantic one is huggingface. providing paraphrasing models and more. another one is wolfram neural network library. can be used freely with wolfram developer engine.",
        "type": "code",
        "location": "/tests/voice_detect_extract_split/spleeter/README.md:1-3"
    },
    "4557": {
        "file_id": 587,
        "content": "This code describes using Spleeter, an open-sourced tool, and mentions two model hosts, Hugging Face and Wolfram Neural Network Library. These libraries provide paraphrasing models and can be utilized with the Wolfram Developer Engine respectively.",
        "type": "comment"
    },
    "4558": {
        "file_id": 588,
        "content": "/tests/voice_detect_extract_split/spleeter/download_models.sh",
        "type": "filepath"
    },
    "4559": {
        "file_id": 588,
        "content": "The code downloads pretrained model files for spleeter, a sound separation tool. It uses curl to retrieve the tarballs from GitHub releases and stores them in \"pretrained_models\" directory. After downloading, it moves the files and changes the directory to execute further tasks related to these models.",
        "type": "summary"
    },
    "4560": {
        "file_id": 588,
        "content": "curl -O -L https://github.com/deezer/spleeter/releases/download/v1.4.0/2stems.tar.gz\ncurl -O -L https://github.com/deezer/spleeter/releases/download/v1.4.0/4stems.tar.gz\ncurl -O -L https://github.com/deezer/spleeter/releases/download/v1.4.0/5stems.tar.gz\nmv {2stems.tar.gz, 4stems.tar.gz, 5stems.tar.gz} pretrained_models\ncd pretrained_models",
        "type": "code",
        "location": "/tests/voice_detect_extract_split/spleeter/download_models.sh:1-6"
    },
    "4561": {
        "file_id": 588,
        "content": "The code downloads pretrained model files for spleeter, a sound separation tool. It uses curl to retrieve the tarballs from GitHub releases and stores them in \"pretrained_models\" directory. After downloading, it moves the files and changes the directory to execute further tasks related to these models.",
        "type": "comment"
    },
    "4562": {
        "file_id": 589,
        "content": "/tests/bilibili_login_get_credential_view_data/tinydb_test.py",
        "type": "filepath"
    },
    "4563": {
        "file_id": 589,
        "content": "This code imports the tinydb library and sets a database location. It creates a TinyDB object at that location, which serves as a lightweight NoSQL document database. The code defines a User query using the tinydb.Query() function and performs an upsert operation on the database, where it either updates or inserts a document based on the provided condition (User.ghi == 123). Note that there is also a commented-out section for updating the database without an insertion operation.",
        "type": "summary"
    },
    "4564": {
        "file_id": 589,
        "content": "import tinydb\ndbLocation = \"test_credential.json\"\ndb = tinydb.TinyDB(dbLocation)\n# table = db.table('mytable')\nUser = tinydb.Query()\ndb.upsert({\"abc\": \"def\", \"ghi\": 123}, User.ghi == 123)  # please specify a condition!\n# db.update({'abc': 'def', 'ghi': 123}) # no insert here!",
        "type": "code",
        "location": "/tests/bilibili_login_get_credential_view_data/tinydb_test.py:1-8"
    },
    "4565": {
        "file_id": 589,
        "content": "This code imports the tinydb library and sets a database location. It creates a TinyDB object at that location, which serves as a lightweight NoSQL document database. The code defines a User query using the tinydb.Query() function and performs an upsert operation on the database, where it either updates or inserts a document based on the provided condition (User.ghi == 123). Note that there is also a commented-out section for updating the database without an insertion operation.",
        "type": "comment"
    },
    "4566": {
        "file_id": 590,
        "content": "/tests/bilibili_login_get_credential_view_data/test_login_successful.py",
        "type": "filepath"
    },
    "4567": {
        "file_id": 590,
        "content": "The code retrieves user credentials from a local database and uses them to log in to Bilibili API. It checks if the user exists, updates the name if necessary, and prints the login status.",
        "type": "summary"
    },
    "4568": {
        "file_id": 590,
        "content": "from bilibili_api.user import get_self_info\nfrom bilibili_api import Credential\n# how to load credential from our stored things?\nfrom lazero.search.api import getHomeDirectory\nimport os\nimport tinydb\nhome = getHomeDirectory()\ndbPath = os.path.join(home, \".bilibili_api.json\")\nimport tinydb\ndb = tinydb.TinyDB(dbPath)\nUser = tinydb.Query()\ndedeuserid = \"397424026\"  # pass it before you do shit!\ndataList = db.search(User.dedeuserid == dedeuserid)  # this will never change i suppose?\nif len(dataList) == 1:\n    data = dataList[0].copy()\n    print(\"try to login credential fetched from db:\", data)\n    oldName = data.pop(\"name\")\n    credential = Credential(**data)\n    from bilibili_api import sync\n    name = sync(get_self_info(credential))[\"name\"]\n    if oldName != name:\n        data[\"name\"] = name\n        db.upsert(data, User.dedeuserid == dedeuserid)\n    print(\"login successful:\", name)",
        "type": "code",
        "location": "/tests/bilibili_login_get_credential_view_data/test_login_successful.py:1-29"
    },
    "4569": {
        "file_id": 590,
        "content": "The code retrieves user credentials from a local database and uses them to log in to Bilibili API. It checks if the user exists, updates the name if necessary, and prints the login status.",
        "type": "comment"
    },
    "4570": {
        "file_id": 591,
        "content": "/tests/bilibili_login_get_credential_view_data/test_login_fail.py",
        "type": "filepath"
    },
    "4571": {
        "file_id": 591,
        "content": "This code retrieves user credentials, logs in using session data, and updates the name if it changed; attempts logging in with expired data to check for failure.",
        "type": "summary"
    },
    "4572": {
        "file_id": 591,
        "content": "from bilibili_api.user import get_self_info\nfrom bilibili_api import Credential\n# how to load credential from our stored things?\nfrom lazero.search.api import getHomeDirectory\nimport os\nimport tinydb\nhome = getHomeDirectory()\ndbPath = os.path.join(home, \".bilibili_api.json\")\nimport tinydb\ndb = tinydb.TinyDB(dbPath)\nUser = tinydb.Query()\ndedeuserid = \"397424026\" # pass it before you do shit!\ndataList = db.search(User.dedeuserid == dedeuserid) # this will never change i suppose?\nif len(dataList) == 1:\n    data = dataList[0].copy()\n    print('try to login credential fetched from db:', data)\n    oldName = data.pop('name')\n    credential = Credential(**{'dedeuserid': dedeuserid,'sessdata':'fakeSessionData'})\n    from bilibili_api import sync\n    name = sync(get_self_info(credential))['name']\n    # 'GetCookieReq.Session' Error:Field validation for 'Session' failed on the 'gte' tag。\n    # don't know how. maybe this works?\n    # if oldName !=name:\n    #     data['name']=name\n    #     db.upsert(data, User.dedeuserid == dedeuserid)",
        "type": "code",
        "location": "/tests/bilibili_login_get_credential_view_data/test_login_fail.py:1-27"
    },
    "4573": {
        "file_id": 591,
        "content": "This code retrieves a user's credential from the database, attempts to log in using the provided session data, and updates the name if it changed. If the name has not changed after logging in, it does not update the database.",
        "type": "comment"
    },
    "4574": {
        "file_id": 591,
        "content": "    # will never succeed.\n    # don't know using some expired sessdata will get what?\n    # maybe will still fail?\n    print('login successful:', name)",
        "type": "code",
        "location": "/tests/bilibili_login_get_credential_view_data/test_login_fail.py:28-31"
    },
    "4575": {
        "file_id": 591,
        "content": "This code block attempts to log in using expired session data, expecting the login to fail. It prints a message indicating whether the login was successful or not.",
        "type": "comment"
    },
    "4576": {
        "file_id": 592,
        "content": "/tests/bilibili_login_get_credential_view_data/test.py",
        "type": "filepath"
    },
    "4577": {
        "file_id": 592,
        "content": "This code allows users to choose between password and SMS login methods, with additional functionality for database storage and geetest validation. It performs a login, retrieves user data, updates the database, and asks about atomic insert in tinydb.",
        "type": "summary"
    },
    "4578": {
        "file_id": 592,
        "content": "from bilibili_api.login import (\n    login_with_password,\n    login_with_sms,\n    send_sms,\n    PhoneNumber,\n    Check,\n)\nfrom bilibili_api.user import get_self_info\nfrom bilibili_api import settings\nfrom bilibili_api import sync, Credential\n# mode = int(input(\"\"\"请选择登录方式：\n# 1. 密码登录\n# 2. 验证码登录\n# 请输入 1/2\n# \"\"\"))\nmode = 2\ncredential = None\n# 关闭自动打开 geetest 验证窗口\nsettings.geetest_auto_open = False\nif mode == 1:\n    # 密码登录\n    username = input(\"请输入手机号/邮箱：\")\n    password = input(\"请输入密码：\")\n    print(\"正在登录。\")\n    c = login_with_password(username, password)\n    if isinstance(c, Check):\n        # 还需验证\n        phone = input(\"需要验证。请输入手机号：\")\n        c.set_phone(PhoneNumber(phone, country=\"+86\"))  # 默认设置地区为中国大陆\n        c.send_code()\n        print(\"已发送验证码。\")\n        code = input(\"请输入验证码：\")\n        credential = c.login(code)\n        print(\"登录成功！\")\n    else:\n        credential = c\nelif mode == 2:\n    # 验证码登录\n    phone = input(\"请输入手机号：\")\n    print(\"正在登录。\")\n    send_sms(PhoneNumber(phone, country=\"+86\"))  # 默认设置地区为中国大陆\n    code = input(\"请输入验证码：\")",
        "type": "code",
        "location": "/tests/bilibili_login_get_credential_view_data/test.py:1-46"
    },
    "4579": {
        "file_id": 592,
        "content": "This code allows the user to choose between two login methods: password or SMS verification. If the user chooses password login, they input their credentials and are logged in immediately if valid. If the user chooses SMS login, they first need to enter their phone number and receive an SMS code. After entering the code, they're logged in. The code also has a setting to disable automatic opening of geetest validation window.",
        "type": "comment"
    },
    "4580": {
        "file_id": 592,
        "content": "    c = login_with_sms(PhoneNumber(phone, country=\"+86\"), code)\n    credential = c\n    print(\"登录成功\")\nelse:\n    print(\"请输入 1/2 ！\")\n    exit()\nfrom lazero.search.api import getHomeDirectory\nimport os\nhome = getHomeDirectory()\ndbPath = os.path.join(home, \".bilibili_api.json\")\nimport tinydb\ndb = tinydb.TinyDB(dbPath)\nif credential != None:\n    name = sync(get_self_info(credential))[\"name\"]\n    print(f\"欢迎，{name}!\")\n    buvid3 = credential.buvid3\n    bili_jct = credential.bili_jct\n    sessdata = credential.sessdata\n    dedeuserid = credential.dedeuserid  # this is userid, better use this instead?\n    User = tinydb.Query()\n    # assume that we are here to fetch valid credentials.\n    db.upsert(\n        {\n            \"name\": name,\n            \"dedeuserid\": dedeuserid,\n            \"bili_jct\": bili_jct,\n            \"buvid3\": buvid3,\n            \"sessdata\": sessdata,\n        },\n        User.dedeuserid == dedeuserid,\n    )\n    # how to perform atomic insert in tinydb?\n    # breakpoint()",
        "type": "code",
        "location": "/tests/bilibili_login_get_credential_view_data/test.py:47-82"
    },
    "4581": {
        "file_id": 592,
        "content": "This code performs a login with SMS and stores the credentials in a database. It first checks if the login was successful, then retrieves the name, buvid3, bili_jct, sessdata, and dedeuserid from the credentials. The code updates the database with this information using an upsert operation, ensuring that the dedeuserid is unique. Finally, it asks how to perform atomic insert in tinydb.",
        "type": "comment"
    },
    "4582": {
        "file_id": 593,
        "content": "/tests/bilibili_login_get_credential_view_data/dump_favourite_list.py",
        "type": "filepath"
    },
    "4583": {
        "file_id": 593,
        "content": "This code loads Bilibili API credentials, fetches user information and favorite lists, processes media data, utilizes TinyDB, and interacts with bilibili_api module. It iterates through elements, extracts bvid, title, updates desc as intro, searches for existing records, and upserts data if not present or loop breaks due to no more elements.",
        "type": "summary"
    },
    "4584": {
        "file_id": 593,
        "content": "from bilibili_api import favorite_list\n# that favourite list is public. i just want that.\n# dedeuserid = \"397424026\"\n# how to?\nfrom bilibili_api.user import get_self_info\nfrom bilibili_api import sync, Credential\n# how to load credential from our stored things?\n# from bilibili_api import user\nfrom lazero.search.api import getHomeDirectory\nimport os\nimport tinydb\nhome = getHomeDirectory()\ndbPath = os.path.join(home, \".bilibili_api.json\")\nimport tinydb\ndb = tinydb.TinyDB(dbPath)\ndbFavList = tinydb.TinyDB(\"bilibiliFavouriteList.json\")\nUser = tinydb.Query()\ndedeuserid = \"397424026\"  # pass it before you do shit!\ndataList = db.search(User.dedeuserid == dedeuserid)  # this will never change i suppose?\nif len(dataList) == 1:\n    data = dataList[0].copy()\n    print(\"try to login credential fetched from db:\", data)\n    oldName = data.pop(\"name\")\n    credential = Credential(**data)\n    from bilibili_api import sync\n    name = sync(get_self_info(credential))[\"name\"]\n    if oldName != name:\n        data[\"name\"] = name\n        db.upsert(data, User.dedeuserid == dedeuserid)",
        "type": "code",
        "location": "/tests/bilibili_login_get_credential_view_data/dump_favourite_list.py:1-34"
    },
    "4585": {
        "file_id": 593,
        "content": "This code aims to load the Bilibili API credentials from stored data and retrieve the user's information using a provided dedeuserid. It utilizes TinyDB for database operations, fetches the home directory, and interacts with the bilibili_api module. If a credential is found in the database, it will update the \"name\" field if necessary and print the retrieved credential information.",
        "type": "comment"
    },
    "4586": {
        "file_id": 593,
        "content": "    print(\"login successful:\", name)\n    # now you have it.\n    result = sync(\n        favorite_list.get_video_favorite_list(int(dedeuserid), None, credential)\n    )\n    print(result)  # None? wtf?\n    favLists = result[\"list\"]\n    for favList in favLists:\n        listId = favList[\"id\"]  # integer.\n        listName = favList[\"title\"]\n        print(\"processing favList:\", listName)\n        page = 0\n        while True:\n            import time\n            time.sleep(3)\n            page += 1\n            print(\"processing page:\", page)\n            result = sync(\n                favorite_list.get_video_favorite_list_content(\n                    listId, page=page, credential=credential\n                )\n            )\n            # import pprint\n            # pprint.pprint(result)\n            has_more = result[\"has_more\"]\n            # print(\"__________result__________\")\n            medias = result[\"medias\"]\n            if type(medias) != list or len(medias) == 0:\n                break\n            breakFlag = False\n            for elem in medias:",
        "type": "code",
        "location": "/tests/bilibili_login_get_credential_view_data/dump_favourite_list.py:35-66"
    },
    "4587": {
        "file_id": 593,
        "content": "Code is fetching user's favorite lists from Bilibili and processing each list's contents page by page. It checks for more content using \"has_more\" flag, fetches media data from the server, and breaks the loop when no more content is available or if the media data type is incorrect.",
        "type": "comment"
    },
    "4588": {
        "file_id": 593,
        "content": "                # print('ELEM:',elem)\n                # breakpoint()\n                # it has description.\n                videoData = {key: elem[key] for key in [\"bvid\", \"title\"]}\n                # here we call 'desc' as 'intro.\n                videoData.update({\"desc\": elem[\"intro\"]})\n                searchResult= dbFavList.search(User.bvid == videoData[\"bvid\"])\n                if len(searchResult) != 0:\n                    breakFlag=True\n                dbFavList.upsert(videoData, User.bvid == videoData[\"bvid\"])\n            if not has_more or breakFlag:\n                break",
        "type": "code",
        "location": "/tests/bilibili_login_get_credential_view_data/dump_favourite_list.py:67-78"
    },
    "4589": {
        "file_id": 593,
        "content": "This code iterates through elements, extracts bvid and title, updates with intro as desc, searches for existing records, upserts data if not already present or if the loop breaks due to no more elements.",
        "type": "comment"
    },
    "4590": {
        "file_id": 594,
        "content": "/tests/bilibili_login_get_credential_view_data/dump_view_history.py",
        "type": "filepath"
    },
    "4591": {
        "file_id": 594,
        "content": "The code fetches credentials from a local database, updates the user's name if necessary, and processes bilibili video history pages in increments of 100 per page, checking for duplicates and stopping upon completion or no more duplicates found.",
        "type": "summary"
    },
    "4592": {
        "file_id": 594,
        "content": "# how to?\nfrom bilibili_api.user import get_self_info\nfrom bilibili_api import Credential\n# how to load credential from our stored things?\nfrom bilibili_api import user\nfrom lazero.search.api import getHomeDirectory\nimport os\nimport tinydb\nhome = getHomeDirectory()\ndbPath = os.path.join(home, \".bilibili_api.json\")\nimport tinydb\ndb = tinydb.TinyDB(dbPath)\nUser = tinydb.Query()\ndedeuserid = \"397424026\"  # pass it before you do shit!\ndataList = db.search(User.dedeuserid == dedeuserid)  # this will never change i suppose?\nif len(dataList) == 1:\n    data = dataList[0].copy()\n    print(\"try to login credential fetched from db:\", data)\n    oldName = data.pop(\"name\")\n    credential = Credential(**data)\n    from bilibili_api import sync\n    name = sync(get_self_info(credential))[\"name\"]\n    if oldName != name:\n        data[\"name\"] = name\n        db.upsert(data, User.dedeuserid == dedeuserid)\n    print(\"login successful:\", name)\n    # now continue.\n    # how many pages you want? infinite?\n    import time\n    page_num = 0\n    dbHistory = tinydb.TinyDB(\"bilibiliHistory.json\")",
        "type": "code",
        "location": "/tests/bilibili_login_get_credential_view_data/dump_view_history.py:1-36"
    },
    "4593": {
        "file_id": 594,
        "content": "Code fetches credential from local database for a specific bilibili user, checks if the name is up-to-date, and updates it if necessary. Then, it continues with further processing while also keeping track of bilibili history in another database file.",
        "type": "comment"
    },
    "4594": {
        "file_id": 594,
        "content": "    while True:\n        time.sleep(3)\n        page_num += 1  # starts with 1\n        print(\"now processing page:\", page_num)\n        result = sync(\n            user.get_self_history(\n                page_num=page_num, per_page_item=100, credential=credential\n            )\n        )\n        # import pprint\n        # pprint.pprint(result)\n        if type(result) != list or len(result) == 0:\n            break\n        breakFlag=False\n        for elem in result:\n            # it has description.\n            videoData = {key: elem[key] for key in [\"bvid\", \"desc\", \"title\"]}\n            searchResult= dbHistory.search(User.bvid == videoData[\"bvid\"])\n            if len(searchResult) != 0:\n                breakFlag=True\n            dbHistory.upsert(videoData, User.bvid == videoData[\"bvid\"])\n        if breakFlag:\n            break",
        "type": "code",
        "location": "/tests/bilibili_login_get_credential_view_data/dump_view_history.py:37-59"
    },
    "4595": {
        "file_id": 594,
        "content": "This code is processing video history pages from a user's account. It fetches data in increments of 100 per page, checks for duplicates before storing the video details, and stops when there are no more pages or duplicate entries found.",
        "type": "comment"
    },
    "4596": {
        "file_id": 595,
        "content": "/tests/bilibili_practices/bilibili_dollar/fetch_related_content.py",
        "type": "filepath"
    },
    "4597": {
        "file_id": 595,
        "content": "The code imports the \"VideosSearch\" class from the \"youtube-search-python\" package and uses it to search for videos related to drawing realistic US Dollars. It fetches the first 10 results, then prints each video's title, ID, author name, channel ID, and view count.",
        "type": "summary"
    },
    "4598": {
        "file_id": 595,
        "content": "#!pip3 install youtube-search-python\nfrom youtubesearchpython import VideosSearch\n# videosSearch = VideosSearch('画人民币', limit = 10)\nvideosSearch = VideosSearch('Draw realistic US Dollar', limit = 10)\n# videosSearch = VideosSearch('NoCopyrightSounds', limit = 2)\n# print(videosSearch.result())\ndata = videosSearch.result()\nfor elem in data[\"result\"]:\n    title = elem[\"title\"]\n    videoId = elem[\"id\"]\n    contentType = elem[\"type\"]\n    authorName = elem[\"channel\"][\"name\"]\n    channelId = elem[\"channel\"][\"id\"]\n    viewCount = elem[\"viewCount\"][\"text\"]\n    print(\"title\",title)\n    print(\"videoId\",videoId)\n    print(\"author\",authorName)\n    print(\"channel ID\",channelId)\n    print(\"viewCount\",viewCount)\n    print(\"_______________________________________\")",
        "type": "code",
        "location": "/tests/bilibili_practices/bilibili_dollar/fetch_related_content.py:1-23"
    },
    "4599": {
        "file_id": 595,
        "content": "The code imports the \"VideosSearch\" class from the \"youtube-search-python\" package and uses it to search for videos related to drawing realistic US Dollars. It fetches the first 10 results, then prints each video's title, ID, author name, channel ID, and view count.",
        "type": "comment"
    }
}