{
    "5100": {
        "file_id": 660,
        "content": "            counter = 0\n            spanList = []\n            target_list = [(a, len(list(b))) for a, b in groupby(mlist)]\n            for a, b in target_list:\n                nextCounter = counter + b\n                if a == target:\n                    spanList.append((counter, nextCounter))\n                counter = nextCounter\n            return spanList\n        # solve diff.\n        xLeftPointsFilteredDiff = np.diff(xLeftPointsFiltered)\n        # xLeftPointsFilteredDiff3 = np.diff(xLeftPointsFilteredDiff)\n        # import matplotlib.pyplot as plt\n        # plt.plot(xLeftPointsFilteredDiff)\n        # plt.plot(xLeftPointsFiltered)\n        # plt.plot(xLeftPoints)\n        # plt.show()\n        # xLeftPointsFilteredDiff3Filtered = Kalman1D(xLeftPointsFilteredDiff3)\n        derivativeThreshold = 3\n        # derivative3Threshold = 3\n        xLeftPointsSignal = (\n            (abs(xLeftPointsFilteredDiff) < derivativeThreshold)\n            .astype(np.uint8)\n            .tolist()\n        )\n        def signalFilter(signal, threshold=10):",
        "type": "code",
        "location": "/tests/video_detector_tests/pip_meanVariance_stablize.py:156-184"
    },
    "5101": {
        "file_id": 660,
        "content": "The code filters and extracts a list of spans from a given input, likely for text processing or analysis purposes. It uses groupby function to split the input into consecutive repetitions of the same element, then iterates over the resulting list of tuples (element, count) to create a new span list based on a target element. The code also performs differential calculations on the filtered xLeftPoints and applies a derivative threshold to generate a binary signal list likely for further processing or visualization purposes.",
        "type": "comment"
    },
    "5102": {
        "file_id": 660,
        "content": "            newSignal = np.zeros(len(signal))\n            signalFiltered = extract_span(xLeftPointsSignal, target=1)\n            newSignalRanges = []\n            for start, end in signalFiltered:\n                length = end - start\n                if length >= threshold:\n                    newSignalRanges.append((start, end))\n                    newSignal[start : end + 1] = 1\n            return newSignal, newSignalRanges\n        xLeftPointsSignalFiltered, newSignalRanges = signalFilter(xLeftPointsSignal, threshold = signalFilterThreshold)\n        xLeftPointsSignalFiltered *= 255\n        mShrink = 2\n        from sklearn.linear_model import LinearRegression\n        target = []\n        for start, end in newSignalRanges:\n            # could we shrink the boundaries?\n            mStart, mEnd = start + mShrink, end - mShrink\n            if mEnd <= mStart:\n                continue\n            sample = xLeftPointsFiltered[mStart:mEnd]\n            std = np.std(sample)\n            if std > stdThreshold:\n                continue",
        "type": "code",
        "location": "/tests/video_detector_tests/pip_meanVariance_stablize.py:185-210"
    },
    "5103": {
        "file_id": 660,
        "content": "This code segment performs signal filtering and preparation for subsequent analysis. It extracts a filtered signal, selects ranges above a threshold, scales the values to 255, applies boundary shrinking, and checks if the standard deviation exceeds a threshold before passing it on for further processing.",
        "type": "comment"
    },
    "5104": {
        "file_id": 660,
        "content": "            model = LinearRegression()\n            X, y = np.array(range(sample.shape[0])).reshape(-1, 1), sample\n            model.fit(X, y)\n            coef = model.coef_[0]  # careful!\n            if abs(coef) > slopeThreshold:\n                continue\n            meanValue = int(np.mean(sample))\n            target.append({\"range\": (start, end), \"mean\": meanValue})\n            # print((start, end), std, coef)\n        newTarget = {}\n        for elem in target:\n            meanStr = str(elem[\"mean\"])\n            mRange = elem[\"range\"]\n            newTarget.update({meanStr: newTarget.get(meanStr, []) + [mRange]})\n        mStart = 0\n        mEnd = len(xLeftPoints)\n        newTarget = getContinualMappedNonSympyMergeResultWithRangedEmpty(\n            newTarget, mStart, mEnd\n        )\n        newTargetSequential = mergedRangesToSequential(newTarget)\n        if (newTargetSequential) == 1:\n            if newTargetSequential[0][0] == \"empty\":\n                # the whole thing is empty now. no need to investigate.\n                print(\"NO STATIC PIP FOUND HERE.\")",
        "type": "code",
        "location": "/tests/video_detector_tests/pip_meanVariance_stablize.py:211-238"
    },
    "5105": {
        "file_id": 660,
        "content": "Performs linear regression on sample data, if slope is within threshold range, adds mean value and range to target list. Combines target elements into newTarget dictionary, and converts newTarget to sequential format. If entire sequential format is empty, prints \"NO STATIC PIP FOUND HERE.\"",
        "type": "comment"
    },
    "5106": {
        "file_id": 660,
        "content": "                return {}\n        else:\n            # newTargetSequential\n            newTargetSequentialUpdated = []\n            for index in range(len(newTargetSequential) - 1):\n                elem = newTargetSequential[index]\n                commandString, commandTimeSpan = elem\n                nextElem = newTargetSequential[index + 1]\n                nextCommandString, nextCommandTimeSpan = nextElem\n                if commandString == \"empty\":\n                    newTargetSequential[index][0] = nextCommandString\n                else:\n                    if nextCommandString == \"empty\":\n                        newTargetSequential[index + 1][0] = commandString\n                    else:  # compare the two!\n                        commandFloat = float(commandString)\n                        nextCommandFloat = float(nextCommandString)\n                        if (\n                            abs(commandFloat - nextCommandFloat)\n                            < commandFloatMergeThreshold\n                        ):",
        "type": "code",
        "location": "/tests/video_detector_tests/pip_meanVariance_stablize.py:239-259"
    },
    "5107": {
        "file_id": 660,
        "content": "This code is updating a list of commands by merging consecutive commands if they are within a certain threshold. It compares the difference between two commands and if it's below a specific value, it updates the list accordingly.",
        "type": "comment"
    },
    "5108": {
        "file_id": 660,
        "content": "                            newTargetSequential[index + 1][0] = commandString\n            # bring this sequential into dict again.\n            answer = sequentialToMergedRanges(newTargetSequential)\n            # print(\"_\"*30, \"ANSWER\",\"_\"*30)\n            # for elem in answer.items():\n            #     print(elem)\n            return answer\n        print(\"[FAILSAFE] SOMEHOW THE CODE SUCKS\")\n        return {}\n    xLeftPoints = data[:, 0, 0]\n    yLeftPoints = data[:, 0, 1]\n    xRightPoints = data[:, 1, 0]\n    yRightPoints = data[:, 1, 1]\n    mPoints = [xLeftPoints, yLeftPoints, xRightPoints, yRightPoints]\n    answers = []\n    for mPoint in mPoints:\n        answer = getSinglePointStableState(mPoint)\n        answers.append(answer)\n        # print(\"_\"*30, \"ANSWER\",\"_\"*30)\n        # for elem in answer.items():\n        #     print(elem)\n    if answers == [{}, {}, {}, {}]:\n        print(\"NO PIP FOUND\")\n        finalCommandDict = {}\n    else:\n        defaultCoord = [0, 0, defaultWidth, defaultHeight]  # deal with it later?",
        "type": "code",
        "location": "/tests/video_detector_tests/pip_meanVariance_stablize.py:260-291"
    },
    "5109": {
        "file_id": 660,
        "content": "This code is performing image analysis and stabilization for PIP (Picture-in-Picture) detection. It first creates newSequential, then converts it back into a dictionary named \"answer.\" The code checks if any PIP was found by comparing the \"answers\" list to four empty dictionaries. If no PIP is detected, it returns an empty dictionary; otherwise, it proceeds further with default coordinates.",
        "type": "comment"
    },
    "5110": {
        "file_id": 660,
        "content": "        defaults = [{str(defaultCoord[index]): [(0, len(data))]} for index in range(4)]\n        for index in range(4):\n            if answers[index] == {}:\n                answers[index] = defaults[index]\n        labels = [\"xleft\", \"yleft\", \"xright\", \"yright\"]\n        commandDict = {}\n        for index, elem in enumerate(answers):\n            label = labels[index]\n            newElem = {\"{}:{}\".format(label, key): elem[key] for key in elem.keys()}\n            commandDict.update(newElem)\n        commandDict = getContinualMappedNonSympyMergeResult(commandDict)\n        commandDictSequential = mergedRangesToSequential(commandDict)\n        def getSpanDuration(span):\n            start, end = span\n            return end - start\n        itemDurationThreshold = 15\n        # print(\"HERE\")\n        # loopCount = 0\n        while True:\n            # print(\"LOOP COUNT:\", loopCount)\n            # loopCount+=1\n            # noAlter = True\n            beforeChange = [item[0] for item in commandDictSequential].copy()\n            for i in range(len(commandDictSequential) - 1):",
        "type": "code",
        "location": "/tests/video_detector_tests/pip_meanVariance_stablize.py:292-318"
    },
    "5111": {
        "file_id": 660,
        "content": "Sets default values for missing answers, converts dictionary format, applies consecutive ranges to sequential format, and enters a while loop that iteratively checks changes in the commandDictSequential list.",
        "type": "comment"
    },
    "5112": {
        "file_id": 660,
        "content": "                currentItem = commandDictSequential[i]\n                nextItem = commandDictSequential[i + 1]\n                currentItemCommand = currentItem[0]\n                currentItemDuration = getSpanDuration(currentItem[1])\n                nextItemCommand = nextItem[0]\n                nextItemDuration = getSpanDuration(nextItem[1])\n                if currentItemDuration < itemDurationThreshold:\n                    if nextItemCommand != currentItemCommand:\n                        # print(\"HERE0\",i, currentItemCommand, nextItemCommand)\n                        commandDictSequential[i][0] = nextItemCommand\n                        # noAlter=False\n                if nextItemDuration < itemDurationThreshold:\n                    if nextItemCommand != currentItemCommand and currentItemDuration >= itemDurationThreshold:\n                        # print(\"HERE1\",i, currentItemCommand, nextItemCommand)\n                        commandDictSequential[i + 1][0] = currentItemCommand\n                        # noAlter=False",
        "type": "code",
        "location": "/tests/video_detector_tests/pip_meanVariance_stablize.py:319-334"
    },
    "5113": {
        "file_id": 660,
        "content": "Checks if current and next commands in commandDictSequential have durations below itemDurationThreshold. If so, adjusts or merges the commands to maintain sequence continuity.",
        "type": "comment"
    },
    "5114": {
        "file_id": 660,
        "content": "            afterChange = [item[0] for item in commandDictSequential].copy()\n            noAlter = beforeChange == afterChange\n            if noAlter:\n                break\n        preFinalCommandDict = sequentialToMergedRanges(commandDictSequential)\n        finalCommandDict = {}\n        for key, elem in preFinalCommandDict.items():\n            # print(key,elem)\n            varNames = [\"xleft\", \"yleft\", \"xright\", \"yright\"]\n            defaultValues = [0, 0, defaultWidth, defaultHeight]\n            for varName, defaultValue in zip(varNames, defaultValues):\n                key = key.replace(\n                    \"{}:empty\".format(varName), \"{}:{}\".format(varName, defaultValue)\n                )\n            # print(key,elem)\n            # breakpoint()\n            import parse\n            formatString = (\n                \"xleft:{xleft:d}|yleft:{yleft:d}|xright:{xright:d}|yright:{yright:d}\"\n            )\n            commandArguments = parse.parse(formatString, key)\n            x, y, w, h = (\n                commandArguments[\"xleft\"],",
        "type": "code",
        "location": "/tests/video_detector_tests/pip_meanVariance_stablize.py:335-358"
    },
    "5115": {
        "file_id": 660,
        "content": "The code checks if the command dictionary remains unchanged after certain operations. If it does not change, the loop is exited. The code then converts the sequential command dictionary to a merged ranges form. It creates an empty final command dictionary and iterates over the pre-final command dictionary items. For each item, it replaces any \"empty\" values with default values for specific variables, such as xleft, yleft, xright, and yright. The code then uses the parse module to parse a format string containing these variable names and their updated values, creating commandArguments that contain the final x, y, w, h values.",
        "type": "comment"
    },
    "5116": {
        "file_id": 660,
        "content": "                commandArguments[\"yleft\"],\n                commandArguments[\"xright\"] - commandArguments[\"xleft\"],\n                commandArguments[\"yright\"] - commandArguments[\"yleft\"],\n            )\n            if w <= 0 or h <= 0:\n                continue\n            cropCommand = \"crop_{}_{}_{}_{}\".format(x, y, w, h)\n            # print(cropCommand)\n            finalCommandDict.update({cropCommand: elem})\n            # print(elem)\n            # the parser shall be in x,y,w,h with keywords.\n            # we might want to parse the command string and reengineer this shit.\n    return finalCommandDict\nobjective = \"discrete\"\n# objective = \"continual\"\n# objective = \"continual_najie\"\nif __name__ == \"__main__\":\n    # better plot this shit.\n    import json\n    if objective == \"continual\":\n        dataDict = json.loads(open(\"pip_meanVariance.json\", \"r\").read())\n    elif objective == 'continual_najie':\n        dataDict = json.loads(open(\"pip_meanVarianceSisterNa.json\", \"r\").read())\n    elif objective == \"discrete\":\n        dataDict = json.loads(open(\"pip_discrete_meanVariance.json\", \"r\").read())",
        "type": "code",
        "location": "/tests/video_detector_tests/pip_meanVariance_stablize.py:359-387"
    },
    "5117": {
        "file_id": 660,
        "content": "This code processes video detector test results and generates a dictionary containing cropped image commands based on the specified objective. It loads data from JSON files depending on the objective (\"continual\", \"continual_najie\", or \"discrete\"). The code then continues to process the resulting data.",
        "type": "comment"
    },
    "5118": {
        "file_id": 660,
        "content": "    else:\n        raise Exception(\"unknown objective: %s\" % objective)\n    # print(len(data)) # 589\n    data = dataDict[\"data\"]\n    defaultWidth, defaultHeight = dataDict[\"width\"], dataDict[\"height\"]\n    if objective in [\"continual\", 'continual_najie']:\n        finalCommandDict = kalmanStablePipRegionExporter(\n            data, defaultWidth, defaultHeight\n        )\n    else:\n        finalCommandDict = sampledStablePipRegionExporter(\n            data, defaultWidth, defaultHeight\n        )\n    import matplotlib.pyplot as plt\n    from matplotlib.patches import Rectangle\n    fig, ax = plt.subplots()\n    def plotRect(ax, x, y, width, height, facecolor):\n        ax.add_patch(\n            Rectangle((x, y), width, height, facecolor=facecolor, fill=True, alpha=0.5)\n        )  # in 0-1\n    ax.plot([[0, 0], [defaultWidth, defaultHeight]])\n    plotRect(ax, 0, 0, defaultWidth, defaultHeight, \"black\")\n    colors = [\"red\", \"yellow\", \"blue\",'orange','white','purple']\n    for index, key in enumerate(finalCommandDict.keys()):\n        import parse",
        "type": "code",
        "location": "/tests/video_detector_tests/pip_meanVariance_stablize.py:388-419"
    },
    "5119": {
        "file_id": 660,
        "content": "This code is handling an objective and preparing a plot for stable PIP region exporter. It raises an exception if the objective is unknown. Depending on the objective, it uses kalmanStablePipRegionExporter or sampledStablePipRegionExporter. It then plots a rectangle on the figure using matplotlib's Rectangle class. Finally, it sets colors for each region in the plot.",
        "type": "comment"
    },
    "5120": {
        "file_id": 660,
        "content": "        commandArguments = parse.parse(\"crop_{x:d}_{y:d}_{w:d}_{h:d}\", key)\n        color = colors[index%len(colors)]\n        rect = [int(commandArguments[name]) for name in [\"x\", \"y\", \"w\", \"h\"]]\n        print(\"RECT\", rect, color, \"SPAN\", finalCommandDict[key])\n        plotRect(ax, *rect, color)\n    # breakpoint()\n    plt.show()",
        "type": "code",
        "location": "/tests/video_detector_tests/pip_meanVariance_stablize.py:421-427"
    },
    "5121": {
        "file_id": 660,
        "content": "The code is creating a rectangular plot using matplotlib. It takes commandArguments as input and uses them to define the x, y, w, and h values of the rectangle. The color of the rectangle is randomly chosen from a list of colors. It then prints the coordinates of the rectangle, the chosen color, and the span of the finalCommandDict[key]. Lastly, it displays the plot using plt.show().",
        "type": "comment"
    },
    "5122": {
        "file_id": 661,
        "content": "/tests/video_detector_tests/motion_gpl.sh",
        "type": "filepath"
    },
    "5123": {
        "file_id": 661,
        "content": "Kills motion process and attempts to capture video from webcam using ffmpeg, then starts motion with the mconfig.conf configuration file. Not suitable for media file processing.",
        "type": "summary"
    },
    "5124": {
        "file_id": 661,
        "content": "killall -s KILL motion\n# ffmpeg -re -i ../../samples/video/LlfeL29BP.mp4 -f v4l2 /dev/video0 &\nmotion -c mconfig.conf\n# to conclude, this is only useful for webcams, not for media file processing.\n# are you sure if you want to capture shits over webcams by this?",
        "type": "code",
        "location": "/tests/video_detector_tests/motion_gpl.sh:1-6"
    },
    "5125": {
        "file_id": 661,
        "content": "Kills motion process and attempts to capture video from webcam using ffmpeg, then starts motion with the mconfig.conf configuration file. Not suitable for media file processing.",
        "type": "comment"
    },
    "5126": {
        "file_id": 662,
        "content": "/tests/video_detector_tests/motion_github.py",
        "type": "filepath"
    },
    "5127": {
        "file_id": 662,
        "content": "This code imports libraries, initializes a motion detector algorithm and sets up video capture. It continuously reads frames from the source, applies an algorithm to create output images, displays them in separate windows, and runs until a frame is not ready or Esc key pressed.",
        "type": "summary"
    },
    "5128": {
        "file_id": 662,
        "content": "# motion detectors are used to track objects. though you may want to separate objects with it.\nimport numpy as np\nimport cv2\nimport pybgs as bgs\n# suspect by static image analysis, and then create bounding box over the thing.\n# check image quality.\nalgorithm = bgs.FrameDifference() # track object we need that.\n# algorithm = bgs.SuBSENSE()\n# video_file = \"../../samples/video/highway_car.avi\"\n# video_file = \"../../samples/video/dog_with_text.mp4\"\nvideo_file = \"../../samples/video/LiEIfnsvn.mp4\" # this one with cropped boundaries. \n# video_file = \"../../samples/video/LlfeL29BP.mp4\"\n# maybe we should consider something else to crop the thing? or not?\n# accumulate the delta over time to see the result?\n# use static detection method.\ncapture = cv2.VideoCapture(video_file)\nwhile not capture.isOpened():\n  capture = cv2.VideoCapture(video_file)\n  cv2.waitKey(1000)\n  print(\"Wait for the header\")\n#pos_frame = capture.get(cv2.cv.CV_CAP_PROP_POS_FRAMES)\n#pos_frame = capture.get(cv2.CV_CAP_PROP_POS_FRAMES)\npos_frame = capture.get(1)",
        "type": "code",
        "location": "/tests/video_detector_tests/motion_github.py:1-29"
    },
    "5129": {
        "file_id": 662,
        "content": "This code imports necessary libraries and initializes a motion detector algorithm (FrameDifference) to track objects in a video. It also defines the video file path and sets up a VideoCapture object. The code waits for the video header, retrieves the current frame position, and is ready to process frames using the motion detection algorithm.",
        "type": "comment"
    },
    "5130": {
        "file_id": 662,
        "content": "while True:\n  flag, frame = capture.read()\n  if flag:\n    cv2.imshow('video', frame)\n    #pos_frame = capture.get(cv2.cv.CV_CAP_PROP_POS_FRAMES)\n    #pos_frame = capture.get(cv2.CV_CAP_PROP_POS_FRAMES)\n    pos_frame = capture.get(1)\n    #print str(pos_frame)+\" frames\"\n    img_output = algorithm.apply(frame)\n    img_bgmodel = algorithm.getBackgroundModel()\n    cv2.imshow('img_output', img_output)\n    cv2.imshow('img_bgmodel', img_bgmodel)\n  else:\n    #capture.set(cv2.cv.CV_CAP_PROP_POS_FRAMES, pos_frame-1)\n    #capture.set(cv2.CV_CAP_PROP_POS_FRAMES, pos_frame-1)\n    #capture.set(1, pos_frame-1)\n    #print \"Frame is not ready\"\n    cv2.waitKey(1000)\n    break\n  if 0xFF & cv2.waitKey(10) == 27:\n    break\n  #if capture.get(cv2.cv.CV_CAP_PROP_POS_FRAMES) == capture.get(cv2.cv.CV_CAP_PROP_FRAME_COUNT):\n  #if capture.get(cv2.CV_CAP_PROP_POS_FRAMES) == capture.get(cv2.CV_CAP_PROP_FRAME_COUNT):\n  #if capture.get(1) == capture.get(cv2.CV_CAP_PROP_FRAME_COUNT):\n    #break\ncv2.destroyAllWindows()",
        "type": "code",
        "location": "/tests/video_detector_tests/motion_github.py:30-62"
    },
    "5131": {
        "file_id": 662,
        "content": "The code continuously reads frames from a video source and displays them. It captures the current frame position, applies an algorithm to create output images, and shows the output and background model images in separate windows. It keeps running until a frame is not ready or the user presses Esc key, closing all windows at the end.",
        "type": "comment"
    },
    "5132": {
        "file_id": 663,
        "content": "/tests/video_detector_tests/mathlib.py",
        "type": "filepath"
    },
    "5133": {
        "file_id": 663,
        "content": "The code uses Sympy to merge overlapping intervals in a list of tuples, creates unified boundaries, and returns final results as merged continual mappings.",
        "type": "summary"
    },
    "5134": {
        "file_id": 663,
        "content": "# not overriding math.\n# do some ranged stuff here...\ndef getContinualNonSympyMergeResult(inputMSetCandidates):\n    # basically the same example.\n    # assume no overlapping here.\n    import sympy\n    def unionToTupleList(myUnion):\n        unionBoundaries = list(myUnion.boundary)\n        unionBoundaries.sort()\n        leftBoundaries = unionBoundaries[::2]\n        rightBoundaries = unionBoundaries[1::2]\n        return list(zip(leftBoundaries, rightBoundaries))\n    def tupleSetToUncertain(mSet):\n        mUncertain = None\n        for start, end in mSet:\n            if mUncertain is None:\n                mUncertain = sympy.Interval(start, end)\n            else:\n                mUncertain += sympy.Interval(start, end)\n        typeUncertain = type(mUncertain)\n        return mUncertain, typeUncertain\n    def mergeOverlappedInIntervalTupleList(intervalTupleList):\n        mUncertain, _ = tupleSetToUncertain(intervalTupleList)\n        mUncertainBoundaryList = list(mUncertain.boundary)\n        mUncertainBoundaryList.sort()",
        "type": "code",
        "location": "/tests/video_detector_tests/mathlib.py:1-28"
    },
    "5135": {
        "file_id": 663,
        "content": "This code defines three functions for set operations involving intervals. The functions are getContinualNonSympyMergeResult, unionToTupleList, and mergeOverlappedInIntervalTupleList. The code uses Sympy library to handle mathematical operations on intervals and merges non-overlapping intervals into a single uncertain variable. It sorts and converts intervals into tuples for further processing.",
        "type": "comment"
    },
    "5136": {
        "file_id": 663,
        "content": "        mergedIntervalTupleList = list(\n            zip(mUncertainBoundaryList[::2], mUncertainBoundaryList[1::2])\n        )\n        return mergedIntervalTupleList\n    # mSet = mergeOverlappedInIntervalTupleList([(0, 1), (2, 3)])\n    # mSet2 = mergeOverlappedInIntervalTupleList([(0.5, 1.5), (1.6, 2.5)])\n    # print(\"MSET\", mSet)\n    # print(\"MSET2\", mSet2)\n    mSetCandidates = [\n        mergeOverlappedInIntervalTupleList(x) for x in inputMSetCandidates\n    ]\n    mSetUnified = [x for y in mSetCandidates for x in y]\n    leftBoundaryList = set([x[0] for x in mSetUnified])\n    rightBoundaryList = set([x[1] for x in mSetUnified])\n    # they may freaking overlap.\n    # if want nearby-merge strategy, simply just expand all intervals, merge them with union and shrink the individual intervals inside union respectively.\n    markers = {\n        \"enter\": {k: [] for k in leftBoundaryList},\n        \"exit\": {k: [] for k in rightBoundaryList},\n    }\n    for index, mSetCandidate in enumerate(mSetCandidates):\n        leftBoundaryListOfCandidate = [x[0] for x in mSetCandidate]",
        "type": "code",
        "location": "/tests/video_detector_tests/mathlib.py:29-55"
    },
    "5137": {
        "file_id": 663,
        "content": "This code defines a function `mergeOverlappedInIntervalTupleList` which takes a list of interval tuples, merges overlapping intervals, and returns the merged list. The main purpose is to unify all the data in the same scope with potential overlap. It first creates a set of left and right boundaries from the unified data, then initializes a `markers` dictionary with \"enter\" and \"exit\" markers for each left boundary. Finally, it iterates over each candidate set, extracting the left boundaries and using them to update the marker dictionary. The final merged interval tuples are not explicitly calculated or returned here, but can be derived from the information in `markers`.",
        "type": "comment"
    },
    "5138": {
        "file_id": 663,
        "content": "        rightBoundaryListOfCandidate = [x[1] for x in mSetCandidate]\n        for leftBoundaryOfCandidate in leftBoundaryListOfCandidate:\n            markers[\"enter\"][leftBoundaryOfCandidate].append(index)  # remap this thing!\n        for rightBoundaryOfCandidate in rightBoundaryListOfCandidate:\n            markers[\"exit\"][rightBoundaryOfCandidate].append(index)  # remap this thing!\n    # now, iterate through the boundaries of mSetUnified.\n    unifiedBoundaryList = leftBoundaryList.union(\n        rightBoundaryList\n    )  # call me a set instead of a list please? now we must sort this thing\n    unifiedBoundaryList = list(unifiedBoundaryList)\n    unifiedBoundaryList.sort()\n    unifiedBoundaryMarks = {}\n    finalMappings = {}\n    # print(\"MARKERS\", markers)\n    # breakpoint()\n    for index, boundary in enumerate(unifiedBoundaryList):\n        previousMark = unifiedBoundaryMarks.get(index - 1, [])\n        enterList = markers[\"enter\"].get(boundary, [])\n        exitList = markers[\"exit\"].get(boundary, [])\n        currentMark = set(previousMark + enterList).difference(set(exitList))",
        "type": "code",
        "location": "/tests/video_detector_tests/mathlib.py:56-77"
    },
    "5139": {
        "file_id": 663,
        "content": "This code creates a set of unified boundaries and maps the markers accordingly. It first gathers the \"enter\" and \"exit\" markers for each boundary, then forms the final mappings by taking the difference between the \"enter\" and \"exit\" lists. The code also sorts the boundaries and retrieves previous marks to form the current mark set.",
        "type": "comment"
    },
    "5140": {
        "file_id": 663,
        "content": "        currentMark = list(currentMark)\n        unifiedBoundaryMarks.update({index: currentMark})\n        # now, handle the change? or not?\n        # let's just deal those empty ones, shall we?\n        if previousMark == []:  # inside it is empty range.\n            # elif currentMark == []:\n            if index == 0:\n                continue  # just the start, no need to note this down.\n            else:\n                finalMappings.update(\n                    {\n                        \"empty\": finalMappings.get(\"empty\", [])\n                        + [(unifiedBoundaryList[index - 1], boundary)]\n                    }\n                )\n            # the end of previous mark! this interval belongs to previousMark\n        else:\n            key = previousMark.copy()\n            key.sort()\n            key = tuple(key)\n            finalMappings.update(\n                {\n                    key: finalMappings.get(key, [])\n                    + [(unifiedBoundaryList[index - 1], boundary)]\n                }\n            )",
        "type": "code",
        "location": "/tests/video_detector_tests/mathlib.py:78-103"
    },
    "5141": {
        "file_id": 663,
        "content": "This code checks if the current mark is empty and updates the finalMappings accordingly. If previousMark is empty, it skips noting down just the start of a range. Otherwise, it sorts and makes a unique key using previousMark, then adds the interval to finalMappings for that key.",
        "type": "comment"
    },
    "5142": {
        "file_id": 663,
        "content": "            # also the end of previous mark! belongs to previousMark.\n    ### NOW THE FINAL OUTPUT ###\n    finalCats = {}\n    for key, value in finalMappings.items():\n        # value is an array containing subInterval tuples.\n        value = mergeOverlappedInIntervalTupleList(value)\n        finalCats.update({key: value})\n    # print(\"______________FINAL CATS______________\")\n    # print(finalCats)\n    return finalCats\ndef getContinualMappedNonSympyMergeResult(mRangesDict, concatSymbol=\"|\", noEmpty=True):\n    mKeyMaps = list(mRangesDict.keys())\n    mSetCandidates = [mRangesDict[key] for key in mKeyMaps]\n    # the next step will automatically merge all overlapped candidates.\n    finalCats = getContinualNonSympyMergeResult(mSetCandidates)\n    finalCatsMapped = {\n        concatSymbol.join([mKeyMaps[k] for k in mTuple]): finalCats[mTuple]\n        for mTuple in finalCats.keys()\n        if type(mTuple) == tuple\n    }\n    if not noEmpty:\n        finalCatsMapped.update(\n            {k: finalCats[k] for k in finalCats.keys() if type(k) != tuple}",
        "type": "code",
        "location": "/tests/video_detector_tests/mathlib.py:104-130"
    },
    "5143": {
        "file_id": 663,
        "content": "This code calculates merged, continual results for a dictionary of sets. It maps the results to a format using a specified concatenation symbol, and allows for empty sets if requested. It uses functions like getContinualNonSympyMergeResult and mergeOverlappedInIntervalTupleList to merge overlapping intervals. The final result is returned as a dictionary of merged continual mappings.",
        "type": "comment"
    },
    "5144": {
        "file_id": 663,
        "content": "        )\n    return finalCatsMapped\n    # default not to output empty set?\ndef getContinualMappedNonSympyMergeResultWithRangedEmpty(\n    mRangesDict, start, end, concatSymbol=\"|\"\n):\n    import uuid\n    emptySetName = str(uuid.uuid4())\n    newRangesDict = mRangesDict.copy()\n    newRangesDict.update({emptySetName: [(start, end)]})\n    newRangesDict = getContinualMappedNonSympyMergeResult(\n        newRangesDict, concatSymbol=\"|\", noEmpty=True\n    )\n    newRangesDict = {\n        key: [\n            (mStart, mEnd)\n            for mStart, mEnd in newRangesDict[key]\n            if mStart >= start and mEnd <= end\n        ]\n        for key in newRangesDict.keys()\n    }\n    newRangesDict = {\n        key: newRangesDict[key]\n        for key in newRangesDict.keys()\n        if newRangesDict[key] != []\n    }\n    finalNewRangesDict = {}\n    for key in newRangesDict.keys():\n        mergedEmptySetName = \"{}{}\".format(concatSymbol, emptySetName)\n        if mergedEmptySetName in key:\n            newKey = key.replace(mergedEmptySetName,\"\")",
        "type": "code",
        "location": "/tests/video_detector_tests/mathlib.py:131-164"
    },
    "5145": {
        "file_id": 663,
        "content": "Function to get a continual mapped non-Sympy merge result with range based on input parameters. It creates a new dictionary with an empty set named UUID, then updates the existing dictionary with this new one. Filters out any ranges that do not fall within the given start and end values. Removes any keys in the newRangesDict dictionary if their corresponding value is an empty list. Finally, iterates over each key in newRangesDict and checks if mergedEmptySetName exists; if it does, it replaces the key with an empty string.",
        "type": "comment"
    },
    "5146": {
        "file_id": 663,
        "content": "            finalNewRangesDict.update({newKey:newRangesDict[key]})\n        elif key == emptySetName:\n            finalNewRangesDict.update({'empty':newRangesDict[key]})\n        else:\n            finalNewRangesDict.update({key:newRangesDict[key]})\n    return finalNewRangesDict\ndef mergedRangesToSequential(renderDict):\n    renderList = []\n    for renderCommandString in renderDict.keys():\n        commandTimeSpans = renderDict[renderCommandString].copy()\n        # commandTimeSpan.sort(key=lambda x: x[0])\n        for commandTimeSpan in commandTimeSpans:\n            renderList.append([renderCommandString, commandTimeSpan].copy())\n    renderList.sort(key=lambda x: x[1][0])\n    return renderList\n    # for renderCommandString, commandTimeSpan in renderList:\n    #     print(renderCommandString, commandTimeSpan)\ndef sequentialToMergedRanges(sequence):\n    mergedRanges = {}\n    for commandString, commandTimeSpan in sequence:\n        mergedRanges.update({commandString: mergedRanges.get(commandString,[])+[commandTimeSpan]})",
        "type": "code",
        "location": "/tests/video_detector_tests/mathlib.py:165-188"
    },
    "5147": {
        "file_id": 663,
        "content": "Function `mergedRangesToSequential` takes a dictionary where keys are commands and values are time spans, sorts them by start time in ascending order, and returns the sorted list of commands with their respective time spans.\n\nFunction `sequentialToMergedRanges` takes a list of command strings and their corresponding start times, groups them by command string, and produces a dictionary with commands as keys and lists of time spans as values.",
        "type": "comment"
    },
    "5148": {
        "file_id": 663,
        "content": "    mergedRanges = getContinualMappedNonSympyMergeResult(mergedRanges)\n    return mergedRanges",
        "type": "code",
        "location": "/tests/video_detector_tests/mathlib.py:189-190"
    },
    "5149": {
        "file_id": 663,
        "content": "This code block retrieves the merged ranges of continuous numbers using getContinualMappedNonSympyMergeResult function and assigns it to variable 'mergedRanges'. Finally, it returns the mergedRanges.",
        "type": "comment"
    },
    "5150": {
        "file_id": 664,
        "content": "/tests/video_detector_tests/frameDifference.py",
        "type": "filepath"
    },
    "5151": {
        "file_id": 664,
        "content": "This function detects motion in a video by comparing frames, calculating differences, applying thresholding and morphology operations, and identifying contours. The code displays two consecutive frames side-by-side using OpenCV and stops when 'Esc' is pressed.",
        "type": "summary"
    },
    "5152": {
        "file_id": 664,
        "content": "import cv2 as cv\nimport numpy as np\nfrom matplotlib import pyplot as plt\ndef motionDetection(videoPath):\n    cap = cv.VideoCapture(videoPath)\n    ret, frame1 = cap.read()\n    ret, frame2 = cap.read()\n    while cap.isOpened():\n        if frame1 is not None and frame2 is not None:\n            pass\n        else:\n            break\n        diff = cv.absdiff(frame1, frame2)\n        diff_gray = cv.cvtColor(diff, cv.COLOR_BGR2GRAY)\n        blur = cv.GaussianBlur(diff_gray, (5, 5), 0)\n        _, thresh = cv.threshold(blur, 20, 255, cv.THRESH_BINARY)\n        dilated = cv.dilate(thresh, None, iterations=3)\n        contours, _ = cv.findContours(\n            dilated, cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE)\n        for contour in contours:\n            (x, y, w, h) = cv.boundingRect(contour)\n            if cv.contourArea(contour) < 900:\n                continue\n            cv.rectangle(frame1, (x, y), (x+w, y+h), (0, 255, 0), 2)\n            cv.putText(frame1, \"Status: {}\".format('Movement'), (10, 20), cv.FONT_HERSHEY_SIMPLEX,\n                        1, (255, 0, 0), 3)",
        "type": "code",
        "location": "/tests/video_detector_tests/frameDifference.py:1-30"
    },
    "5153": {
        "file_id": 664,
        "content": "This function performs motion detection by comparing successive frames in a video, calculates the difference, applies thresholding and morphology operations, and finally detects contours to identify areas with significant changes.",
        "type": "comment"
    },
    "5154": {
        "file_id": 664,
        "content": "        # cv.drawContours(frame1, contours, -1, (0, 255, 0), 2)\n        cv.imshow(\"Video\", frame1)\n        frame1 = frame2\n        ret, frame2 = cap.read()\n        if cv.waitKey(50) == 27:\n            break\n    cap.release()\n    cv.destroyAllWindows()\nif __name__ == \"__main__\":\n    motionDetection(\"../../samples/video/LiEIfnsvn.mp4\")",
        "type": "code",
        "location": "/tests/video_detector_tests/frameDifference.py:32-46"
    },
    "5155": {
        "file_id": 664,
        "content": "This code displays two consecutive frames of a video side by side, highlighting the difference between them using OpenCV. It reads frames from a video file and continuously checks for user input to break the loop when key 'Esc' is pressed.",
        "type": "comment"
    },
    "5156": {
        "file_id": 665,
        "content": "/tests/video_detector_tests/detectron2_norfair.py",
        "type": "filepath"
    },
    "5157": {
        "file_id": 665,
        "content": "The code uses Detectron2 for object detection, tracks \"person\" or \"dog\", updates tracked_objects, and displays bounding boxes. It utilizes OpenCV for video display and waits for 'q' to terminate, closing windows upon exiting.",
        "type": "summary"
    },
    "5158": {
        "file_id": 665,
        "content": "import cv2\nimport numpy as np\nfrom detectron2.config import get_cfg\nfrom detectron2.engine import DefaultPredictor\nfrom cocoNames import cocoRealName\nfrom norfair import Detection, Tracker, Video, draw_tracked_objects\ndef euclidean_distance(detection, tracked_object):\n    return np.linalg.norm(detection.points - tracked_object.estimate)\n# Set up Detectron2 object detector\ncfg = get_cfg()\ncfg.merge_from_file(\"norfair/demos/faster_rcnn_R_50_FPN_3x.yaml\")\ncfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5 # looks like it does not recognize dog.\n# cfg.MODEL.WEIGHTS = \"detectron2://COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl\"\ncfg.MODEL.WEIGHTS = \"/root/Desktop/works/pyjom/tests/video_detector_tests/detectron2_models/model_final_f10217.pkl\"\n# it is stored in s3\n# https://dl.fbaipublicfiles.com/detectron2/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl\n# download cache: /root/.torch/iopath_cache/detectron2/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl",
        "type": "code",
        "location": "/tests/video_detector_tests/detectron2_norfair.py:1-21"
    },
    "5159": {
        "file_id": 665,
        "content": "The code imports necessary libraries and sets up a Detectron2 object detector using pre-trained weights for instance segmentation. It also defines a function to calculate Euclidean distance between detection and tracked objects. The configuration file specifies the model architecture, which is R_50_FPN with three stages and the specific weights (model_final_f10217.pkl) to be used for detection. The weights can either be downloaded from a public S3 storage or retrieved from the local cache if already downloaded.",
        "type": "comment"
    },
    "5160": {
        "file_id": 665,
        "content": "detector = DefaultPredictor(cfg)\n# what are the classes output by the model?\n# Norfair\nvideo_path = \"/root/Desktop/works/pyjom/samples/video/dog_with_text.mp4\"\n# video_path = \"/root/Desktop/works/pyjom/samples/video/LlfeL29BP.mp4\"\nvideo = Video(input_path=video_path)\ntracker = Tracker(distance_function=euclidean_distance, distance_threshold=400,hit_inertia_min=2,hit_inertia_max=20,initialization_delay=1) # what the heck?\ntracked_objects = None\ndisplay=True\nfor index, frame in enumerate(video): # we need to speed up.\n    if index%10 == 0:\n        detections = detector(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n        print(\"original detections:\",detections)\n        instances = detections[\"instances\"]\n        print(\"instances:\",instances)\n        # breakpoint()\n        pred_classes = instances.pred_classes\n        if len(pred_classes) == 0:\n            continue\n        detections2=[]\n        for index,class_ in enumerate(pred_classes):\n            print(\"index:\",index)\n            class_ = int(class_.cpu().numpy().tolist())",
        "type": "code",
        "location": "/tests/video_detector_tests/detectron2_norfair.py:23-48"
    },
    "5161": {
        "file_id": 665,
        "content": "The code initializes a video detector using Detector class and then processes each frame of the video. It predicts instances in each frame, prints detected classes and instances, and continues only if there are predictions. The tracker is used to track objects over frames, but its parameters might need clarification. Speedup is mentioned as needed, which implies potential optimizations.",
        "type": "comment"
    },
    "5162": {
        "file_id": 665,
        "content": "            print(\"class:\",class_)\n            box = instances.pred_boxes.tensor[index].cpu().numpy().tolist()\n            box = [int(x) for x in box]\n            score = float(instances.scores[index].cpu().numpy().tolist())\n            print('box:',box)\n            print('score:',score)\n            className = cocoRealName[class_]\n            # we filter our targets.\n            if className not in [\"person\",\"dog\"]:\n                continue\n            mdata = {\"box\":box,\"class\":{\"id\":class_,\"name\":className}}\n            det = Detection(instances.pred_boxes.get_centers()[index].cpu().numpy(),scores=np.array([score]),data=mdata)\n            detections2.append(det)\n            # breakpoint()\n        # detections = [Detection(p) for p in instances.pred_boxes.get_centers().cpu().numpy()] # what is this instance anyway?\n        # you would lost data you dick!\n        print(\"detections2\",detections2)\n        tracked_objects = tracker.update(detections=detections2)\n        # print(detections)\n        print(\"tracked objects:\",tracked_objects) # you don't track shit?",
        "type": "code",
        "location": "/tests/video_detector_tests/detectron2_norfair.py:49-68"
    },
    "5163": {
        "file_id": 665,
        "content": "This code is filtering and creating detection objects for \"person\" or \"dog\" instances from a given dataset. It prints the box coordinates, score, and class name before adding it to the detections2 list. The code then updates tracked_objects using the tracker function with the detections2 list.",
        "type": "comment"
    },
    "5164": {
        "file_id": 665,
        "content": "    if tracked_objects is not None:\n        if tracked_objects!=[]:\n            # there is no bounding box avaliable?\n            for obj in tracked_objects:\n                point = obj.estimate[0]\n                position = tuple(point.astype(int))\n                color = (255,0,0)\n                # breakpoint()\n                name = obj.last_detection.data[\"class\"][\"name\"]\n                cv2.circle(\n                        frame,\n                        position,\n                        radius=100,\n                        color=color,\n                        thickness=2,\n                    )\n                cv2.putText(\n                    frame,\n                    \"[{}][{}]\".format(str(obj.id),name),\n                    (position[0]-100,position[1]),\n                    cv2.FONT_HERSHEY_SIMPLEX,\n                    2,\n                    (0,255,0),\n                    3,\n                    cv2.LINE_AA,\n                )\n            # breakpoint()\n        # i want to draw you in a different way.\n        # draw_tracked_objects(frame, tracked_objects,color=(255,0,0))",
        "type": "code",
        "location": "/tests/video_detector_tests/detectron2_norfair.py:69-97"
    },
    "5165": {
        "file_id": 665,
        "content": "The code checks if there are any tracked objects and then proceeds to draw bounding boxes around them, add labels for the objects' class names, and display their IDs on the frame using OpenCV functions. Additionally, it offers an alternative way to draw the objects in a different color.",
        "type": "comment"
    },
    "5166": {
        "file_id": 665,
        "content": "    if display:\n        cv2.imshow(\"window\",frame)\n        key  =  cv2.waitKey(1) & 0xff\n        if key == ord('q'):\n            break\n        # maybe we shall print this shit somehow.\n    # video.write(frame) # you write what?\nif display:\n    cv2.destroyAllWindows()",
        "type": "code",
        "location": "/tests/video_detector_tests/detectron2_norfair.py:98-106"
    },
    "5167": {
        "file_id": 665,
        "content": "The code displays a frame from a video using OpenCV's imshow function and waits for a user input (key) to terminate. The key input is checked if it matches the character 'q', which signals a break in the loop. OpenCV's destroyAllWindows() is called to close the window when display is enabled.",
        "type": "comment"
    },
    "5168": {
        "file_id": 666,
        "content": "/tests/video_detector_tests/detectron2_model_zoo_url.py",
        "type": "filepath"
    },
    "5169": {
        "file_id": 666,
        "content": "This code maps Detectron2 COCO model names to checkpoint files, defining configurations for trained parameters and providing pretrained model URLs and checkpoints.",
        "type": "summary"
    },
    "5170": {
        "file_id": 666,
        "content": "from typing import Optional\nclass _ModelZooUrls(object):\n    \"\"\"\n    Mapping from names to officially released Detectron2 pre-trained models.\n    \"\"\"\n    S3_PREFIX = \"https://dl.fbaipublicfiles.com/detectron2/\"\n    # format: {config_path.yaml} -> model_id/model_final_{commit}.pkl\n    CONFIG_PATH_TO_URL_SUFFIX = {\n        # COCO Detection with Faster R-CNN\n        \"COCO-Detection/faster_rcnn_R_50_C4_1x\": \"137257644/model_final_721ade.pkl\",\n        \"COCO-Detection/faster_rcnn_R_50_DC5_1x\": \"137847829/model_final_51d356.pkl\",\n        \"COCO-Detection/faster_rcnn_R_50_FPN_1x\": \"137257794/model_final_b275ba.pkl\",\n        \"COCO-Detection/faster_rcnn_R_50_C4_3x\": \"137849393/model_final_f97cb7.pkl\",\n        \"COCO-Detection/faster_rcnn_R_50_DC5_3x\": \"137849425/model_final_68d202.pkl\",\n        \"COCO-Detection/faster_rcnn_R_50_FPN_3x\": \"137849458/model_final_280758.pkl\",\n        \"COCO-Detection/faster_rcnn_R_101_C4_3x\": \"138204752/model_final_298dad.pkl\",\n        \"COCO-Detection/faster_rcnn_R_101_DC5_3x\": \"138204841/model_final_3e0943.pkl\",",
        "type": "code",
        "location": "/tests/video_detector_tests/detectron2_model_zoo_url.py:2-20"
    },
    "5171": {
        "file_id": 666,
        "content": "The code defines a class \"_ModelZooUrls\" that provides a mapping between Detectron2 pre-trained model names and their respective URLs. It uses the \"S3_PREFIX\" to specify the base URL for downloading models and stores each model's path in the \"CONFIG_PATH_TO_URL_SUFFIX\" dictionary. The code includes various pre-trained COCO Detection models, such as Faster R-CNN with different architectures and scales.",
        "type": "comment"
    },
    "5172": {
        "file_id": 666,
        "content": "        \"COCO-Detection/faster_rcnn_R_101_FPN_3x\": \"137851257/model_final_f6e8b1.pkl\",\n        \"COCO-Detection/faster_rcnn_X_101_32x8d_FPN_3x\": \"139173657/model_final_68b088.pkl\",\n        # COCO Detection with RetinaNet\n        \"COCO-Detection/retinanet_R_50_FPN_1x\": \"190397773/model_final_bfca0b.pkl\",\n        \"COCO-Detection/retinanet_R_50_FPN_3x\": \"190397829/model_final_5bd44e.pkl\",\n        \"COCO-Detection/retinanet_R_101_FPN_3x\": \"190397697/model_final_971ab9.pkl\",\n        # COCO Detection with RPN and Fast R-CNN\n        \"COCO-Detection/rpn_R_50_C4_1x\": \"137258005/model_final_450694.pkl\",\n        \"COCO-Detection/rpn_R_50_FPN_1x\": \"137258492/model_final_02ce48.pkl\",\n        \"COCO-Detection/fast_rcnn_R_50_FPN_1x\": \"137635226/model_final_e5f7ce.pkl\",\n        # COCO Instance Segmentation Baselines with Mask R-CNN\n        \"COCO-InstanceSegmentation/mask_rcnn_R_50_C4_1x\": \"137259246/model_final_9243eb.pkl\",\n        \"COCO-InstanceSegmentation/mask_rcnn_R_50_DC5_1x\": \"137260150/model_final_4f86c3.pkl\",\n        \"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_1x\": \"137260431/model_final_a54504.pkl\",",
        "type": "code",
        "location": "/tests/video_detector_tests/detectron2_model_zoo_url.py:21-34"
    },
    "5173": {
        "file_id": 666,
        "content": "This code maps various Detectron2 models (e.g., faster_rcnn, retinanet, mask_rcnn) to their corresponding pre-trained model files stored in specific URLs or locations. These models are used for tasks like instance segmentation and detection on the COCO dataset.",
        "type": "comment"
    },
    "5174": {
        "file_id": 666,
        "content": "        \"COCO-InstanceSegmentation/mask_rcnn_R_50_C4_3x\": \"137849525/model_final_4ce675.pkl\",\n        \"COCO-InstanceSegmentation/mask_rcnn_R_50_DC5_3x\": \"137849551/model_final_84107b.pkl\",\n        \"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x\": \"137849600/model_final_f10217.pkl\",\n        \"COCO-InstanceSegmentation/mask_rcnn_R_101_C4_3x\": \"138363239/model_final_a2914c.pkl\",\n        \"COCO-InstanceSegmentation/mask_rcnn_R_101_DC5_3x\": \"138363294/model_final_0464b7.pkl\",\n        \"COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x\": \"138205316/model_final_a3ec72.pkl\",\n        \"COCO-InstanceSegmentation/mask_rcnn_X_101_32x8d_FPN_3x\": \"139653917/model_final_2d9806.pkl\",  # noqa\n        # New baselines using Large-Scale Jitter and Longer Training Schedule\n        \"new_baselines/mask_rcnn_R_50_FPN_100ep_LSJ\": \"42047764/model_final_bb69de.pkl\",\n        \"new_baselines/mask_rcnn_R_50_FPN_200ep_LSJ\": \"42047638/model_final_89a8d3.pkl\",\n        \"new_baselines/mask_rcnn_R_50_FPN_400ep_LSJ\": \"42019571/model_final_14d201.pkl\",",
        "type": "code",
        "location": "/tests/video_detector_tests/detectron2_model_zoo_url.py:35-45"
    },
    "5175": {
        "file_id": 666,
        "content": "This code maps different model names to their corresponding checkpoint files in the Detectron2 Model Zoo. It includes COCO instance segmentation models and new baselines with Large-Scale Jitter and longer training schedules.",
        "type": "comment"
    },
    "5176": {
        "file_id": 666,
        "content": "        \"new_baselines/mask_rcnn_R_101_FPN_100ep_LSJ\": \"42025812/model_final_4f7b58.pkl\",\n        \"new_baselines/mask_rcnn_R_101_FPN_200ep_LSJ\": \"42131867/model_final_0bb7ae.pkl\",\n        \"new_baselines/mask_rcnn_R_101_FPN_400ep_LSJ\": \"42073830/model_final_f96b26.pkl\",\n        \"new_baselines/mask_rcnn_regnetx_4gf_dds_FPN_100ep_LSJ\": \"42047771/model_final_b7fbab.pkl\",  # noqa\n        \"new_baselines/mask_rcnn_regnetx_4gf_dds_FPN_200ep_LSJ\": \"42132721/model_final_5d87c1.pkl\",  # noqa\n        \"new_baselines/mask_rcnn_regnetx_4gf_dds_FPN_400ep_LSJ\": \"42025447/model_final_f1362d.pkl\",  # noqa\n        \"new_baselines/mask_rcnn_regnety_4gf_dds_FPN_100ep_LSJ\": \"42047784/model_final_6ba57e.pkl\",  # noqa\n        \"new_baselines/mask_rcnn_regnety_4gf_dds_FPN_200ep_LSJ\": \"42047642/model_final_27b9c1.pkl\",  # noqa\n        \"new_baselines/mask_rcnn_regnety_4gf_dds_FPN_400ep_LSJ\": \"42045954/model_final_ef3a80.pkl\",  # noqa\n        # COCO Person Keypoint Detection Baselines with Keypoint R-CNN\n        \"COCO-Keypoints/keypoint_rcnn_R_50_FPN_1x\": \"137261548/model_final_04e291.pkl\",",
        "type": "code",
        "location": "/tests/video_detector_tests/detectron2_model_zoo_url.py:46-56"
    },
    "5177": {
        "file_id": 666,
        "content": "This code defines a mapping of model names to their corresponding final pickle files. The models are Detectron2 COCO Person Keypoint Detection Baselines and include variations of Mask R-CNN, Mask R-CNN with RegNetX/Y, and the COCO-Keypoints Keypoint R-CNN.",
        "type": "comment"
    },
    "5178": {
        "file_id": 666,
        "content": "        \"COCO-Keypoints/keypoint_rcnn_R_50_FPN_3x\": \"137849621/model_final_a6e10b.pkl\",\n        \"COCO-Keypoints/keypoint_rcnn_R_101_FPN_3x\": \"138363331/model_final_997cc7.pkl\",\n        \"COCO-Keypoints/keypoint_rcnn_X_101_32x8d_FPN_3x\": \"139686956/model_final_5ad38f.pkl\",\n        # COCO Panoptic Segmentation Baselines with Panoptic FPN\n        \"COCO-PanopticSegmentation/panoptic_fpn_R_50_1x\": \"139514544/model_final_dbfeb4.pkl\",\n        \"COCO-PanopticSegmentation/panoptic_fpn_R_50_3x\": \"139514569/model_final_c10459.pkl\",\n        \"COCO-PanopticSegmentation/panoptic_fpn_R_101_3x\": \"139514519/model_final_cafdb1.pkl\",\n        # LVIS Instance Segmentation Baselines with Mask R-CNN\n        \"LVISv0.5-InstanceSegmentation/mask_rcnn_R_50_FPN_1x\": \"144219072/model_final_571f7c.pkl\",  # noqa\n        \"LVISv0.5-InstanceSegmentation/mask_rcnn_R_101_FPN_1x\": \"144219035/model_final_824ab5.pkl\",  # noqa\n        \"LVISv0.5-InstanceSegmentation/mask_rcnn_X_101_32x8d_FPN_1x\": \"144219108/model_final_5e3439.pkl\",  # noqa",
        "type": "code",
        "location": "/tests/video_detector_tests/detectron2_model_zoo_url.py:57-67"
    },
    "5179": {
        "file_id": 666,
        "content": "This code is a dictionary mapping model names to their corresponding checkpoint files. These models are for Detectron2's object detection, keypoint estimation, and segmentation tasks on COCO and LVIS datasets. The checkpoint files store the trained model parameters for each specific configuration.",
        "type": "comment"
    },
    "5180": {
        "file_id": 666,
        "content": "        # Cityscapes & Pascal VOC Baselines\n        \"Cityscapes/mask_rcnn_R_50_FPN\": \"142423278/model_final_af9cf5.pkl\",\n        \"PascalVOC-Detection/faster_rcnn_R_50_C4\": \"142202221/model_final_b1acc2.pkl\",\n        # Other Settings\n        \"Misc/mask_rcnn_R_50_FPN_1x_dconv_c3-c5\": \"138602867/model_final_65c703.pkl\",\n        \"Misc/mask_rcnn_R_50_FPN_3x_dconv_c3-c5\": \"144998336/model_final_821d0b.pkl\",\n        \"Misc/cascade_mask_rcnn_R_50_FPN_1x\": \"138602847/model_final_e9d89b.pkl\",\n        \"Misc/cascade_mask_rcnn_R_50_FPN_3x\": \"144998488/model_final_480dd8.pkl\",\n        \"Misc/mask_rcnn_R_50_FPN_3x_syncbn\": \"169527823/model_final_3b3c51.pkl\",\n        \"Misc/mask_rcnn_R_50_FPN_3x_gn\": \"138602888/model_final_dc5d9e.pkl\",\n        \"Misc/scratch_mask_rcnn_R_50_FPN_3x_gn\": \"138602908/model_final_01ca85.pkl\",\n        \"Misc/scratch_mask_rcnn_R_50_FPN_9x_gn\": \"183808979/model_final_da7b4c.pkl\",\n        \"Misc/scratch_mask_rcnn_R_50_FPN_9x_syncbn\": \"184226666/model_final_5ce33e.pkl\",\n        \"Misc/panoptic_fpn_R_101_dconv_cascade_gn_3x\": \"139797668/model_final_be35db.pkl\",",
        "type": "code",
        "location": "/tests/video_detector_tests/detectron2_model_zoo_url.py:68-81"
    },
    "5181": {
        "file_id": 666,
        "content": "This code defines model configurations and their corresponding checkpoint file paths for various tasks like Cityscapes, Pascal VOC detection, and panoptic segmentation. These configurations include different architectures and training strategies such as syncBN and gn. The checkpoint files store the trained model parameters which can be loaded to replicate the results.",
        "type": "comment"
    },
    "5182": {
        "file_id": 666,
        "content": "        \"Misc/cascade_mask_rcnn_X_152_32x8d_FPN_IN5k_gn_dconv\": \"18131413/model_0039999_e76410.pkl\",  # noqa\n        # D1 Comparisons\n        \"Detectron1-Comparisons/faster_rcnn_R_50_FPN_noaug_1x\": \"137781054/model_final_7ab50c.pkl\",  # noqa\n        \"Detectron1-Comparisons/mask_rcnn_R_50_FPN_noaug_1x\": \"137781281/model_final_62ca52.pkl\",  # noqa\n        \"Detectron1-Comparisons/keypoint_rcnn_R_50_FPN_1x\": \"137781195/model_final_cce136.pkl\",\n    }\n    @staticmethod\n    def query(config_path: str) -> Optional[str]:\n        \"\"\"\n        Args:\n            config_path: relative config filename\n        \"\"\"\n        name = config_path.replace(\".yaml\", \"\").replace(\".py\", \"\")\n        if name in _ModelZooUrls.CONFIG_PATH_TO_URL_SUFFIX:\n            suffix = _ModelZooUrls.CONFIG_PATH_TO_URL_SUFFIX[name]\n            return _ModelZooUrls.S3_PREFIX + name + \"/\" + suffix\n        return None\ndef get_checkpoint_url(config_path):\n    \"\"\"\n    Returns the URL to the model trained using the given config\n    Args:\n        config_path (str): config file name relative to detectron2's \"configs/\"",
        "type": "code",
        "location": "/tests/video_detector_tests/detectron2_model_zoo_url.py:82-107"
    },
    "5183": {
        "file_id": 666,
        "content": "This code provides a function to query the model URL and checkpoint from a given configuration path. It maps specific configurations to their respective URL suffixes and uses them to generate the model's URL, including a prefix. The function returns the URL if a valid mapping is found; otherwise, it returns None.",
        "type": "comment"
    },
    "5184": {
        "file_id": 666,
        "content": "            directory, e.g., \"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_1x.yaml\"\n    Returns:\n        str: a URL to the model\n    \"\"\"\n    url = _ModelZooUrls.query(config_path)\n    if url is None:\n        raise RuntimeError(\"Pretrained model for {} is not available!\".format(config_path))\n    return url\nif __name__ == \"__main__\":\n    test_config = \"Detectron1-Comparisons/mask_rcnn_R_50_FPN_noaug_1x\"\n    url = get_checkpoint_url(test_config)\n    print(\"model name:\",test_config)\n    print(\"model url:\",url)",
        "type": "code",
        "location": "/tests/video_detector_tests/detectron2_model_zoo_url.py:108-122"
    },
    "5185": {
        "file_id": 666,
        "content": "This code defines a function `get_checkpoint_url` that returns the URL of a pretrained model based on its configuration path. It also checks if a valid URL exists for the given config, and raises an error if not. The provided example demonstrates how to use this function with a specific config, printing the model name and URL.",
        "type": "comment"
    },
    "5186": {
        "file_id": 667,
        "content": "/tests/video_detector_tests/cocoNames.py",
        "type": "filepath"
    },
    "5187": {
        "file_id": 667,
        "content": "The code defines two dictionaries, \"cocoName\" and \"cocoRealName\", used for image classification tasks based on the MS COCO dataset. It maps labels to object names and indexes respectively, correcting 0-indexing in dataset labels.",
        "type": "summary"
    },
    "5188": {
        "file_id": 667,
        "content": "cocoName = {0: '__background__',\n\t 1: 'person',\n\t 2: 'bicycle',\n\t 3: 'car',\n\t 4: 'motorcycle',\n\t 5: 'airplane',\n\t 6: 'bus',\n\t 7: 'train',\n\t 8: 'truck',\n\t 9: 'boat',\n\t 10: 'traffic light',\n\t 11: 'fire hydrant',\n\t 12: 'stop sign',\n\t 13: 'parking meter',\n\t 14: 'bench',\n\t 15: 'bird',\n\t 16: 'cat',\n\t 17: 'dog',\n\t 18: 'horse',\n\t 19: 'sheep',\n\t 20: 'cow',\n\t 21: 'elephant',\n\t 22: 'bear',\n\t 23: 'zebra',\n\t 24: 'giraffe',\n\t 25: 'backpack',\n\t 26: 'umbrella',\n\t 27: 'handbag',\n\t 28: 'tie',\n\t 29: 'suitcase',\n\t 30: 'frisbee',\n\t 31: 'skis',\n\t 32: 'snowboard',\n\t 33: 'sports ball',\n\t 34: 'kite',\n\t 35: 'baseball bat',\n\t 36: 'baseball glove',\n\t 37: 'skateboard',\n\t 38: 'surfboard',\n\t 39: 'tennis racket',\n\t 40: 'bottle',\n\t 41: 'wine glass',\n\t 42: 'cup',\n\t 43: 'fork',\n\t 44: 'knife',\n\t 45: 'spoon',\n\t 46: 'bowl',\n\t 47: 'banana',\n\t 48: 'apple',\n\t 49: 'sandwich',\n\t 50: 'orange',\n\t 51: 'broccoli',\n\t 52: 'carrot',\n\t 53: 'hot dog',\n\t 54: 'pizza',\n\t 55: 'donut',\n\t 56: 'cake',\n\t 57: 'chair',\n\t 58: 'couch',\n\t 59: 'potted plant',\n\t 60: 'bed',\n\t 61: 'dining table',\n\t 62: 'toilet',\n\t 63: 'tv',",
        "type": "code",
        "location": "/tests/video_detector_tests/cocoNames.py:1-64"
    },
    "5189": {
        "file_id": 667,
        "content": "This code defines a dictionary named \"cocoName\" that maps integer labels to object names, used for image classification tasks based on the MS COCO dataset.",
        "type": "comment"
    },
    "5190": {
        "file_id": 667,
        "content": "\t 64: 'laptop',\n\t 65: 'mouse',\n\t 66: 'remote',\n\t 67: 'keyboard',\n\t 68: 'cell phone',\n\t 69: 'microwave',\n\t 70: 'oven',\n\t 71: 'toaster',\n\t 72: 'sink',\n\t 73: 'refrigerator',\n\t 74: 'book',\n\t 75: 'clock',\n\t 76: 'vase',\n\t 77: 'scissors',\n\t 78: 'teddy bear',\n\t 79: 'hair drier',\n\t 80: 'toothbrush'}\ncocoRealName = {k-1:cocoName[k] for k in cocoName.keys()}",
        "type": "code",
        "location": "/tests/video_detector_tests/cocoNames.py:65-83"
    },
    "5191": {
        "file_id": 667,
        "content": "The code defines a dictionary named \"cocoRealName\" that maps object names to corresponding COCO indexes. It uses a dictionary comprehension to subtract 1 from each key in the original \"cocoName\" dictionary, which assumes an offset of 0-indexing in the dataset labels.",
        "type": "comment"
    },
    "5192": {
        "file_id": 668,
        "content": "/tests/video_detector_tests/siamMask/setup.sh",
        "type": "filepath"
    },
    "5193": {
        "file_id": 668,
        "content": "Downloading and setting up SiamMask from GitHub, then retrieving pre-trained model files for VOT and DAVIS datasets.",
        "type": "summary"
    },
    "5194": {
        "file_id": 668,
        "content": "git clone https://github.com/foolwood/SiamMask.git && cd SiamMask\nexport SiamMask=$PWD\ncd $SiamMask/experiments/siammask_sharp\nwget http://www.robots.ox.ac.uk/~qwang/SiamMask_VOT.pth\nwget http://www.robots.ox.ac.uk/~qwang/SiamMask_VOT_LD.pth\nwget http://www.robots.ox.ac.uk/~qwang/SiamMask_DAVIS.pth",
        "type": "code",
        "location": "/tests/video_detector_tests/siamMask/setup.sh:1-6"
    },
    "5195": {
        "file_id": 668,
        "content": "Downloading and setting up SiamMask from GitHub, then retrieving pre-trained model files for VOT and DAVIS datasets.",
        "type": "comment"
    },
    "5196": {
        "file_id": 669,
        "content": "/tests/video_detector_tests/siamMask/demo.sh",
        "type": "filepath"
    },
    "5197": {
        "file_id": 669,
        "content": "This script changes directory to \"SiamMask\" and sets environment variables for running a SiamMask demo using Python 3. It resumes from the \"SiamMask_DAVIS.pth\" file with configuration from \"config_davis.json\".",
        "type": "summary"
    },
    "5198": {
        "file_id": 669,
        "content": "cd SiamMask\nexport SiamMask=$PWD\n# cd $SiamMask/experiments/siammask_sharp\n# cd $SiamMask/experiments/siammask_sharp\n# export PYTHONPATH=$PWD:$PYTHONPATH\n# which python3\npython3 -m tools.demo --resume experiments/siammask_sharp/SiamMask_DAVIS.pth --config experiments/siammask_sharp/config_davis.json",
        "type": "code",
        "location": "/tests/video_detector_tests/siamMask/demo.sh:1-7"
    },
    "5199": {
        "file_id": 669,
        "content": "This script changes directory to \"SiamMask\" and sets environment variables for running a SiamMask demo using Python 3. It resumes from the \"SiamMask_DAVIS.pth\" file with configuration from \"config_davis.json\".",
        "type": "comment"
    }
}