{
    "1800": {
        "file_id": 171,
        "content": "# socket.setdefaulttimeout(SOCKET_TIMEOUT)\nfrom test_commons import *\nfrom pyjom.primitives import *\nfrom pyjom.medialang.core import *\nfrom pyjom.videotoolbox import resetMilvusVideoDeduplicationCollection\nautoArgs = {\"subtitle_detector\": {\"timestep\": 0.2}}\ntemplate_names = [\"subtitle_detector.mdl.j2\"]\nDEBUG_STATE=False # let's see how far it goes.\n# warning: if you want to post it, you must review, and you must not use 'fast' mode aka preview.\n# you want musictoolbox? well shit...\n# just because you want download music.\n# also where are the places for 'video/audio/voice/artwork' generation?\n# maybe it is not the time to use such kind of things... you know the ram best.\nfrom pyjom.platforms.bilibili.postMetadata import getBilibiliPostMetadataForDogCat\n# decide to do this in sync.\n# preconfigure the dog_or_cat value.\n# dog_or_cat = random.choice([\"dog\", \"cat\"])  # strange.\ndog_or_cat = \"dog\"\n# we need preconfigured things.\nbgmCacheSetName = \"bilibili_cached_bgm_set\"\nfrom pyjom.languagetoolbox import paraphraser",
        "type": "code",
        "location": "/tests/test_auto_dog_video_giphy_online_producer.py:38-65"
    },
    "1801": {
        "file_id": 171,
        "content": "This code imports necessary modules, sets default timeout, defines autoArgs and template_names, enables debugging, imports postMetadata for Bilibili, preconfigures dog\\_or\\_cat value as \"dog\", and imports paraphraser from languageToolbox.",
        "type": "comment"
    },
    "1802": {
        "file_id": 171,
        "content": "import random\ndef myParaphraser(content:str):# TODO: limit and chop large group of text into chunks, process them individually.\n    methods = [\"clueai_free\", \n    # till we get it.\n    # \"cn_nlp_online\", \n    \"baidu_translator\"]\n    random.shuffle(methods)\n    for method in methods:\n        output, success = paraphraser(content, method =method )\n        if not success:\n            output = content\n        else:\n            break\n    return output\npostMetadataGeneratorPrimitive = getBilibiliPostMetadataForDogCat(\n    dog_or_cat=dog_or_cat,\n    bgmCacheSetName=bgmCacheSetName,\n    bgmCacheAutoPurge=True,  # autopurge bgm, not sure we are using the latest bgm!\n    customParaphraser=myParaphraser\n)  # metadata you can fetch from database, maybe you can preprocess this.\nMAX_ITER = 10  # stop on ten trials.\nfrom lazero.utils.tools import iteratorWrapper\npostMetadataGenerator = iteratorWrapper(\n    postMetadataGeneratorPrimitive, init_repeat=0, max_iter=MAX_ITER, before_yield = resetMilvusVideoDeduplicationCollection",
        "type": "code",
        "location": "/tests/test_auto_dog_video_giphy_online_producer.py:66-90"
    },
    "1803": {
        "file_id": 171,
        "content": "This code defines a function `myParaphraser` that takes a string and paraphrases it using multiple methods in random order. It then applies the paraphrased text if successful, otherwise keeps the original content. The code also sets up a metadata generator for Bilibili dog or cat posts using the `myParaphraser` function, with a maximum of 10 trials before stopping. The metadata can be fetched from a database and preprocessed.",
        "type": "comment"
    },
    "1804": {
        "file_id": 171,
        "content": ")\npostMetadataGenerator.__next__()  # for getting some bgm, just in case.\n# really?\n# [DONE] i think you need some superpower over this postMetadataGenerator.\n# kwargs: init_repeat=0, repeat=0, max_iter=MAX_ITER (take care of \"repeat\" related arguments)\n# [DONE] i also think you should alter the title and intro with paraphraser.\n# TODO: check if video is properly registered to video recommendation server.\n# TODO: check video recommendation server is \"properly\" recommending all related videos\n# [DONE] control dog/cat shits, by stopping the iterator!\nmetaTopics = {\n    \"dog\": {\n        \"static\": [[\"dog\", \"puppy\"]],\n        \"dynamic\": [\n            [\"samoyed\", \"husky\", \"teddy\", \"chiwawa\"],\n            [\"meme\"],\n            [\"funny\", \"cute\", \"love\"],\n        ],\n    },\n    \"cat\": {\n        \"static\": [[\"cat\", \"kitten\"]],\n        \"dynamic\": [[\"purr\", \"paws\", \"meme\"], [\"funny\", \"cute\"]],\n    },\n}\n# when use 'complete test' it stops iterating.\n# maybe because the last one is a generator. goddamn it.\ndef cleanupMedialangTmpdir():",
        "type": "code",
        "location": "/tests/test_auto_dog_video_giphy_online_producer.py:91-121"
    },
    "1805": {
        "file_id": 171,
        "content": "This code snippet seems to be responsible for handling different types of metadata related to dogs and cats, as well as testing the functionality of video recommendations. It includes setting up dynamic topics based on specific breeds and actions, using a paraphraser to alter titles and intros, and checking if videos are properly registered and recommended by the video recommendation server. The code also mentions cleaning up temporary files when running a complete test. However, there seems to be some confusion about certain aspects of the postMetadataGenerator and potential issues with iterating through it.",
        "type": "comment"
    },
    "1806": {
        "file_id": 171,
        "content": "    tmpdirPath = \"/dev/shm/medialang\"\n    files_and_dirs = os.listdir(tmpdirPath)\n    for f in files_and_dirs:\n        fpath = os.path.join(tmpdirPath, f)\n        if os.path.isfile(fpath):\n            os.remove(fpath)\nfrom pyjom.commons import getRedisCachedSet\nfrom pyjom.musictoolbox import neteaseMusic\ndef makeTemplateConfigsGenerator():\n    NMClient = neteaseMusic()\n    while True:\n        # download one music, either from hottest songs or from fetched music list.\n        # even if we search for the name, we will randomly choose the song to avoid problems.\n        # you must download the file in a fixed location.\n        while True:\n            bgmCacheSet = getRedisCachedSet(bgmCacheSetName)\n            keywords = random.choice(list(bgmCacheSet)).strip()\n            if len(keywords) > 0:\n                (\n                    music_content,\n                    music_format,\n                ), lyric_string = NMClient.getMusicAndLyricWithKeywords(\n                    keywords, similar=random.choice([True, False])",
        "type": "code",
        "location": "/tests/test_auto_dog_video_giphy_online_producer.py:122-148"
    },
    "1807": {
        "file_id": 171,
        "content": "This code generates a random keyword from a Redis set and uses the NeteaseMusic API to download music content, format, and lyric string related to that keyword. The music is downloaded to a fixed location. If no keywords are found or they're empty, it keeps searching for new ones.",
        "type": "comment"
    },
    "1808": {
        "file_id": 171,
        "content": "                )\n                if music_content is not None:\n                    break\n        with tempfile.NamedTemporaryFile(\n            \"wb\", suffix=\".{}\".format(music_format)\n        ) as music_file:\n            with tempfile.NamedTemporaryFile(\"w+\", suffix=\".lrc\") as lyric_file:\n                musicFilePath, lyricPath = music_file.name, lyric_file.name\n                music_file.write(music_content)\n                music_file.seek(0)\n                if lyric_string is not None:\n                    lyric_file.write(lyric_string)\n                    lyric_file.seek(0)\n                else:\n                    lyricPath = None\n                data = {\n                    \"debug\": DEBUG_STATE,  # we need to preview this video.\n                    # use generator instead.\n                    \"music\": {\n                        \"filepath\": musicFilePath,  # these things were not right.\n                        # how to get this music file? by bgm search?\n                        # \"filepath\": \"/root/Desktop/works/pyjom/tests/music_analysis/exciting_bgm.mp3\",  # these things were not right.",
        "type": "code",
        "location": "/tests/test_auto_dog_video_giphy_online_producer.py:149-170"
    },
    "1809": {
        "file_id": 171,
        "content": "This code snippet creates temporary music and lyric files, writes music content and lyric string into them if available, and stores their file paths in a dictionary along with the debug state. It aims to preview a video using the given music and lyrics. The music file path may be set via bgm search or by specifying it directly.",
        "type": "comment"
    },
    "1810": {
        "file_id": 171,
        "content": "                        \"lyric_path\": lyricPath,  ## you can choose not to pass the lyric_path anyway. also format different than .lrc is on the way?\n                    },\n                    \"font\": \"/root/.local/share/fonts/simhei.ttf\",\n                    # \"font\": \"/root/.local/share/fonts/simyou.ttf\", # 幼圆可能打不出来\n                    \"policy\": {},\n                    \"maxtime\": 7.8,\n                    \"mintime\": 2,  # we've write this shit!\n                    \"render_ass\": lyricPath is not None,\n                    # also determine how to translate the lyrics, whether to translate or not.\n                    \"translate\": lyricPath is not None,  # default: False\n                    # are you sure you want to use deepl? this is hard to configure. especially the goddamn proxy.\n                    # you can simply implement the method to cofigure and test ping for websites in lazero library so we can share the same code.\n                    # or you can borrow code from the web. some clash manager library for python.",
        "type": "code",
        "location": "/tests/test_auto_dog_video_giphy_online_producer.py:171-183"
    },
    "1811": {
        "file_id": 171,
        "content": "This code is defining a video producer for Giphy that generates subtitles for videos. The `lyric_path` can be chosen to not be passed, and different lyric file formats are being considered. The font for the subtitles is set as '/root/.local/share/fonts/simhei.ttf'. The policy, maximum time (`maxtime`), minimum time (`mintime`) and whether to render assubtitles (`render_ass`) are determined based on `lyricPath`. Translation is set to be done if the `lyricPath` is not None.",
        "type": "comment"
    },
    "1812": {
        "file_id": 171,
        "content": "                    \"translate_method\": \"baidu\",  # default: baidu, random, deepl\n                    # damn cold for this mac!\n                    \"ass_template_configs\": {},\n                    \"assStyleConfig\": {},\n                }\n                yield data\ntemplateConfigsGenerator = makeTemplateConfigsGenerator()\nwbRev = OnlineAutoContentProducer(\n    afterPosting=cleanupMedialangTmpdir,\n    source=\"giphy\",\n    fast=False,\n    metaTopic=metaTopics[dog_or_cat],\n    # fast= True,  # pass this flag to medialang export engine\n    template=\"pets_with_music_online\",\n    postMetadataGenerator=postMetadataGenerator,\n    template_configs=templateConfigsGenerator,\n    # you can also translate funny videos from youtube.\n    # dummy_auto=False,\n    # args=autoArgs,\n    # semiauto=False # i do not want to comment shit.\n)\ndef completeTest():\n    wbRev.main()\ndef partialMedialangRenderTest(medialangScript, medialangTmpdir, verbose=True):\n    # copy that script to my dear clipboard please?\n    medialangObject = Medialang(",
        "type": "code",
        "location": "/tests/test_auto_dog_video_giphy_online_producer.py:184-215"
    },
    "1813": {
        "file_id": 171,
        "content": "This code is creating an instance of the OnlineAutoContentProducer class with specific arguments. The producer is set to work with Giphy content and use the \"pets_with_music_online\" template. It also yields data, generates template configs, and provides a postMetadataGenerator. The completeTest function calls the main method on the producer instance, while partialMedialangRenderTest takes a medialangScript and renders it in the given temporary directory (medialangTmpdir).",
        "type": "comment"
    },
    "1814": {
        "file_id": 171,
        "content": "        script=medialangScript, verbose=verbose, medialangTmpdir=medialangTmpdir\n    )\n    result = medialangObject.execute()\n    return result\ndef PMRT_0(scriptFilePath, medialangTmpdir, verbose=True):\n    with open(scriptFilePath, \"r\") as f:\n        medialangScript = f.read()\n    return partialMedialangRenderTest(medialangScript, medialangTmpdir, verbose=verbose)\nfrom lazero.filesystem import tmpdir\n# from contextlib import AbstractContextManager\n# class tmpdir(AbstractContextManager):\n#     \"\"\"Context manager to suppress specified exceptions\n#     After the exception is suppressed, execution proceeds with the next\n#     statement following the with statement.\n#          with suppress(FileNotFoundError):\n#              os.remove(somefile)\n#          # Execution still resumes here if the file was already removed\n#     \"\"\"\n#     def __init__(self, path=None):\n#         assert os.path.isabs(path)\n#         self._tmpdir = path\n#     def __enter__(self):\n#         print(\"temporary directory: %s\" % self._tmpdir)",
        "type": "code",
        "location": "/tests/test_auto_dog_video_giphy_online_producer.py:216-248"
    },
    "1815": {
        "file_id": 171,
        "content": "The code defines a function `partialMedialangRenderTest` that takes a medialang script, temporary directory, and verbosity level as input. It creates an object `medialangObject`, executes it, and returns the result. Additionally, there's another function `PMRT_0` that uses this `partialMedialangRenderTest` function to execute a medialang script provided by a file path in a temporary directory. Finally, there is a class `tmpdir` which seems to be used as a context manager to suppress exceptions.",
        "type": "comment"
    },
    "1816": {
        "file_id": 171,
        "content": "#         if os.path.exists(self._tmpdir): shutil.rmtree(self._tmpdir)\n#         os.makedirs(self._tmpdir)\n#         return self._tmpdir\n#     def __exit__(self, exctype, excinst, exctb):\n#         # try not to handle exceptions?\n#         tempdir = self._tmpdir\n#         print(\"cleaning tempdir: %s\" % tempdir)\n#         shutil.rmtree(tempdir)\n#         return False\nif __name__ == \"__main__\":\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"-p\", \"--partial\", action=\"store_true\", default=False)\n    args = parser.parse_args()\n    # print('args.partial:', args.partial)\n    # breakpoint()\n    COMPLETE_TEST = not args.partial\n    if COMPLETE_TEST:\n        completeTest()\n    # so we don't have to run it all the time. really?\n    else:\n        # scriptFilePath = \"/root/Desktop/works/pyjom/tests/medialang_tests/aef2ab90-6414-4b55-a40e-63014e5648a8.mdl\"\n        # set this scriptFilePath to something else.\n        scriptFilePath = \"/root/Desktop/works/pyjom/samples/medialang/dog_cat_test_nofast.mdl\"  # make it real, not preview.",
        "type": "code",
        "location": "/tests/test_auto_dog_video_giphy_online_producer.py:249-275"
    },
    "1817": {
        "file_id": 171,
        "content": "This code snippet is from a test file that creates a temporary directory, performs some operations within it, and then cleans up by removing the directory. The code also checks whether to run a complete or partial test based on command-line arguments, and sets the script file path accordingly.",
        "type": "comment"
    },
    "1818": {
        "file_id": 171,
        "content": "        # scriptFilePath = \"/root/Desktop/works/pyjom/samples/medialang/dog_cat_test.mdl\" # make it real, not preview.\n        # a special hack\n        # import tempfile\n        with tmpdir(path=\"/dev/shm/medialang\") as medialangTmpdir:\n            print(\n                \"MEDIALANG SUPER TMPDIR:\", medialangTmpdir\n            )  # as some sort of protection.\n            # /dev/shm/medialang/<randomString>/<randomUUID>.mp4 -> /dev/shm/medialang/<randomUUID>.mp4\n            result = PMRT_0(scriptFilePath, medialangTmpdir, verbose=False)\n            editly_outputPath, medialang_item_list = result  # this just return none!\n            # data -> editly json\n            # this output path is modified. we shall change this.\n            outPath = editly_outputPath  # WE SHALL MUTE IT!\n            # print(editly_json.keys())\n            print(\"MEDIA SAVE PATH (MAYBE YOU CAN PLAY IT?):\", outPath)\n            breakpoint()\n            # import json\n            # data_array -> input of dot processor? check it out.\n            # breakpoint() # what is this?",
        "type": "code",
        "location": "/tests/test_auto_dog_video_giphy_online_producer.py:276-295"
    },
    "1819": {
        "file_id": 171,
        "content": "The code sets a temporary directory path (medialangTmpdir), uses the PMRT_0 function with scriptFilePath and medialangTmpdir as arguments, and returns editly_outputPath and medialang_item_list. It then modifies outPath and prints it. The code also includes potential use of json and breakpoint() for debugging.",
        "type": "comment"
    },
    "1820": {
        "file_id": 172,
        "content": "/tests/unittest_full_text_search_peewee_sqlite.py",
        "type": "filepath"
    },
    "1821": {
        "file_id": 172,
        "content": "The code imports modules and sets up a SQLite database for full-text search. It defines a model class, populates the table with data, adds/updates a video, searches \"python world\" using BM25 algorithm, limits results to 2, and prints each result. Debugging breakpoints are included.",
        "type": "summary"
    },
    "1822": {
        "file_id": 172,
        "content": "from peewee import *\nfrom playhouse.sqlite_ext import SqliteExtDatabase, FTSModel, SearchField, RowIDField\ndb_path = \"test_fulltext_search.db\"\ndb = SqliteExtDatabase(\n    db_path, pragmas={\"journal_mode\": \"wal\", \"cache_size\": -1024 * 64}\n)\nclass BilibiliVideoIndex(FTSModel):\n    rowid = RowIDField()  # this does not support\n    title = SearchField()\n    content = SearchField()\n    class Meta:\n        database = None  # that's good.\n        options = {\"tokenize\": \"porter\"}  # you need manually separate some\ndb.create_tables([BilibiliVideoIndex])\nimport uuid\nrandomContent = lambda: str(uuid.uuid4())\nobject, flag = BilibiliVideoIndex.get_and_update_or_create(\n    rowid=1, title=randomContent(), content=randomContent(), _unique_keys=[\"rowid\"]\n)\nBilibiliVideoIndex.get_and_update_or_create(\n    rowid=2,\n    title=\"hello world\",\n    content=\"learn python the hard way\",\n    _unique_keys=[\"rowid\"],\n)\nBilibiliVideoIndex.get_and_update_or_create(\n    rowid=3,\n    title=\"hello world\",\n    content=\"learn python the hard way\",\n    _unique_keys=[\"rowid\"],",
        "type": "code",
        "location": "/tests/unittest_full_text_search_peewee_sqlite.py:1-42"
    },
    "1823": {
        "file_id": 172,
        "content": "This code imports necessary modules and sets up a SQLite database with full-text search capabilities. It defines a model class, BilibiliVideoIndex, and creates its corresponding table in the database. Using the get_and_update_or_create method, it populates the table with data for three records, ensuring uniqueness based on the rowid field.",
        "type": "comment"
    },
    "1824": {
        "file_id": 172,
        "content": ")\nBilibiliVideoIndex.get_and_update_or_create(\n    rowid=4,\n    title=\"hello world\",\n    content=\"learn python the hard way\",\n    _unique_keys=[\"rowid\"],\n)\nprint(object)\nprint(flag)\nprint(object.rowid, object.title, object.content)\n# don't know what magic is inside. whatever.\n# updated. my lord.\n# now search for it.\nterm = \"python world\"\nresults = BilibiliVideoIndex.search_bm25(term).limit(2)  # just how many?\n# breakpoint()\n# it does have the limit.\n# it is ordered.\nfor result in results:\n    print(\"RESULT\", result)\n    breakpoint()",
        "type": "code",
        "location": "/tests/unittest_full_text_search_peewee_sqlite.py:43-68"
    },
    "1825": {
        "file_id": 172,
        "content": "Code adds a video to BilibiliVideoIndex, updates it, and searches for \"python world\" using BM25 algorithm. Limits search results to 2, then prints each result. Breakpoints inserted for debugging.",
        "type": "comment"
    },
    "1826": {
        "file_id": 173,
        "content": "/tests/unittest_bezier_fitting_bias_skew_baidu_resnet_animals_detection_hyperopt.py",
        "type": "filepath"
    },
    "1827": {
        "file_id": 173,
        "content": "This code defines Bezier curve functions and applies them in an exponential network model for animal detection. It uses hyperparameter optimization in the Hyperopt library to represent results, tune input parameters, and find the best loss value.",
        "type": "summary"
    },
    "1828": {
        "file_id": 173,
        "content": "import numpy as np\nimport bezier\n# BEST: {'input_bias': 0.0830047243746045, 'skew': -0.4986098769473948}\n# maybe not so right?\ndef bezierCurve(start=(0, 0), end=(1, 1), skew=0):\n    # skew: (-0.5,0.5) otherwise this shit will look ugly.\n    assert skew >= -0.5\n    assert skew <= 0.5\n    x_start, y_start = start\n    x_end, y_end = end\n    x_diff = x_end - x_start\n    y_diff = y_end - y_start\n    nodes1 = np.asfortranarray(\n        [\n            [x_start, x_diff * (0.5 + skew), x_end],\n            [y_start, y_diff * (0.5 - skew), y_end],\n        ]\n    )\n    curve1 = bezier.Curve(nodes1, degree=2)\n    curve_params = {\"x_start\": x_start, \"x_diff\": x_diff, \"x_end\": x_end}\n    return curve1, curve_params\ndef evaluateBezierCurve(input_value: float, curve, curve_params: dict):\n    x_start = curve_params[\"x_start\"]\n    x_end = curve_params[\"x_end\"]\n    assert x_start <= input_value\n    assert x_end >= input_value\n    x_diff = curve_params[\"x_diff\"]\n    s = (input_value - x_start) / x_diff\n    points = curve.evaluate(s)\n    # we only get the single point.",
        "type": "code",
        "location": "/tests/unittest_bezier_fitting_bias_skew_baidu_resnet_animals_detection_hyperopt.py:1-33"
    },
    "1829": {
        "file_id": 173,
        "content": "This code defines a function `bezierCurve()` that creates a Bezier curve with optional skew parameter and returns the curve object and its parameters. The `evaluateBezierCurve()` function evaluates a given input value on the Bezier curve.",
        "type": "comment"
    },
    "1830": {
        "file_id": 173,
        "content": "    point = points.T[0]\n    x, y = point\n    result = y\n    return result\ndef multiParameterExponentialNetwork(\n    *args,\n    input_bias=0.05,\n    curve_function=bezierCurve,\n    curve_function_kwargs={\"start\": (0, 0), \"end\": (1, 1), \"skew\": 0},\n    evaluate_function=evaluateBezierCurve\n):\n    curve, curve_params = curve_function(**curve_function_kwargs)\n    value = evaluate_function(input_bias, curve, curve_params)\n    for index, input_value in enumerate(args):\n        apply_list = [input_value] * (index + 1)\n        for apply_item in apply_list:\n            value += (1 - value) * evaluate_function(apply_item, curve, curve_params)\n    return value\n# params = (0.2,0.1,0.1)\n##################################################\n# [('cat', 0.23492032289505005), ('cat', 0.14728288352489471), ('cat', 0.13097935914993286)]\n# [('cat', 0.29809582233428955), ('cat', 0.2462661862373352), ('cat', 0.13935738801956177)]\ntest_params = [\n    # [('cat', 0.3532187342643738), ('cat', 0.22708916664123535), (None, 0.11154596507549286)],0.7],",
        "type": "code",
        "location": "/tests/unittest_bezier_fitting_bias_skew_baidu_resnet_animals_detection_hyperopt.py:34-61"
    },
    "1831": {
        "file_id": 173,
        "content": "This code defines a function `multiParameterExponentialNetwork` which takes input parameters and applies an exponential network model using a given curve function, evaluate function, and additional arguments. The result is returned after applying the exponential network model to each input parameter. Testing with different sets of parameters is shown in the last part of the code.",
        "type": "comment"
    },
    "1832": {
        "file_id": 173,
        "content": "    ##################################################\n    [\n        [\n            (\"cat\", 0.15381687879562378),\n            (\"cat\", 0.14100512862205505),\n            (\"cat\", 0.11225848644971848),\n        ],\n        0.7,\n    ],\n    # params = (0.2,0.1,0.1)\n    # source = \"/root/Desktop/works/pyjom/samples/image/samoyed.jpeg\"\n    # [('dog', 0.8835851550102234), ('dog', 0.08754527568817139), ('dog', 0.008648859336972237)]\n    # source = \"/root/Desktop/works/pyjom/samples/image/dog_saturday_night.jpg\"\n    [\n        [\n            (None, 0.33663231134414673),\n            (\"dog\", 0.32254937291145325),\n            (\"dog\", 0.0494903139770031),\n        ],\n        0.7,\n    ],\n]  # select the typical things for evaluation.\n# not animal? wtf?\n# source = \"/root/Desktop/works/pyjom/samples/image/porn_shemale.jpeg\" # definitely not animal\n# [(None, 0.9894463419914246), ('dog', 1.564090962347109e-05), ('dog', 1.3550661606132053e-05)]\n# source = \"/root/Desktop/works/pyjom/samples/image/is_this_duck.bmp\"\n# [(None, 0.9864748120307922), ('dog', 1.2670795513258781e-05), (None, 9.569253961672075e-06)]",
        "type": "code",
        "location": "/tests/unittest_bezier_fitting_bias_skew_baidu_resnet_animals_detection_hyperopt.py:62-88"
    },
    "1833": {
        "file_id": 173,
        "content": "This code represents a list of animal detection results, where each element consists of the detected animal category and its corresponding probability. It also contains an optional \"None\" entry for non-animal detections. The list is used to evaluate typical scenarios with different images and animals, including some anomalies like non-animal images or images with extremely low detection probabilities.",
        "type": "comment"
    },
    "1834": {
        "file_id": 173,
        "content": "# source = \"/root/Desktop/works/pyjom/samples/image/pig_really.bmp\" # it's really a dog, but it is so ugly so i don't want to admit.\n# [(None, 0.35919442772865295), ('dog', 0.16199783980846405), ('dog', 0.07987158000469208)]\n# source = \"/root/Desktop/works/pyjom/samples/image/miku_on_green.png\"\n# besides calculating \"DOG\" or \"CAT\" we are also concerned about \"NONE\"\n# [(None, 0.9998186230659485), (None, 1.7534730432089418e-06), (None, 7.280816021193459e-07)]\n# source = \"/root/Desktop/works/pyjom/samples/image/dog_with_text.jpg\" # no dog\n#  [(None, 0.9998675584793091), ('dog', 2.565316492564307e-07), (None, 1.562129767762599e-07)]\n# source = \"/root/Desktop/works/pyjom/samples/image/dog_with_text2.png\" # has dog\n#  [(None, 0.8876796960830688), ('dog', 0.0498274527490139), ('dog', 0.02175540290772915)]\n# a little, but not focused.\n# input_bias = 0.05\n# skew = -0.5\n# change these two things.\nfrom lazero.utils.logger import sprint\nimport hyperopt\nfrom hyperopt import fmin, tpe, space_eval\ndef evaluate_params(input_bias, skew):",
        "type": "code",
        "location": "/tests/unittest_bezier_fitting_bias_skew_baidu_resnet_animals_detection_hyperopt.py:89-109"
    },
    "1835": {
        "file_id": 173,
        "content": "This code snippet is using hyperparameter optimization to tune the input_bias and skew parameters for a machine learning model. The code provides sample inputs and expected outputs, demonstrating how these hyperparameters affect the results of the model. It then uses the Hyperopt library to perform the optimization.",
        "type": "comment"
    },
    "1836": {
        "file_id": 173,
        "content": "    curve_function_kwargs = {\n        \"start\": (0, 0),\n        \"end\": (1, 1),\n        \"skew\": skew,\n    }  # maximize the output.\n    difference_items = []\n    for subject_id, (test_param, target_output) in enumerate(test_params):\n        differences = []\n        for index, (label, confidence) in enumerate(test_param):\n            scope = test_param[index:]\n            scope_confidences = [elem[1] for elem in scope if elem[0] == label]\n            output = multiParameterExponentialNetwork(\n                *scope_confidences,\n                input_bias=input_bias,\n                curve_function_kwargs=curve_function_kwargs\n            )\n            print(\"test subject_id:\", subject_id)\n            print(\"label:\", label)\n            print(\"output:\", output)\n            print(\"target_output:\", target_output)\n            absolute_difference = abs(target_output - output)\n            sprint(\"absolute difference:\", absolute_difference)\n            differences.append((label, absolute_difference))\n        mLabels = [\"dog\", \"cat\"]",
        "type": "code",
        "location": "/tests/unittest_bezier_fitting_bias_skew_baidu_resnet_animals_detection_hyperopt.py:110-133"
    },
    "1837": {
        "file_id": 173,
        "content": "This code defines a function that takes in a set of test parameters and their corresponding target outputs. It then calculates the output from a neural network with the given parameters, using a specific curve function with skew factor. The differences between the calculated output and the target output are recorded for each parameter combination, and printed along with other information such as subject ID and label.",
        "type": "comment"
    },
    "1838": {
        "file_id": 173,
        "content": "        best_params_dict = {}\n        for label, difference in differences:\n            if label in mLabels:\n                previousDifference = best_params_dict.get(label, 1)\n                if previousDifference > difference:\n                    best_params_dict[label] = difference\n        final_differences = []\n        for mLabel in mLabels:\n            d = best_params_dict.get(mLabel, 1)\n            final_differences.append(d)\n        difference_item = min(final_differences)\n        difference_items.append(difference_item)\n    final_difference = sum(difference_items)\n    sprint(\"FINAL DIFFERENCE:\", final_difference)\n    return final_difference\ndef objective(args):\n    skew, input_bias = args\n    # print(args)\n    print(\"skew:\", skew)\n    sprint(\"input_bias:\", input_bias)\n    # it is just a tuple.\n    # breakpoint()\n    value = evaluate_params(input_bias, skew)\n    return value\nspace = (\n    hyperopt.hp.uniform(\"skew\", -0.5, 0),\n    hyperopt.hp.uniform(\"input_bias\", 0, 0.1),\n)\nbest = fmin(objective, space, algo=tpe.suggest, max_evals=100)",
        "type": "code",
        "location": "/tests/unittest_bezier_fitting_bias_skew_baidu_resnet_animals_detection_hyperopt.py:134-167"
    },
    "1839": {
        "file_id": 173,
        "content": "This code uses hyperparameter optimization to find the best skew and input_bias values for a model. It calculates the final difference by iterating over the labels, comparing previous differences with new ones, and finding the minimum difference in each label. The objective function evaluates the parameters for a given input and returns a value. Hyperopt's tpe algorithm is used to search for the best combination of skew and input_bias within the specified ranges, and max_evals sets the maximum number of evaluations to be performed.",
        "type": "comment"
    },
    "1840": {
        "file_id": 173,
        "content": "# sprint(\"EVAL:\",space_eval(space, best))\nbest_loss = objective((best[\"skew\"], best[\"input_bias\"]))\nsprint(\"BEST LOSS:\", best_loss)\nsprint(\"BEST:\", best)",
        "type": "code",
        "location": "/tests/unittest_bezier_fitting_bias_skew_baidu_resnet_animals_detection_hyperopt.py:168-172"
    },
    "1841": {
        "file_id": 173,
        "content": "These lines evaluate the best hyperparameters found and print the loss value, as well as the entire set of hyperparameters.",
        "type": "comment"
    },
    "1842": {
        "file_id": 174,
        "content": "/tests/unittest_bezier_evaluate.py",
        "type": "filepath"
    },
    "1843": {
        "file_id": 174,
        "content": "This code initializes a bezier curve and tests it in two cases: plotting with Seaborn and Matplotlib, or evaluating based on user input. It prints the value of 'axis' without context.",
        "type": "summary"
    },
    "1844": {
        "file_id": 174,
        "content": "import bezier\nimport numpy as np\nskew = -0.5  # skew: (-0.5,0.5) otherwise this shit will look ugly.\nx_start, y_start = 0, 0\nx_end, y_end = 1, 1\nx_diff = x_end - x_start\ny_diff = y_end - y_start\nnodes1 = np.asfortranarray(\n    [\n        [x_start, x_diff * (0.5 + skew), x_end],\n        [y_start, y_diff * (0.5 - skew), y_end],\n    ]\n)\ncurve1 = bezier.Curve(nodes1, degree=2)\n# import seaborn\n# seaborn.set()\ntest_case = \"evaluate\"\nif test_case == \"plot\":\n    axis = curve1.plot(num_pts=256)\n    import matplotlib.pyplot as plt\n    # plt.plot(axis)\n    plt.show()\nelif test_case == \"evaluate\":\n    print(\"type q to quit evaluation\")\n    while True:\n        s = input(\"s> \")\n        if s == \"q\":\n            print(\"quitting...\")\n            break\n        try:\n            s = float(s)\n            points = curve1.evaluate(s)\n            # we only get the single point.\n            point = points.T[0]\n            x, y = point\n            print(\"x: %f, y: %f\" % (x, y))\n        except:\n            print(\"ERROR: Invalid input value: %s\" % s)",
        "type": "code",
        "location": "/tests/unittest_bezier_evaluate.py:1-43"
    },
    "1845": {
        "file_id": 174,
        "content": "Code initializes a bezier curve with specified nodes, handles two test cases - plot and evaluate. In plot case, the curve is plotted using Seaborn and Matplotlib libraries. For the evaluate case, it continuously asks for user input (type 'q' to quit), evaluates the curve at the given point, and prints the x and y coordinates of the evaluated point.",
        "type": "comment"
    },
    "1846": {
        "file_id": 174,
        "content": "    # print(axis)",
        "type": "code",
        "location": "/tests/unittest_bezier_evaluate.py:44-44"
    },
    "1847": {
        "file_id": 174,
        "content": "This line prints the value of variable 'axis' without any context or further processing.",
        "type": "comment"
    },
    "1848": {
        "file_id": 175,
        "content": "/tests/unittest_bilibili_login.py",
        "type": "filepath"
    },
    "1849": {
        "file_id": 175,
        "content": "Checks if the test variable is 1, if true, imports necessary modules and attempts to remove existing credential file. If the test variable is not 1, tries to return a value using two different methods, handles potential errors and prints the result.",
        "type": "summary"
    },
    "1850": {
        "file_id": 175,
        "content": "test = 2\nif test == 1:\n    import os\n    credpath = \"/root/.bilibili_api.json\"\n    if os.path.exists(credpath):\n        os.remove(credpath)\n    from test_commons import *\n    from pyjom.platforms.bilibili.credentials import (\n        getCredentialByDedeUserId,\n        getCredentialViaSMS,\n    )\n    # myvalue = getCredentialViaSMS()\n    # print(myvalue)\n    val = getCredentialByDedeUserId()\n    print(val)\nelse:\n    # you may want to remove database.\n    # how the fuck you can do that?\n    # not possible. \"RETURN OUTSIDE OF FUNCTION\"\n    def myfunction():\n        try:\n            # exec('val= 1234'+';break'*1000)\n            val = eval(\"1234\")\n        except:\n            ...\n        print(val)\n    value = myfunction()\n    print(value)",
        "type": "code",
        "location": "/tests/unittest_bilibili_login.py:1-34"
    },
    "1851": {
        "file_id": 175,
        "content": "Checks if the test variable is 1, if true, imports necessary modules and attempts to remove existing credential file. If the test variable is not 1, tries to return a value using two different methods, handles potential errors and prints the result.",
        "type": "comment"
    },
    "1852": {
        "file_id": 176,
        "content": "/tests/unittest_ffmpeg_overlay_boxblur.py",
        "type": "filepath"
    },
    "1853": {
        "file_id": 176,
        "content": "The code uses ffmpeg library to apply Gaussian blur filter and scale video stream, then creates a second layer, overlays both layers, and outputs the processed video stream. It sets output dimensions, uses \"scale\" and \"gblur\" or \"boxblur\" filters, scales video stream with aspect ratio preservation, and outputs file to temporary directory.",
        "type": "summary"
    },
    "1854": {
        "file_id": 176,
        "content": "# ffmpeg对视频实现高斯模糊，给视频上下加模糊背景\n# ffmpeg实现视频高斯模糊拓边效果\nimport ffmpeg\nsource = \"/root/Desktop/works/pyjom/samples/video/cute_cat_gif.gif\"\nstream = ffmpeg.input(source)\nvideo_stream = stream.video\n# the damn thing because they are from the same file! fuck!\n# layer_0 = video_stream.filter(\"scale\", w=1080, h=1920).filter(\"boxblur\", 10) # this is default?\n# however, you need to generalize it here.\n# output_width = 1080\n# output_height = 1920\noutput_height = 1080\noutput_width = 1920\nlayer_0 = video_stream.filter(\"scale\", w=output_width, h=output_height).filter(\n    \"gblur\", sigma=9\n)  # this is default?\n# print('layer_0 args', layer_0.get_args())\nlayer_1 = video_stream.filter(\n    \"scale\",\n    w=\"min(floor(iw*{}/ih),{})\".format(output_height, output_width),\n    h=\"min(floor(ih*{}/iw),{})\".format(output_width, output_height),\n)\n# print('layer_1 args', layer_1.get_args())\n## in case you failed to generalize this shit...\noutput_stream = layer_0.overlay(layer_1, x=\"floor((W-w)/2)\", y=\"floor((H-h)/2)\")\n# print('output_stream args', output_stream.get_args())",
        "type": "code",
        "location": "/tests/unittest_ffmpeg_overlay_boxblur.py:1-40"
    },
    "1855": {
        "file_id": 176,
        "content": "The code uses ffmpeg library to apply Gaussian blur filter and scale video stream. It sets output dimensions, applies \"scale\" filter with given width and height, then applies \"gblur\" or \"boxblur\" filter. Then, it creates a second layer by scaling the video stream with aspect ratio preservation and overlays both layers using specific coordinates. Finally, it outputs the processed video stream.",
        "type": "comment"
    },
    "1856": {
        "file_id": 176,
        "content": "from lazero.filesystem import tmpdir\npath = \"/dev/shm/medialang\"\nimport os\nwith tmpdir(path=path) as T:\n    filepath = os.path.join(path, \"output.mp4\")\n    # args = ffmpeg.get_args(output_stream)\n    # print(args)\n    output_args = {\"preset\": \"veryfast\"}  # seems like it won't speed up so much?\n    ffmpeg.output(output_stream, filepath, **output_args).run(overwrite_output=True)\n    print(\"output file location:\", filepath)\n    breakpoint()",
        "type": "code",
        "location": "/tests/unittest_ffmpeg_overlay_boxblur.py:42-54"
    },
    "1857": {
        "file_id": 176,
        "content": "The code sets a temporary directory path, joins it with the file name, creates FFmpeg output arguments with a fast preset, and then runs an FFmpeg command to output the stream to the file in the temporary directory. The output file location is printed.",
        "type": "comment"
    },
    "1858": {
        "file_id": 177,
        "content": "/tests/test_weibo_pets.sh",
        "type": "filepath"
    },
    "1859": {
        "file_id": 177,
        "content": "This code is running a Python script called 'test_weibo_pets.py'. This could be part of an automated testing process, likely to test the functionality of a Weibo Pets module or application.",
        "type": "summary"
    },
    "1860": {
        "file_id": 177,
        "content": "python3 test_weibo_pets.py",
        "type": "code",
        "location": "/tests/test_weibo_pets.sh:1-1"
    },
    "1861": {
        "file_id": 177,
        "content": "This code is running a Python script called 'test_weibo_pets.py'. This could be part of an automated testing process, likely to test the functionality of a Weibo Pets module or application.",
        "type": "comment"
    },
    "1862": {
        "file_id": 178,
        "content": "/tests/unittest_extract_tags_tfidf.py",
        "type": "filepath"
    },
    "1863": {
        "file_id": 178,
        "content": "This code is processing Chinese text using the Jieba library to tokenize it and filter out stop words. It then extracts the top 5 keywords using NLTK's jieba.analyse module. The output is the extracted tags printed on the console.",
        "type": "summary"
    },
    "1864": {
        "file_id": 178,
        "content": "text = \"Flask的路由,视图和相关配置\"  # just a sample please?\nfrom nltk.corpus import stopwords\nmyStopwords = stopwords.words([\"chinese\", \"english\"])\nimport jieba.analyse as ana\nimport jieba\nwords = jieba.lcut(text)\nwords_filtered = []\nfor word in words:\n    if word.lower() not in myStopwords:\n        words_filtered.append(word)\ntext_splited = \" \".join(words_filtered)\ntags = ana.extract_tags(\n    text_splited,\n    topK=5,\n)\nprint(tags)\n# seems like you can only change the source to make it into somewhat solveable problem.",
        "type": "code",
        "location": "/tests/unittest_extract_tags_tfidf.py:1-24"
    },
    "1865": {
        "file_id": 178,
        "content": "This code is processing Chinese text using the Jieba library to tokenize it and filter out stop words. It then extracts the top 5 keywords using NLTK's jieba.analyse module. The output is the extracted tags printed on the console.",
        "type": "comment"
    },
    "1866": {
        "file_id": 179,
        "content": "/tests/unittest_convolution_bilibili_translate_text_detect.py",
        "type": "filepath"
    },
    "1867": {
        "file_id": 179,
        "content": "This code imports libraries, defines image and video processing functions, reads a JSON file, applies these functions to create the final image, processes bounding boxes, creates rectangles, blurs, visualizes, and displays images while waiting for key presses.",
        "type": "summary"
    },
    "1868": {
        "file_id": 179,
        "content": "import json\nfrom test_commons import *\nfrom pyjom.commons import *\nimport cv2\ndef getVideoPixels(videoPath):\n    from MediaInfo import MediaInfo\n    info = MediaInfo(filename=videoPath)\n    infoData = info.getInfo()\n    # print(infoData)\n    # breakpoint()\n    defaultWidth = infoData[\"videoWidth\"]\n    defaultHeight = infoData[\"videoHeight\"]\n    return defaultWidth, defaultHeight\n# easy gig, you said.\n# basePath = \"/Users/jamesbrown/desktop/works/pyjom_remote\"\nbasePath = \"/root/Desktop/works/pyjom\"\ntargetFile = (\n    basePath + \"/tests/bilibili_practices/bilibili_video_translate/japan_day.json\"\n)\noriginalFile = (\n    basePath + \"/tests/bilibili_practices/bilibili_video_translate/japan_day.webm\"\n)\n# visualization can only be done here?\n# where is the original file?\nmJson = json.loads(open(targetFile, \"r\", encoding=\"utf-8\").read())\nimport numpy as np\nwidth, height = getVideoPixels(originalFile)\ndef getBlackPicture(width, height):\n    blackPicture = np.zeros((height, width, 1), dtype=\"uint8\")  # this is grayscale.\n    return blackPicture",
        "type": "code",
        "location": "/tests/unittest_convolution_bilibili_translate_text_detect.py:1-41"
    },
    "1869": {
        "file_id": 179,
        "content": "The code is importing necessary libraries and defining a function `getVideoPixels` to retrieve the default video width and height from a given video file. It then sets the base path, target file, and original file paths. The code reads the target JSON file, loads it into a variable `mJson`, and retrieves the video dimensions using the `getVideoPixels` function. Finally, it defines a function `getBlackPicture` to create a black grayscale image with the specified width and height.",
        "type": "comment"
    },
    "1870": {
        "file_id": 179,
        "content": "mKeys = list(mJson.keys())\nmIntKeys = [int(x) for x in mKeys]\nminKey, maxKey = min(mIntKeys), max(mIntKeys)\n# imutils is created by pyimagesearch.\nfrom imutils.object_detection import non_max_suppression\ndef getConvBlurredCurrentShot(blurredSpan, span=5):\n    # honor the most the latest one.\n    mImage = None\n    for index, blurredImage in enumerate(blurredSpan):\n        ratio = index / span\n        if mImage is None:\n            mImage = blurredImage * ratio\n        else:\n            mImage += blurredImage * ratio\n    # print(mImage.shape)\n    # breakpoint()\n    # change this mImage.\n    mImage = mImage > 128\n    mImage = mImage.astype(np.uint8)\n    mImage = mImage * 255\n    return mImage\n    # return 256*((mImage>128).astype(np.uint8))\nconvolutionSpan = 20\nconvolutionBoundingBoxSpan = []\nconvolutionBlurredSpan = []\nfor intKey in range(minKey, maxKey + 1):\n    strKey = str(intKey)\n    target = mJson[strKey]\n    boundingBoxes = []\n    for item in target:\n        location = item[0]\n        text, confidence = item[1]\n        # print(\"location\",location) # four points. do not know if there is any rotation here.",
        "type": "code",
        "location": "/tests/unittest_convolution_bilibili_translate_text_detect.py:44-86"
    },
    "1871": {
        "file_id": 179,
        "content": "This code defines a function `getConvBlurredCurrentShot` that averages multiple blurred images to create a final image. It also initializes variables for convolution bounding boxes and blurred spans based on a range of keys in `mJson`. The resulting image is then thresholded and converted to 8-bit format before being returned.",
        "type": "comment"
    },
    "1872": {
        "file_id": 179,
        "content": "        if confidence > 0.7:\n            npLocation = np.array(location)\n            xlocs = npLocation[:, 0]\n            ylocs = npLocation[:, 1]\n            # print(xlocs)\n            # print(ylocs)\n            # breakpoint()\n            minX, maxX = min(xlocs), max(xlocs)\n            minY, maxY = min(ylocs), max(ylocs)\n            boundingBox = [minX, minY, maxX, maxY]\n            boundingBoxes.append(boundingBox.copy())\n            # breakpoint()\n        # print(\"text\", text)\n        # print(\"confidence\", confidence)\n    convolutionBoundingBoxSpan.append(boundingBoxes.copy())\n    if len(convolutionBoundingBoxSpan) > convolutionSpan:\n        convolutionBoundingBoxSpan.pop(0)\n    # do your calculation!\n    flatSpan = [y for x in convolutionBoundingBoxSpan for y in x]\n    flatSpan = np.array(flatSpan)\n    currentNonOverlappingBoxes = non_max_suppression(flatSpan)\n    # print(intKey,target)\n    # this time we do not care about the text inside.\n    blackPicture = getBlackPicture(width, height)\n    for rectangle in flatSpan:",
        "type": "code",
        "location": "/tests/unittest_convolution_bilibili_translate_text_detect.py:87-111"
    },
    "1873": {
        "file_id": 179,
        "content": "The code processes bounding boxes from a convolution operation, filters them based on confidence score, and performs non-maximum suppression to eliminate overlapping boxes. It then creates an array of non-overlapping bounding boxes and generates a black picture with the same width and height as the original image.",
        "type": "comment"
    },
    "1874": {
        "file_id": 179,
        "content": "        # make it all int.\n        x0, y0, x1, y1 = [int(num) for num in rectangle]\n        loc0 = (x0, y0)\n        loc1 = (x1, y1)\n        cv2.rectangle(\n            blackPicture, loc0, loc1, 255, cv2.FILLED\n        )  # we fill so we can merge shits.\n    blackPictureBlurred = cv2.GaussianBlur(blackPicture, (33, 33), 0)\n    convolutionBlurredSpan.append(blackPictureBlurred.copy())\n    if len(convolutionBlurredSpan) > convolutionSpan:\n        convolutionBlurredSpan.pop(0)\n    currentBlackPictureBlurred = getConvBlurredCurrentShot(\n        convolutionBlurredSpan, span=convolutionSpan\n    )\n    # print(currentBlackPictureBlurred.shape)\n    print(\"boundingBoxes:\", len(flatSpan))\n    if len(flatSpan) == 0:\n        continue\n    contours = cv2.findContours(\n        currentBlackPictureBlurred, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n    )\n    contours = contours[0] if len(contours) == 2 else contours[1]\n    currentBoundingBoxesVisualize = getBlackPicture(width, height)\n    for i in contours:\n        x, y, w, h = cv2.boundingRect(i)",
        "type": "code",
        "location": "/tests/unittest_convolution_bilibili_translate_text_detect.py:112-142"
    },
    "1875": {
        "file_id": 179,
        "content": "The code creates a rectangle from input, fills it in the black picture, blurs the filled image, appends it to a list if length is less than convolutionSpan, pops oldest if length exceeds convolutionSpan, gets the current blurred image from the list, prints the bounding boxes count, and if no elements in flatSpan, continues. It then finds contours in the current blurred image and creates a new image for visualization of bounding rectangles.",
        "type": "comment"
    },
    "1876": {
        "file_id": 179,
        "content": "        cv2.rectangle(currentBoundingBoxesVisualize, (x, y), (x + w, y + h), 255, 4)\n    cv2.imshow(\"IMAGE\", currentBoundingBoxesVisualize)\n    cv2.waitKey(10)\n    print(\"showing image:\", intKey)\n    # print\n    # cv2.waitKey(1000)\n    # print(\"NON OVERLAPPING BOXES:\")\n    # print(currentNonOverlappingBoxes)\n    # we need to visualize this shit.\n    # breakpoint()\ncv2.destroyAllWindows()\nprint(\"THE END\")",
        "type": "code",
        "location": "/tests/unittest_convolution_bilibili_translate_text_detect.py:143-156"
    },
    "1877": {
        "file_id": 179,
        "content": "This code snippet is responsible for visualizing bounding boxes, displaying an image, and waiting for a key press. It prints the non-overlapping boxes but may require visualization. The code will close all windows at the end with a final message \"THE END\".",
        "type": "comment"
    },
    "1878": {
        "file_id": 180,
        "content": "/tests/unittest_chain.py",
        "type": "filepath"
    },
    "1879": {
        "file_id": 180,
        "content": "This code seems to be importing a module called \"chain\" for chaining functions, despite the author expressing doubt about its necessity due to list processing power.",
        "type": "summary"
    },
    "1880": {
        "file_id": 180,
        "content": "# how to chain functions?\n# it is hard. we have list processing power. why fucking bother?\nimport chain",
        "type": "code",
        "location": "/tests/unittest_chain.py:1-3"
    },
    "1881": {
        "file_id": 180,
        "content": "This code seems to be importing a module called \"chain\" for chaining functions, despite the author expressing doubt about its necessity due to list processing power.",
        "type": "comment"
    },
    "1882": {
        "file_id": 181,
        "content": "/tests/unittest_bilibili_video_upload.py",
        "type": "filepath"
    },
    "1883": {
        "file_id": 181,
        "content": "This code uses FFmpeg to generate a temporary video and cover image, sets parameters, and uploads the video on Bilibili platform. It utilizes tempfile and uuid modules for handling temporary files and generating random strings. The function call with `multithread=True` tests if it's working with credentials, and debugging is planned for further improvements.",
        "type": "summary"
    },
    "1884": {
        "file_id": 181,
        "content": "from test_commons import *\nimport os\nfrom pyjom.platforms.bilibili.uploader import uploadVideo\nimport uuid\nrandomString = str(uuid.uuid4())\n# import ffmpeg\n# how about let's generate shit?\n# use multithread uploader instead of that.\nimport tempfile\n# import random\nduration = 5\nwith tempfile.NamedTemporaryFile(suffix=\".jpeg\") as pic:\n    cover_path = pic.name\n    with tempfile.NamedTemporaryFile(suffix=\".mp4\") as f:\n        videoPath = f.name\n        command = f\"\"\"ffmpeg -y -f lavfi -i nullsrc=s=1920x1080 -filter_complex \"geq=random(1)*255:128:128;aevalsrc=-2+random(0)\" -t {duration:.2f} {videoPath}\"\"\"\n        os.system(command)\n        picgen_command = f\"\"\"ffmpeg -y -i {videoPath} -ss 1 {cover_path}\"\"\"\n        os.system(picgen_command)\n        print(\"uploading video\")\n        reply = uploadVideo(\n            description=\"test video\",\n            dynamic=\"nothing\",\n            tagString=\"狗狗\",\n            title=\"just a test {}\".format(randomString),\n            videoPath=videoPath,\n            cover_path=cover_path,",
        "type": "code",
        "location": "/tests/unittest_bilibili_video_upload.py:1-29"
    },
    "1885": {
        "file_id": 181,
        "content": "This code generates a temporary video and cover image using FFmpeg, sets necessary parameters such as description, title, tagString, dynamic and calls the uploadVideo function to upload the video on Bilibili platform. The code also utilizes tempfile module for handling temporary files and uuid module for generating random strings.",
        "type": "comment"
    },
    "1886": {
        "file_id": 181,
        "content": "            multithread=True,\n        )  # it is with credential right now.\n        print(\"reply:\", reply)  # reply true? what the fuck?\n        print(\"----\")\n        breakpoint()",
        "type": "code",
        "location": "/tests/unittest_bilibili_video_upload.py:30-34"
    },
    "1887": {
        "file_id": 181,
        "content": "Function call with `multithread=True` to test whether it's working with credentials. The reply is true, and the breakpoint is set for further debugging.",
        "type": "comment"
    },
    "1888": {
        "file_id": 182,
        "content": "/tests/unittest_cv2_rectangle.py",
        "type": "filepath"
    },
    "1889": {
        "file_id": 182,
        "content": "This code imports necessary libraries, defines a function to create a black image of given dimensions, creates a black image, draws a rectangle on it with white color, displays the image, and waits for any key press before exiting.",
        "type": "summary"
    },
    "1890": {
        "file_id": 182,
        "content": "from test_commons import *\nfrom pyjom.commons import *\nimport cv2\nimport numpy as np\ndef getBlackPicture(width, height):\n    blackPicture = np.zeros((height, width, 3), dtype=\"uint8\")  # this is grayscale.\n    return blackPicture\nblackPicture = getBlackPicture(500, 500)\ncv2.rectangle(blackPicture, (200, 200), (300, 300), (255, 255, 255), 3)\ncv2.imshow(\"image\", blackPicture)\ncv2.waitKey(0)",
        "type": "code",
        "location": "/tests/unittest_cv2_rectangle.py:1-16"
    },
    "1891": {
        "file_id": 182,
        "content": "This code imports necessary libraries, defines a function to create a black image of given dimensions, creates a black image, draws a rectangle on it with white color, displays the image, and waits for any key press before exiting.",
        "type": "comment"
    },
    "1892": {
        "file_id": 183,
        "content": "/tests/unittest_async_function_type.py",
        "type": "filepath"
    },
    "1893": {
        "file_id": 183,
        "content": "The code defines two async functions: `randomFunction` and `randomFunctionGenerator`. It determines the types of these functions using `type()`, and compares them to various built-in types. The code prints the results of these comparisons, mentioning that async generators can only be used within async methods, and there is no breakpoint support for async functions. Finally, it assigns a value to `data` by calling `randomFunction`, converts it to sync using `bilibili_api.sync()`, and prints the type and value of `data`.",
        "type": "summary"
    },
    "1894": {
        "file_id": 183,
        "content": "async def randomFunction():\n    return 1\nasync def randomFunctionGenerator():\n    yield await randomFunction()\nfrom bilibili_api import sync\nimport types\ntype0 = type(randomFunction)\ntype1 = type(randomFunction())\ntype2 = types.AsyncGeneratorType\ntype3 = type(randomFunctionGenerator())\ntype4 = types.CoroutineType\ntype5 = type(randomFunctionGenerator)\nprint(type0, type1, type2, type3, type4, type5)\nprint(type1 == type4)\nprint(type2 == type3)\n# async generator can only be used within async methods.\n# no breakpoint support for async functions? wtf?\n# data = randomFunctionGenerator() # this is async generator. different!\ndata = randomFunction()\ndata = sync(data)\n# # not good.\nprint(type(data), data)",
        "type": "code",
        "location": "/tests/unittest_async_function_type.py:1-27"
    },
    "1895": {
        "file_id": 183,
        "content": "The code defines two async functions: `randomFunction` and `randomFunctionGenerator`. It determines the types of these functions using `type()`, and compares them to various built-in types. The code prints the results of these comparisons, mentioning that async generators can only be used within async methods, and there is no breakpoint support for async functions. Finally, it assigns a value to `data` by calling `randomFunction`, converts it to sync using `bilibili_api.sync()`, and prints the type and value of `data`.",
        "type": "comment"
    },
    "1896": {
        "file_id": 184,
        "content": "/tests/test_talib_stream_ema.py",
        "type": "filepath"
    },
    "1897": {
        "file_id": 184,
        "content": "This code is testing the speed of two different methods for calculating a Simple Moving Average (SMA) using Talib library. The first method uses Function API and the second method uses Streaming API. It measures the time taken to execute each method and prints the results along with original data.",
        "type": "summary"
    },
    "1898": {
        "file_id": 184,
        "content": "import talib\nfrom talib import stream\nimport numpy as np\n# check the difference\nimport timeit\nclose = np.random.random(100)\nprint(close.dtype)\nbreakpoint()\n# close = np.append(close,10)\nclose = np.append(close[1:], 10)\nmtime = timeit.timeit(lambda: np.append(close, 10), number=1)  # why so many times?\n# the Function API\n# really don't know which is faster.\noutput = timeit.timeit(\n    lambda: talib.SMA(close), number=1\n)  # why you take it so damn long?\n# the Streaming API\nlatest = timeit.timeit(lambda: stream.SMA(close[-20:]), number=1)\nprint(output)\nprint(latest)\nprint(close)\nprint(mtime)  # why taking so long?",
        "type": "code",
        "location": "/tests/test_talib_stream_ema.py:1-28"
    },
    "1899": {
        "file_id": 184,
        "content": "This code is testing the speed of two different methods for calculating a Simple Moving Average (SMA) using Talib library. The first method uses Function API and the second method uses Streaming API. It measures the time taken to execute each method and prints the results along with original data.",
        "type": "comment"
    }
}