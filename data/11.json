{
    "1100": {
        "file_id": 96,
        "content": "from pyjom.medialang.commons import *\nimport ffmpeg\ndef videoFsProcessor(videoPath,args={},previous = None, medialangTmpDir = medialangTmpDir):\n    if args == {}:\n        return videoPath\n    newVideoPath = getTmpMediaName(medialangTmpDir = medialangTmpDir)\n    return newVideoPath\ndef audioFsProcessor(audioPath,args={},previous = None, medialangTmpDir = medialangTmpDir):\n    if args == {}:\n        return audioPath\n    newAudioPath = getTmpMediaName(medialangTmpDir = medialangTmpDir)\n    return newAudioPath\ndef imageFsProcessor(imagePath,args={},previous = None, medialangTmpDir = medialangTmpDir):\n    if args == {}:\n        return imagePath\n    newImagePath = getTmpMediaName(medialangTmpDir = medialangTmpDir)\n    return newImagePath\ndef fsProcessor(item,previous=None, verbose=True, medialangTmpDir = medialangTmpDir):\n    path = item.path # it exists!\n    fbase = os.path.basename(path)\n    args = item.args\n    mediatype = getFileType(fbase) # mediatype not sure yet.\n    if verbose:\n        print(\"media path:\",path)",
        "type": "code",
        "location": "/pyjom/medialang/processors/mediaProcessor/filesystemProcessor.py:1-28"
    },
    "1101": {
        "file_id": 96,
        "content": "The code contains three functions: videoFsProcessor, audioFsProcessor, and imageFsProcessor. Each function takes a media file path, optional arguments, and temporary directory as inputs. If the arguments are empty, the function returns the original file path; otherwise, it generates a new temporary file path using getTmpMediaName and returns that instead. The fsProcessor function determines the media type (video, audio, or image) based on the file's extension, selects the appropriate processor function, and applies it to the file path. If verbose is True, it prints the media path.",
        "type": "comment"
    },
    "1102": {
        "file_id": 96,
        "content": "        print(\"media type:\",mediatype)\n    # handle to ffmpeg.\n    mediaFunctions = {\"video\":videoFsProcessor,\"audio\":audioFsProcessor,\"image\":imageFsProcessor}\n    data = mediaFunctions[mediatype](path,args=args,previous=previous, medialangTmpDir = medialangTmpDir)\n    return data",
        "type": "code",
        "location": "/pyjom/medialang/processors/mediaProcessor/filesystemProcessor.py:29-33"
    },
    "1103": {
        "file_id": 96,
        "content": "This code segment handles media processing based on the type. It prints the media type, creates a dictionary of function handlers for video, audio, and image, processes the media using the corresponding function, and returns the result.",
        "type": "comment"
    },
    "1104": {
        "file_id": 97,
        "content": "/pyjom/medialang/processors/mediaProcessor/__init__.py",
        "type": "filepath"
    },
    "1105": {
        "file_id": 97,
        "content": "Importing filesystemProcessor module from pyjom.medialang.processors.mediaProcessor package.",
        "type": "summary"
    },
    "1106": {
        "file_id": 97,
        "content": "from pyjom.medialang.processors.mediaProcessor.filesystemProcessor import *",
        "type": "code",
        "location": "/pyjom/medialang/processors/mediaProcessor/__init__.py:1-1"
    },
    "1107": {
        "file_id": 97,
        "content": "Importing filesystemProcessor module from pyjom.medialang.processors.mediaProcessor package.",
        "type": "comment"
    },
    "1108": {
        "file_id": 98,
        "content": "/pyjom/medialang/processors/dotProcessor/videoProcessor.py",
        "type": "filepath"
    },
    "1109": {
        "file_id": 98,
        "content": "The code utilizes Editly and Fmpeg for video filtering, detects text/logos, crop regions, and handles errors while generating UUIDs, storing temp videos, modifying clip properties, setting layer durations, adding clips to templates, and executes with formatted output.",
        "type": "summary"
    },
    "1110": {
        "file_id": 98,
        "content": "from pyjom.medialang.functions import *\nfrom pyjom.medialang.commons import *\nfrom pyjom.mathlib import *\nfrom pyjom.videotoolbox import *\nimport tempfile\nimport ffmpeg\ndef executeEditlyScript(medialangTmpDir, editly_json):\n    editlyJsonSavePath = os.path.join(medialangTmpDir, \"editly.json\")\n    with open(editlyJsonSavePath, \"w+\", encoding=\"utf8\") as f:\n        f.write(json.dumps(editly_json, ensure_ascii=False))\n    print(\"EXECUTING EDITLY JSON AT %s\" % editlyJsonSavePath)\n    commandline = [\"xvfb-run\", \"editly\", \"--json\", editlyJsonSavePath]\n    print(commandline)\n    status = subprocess.run(commandline)  # is it even successful?\n    returncode = status.returncode\n    assert returncode == 0\n    print(\"RENDER SUCCESSFUL\")\nfrom typing import Literal, List\ndef ffmpegVideoPreProductionFilter(\n    filepath,  # this is actually a video path. must be video here.\n    start=None,\n    end=None,\n    cachePath=None,\n    audio=False,\n    epsilon=0.000001,\n    filters: List[\n        Literal[\n            \"minterpolate\",  # add time-saver option for this shit. use 'blend' instead of motion vector based compensation.",
        "type": "code",
        "location": "/pyjom/medialang/processors/dotProcessor/videoProcessor.py:1-34"
    },
    "1111": {
        "file_id": 98,
        "content": "The code is a Python script that contains functions for video processing using the Editly library and FFmpeg. It defines the `executeEditlyScript` function which takes a temporary directory and an Editly JSON object, saves it to disk, executes the Editly command line tool with the saved JSON file, and asserts that the operation was successful. The script also includes the `ffmpegVideoPreProductionFilter` function which processes video files using FFmpeg filters, optional start/end time parameters, audio flag, and a list of filter options like \"minterpolate\".",
        "type": "comment"
    },
    "1112": {
        "file_id": 98,
        "content": "            \"removegrain\",\n            \"bilateral\",\n            \"randomFlip\",\n            \"superResolution\",\n            \"pipCrop\",\n            \"textRemoval\",\n            \"logoRemoval\",\n            \"minterpolate_mi_mode=blend\",\n        ]\n    ] = [  # what is slowing us down?\n        \"pipCrop\",\n        \"textRemoval\",  # we got a problem here?\n        \"logoRemoval\",\n        \"randomFlip\",  # these are common\n        \"superResolution\",  # optional below\n        # \"minterpolate\",\n        # \"minterpolate_mi_mode=blend\", # this might be the problem.\n        # \"removegrain\",\n        # \"bilateral\",\n    ],\n    preview=True,\n    # padding=True,\n    paddingBlur=True,\n    output_width: int = 1920,\n    output_height: int = 1080,\n):  # what is the type of this shit?\n    # enable that 'fast' flag? or we use low_resolution ones? not good since that will ruin our detection system!\n    # anyway it will get processed? or not?\n    # uncertain. very uncertain.\n    def paddingBlurFilter(stream, mWidth=1920, mHeight=1080):\n        # video_stream = stream.video",
        "type": "code",
        "location": "/pyjom/medialang/processors/dotProcessor/videoProcessor.py:35-65"
    },
    "1113": {
        "file_id": 98,
        "content": "This code defines a function `paddingBlurFilter` that takes a video stream as input and applies padding and blur effects if the `paddingBlur` parameter is set to `True`. The video stream's width and height are set to 1920x1080 unless specified otherwise. The code also provides a list of processing steps, where some are marked as common, optional, or potentially causing problems.",
        "type": "comment"
    },
    "1114": {
        "file_id": 98,
        "content": "        video_stream = stream\n        video_stream_split = video_stream.split()\n        output_width = mWidth\n        output_height = mHeight\n        layer_0 = (\n            video_stream_split[0]\n            .filter(\"scale\", w=output_width, h=output_height)\n            .filter(\"gblur\", sigma=9)\n        )\n        layer_1 = video_stream_split[1].filter(\n            \"scale\",\n            w=\"min(floor(iw*{}/ih),{})\".format(output_height, output_width),\n            h=\"min(floor(ih*{}/iw),{})\".format(output_width, output_height),\n        )\n        output_stream = layer_0.overlay(layer_1, x=\"floor((W-w)/2)\", y=\"floor((H-h)/2)\")\n        return output_stream\n    def paddingFilter(stream, mWidth=1920, mHeight=1080):\n        width = \"max(iw, ceil(ih*max({}/{}, iw/ih)))\".format(mWidth, mHeight)\n        height = \"max(ih, ceil(iw*max({}/{}, ih/iw)))\".format(mHeight, mWidth)\n        x = \"max(0,floor(({}-iw)/2))\".format(width)\n        y = \"max(0,floor(({}-ih)/2))\".format(height)\n        return (\n            stream.filter(\n                \"pad\", width=width, height=height, x=x, y=y, color=\"black\"",
        "type": "code",
        "location": "/pyjom/medialang/processors/dotProcessor/videoProcessor.py:66-90"
    },
    "1115": {
        "file_id": 98,
        "content": "This code defines two functions: `videoProcessor` and `paddingFilter`. The `videoProcessor` function takes a video stream, scales and blurs the first layer of the split video, scales the second layer maintaining aspect ratio, then overlays both layers with padding. The `paddingFilter` function calculates the width and height for padding to center the video. Both functions take optional parameters `mWidth` and `mHeight` for the output resolution.",
        "type": "comment"
    },
    "1116": {
        "file_id": 98,
        "content": "            )  # here to control the padding logic, decide how to 'blur' the thing!\n            .filter(\"scale\", w=mWidth, h=mHeight)\n            .filter(\"setsar\", 1)\n        )\n    assert cachePath is not None\n    assert start is not None\n    assert end is not None\n    # from 4 to 10 seconds?\n    defaultWidth, defaultHeight = getVideoWidthHeight(filepath)\n    previewRatio = 1\n    if preview:\n        previewWidth, previewHeight = getVideoPreviewPixels(filepath)\n        previewRatio = previewWidth / defaultWidth\n        def previewFilter(stream):\n            # maintain original ratio?\n            return stream.filter(\n                \"scale\",\n                \"ceil((iw*{})/4)*4\".format(previewRatio),\n                \"ceil((ih*{})/4)*4\".format(previewRatio),\n            )\n    # stream = ffmpeg.hflip(stream)\n    # this fliping may be useful for copyright evasion, but not very useful for filtering. it just adds more computational burden.\n    # we just need to crop this.\n    # stream = ffmpeg.output(stream, cachePath)",
        "type": "code",
        "location": "/pyjom/medialang/processors/dotProcessor/videoProcessor.py:91-117"
    },
    "1117": {
        "file_id": 98,
        "content": "Code snippet performs video processing operations including scaling, setting aspect ratio, and potentially applying a preview filter based on the input filepath. It also considers cropping instead of flipping for improved efficiency and removes any unnecessary computational burden.",
        "type": "comment"
    },
    "1118": {
        "file_id": 98,
        "content": "    # ffmpeg.run(stream, overwrite_output=True)\n    # procedureList = []\n    # stream = ffmpeg.input\n    # no_processing = True # change this flag if anything need to change in original video according to filter results.\n    # logo removal/text removal first, pipCrop last.\n    # if overlap, we sort things.\n    # if not, no sorting is needed.\n    mDict = {}\n    def delogoFilter(stream, commandParams):\n        return stream.filter(\n            \"delogo\",\n            x=commandParams[\"x\"],\n            y=commandParams[\"y\"],\n            w=commandParams[\"w\"],\n            h=commandParams[\"h\"],\n        )\n    def cropFilter(stream, commandParams):\n        return stream.filter(\n            \"crop\",\n            x=commandParams[\"x\"],\n            y=commandParams[\"y\"],\n            w=commandParams[\"w\"],\n            h=commandParams[\"h\"],\n        )\n    def filterCommandStringParser(filterCommandString):\n        args_with_kwargs = filterCommandString.split(\":\")\n        args = []\n        kwargs = {}\n        for elem in args_with_kwargs:",
        "type": "code",
        "location": "/pyjom/medialang/processors/dotProcessor/videoProcessor.py:118-151"
    },
    "1119": {
        "file_id": 98,
        "content": "This code defines functions for applying filters to video streams using ffmpeg. It defines delogoFilter() and cropFilter() functions that take stream and commandParams as input, apply respective filters using ffmpeg, and return the filtered stream. The filterCommandStringParser() function parses filter command strings into arguments and keyword arguments.",
        "type": "comment"
    },
    "1120": {
        "file_id": 98,
        "content": "            if \"=\" in elem:\n                key, value = elem.split(\"=\")\n                kwargs.update({key: value})\n            else:\n                args.append(elem)\n        return args, kwargs\n    def ffmpegStringFilter(stream, commandString):\n        filterName = commandString.split(\"_\")[0]\n        filterPrefix = \"{}_\".format(filterName)\n        filterCommandString = commandString[len(filterPrefix) :]\n        args, kwargs = filterCommandStringParser(filterCommandString)\n        # print(commandString)\n        # print(args, kwargs)\n        # breakpoint()\n        return stream.filter(\"scale\", *args, **kwargs)\n    # TODO: FIX THIS SHIT!\n    # raise Exception(\"TODO: FIX THIS SHIT!\")\n    # these things are ordered to be the last ones. just flags.\n    from caer.video.frames_and_fps import get_duration\n    video_start = 0\n    video_end = get_duration(filepath)\n    if \"randomFlip\" in filters:\n        if random.random() > 0.5:\n            mDict.update({\"hflip\": [(start, end)]})\n    if \"superResolution\" in filters:",
        "type": "code",
        "location": "/pyjom/medialang/processors/dotProcessor/videoProcessor.py:152-180"
    },
    "1121": {
        "file_id": 98,
        "content": "The code defines a function `ffmpegStringFilter` which takes in a stream and command string. It splits the command string into its filter name, filter prefix, and filter command string. Then it calls another function `filterCommandStringParser` to parse the filter command string into arguments (args) and keyword arguments (kwargs). Finally, it returns the filtered video stream using the parsed arguments. Additionally, there are two TODO comments to fix some issues later on.",
        "type": "comment"
    },
    "1122": {
        "file_id": 98,
        "content": " # not working for extremely poor quality images. however, we can fetch these video elsewhere. no need to repair.\n        mDict.update({\"scale_w=iw*2:h=ih*2:flags=lanczos\": [(start, end)]})\n        # how to parse this shit?\n    simpleFilters = [\"minterpolate\", \"removegrain\", \"bilateral\"]\n    for filterName in simpleFilters:\n        for myFilter in filters:\n            if myFilter.startswith(filterName):\n                # if filterName == \"minterpolate\":\n                # filterName += \"_mi_mode=blend\"\n                # print(\"FILTER NAME:\", filterName)\n                # breakpoint()\n                mDict.update({filterName: [(start, end)]})\n    pipCropDicts = None\n    if \"pipCrop\" in filters:\n        # remember: if pip crop makes any of our logoRemoval or textRemoval filters invalid, we do not execute them.\n        # also it will affect parameters of logoRemoval.\n        pipCropDicts = detectPipRegionOverTime(filepath, start, end)\n        mDict.update(pipCropDicts)  # using default settings?\n        # pass",
        "type": "code",
        "location": "/pyjom/medialang/processors/dotProcessor/videoProcessor.py:180-199"
    },
    "1123": {
        "file_id": 98,
        "content": "Code snippet is updating a dictionary with specified filters and their corresponding time intervals. It checks if any 'pipCrop' filter is present, and if so, it retrieves the PIP crop regions over time using a separate function 'detectPipRegionOverTime'. The retrieved crop regions are then added to the dictionary using default settings.",
        "type": "comment"
    },
    "1124": {
        "file_id": 98,
        "content": "    if \"textRemoval\" in filters:\n        # process the video, during that duration. fast seek avaliable?\n        mDict.update(detectTextRegionOverTime(filepath, start, end))\n        # pass\n    if \"logoRemoval\" in filters:\n        # dual safe? no?\n        # the dict is not hashable. warning!\n        stationaryLogoDicts = detectStationaryLogoOverTime(\n            filepath, start, end, pipCropDicts=pipCropDicts\n        )  # this need to be improvised. if it is long, we need to do another check.\n        if video_end > 30:\n            stationaryLogoDicts.update(\n                detectStationaryLogoOverTime(\n                    filepath, video_start, video_end, cornersOnly=False, top_k=5\n                )  # are you sure? wtf?\n                # i mean area size similar than one of the corners.\n            )\n        # reprocess these things. really?\n        mDict.update(stationaryLogoDicts)  # output logo mask. or not.\n        # estimate the shape with multiple rectangles? packing algorithm?\n        # polygon to rectangle? decomposition?",
        "type": "code",
        "location": "/pyjom/medialang/processors/dotProcessor/videoProcessor.py:200-220"
    },
    "1125": {
        "file_id": 98,
        "content": "The code checks if \"textRemoval\" and \"logoRemoval\" are in the filters. If \"textRemoval\" is present, it detects text regions over time for the given video filepath, start, and end duration. If \"logoRemoval\" is present, it detects stationary logos using detectStationaryLogoOverTime function and updates mDict with the results, but only if the video length is greater than 30 seconds. It also considers updating mDict with additional stationary logos if needed by calling detectStationaryLogoOverTime again. Finally, it updates mDict with the stationary logo dictionaries and suggests using a packing algorithm to estimate shapes with multiple rectangles.",
        "type": "comment"
    },
    "1126": {
        "file_id": 98,
        "content": "        # pass\n    MAX_INT = 999999\n    commandValueMap = {\n        \"empty\": -1,\n        \"delogo\": 0,\n        \"crop\": 1,\n        \"removegrain\": 2,\n        \"bilateral\": 2,\n        \"scale\": 3,  ## wtf?\n        \"minterpolate\": 4,\n        \"hflip\": MAX_INT,\n        \"vflip\": MAX_INT,\n    }  # no scale filter shall present. we do not provide such creep. editly will handle it.\n    # commandValueMap.update(simpleFiltersValueMap)\n    renderDict = getContinualMappedNonSympyMergeResultWithRangedEmpty(mDict, start, end)\n    # now we consider the rendering process. how?\n    # shall we line it up?\n    if (\n        list(renderDict.keys()) == [\"empty\"] and not preview\n    ):  # not preview! so we need not to downscale this thing.\n        # nothing happens. just return the original shit.\n        return filepath\n    renderList = mergedRangesToSequential(renderDict)\n    renderVideoStreamList = []\n    if audio:\n        # we may want audio sometime, but not this time.\n        renderAudioStream = ffmpeg.input(filepath, ss=start, to=end).audio",
        "type": "code",
        "location": "/pyjom/medialang/processors/dotProcessor/videoProcessor.py:221-248"
    },
    "1127": {
        "file_id": 98,
        "content": "The code defines a commandValueMap for different video processing commands and checks if the renderDict contains only \"empty\" key. If it does, it returns the original file path without any processing. If not, it converts merged rendering ranges into sequential format and creates a list of renderVideoStreams (excluding audio if specified).",
        "type": "comment"
    },
    "1128": {
        "file_id": 98,
        "content": "    # for elem in renderList:\n    #     print(elem)\n    # breakpoint()\n    # videoDuration = getVideoDuration(videoPath)\n    for renderCommandIndex, (renderCommandString, commandTimeSpan) in enumerate(\n        renderList\n    ):\n        print(\"#{}\".format(renderCommandIndex), renderCommandString, commandTimeSpan)\n        mStart, mEnd = commandTimeSpan\n        mStart = max(start, mStart)\n        mEnd = min(mEnd, end)\n        clipDuration = mEnd - mStart\n        if clipDuration < epsilon:\n            continue  # if so, this clip is shit.\n        # print(\"CLIP TIMESPAN:\", mStart, mEnd)\n        stream = ffmpeg.input(\n            filepath, ss=mStart, to=mEnd, hwaccel=\"vulkan\"\n        ).video  # no audio? seriously?\n        # this is video stream.\n        if renderCommandString == \"empty\":\n            pass  # do not continue since maybe we have preview filter below?\n            # still need to append shit below. we cannot skip this loop.\n        # do nothing.\n        else:\n            renderCommands = renderCommandString.split(\"|\")",
        "type": "code",
        "location": "/pyjom/medialang/processors/dotProcessor/videoProcessor.py:249-274"
    },
    "1129": {
        "file_id": 98,
        "content": "The code iterates through a list of render commands, each with a time span. It prints each command and the corresponding timespan, skips any clip duration less than epsilon, and gets the video stream using ffmpeg for specified start and end times. If the command is \"empty\", it does nothing; otherwise, it splits the command into separate render commands.",
        "type": "comment"
    },
    "1130": {
        "file_id": 98,
        "content": "            # sort all commands?\n            renderCommands.sort(\n                key=lambda command: commandValueMap[command.split(\"_\")[0]]\n            )\n            from pyjom.mathlib import uniq\n            for renderCommand in uniq(renderCommands):\n                # print('RENDER COMMAND:',renderCommand, \"SPAN\", mStart, mEnd)\n                # breakpoint()\n                if renderCommand == \"empty\":\n                    # yeah we have failsafe.\n                    continue\n                if \"_\" not in renderCommand:\n                    stream = stream.filter(renderCommand)\n                elif \"=\" in renderCommand:\n                    stream = ffmpegStringFilter(\n                        stream, renderCommand\n                    )  # do not check for validity!\n                else:\n                    # non standard filter formats below. be warned.\n                    for prefix, keyword in [\n                        (\"{}_\".format(k), k) for k in [\"delogo\", \"crop\"]\n                    ]:\n                        if renderCommand.startswith(prefix):",
        "type": "code",
        "location": "/pyjom/medialang/processors/dotProcessor/videoProcessor.py:275-298"
    },
    "1131": {
        "file_id": 98,
        "content": "Code is sorting commands based on a given key and applying them to the stream. If a command doesn't have an operator or contains non-standard filter formats, it applies prefix-based actions. The code handles different types of filters including 'delogo', 'crop', etc., and applies them accordingly to the stream.",
        "type": "comment"
    },
    "1132": {
        "file_id": 98,
        "content": "                            import parse\n                            commandParams = parse.parse(\n                                keyword + \"_{x:d}_{y:d}_{w:d}_{h:d}\", renderCommand\n                            )\n                            # print(defaultWidth, defaultHeight)\n                            mX, mY, mW, mH = (\n                                commandParams[\"x\"],\n                                commandParams[\"y\"],\n                                commandParams[\"w\"],\n                                commandParams[\"h\"],\n                            )\n                            status, XYWH = checkXYWH(\n                                (mX, mY, mW, mH), (defaultWidth, defaultHeight)\n                            )\n                            if not status:\n                                # cannot process this delogo filter since its parameters are outraged.\n                                # shall we warn you?\n                                # print(\"SOMEHOW DELOGO IS NOT WORKING PROPERLY\")\n                                # breakpoint()",
        "type": "code",
        "location": "/pyjom/medialang/processors/dotProcessor/videoProcessor.py:299-318"
    },
    "1133": {
        "file_id": 98,
        "content": "The code imports the \"parse\" module and parses a command string using a specific format. It then extracts x, y, w, and h values from the parsed command and checks if they match the default width and height. If not, it cannot process this delogo filter and may warn or raise an issue.",
        "type": "comment"
    },
    "1134": {
        "file_id": 98,
        "content": "                                # maybe it's not because of out of bounds error\n                                print(\"_\" * 30)\n                                print(\n                                    \"ABNORMAL {} FILTER PARAMS:\".format(\n                                        keyword.upper()\n                                    ),\n                                    commandParams,\n                                )\n                                print(\n                                    \"maxX: {} maxY: {}\".format(\n                                        commandParams[\"x\"] + commandParams[\"w\"],\n                                        commandParams[\"y\"] + commandParams[\"h\"],\n                                    )\n                                )\n                                print(\"VALID BOUNDARIES:\", defaultWidth, defaultHeight)\n                                print(\"_\" * 30)\n                                continue\n                            else:\n                                (mX, mY, mW, mH) = XYWH",
        "type": "code",
        "location": "/pyjom/medialang/processors/dotProcessor/videoProcessor.py:319-337"
    },
    "1135": {
        "file_id": 98,
        "content": "If out of bounds error occurs, it prints abnormal filter params and boundary information before continuing execution.",
        "type": "comment"
    },
    "1136": {
        "file_id": 98,
        "content": "                                commandParams = {\"x\": mX, \"y\": mY, \"w\": mW, \"h\": mH}\n                            # mX1, mY1 = mX+mW, mY+mH\n                            # if mX1>defaultWidth or mY1>defaultHeight: # opecv to be blamed?\n                            #     print(\"DELOGO ERROR:\")\n                            #     print(mX1,defaultWidth,mY1,defaultHeight)\n                            #     breakpoint()\n                            # we also need to consider if this is necessary.\n                            if keyword == \"delogo\":\n                                stream = delogoFilter(stream, commandParams)\n                            elif keyword == \"crop\":\n                                stream = cropFilter(stream, commandParams)\n                                # TODO: the main shit happens here is that if pip region is detected, it (the crop region) will not maintain the width to height ratio. you might need padding, and that's what we about to do here. you may also extract that clip as standalone material.",
        "type": "code",
        "location": "/pyjom/medialang/processors/dotProcessor/videoProcessor.py:338-349"
    },
    "1137": {
        "file_id": 98,
        "content": "This code checks if the keyword is \"delogo\" or \"crop\". If it's \"crop\", it applies a crop filter to the stream and checks if the pipeline region is detected. If so, it might need padding or could be extracted as standalone material.",
        "type": "comment"
    },
    "1138": {
        "file_id": 98,
        "content": "                                # more inspection is needed for comprehensive reasoning.\n        if paddingBlur:\n            stream = paddingBlurFilter(\n                stream, mWidth=output_width, mHeight=output_height\n            )\n        else:\n            stream = paddingFilter(stream, mWidth=output_width, mHeight=output_height)\n        if preview:  # final filter? need us to crop this?\n            stream = previewFilter(\n                stream\n            )  # just preview, no need to set output width/height!\n            # do nothing here! (no fx.)\n        # and?\n        # we need to concat these shit!\n        # print(stream)\n        # print(dir(stream))\n        # breakpoint()\n        # import copy\n        # print(stream)\n        renderVideoStreamList.append(stream)\n    # for x in renderVideoStreamList:\n    #     print(x)\n    # print(len(renderVideoStreamList))\n    # breakpoint()\n    # breakpoint()\n    renderVideoStream = ffmpeg.concat(*renderVideoStreamList)\n    # detect if there is really anything audio related!",
        "type": "code",
        "location": "/pyjom/medialang/processors/dotProcessor/videoProcessor.py:350-376"
    },
    "1139": {
        "file_id": 98,
        "content": "The code is processing video streams with optional padding, blur, and preview filters. The resulting streams are concatenated into a single video using the ffmpeg library.",
        "type": "comment"
    },
    "1140": {
        "file_id": 98,
        "content": "    if audio:\n        renderStream = ffmpeg.output(renderVideoStream, renderAudioStream, cachePath)\n    else:\n        renderStream = ffmpeg.output(renderVideoStream, cachePath)\n    # DEBUG #\n    # args = renderStream.get_args()\n    # print(args)\n    # breakpoint()\n    # DEBUG #\n    renderStream.run(overwrite_output=True)\n    return cachePath\ndef dotVideoProcessor(\n    item, previous, format=None, verbose=True, medialangTmpDir=\"/dev/shm/medialang/\"\n):\n    # print(\"DOTVIDEO ARGS:\", item, previous, format)\n    # this item is the video output config, medialang item.\n    itemArgs = item.args\n    if format is None:\n        format = item.path.split(\".\")[-1]\n    backend = itemArgs.get(\n        \"backend\", \"editly\"  # this is mere assumption!\n    )  # so all things will be assumed to put directly into editly render json, unless explicitly specified under other medialang or other backend and need to be resolved into media file format before rendering. sure?\n    fast = itemArgs.get(\"fast\", True)\n    bgm = itemArgs.get(\"bgm\", None)",
        "type": "code",
        "location": "/pyjom/medialang/processors/dotProcessor/videoProcessor.py:377-402"
    },
    "1141": {
        "file_id": 98,
        "content": "The code is checking if there's an audio stream present. If so, it combines the video and audio streams using ffmpeg. Otherwise, it only processes the video stream. The DEBUG section is for debugging purposes and logs arguments of the rendered stream. Finally, the code runs the rendering process and returns the cache path. This function aims to generate a video output configuration from medialang item, assuming the backend as \"editly\" if not specified.",
        "type": "comment"
    },
    "1142": {
        "file_id": 98,
        "content": "    # outputPath = itemArgs.get(\"\",None)\n    randomUUID = str(uuid.uuid4())\n    outputPath = os.path.join(\n        medialangTmpDir, randomUUID + \".\" + format\n    )  # this is temporary!\n    # usually we choose to use something under medialang tempdir as the storage place.\n    print(\"medialang config:\", format, backend, fast, bgm)\n    # the \"previous\" is the clips, was fucked, filled with non-existant intermediate mpegts files, but no source was out there.\n    # this is initially decided to output mp4, however you might want to decorate it.\n    if verbose:\n        print(\"_________INSIDE DOT VIDEO PROCESSOR_________\")\n        print(\"ITEM:\", item)\n        print(\"PREVIOUS:\", previous)\n        print(\"_________INSIDE DOT VIDEO PROCESSOR_________\")\n    with tempfile.TemporaryDirectory(\n        dir=medialangTmpDir\n    ) as tmpdirname:  # maybe you should take care of the directory prefix?\n        # wtf are you doing over here?\n        # find out where our cache leads to!\n        # maybe the final product is one move away.",
        "type": "code",
        "location": "/pyjom/medialang/processors/dotProcessor/videoProcessor.py:403-423"
    },
    "1143": {
        "file_id": 98,
        "content": "This code snippet generates a random UUID and stores the video temporarily in a specified directory. It ensures that the output is under medialang's temporary directory, and the verbose parameter allows printing details about the item, previous clip, and other relevant information. The code also utilizes tempfile.TemporaryDirectory to manage the temporary directory.",
        "type": "comment"
    },
    "1144": {
        "file_id": 98,
        "content": "        tmpdirname = os.path.abspath(tmpdirname)\n        print(\"created temporary directory\", tmpdirname)\n        output_path = os.path.join(\n            tmpdirname, randomUUID + \".\" + format\n        )  # this is temporary!\n        # that is the tweak. we have successfully changed the directory!\n        if backend == \"editly\":\n            # iterate through all items.\n            template = {\n                \"width\": 1920,\n                \"height\": 1080,\n                \"fast\": fast,\n                \"fps\": 60,\n                \"outPath\": output_path,\n                \"defaults\": {\"transition\": None},\n                \"clips\": [],\n            }\n            if bgm is not None:\n                template.update({\"audioFilePath\": bgm})\n            for elem in previous:\n                duration = 3  # default duration\n                clip = {\n                    \"duration\": duration,\n                    \"layers\": [],\n                }\n                layer_durations = []\n                for layerElem in elem:\n                    layer = None",
        "type": "code",
        "location": "/pyjom/medialang/processors/dotProcessor/videoProcessor.py:424-452"
    },
    "1145": {
        "file_id": 98,
        "content": "The code creates a temporary directory, generates a unique output path for the edited video file, and sets up a template for processing using Editly backend. The template includes parameters such as width, height, fast mode, fps, output path, audio file path (if provided), duration of each clip, and layers in each clip. A default transition is also included.",
        "type": "comment"
    },
    "1146": {
        "file_id": 98,
        "content": "                    # print(layerElem) # {\"item\":<item>, \"cache\": <cache_path>}\n                    cachePath = layerElem[\"cache\"]\n                    # breakpoint()\n                    layerElemItem = layerElem[\"item\"]\n                    filepath = layerElemItem.path\n                    # what type is this damn media?\n                    filetype = getFileType(filepath)\n                    if layerElemItem.args.get(\"backend\", \"editly\") == \"editly\":\n                        if filetype == \"video\":\n                            videoInfo = get_media_info(filepath)\n                            endOfVideo = videoInfo[\"duration\"]\n                            cutFrom = layerElemItem.args.get(\"cutFrom\", 0)\n                            cutTo = layerElemItem.args.get(\"cutTo\", endOfVideo)\n                            layerOriginalDuration = cutTo - cutFrom\n                            mute = layerElemItem.args.get(\"slient\", False)\n                            processedFilePath = ffmpegVideoPreProductionFilter(\n                                filepath,",
        "type": "code",
        "location": "/pyjom/medialang/processors/dotProcessor/videoProcessor.py:453-471"
    },
    "1147": {
        "file_id": 98,
        "content": "This code segment is responsible for processing video layers in a media project. It first retrieves the cache path and filepath from the input layer element, then determines the file type using getFileType function. If the backend is set to \"editly\" and the file type is a video, it extracts information about the video's duration and any specified cut range within the layer element. Finally, it applies ffmpegVideoPreProductionFilter to process the video according to the specified parameters.",
        "type": "comment"
    },
    "1148": {
        "file_id": 98,
        "content": "                                start=cutFrom,\n                                end=cutTo,\n                                cachePath=cachePath,\n                                preview=fast,\n                                audio=not mute,\n                            )\n                            # what is this filepath? man how do i handle this?\n                            videoFilePath = processedFilePath\n                            # get video information!\n                            # if processed:\n                            # this must be true now.\n                            cutFrom = 0\n                            cutTo = layerOriginalDuration\n                            speed = layerElemItem.args.get(\"speed\", 1)\n                            # was wrong.\n                            layerDuration = (cutTo - cutFrom) / speed\n                            layer_durations.append(layerDuration)\n                            layer = {\n                                \"type\": \"video\",\n                                \"path\": videoFilePath,",
        "type": "code",
        "location": "/pyjom/medialang/processors/dotProcessor/videoProcessor.py:472-492"
    },
    "1149": {
        "file_id": 98,
        "content": "This code is processing a video file and saving it to a specified path. It sets the start and end time of the clip, determines the speed of playback, and appends the duration of the layer to a list. The processed video file's path is stored in \"videoFilePath\".",
        "type": "comment"
    },
    "1150": {
        "file_id": 98,
        "content": "                                \"resizeMode\": \"contain\",\n                                \"cutFrom\": cutFrom,\n                                \"cutTo\": cutTo,\n                                # that's how we mute it.\n                                \"mixVolume\": 1 - int(mute),\n                            }\n                            removeKeys = []\n                            for key, elem in layer.items():\n                                if elem is None:\n                                    removeKeys.append(key)\n                            for key in removeKeys:\n                                del layer[key]\n                    if layer is not None:\n                        clip[\"layers\"].append(layer)\n                    else:\n                        raise Exception(\"NOT IMPLEMENTED LAYER FORMAT:\", layerElem)\n                maxDuration = max(layer_durations)\n                clip[\"duration\"] = maxDuration\n                template[\"clips\"].append(clip)\n                # then just execute this template, or let's just view it.",
        "type": "code",
        "location": "/pyjom/medialang/processors/dotProcessor/videoProcessor.py:493-512"
    },
    "1151": {
        "file_id": 98,
        "content": "The code is modifying a clip's properties, such as mute and resize mode, based on input parameters like cutFrom and cutTo. It also handles None values in the layer dictionary by removing them. The code then sets the maximum duration of all layers, adds the modified clip to the template, and finally executes or views the template.",
        "type": "comment"
    },
    "1152": {
        "file_id": 98,
        "content": "            if verbose:\n                print(\"________________editly template________________\")\n                print(\n                    json.dumps(template, ensure_ascii=False, indent=4)\n                )  # let's view it elsewhere? or in `less`?\n                print(\"________________editly template________________\")\n            # breakpoint()\n            # return template\n            executeEditlyScript(medialangTmpDir, template)\n            os.rename(output_path, outputPath)\n            return outputPath",
        "type": "code",
        "location": "/pyjom/medialang/processors/dotProcessor/videoProcessor.py:513-523"
    },
    "1153": {
        "file_id": 98,
        "content": "This code snippet checks if verbose is true and prints the editly template in a formatted way. It then executes the Editly script, renames the output file from `output_path` to `outputPath`, and finally returns the output path.",
        "type": "comment"
    },
    "1154": {
        "file_id": 99,
        "content": "/pyjom/medialang/processors/dotProcessor/jsonProcessor.py",
        "type": "filepath"
    },
    "1155": {
        "file_id": 99,
        "content": "This function dotJsonProcessor takes an item, a previous item, and optional verbose and medialangTmpDir parameters. It extracts the processor name from the item's arguments, gets the corresponding Medialang function using getMedialangFunction, checks if it exists or not, and then applies the processor on the previous item using keywordDecorator. The output of this processing is returned.",
        "type": "summary"
    },
    "1156": {
        "file_id": 99,
        "content": "from pyjom.medialang.functions import *\nfrom pyjom.medialang.commons import *\ndef dotJsonProcessor(item, previous, verbose=True, medialangTmpDir=\"/dev/shm/medialang/\"):\n    # must contain something.\n    args = item.args\n    processorName = args[\"processor\"]\n    processor = getMedialangFunction(processorName)\n    if processor is None:\n        medialangFatalError(\"processor {} not found.\".format(processorName), __file__)\n    print(\"Using JSON processor:\", processorName)\n    args.pop(\"processor\")\n    # breakpoint()\n    output = keywordDecorator(processor, **args)(previous)  # what is this shit?\n    return output",
        "type": "code",
        "location": "/pyjom/medialang/processors/dotProcessor/jsonProcessor.py:1-16"
    },
    "1157": {
        "file_id": 99,
        "content": "This function dotJsonProcessor takes an item, a previous item, and optional verbose and medialangTmpDir parameters. It extracts the processor name from the item's arguments, gets the corresponding Medialang function using getMedialangFunction, checks if it exists or not, and then applies the processor on the previous item using keywordDecorator. The output of this processing is returned.",
        "type": "comment"
    },
    "1158": {
        "file_id": 100,
        "content": "/pyjom/medialang/processors/dotProcessor/__init__.py",
        "type": "filepath"
    },
    "1159": {
        "file_id": 100,
        "content": "The code imports necessary classes from jsonProcessor and videoProcessor modules, then creates a dictionary called dotProcessors, which maps file extensions to their respective processors. The .json extension is associated with the dotJsonProcessor, while .mp4 is associated with the keywordDecorator-decorated dotVideoProcessor.",
        "type": "summary"
    },
    "1160": {
        "file_id": 100,
        "content": "from pyjom.medialang.processors.dotProcessor.jsonProcessor import *\nfrom pyjom.medialang.processors.dotProcessor.videoProcessor import *\nfrom pyjom.commons import keywordDecorator\ndotProcessors = {\".json\":dotJsonProcessor, \".mp4\": keywordDecorator(dotVideoProcessor,format=\"mp4\")}",
        "type": "code",
        "location": "/pyjom/medialang/processors/dotProcessor/__init__.py:1-5"
    },
    "1161": {
        "file_id": 100,
        "content": "The code imports necessary classes from jsonProcessor and videoProcessor modules, then creates a dictionary called dotProcessors, which maps file extensions to their respective processors. The .json extension is associated with the dotJsonProcessor, while .mp4 is associated with the keywordDecorator-decorated dotVideoProcessor.",
        "type": "comment"
    },
    "1162": {
        "file_id": 101,
        "content": "/pyjom/medialang/functions/__init__.py",
        "type": "filepath"
    },
    "1163": {
        "file_id": 101,
        "content": "The code imports detectors from the medialangFunctions module and creates a dictionary of available media language functions. The getMedialangFunction function searches the dictionary for the specified function name, returns it if found, and prints its type and name. If not found, it returns None.",
        "type": "summary"
    },
    "1164": {
        "file_id": 101,
        "content": "from pyjom.medialang.functions.detectors import *\nmedialangFunctions = {\"detector\": medialangDetectors}\ndef getMedialangFunction(function):\n    for key in medialangFunctions:\n        mgroup = medialangFunctions[key]\n        for key2 in mgroup:\n            if key2 == function:\n                function = mgroup[key2]\n                print(\"function type:\", key)\n                print(\"function name:\", key2)\n                return function\n    return None",
        "type": "code",
        "location": "/pyjom/medialang/functions/__init__.py:1-15"
    },
    "1165": {
        "file_id": 101,
        "content": "The code imports detectors from the medialangFunctions module and creates a dictionary of available media language functions. The getMedialangFunction function searches the dictionary for the specified function name, returns it if found, and prints its type and name. If not found, it returns None.",
        "type": "comment"
    },
    "1166": {
        "file_id": 102,
        "content": "/pyjom/medialang/functions/detectors/blackoutDetector.py",
        "type": "filepath"
    },
    "1167": {
        "file_id": 102,
        "content": "This code detects blackouts in media by calculating scores per frame block and storing results. It works for videos and images, using OpenCV or videoFrameIterator. The code updates metadata dictionaries and appends updated results to a list before returning the final list of results.",
        "type": "summary"
    },
    "1168": {
        "file_id": 102,
        "content": "from .mediaDetector import *\ndef blackoutIdentifier(frame_a, cut=3, threshold=30, method=\"average\"):\n    assert cut >= 1\n    methods = {\"average\": np.average, \"max\": np.max, \"min\": np.min}\n    mshape = frame_a.shape\n    width, height = mshape[:2]\n    mcut = int(min(width, height) / cut)\n    if len(mshape) == 3:\n        result = methods[method](result, axis=2)\n        shape0 = int(width / mcut)\n    shape1 = int(height / mcut)\n    diff = np.zeros((shape0, shape1)).tolist()\n    # mapping = {}\n    for x in range(shape0):\n        for y in range(shape1):\n            area = result[x * mcut : (x + 1) * mcut, y * mcut : (y + 1) * mcut]\n            score = (area < threshold).astype(int)\n            diff[x][y] = score.sum() / score.size\n    return {\n        \"blackout\": diff,\n        \"blocksize\": mcut,\n    }  # required for recovering center points.\ndef blackoutDetector(mediapaths, cut=3, threshold=30, method=\"average\", timestep=0.2):\n    # any better detectors? deeplearning?\n    results = []\n    data_key = \"blackout_score\"",
        "type": "code",
        "location": "/pyjom/medialang/functions/detectors/blackoutDetector.py:1-30"
    },
    "1169": {
        "file_id": 102,
        "content": "blackoutIdentifier function: Calculates blackout score per block of a frame using average, max, or min method; accepts cut, threshold, and method parameters.\n\nblackoutDetector function: Detects blackouts across multiple media paths by calling blackoutIdentifier; stores results in 'data_key' for further use.",
        "type": "comment"
    },
    "1170": {
        "file_id": 102,
        "content": "    for mediapath in mediapaths:\n        print(\"mediapath:\", mediapath)\n        mediatype = getFileType(mediapath)\n        print(\"subtitle of mediatype:\", mediatype)\n        assert mediatype in [\"video\", \"image\"]  # gif? anything like that?\n        result = {\"type\": mediatype, data_key: {}}\n        config = {\"cut\": cut, \"threshold\": threshold, \"method\": method}\n        if mediatype == \"image\":\n            data = cv2.imread(mediapath)\n            data = keywordDecorator(blackoutIdentifier, **config)(data)\n            result[data_key].update({\"blackout_detector\": data})\n            result[data_key].update({\"config\": config})\n        else:\n            keyword = \"blackout_detector\"\n            mdata, metadata = videoFrameIterator(\n                mediapath,\n                data_producer=keywordDecorator(blackoutIdentifier, **config),\n                framebatch=1,\n                timestep=timestep,\n                keyword=keyword,\n            )\n            metadata.update({\"config\": config})\n            result[data_key][keyword] = mdata",
        "type": "code",
        "location": "/pyjom/medialang/functions/detectors/blackoutDetector.py:31-54"
    },
    "1171": {
        "file_id": 102,
        "content": "This code iterates over mediapaths, detects media type (video or image), applies blackout detection configuration, and stores the result in a dictionary. For images, it uses OpenCV to read image data and apply keyword decorator for blackout identification. For videos, it uses videoFrameIterator with keyword decorator as data producer for frame-by-frame blackout detection.",
        "type": "comment"
    },
    "1172": {
        "file_id": 102,
        "content": "            result[data_key].update(metadata)\n        results.append(result)\n    return results",
        "type": "code",
        "location": "/pyjom/medialang/functions/detectors/blackoutDetector.py:55-57"
    },
    "1173": {
        "file_id": 102,
        "content": "This code block takes a metadata dictionary and updates it to an existing data key in the result dictionary. After updating, it appends the updated result dictionary to the results list and returns the final list of results.",
        "type": "comment"
    },
    "1174": {
        "file_id": 103,
        "content": "/pyjom/medialang/functions/detectors/__init__.py",
        "type": "filepath"
    },
    "1175": {
        "file_id": 103,
        "content": "This code imports various detector functions, defines a medialang input function, and creates a dictionary of detectors for processing media data. It handles potential problematic inputs.",
        "type": "summary"
    },
    "1176": {
        "file_id": 103,
        "content": "# from pyjom.medialang.functions.detectors.mediaDetector import *\nfrom .blackoutDetector import *\nfrom .subtitleDetector import *\nfrom .videoDiffDetector import *\nfrom .yolov5_Detector import *\nfrom .frameborder_Detector import *\n# maybe these shits are gonna ruin my life...\ndef getMedialangInputFixed(medialangPathsInput):\n    for fbase0 in medialangPathsInput:\n        if type(fbase0) == str:\n            yield fbase0\n        elif (\n            type(fbase0) == list\n            and len(fbase0) == 1\n            and type(fbase0[0] == dict)\n            and \"cache\" in fbase0[0].keys()\n        ):\n            yield fbase0[0][\"cache\"]\n        else:\n            print(\"weird medialang detector input\")\n            print(fbase0)\n        # then it must be the medialang shit.\ndef processInputWrapperFunction(function, wrapperFunction):\n    def mFunction(data, *args, **kwargs):\n        return function(wrapperFunction(data), *args, **kwargs)\n    return mFunction\nmedialangDetectors = {\n    \"subtitle_detector\": mediaSubtitleDetector,",
        "type": "code",
        "location": "/pyjom/medialang/functions/detectors/__init__.py:1-36"
    },
    "1177": {
        "file_id": 103,
        "content": "This code imports various detector functions, defines a function for getting medialang input, and creates a dictionary of detectors. It seems to be part of a larger program used for processing media data. The comment on line 30 suggests that the code is concerned with potentially difficult or problematic inputs.",
        "type": "comment"
    },
    "1178": {
        "file_id": 103,
        "content": "    \"framediff_detector\": videoDiffDetector,\n    \"blackout_detector\": blackoutDetector,\n    \"yolov5_detector\": yolov5_Detector,\n    \"frameborder_detector\": frameborder_Detector,\n}\nmedialangDetectors = { # strange. i don't feel it.\n    key: processInputWrapperFunction(medialangDetectors[key], getMedialangInputFixed)\n    for key in medialangDetectors.keys()\n}",
        "type": "code",
        "location": "/pyjom/medialang/functions/detectors/__init__.py:37-46"
    },
    "1179": {
        "file_id": 103,
        "content": "This code initializes and processes media language detectors with input wrappers. It maps detector names to corresponding functions and applies a processing function to each detector for handling inputs.",
        "type": "comment"
    },
    "1180": {
        "file_id": 104,
        "content": "/pyjom/medialang/functions/detectors/frameborder_Detector.py",
        "type": "filepath"
    },
    "1181": {
        "file_id": 104,
        "content": "The code detects squares in images using blurring, edge detection, and HoughLines for lines. It processes video frames for object detection, OCR adjustments, and removes premature rectangles while applying theta filtering. It performs calculations, checks rectangles, and stores data using background models and contours to detect frame changes.",
        "type": "summary"
    },
    "1182": {
        "file_id": 104,
        "content": "from .mediaDetector import *\nimport numpy as np\nimport cv2\nimport pybgs as bgs\nimport talib\nimport uuid\nimport itertools\nimport copy\n# consider merging this project with autoup, or just borrow some of its content.\n# assume you not to run many instances at once?\n# how to identify same video in a sequence?\n# maybe you can paint translated words with paddleocr?\n# framedifference can only be applied to videos, not freaking images.\ndef huffline_stillImage_Identifier(mediapath,**config): # wtf?\n    img = cv2.imread(mediapath)\n    line_thresh =  config[\"line_thresh\"]\n    includeBoundaryLines = config[\"includeBoundaryLines\"] # applied to those cornered crops.\\\n    blurKernel = config[\"blurKernel\"]\n    blurred = cv2.GaussianBlur(img,blurKernel, 0)\n    edges = cv2.Canny(blurred,20,210,apertureSize = 3) # great.\n    lines = cv2.HoughLines(edges,1,np.pi/180,line_thresh)\n    angle_error = config[\"angle_error\"]   # this can only detect square things, absolute square.\n    # we need to know horizontal and vertical lines, when they cross we get points.",
        "type": "code",
        "location": "/pyjom/medialang/functions/detectors/frameborder_Detector.py:1-26"
    },
    "1183": {
        "file_id": 104,
        "content": "This code reads an image and applies Gaussian blur and Canny edge detection. It then uses the HoughLines algorithm to identify straight lines in the image. The identified horizontal and vertical lines can be used to detect square objects, but this implementation may have limitations as it assumes squares only.",
        "type": "comment"
    },
    "1184": {
        "file_id": 104,
        "content": "    frameHeight, frameWidth = img.shape[:2]\n    # print(\"height: \", frameHeight)\n    # print(\"width: \", frameWidth)\n    mlines = {\"horizontal\":[], \"vertical\":[]}\n    if includeBoundaryLines:\n        originPoint = (0,0)\n        cornerPoint = (frameWidth,frameHeight)\n        mlines[\"horizontal\"].append(originPoint)\n        mlines[\"horizontal\"].append(cornerPoint)\n        mlines[\"vertical\"].append(originPoint)\n        mlines[\"vertical\"].append(cornerPoint)\n    if lines is None: lines = []\n    for line in lines:\n        for r_theta in line:\n            # breakpoint()\n            r,theta = r_theta.tolist()\n            # Stores the value of cos(theta) in a\n            # filter detected lines?\n            # theta filter:\n            if not abs(theta % (np.pi/2) )< angle_error:\n                continue # this is filtering.\n            # print(\"line parameter:\",r,theta)\n            a = np.cos(theta)\n            # Stores the value of sin(theta) in b\n            b = np.sin(theta)\n            # x0 stores the value rcos(theta)",
        "type": "code",
        "location": "/pyjom/medialang/functions/detectors/frameborder_Detector.py:27-54"
    },
    "1185": {
        "file_id": 104,
        "content": "This code calculates the image height and width, creates horizontal and vertical line lists, appends origin and corner points to each list if including boundary lines is enabled. It also checks if lines are provided as input, filters out lines with angle error, and computes cos(theta) and sin(theta) for line parameter calculations.",
        "type": "comment"
    },
    "1186": {
        "file_id": 104,
        "content": "            x0 = a*r\n            # y0 stores the value rsin(theta)\n            y0 = b*r\n            # x1 stores the rounded off value of (rcos(theta)-1000sin(theta))\n            x1 = int(x0 + 1000*(-b))\n            # y1 stores the rounded off value of (rsin(theta)+1000cos(theta))\n            y1 = int(y0 + 1000*(a))\n            # x2 stores the rounded off value of (rcos(theta)+1000sin(theta))\n            x2 = int(x0 - 1000*(-b))\n            # y2 stores the rounded off value of (rsin(theta)-1000cos(theta))\n            y2 = int(y0 - 1000*(a))\n            # cv2.line draws a line in img from the point(x1,y1) to (x2,y2).\n            # (0,0,255) denotes the colour of the line to be\n            #drawn. In this case, it is red.\n            df_x = abs(x1-x2)\n            df_y = abs(y1-y2)\n            lineType = \"vertical\"\n            if df_x > df_y:\n                lineType = \"horizontal\"\n            # we just need one single point and lineType.\n            linePoint = (x1,y1)\n            mlines[lineType].append(linePoint)",
        "type": "code",
        "location": "/pyjom/medialang/functions/detectors/frameborder_Detector.py:55-83"
    },
    "1187": {
        "file_id": 104,
        "content": "This code calculates the coordinates of a line using trigonometry (a*r and b*r for x0 and y0) with an additional 1000 multipliers in the calculations. It then determines the rounded values for four points based on these coordinates, creating lines for vertical or horizontal segments as needed. The calculated line data is stored in a list called mlines.",
        "type": "comment"
    },
    "1188": {
        "file_id": 104,
        "content": "            # cv2.line(img,(x1,y1), (x2,y2), (0,0,255),2)\n            # would not draw lines this time. draw found rects instead.\n    # get rectangle points. or just all possible rectangles?\n    # enumerate all possible lines.\n    rects =[] # list of rectangles\n    if len(mlines[\"horizontal\"]) < 2 or len(mlines[\"vertical\"]) < 2:\n        # print(\"unable to form rectangles.\")\n        # return [] # no rect.\n        pass\n    else:\n        for line_h1, line_h2 in itertools.combinations(mlines[\"horizontal\"],2):\n            ymin, ymax = list(sorted([line_h1[1],line_h2[1]]))\n            for line_v1, line_v2 in itertools.combinations(mlines[\"vertical\"], 2):\n                xmin, xmax = list(sorted([line_v1[0],line_v2[0]]))\n                rect = ((xmin,ymin),(xmax,ymax))\n                rects.append(rect)\n        # print(\"RECT DICT MAIN LIST:\")\n        # print(rect_dict_main_list) # maybe i want this shit?\n    return rects\ndef huffline_horizontal_vertical_FrameIterator(mediapath,**config):\n    video_file = mediapath # this one with cropped boundaries.",
        "type": "code",
        "location": "/pyjom/medialang/functions/detectors/frameborder_Detector.py:84-105"
    },
    "1189": {
        "file_id": 104,
        "content": "This function generates rectangles based on horizontal and vertical lines detected in the image. It requires a minimum of two horizontal and two vertical lines for rectangle formation. The function combines these lines to find the bounding box coordinates, then stores them as rectangles in a list before returning the list of rectangles.",
        "type": "comment"
    },
    "1190": {
        "file_id": 104,
        "content": "    video = cv2.VideoCapture(video_file)\n    def rectMerge(oldRect, newRect,delta_thresh = config[\"delta_thresh\"]):\n        # if very much alike, we merge these rects.\n        # what about those rect that overlaps? we check exactly those who overlaps.\n        # 1. check all new rects against all old rects. if they overlap, highly alike (or not) then mark it as having_alike_rect (or not) and append to new old rect list. <- after those old rects have been marked with alike sign, one cannot revoke the sign. still remaining new rects will be checked against them.\n        # 2. while checking, if not very alike then append newRect to new rect list.\n        # 3. if one old rect has not yet been checked as having_alike_rect then cut its life. otherwise extend its life, though not extend above max_rect_life.\n        (old_x1,old_y1), (old_x2, old_y2) = oldRect\n        (new_x1,new_y1), (new_x2, new_y2) = newRect\n        # too many rects?\n        old_w = old_x2-old_x1\n        old_h = old_y2-old_y1\n        det_x1 = abs(new_x1 - old_x1)/ old_w",
        "type": "code",
        "location": "/pyjom/medialang/functions/detectors/frameborder_Detector.py:107-122"
    },
    "1191": {
        "file_id": 104,
        "content": "The code defines a function `rectMerge` that merges similar rectangles based on their overlap and likeness. It first checks all new rectangles against all old rectangles for overlap, marks them as having_alike_rect if highly alike or not, appends new rectangles to a new rectangle list, and extends the life of old rectangles without exceeding max_rect_life.",
        "type": "comment"
    },
    "1192": {
        "file_id": 104,
        "content": "        det_x2 = abs(new_x2 - old_x2)/ old_w\n        det_y1 = abs(new_y1 - old_y1)/ old_h\n        det_y2 = abs(new_y2 - old_y2)/ old_h\n        # print(\"deltas:\",det_x1, det_x2, det_y1, det_y2)\n        having_alike_rect =  (det_x1 < delta_thresh) and (det_y1 < delta_thresh) and (det_x2 < delta_thresh ) and (det_y2 < delta_thresh)\n        myRect = newRect\n        if having_alike_rect:\n            myRect = oldRect\n        return myRect, having_alike_rect\n    def rectSurge(oldRectList, newRectList,diff_img_output,delta_thresh = config[\"delta_thresh\"], min_rect_life = config[\"min_rect_life\"], max_rect_life = config[\"max_rect_life\"],max_rect_list_length = 30, rect_area_threshold = 0.05):\n        yd,xd = diff_img_output.shape\n        aread = yd*xd\n        min_area_thresh = rect_area_threshold * aread\n        def getRectArea(rect):\n            (x0,y0),(x1,y1) = rect\n            return (x1-x0)*(y1-y0)\n        def getDiff(rect,diff_img_output):\n            (x0,y0),(x1,y1) = rect\n            diff_area = diff_img_output[y0:y1,x0:x1]",
        "type": "code",
        "location": "/pyjom/medialang/functions/detectors/frameborder_Detector.py:123-146"
    },
    "1193": {
        "file_id": 104,
        "content": "Function detects changes in rectangles by comparing their dimensions and returns the updated rectangle and a boolean value if they are similar or different. It also calculates the area threshold based on image size, and provides functions for getting rectangle area and difference with diff_img_output.",
        "type": "comment"
    },
    "1194": {
        "file_id": 104,
        "content": "            return np.sum(diff_area)\n        def getScore(rect,totalArea,r0=2,r1=5,key=\"rect\"):\n            # if key:\n            #     rect = x[\"rect\"]\n            # else: rect = x\n            area = getRectArea(rect)\n            diff = getDiff(rect,diff_img_output)\n            val1 = diff/area\n            val2 = diff/totalArea\n            return r0*val1+r1*val2\n        newToOldDictList = []\n        oldRectDictList = [{\"rect\":x[\"rect\"], \"alike\":False, \"life\":x[\"life\"],\"uuid\":x[\"uuid\"]} for x in oldRectList if getRectArea(x[\"rect\"]) > min_area_thresh] # actually they are all dict lists. you can pass an empty list as oldRectList anyway.\n        newRectList = [x for x in newRectList if getRectArea(x) > min_area_thresh] # get something else.\n        oldRectDictList = list(reversed(sorted(oldRectDictList,key=lambda x: getScore(x[\"rect\"],aread))))[:max_rect_list_length]\n        newRectList = list(reversed(sorted(newRectList,key=lambda x: getScore(x,aread))))[:max_rect_list_length]\n        # oldRectDictLis",
        "type": "code",
        "location": "/pyjom/medialang/functions/detectors/frameborder_Detector.py:147-164"
    },
    "1195": {
        "file_id": 104,
        "content": "The code defines a function to detect and process rectangles based on their area and difference from an image. It creates two lists of rectangle dictionaries, one for new and old rectangles, and sorts them based on a score calculated using the getScore() function. The sorted lists are then truncated to the max_rect_list_length value.",
        "type": "comment"
    },
    "1196": {
        "file_id": 104,
        "content": "t = sorted(oldRectDictList,key=lambda x:getRectArea(x[\"rect\"]), reverse=True) # not good since we got other freaking shits.\n        # print(\"OLDRECTDICTLIST:\",oldRectDictList)\n        # print(\"NEW RECT LENGTH:\",len(newRectList))\n        for newRect in newRectList:\n            needAppend = True\n            for index, oldRectDict in enumerate(oldRectDictList):\n                # print(\"ENUMERATING OLD INDEX:\",index)\n                oldRect = oldRectDict[\"rect\"]\n                _, having_alike_rect = rectMerge(oldRect,newRect,delta_thresh=delta_thresh)\n                if having_alike_rect:\n                    needAppend = False\n                    if not oldRectDict[\"alike\"]:\n                        # print(\"SET ALIKE:\",index,oldRect)\n                        oldRectDictList[index][\"alike\"] = True\n                    # ignore myRect.\n            if needAppend:\n                newToOldDictList.append({\"rect\":newRect,\"life\":1,\"uuid\":str(uuid.uuid4())}) # make sure it is not duplicated?\n                # if appended we shall break this loop. but when shall we append?",
        "type": "code",
        "location": "/pyjom/medialang/functions/detectors/frameborder_Detector.py:164-182"
    },
    "1197": {
        "file_id": 104,
        "content": "Code snippet is sorting a list of oldRectDictList based on the area of rectangles in reverse order. It then compares each new rectangle with old rectangles and updates their alike status accordingly. If no match is found, it adds a new entry to newToOldDictList.",
        "type": "comment"
    },
    "1198": {
        "file_id": 104,
        "content": "        oldToOldDictList = []\n        # print(\"OLD RECT LENGTH:\",len(oldRectDictList))\n        for oldRectDict in oldRectDictList:\n            alike = oldRectDict[\"alike\"]\n            life = oldRectDict[\"life\"]\n            oldRect = oldRectDict[\"rect\"]\n            myUUID = oldRectDict[\"uuid\"]\n            if not alike:\n                life -=1\n            else:\n                life +=1\n                life = min(max_rect_life, life)\n            if life <= min_rect_life:\n                continue\n            oldToOldDictList.append({\"rect\":oldRect,\"life\":life,\"uuid\":myUUID})\n        return oldToOldDictList + newToOldDictList # a combination.\n    def updateTotalRects(oldTotalRectDict,rectList,currentFrameIndex,diffFrame,minRectArea = 1):\n        for elem in rectList:\n            uuid = elem[\"uuid\"]\n            rect = elem[\"rect\"]\n            (x0,y0),(x1,y1) = rect\n            rectArea = (x1-x0)*(y1-y0)\n            if rectArea <minRectArea:\n                continue\n            if uuid not in oldTotalRectDict.keys():",
        "type": "code",
        "location": "/pyjom/medialang/functions/detectors/frameborder_Detector.py:183-209"
    },
    "1199": {
        "file_id": 104,
        "content": "The code is iterating over old and new rectangles, updating their life count and filtering out rectangles with areas below a certain threshold. It then combines the old and new rectangle lists to form a single list.",
        "type": "comment"
    }
}