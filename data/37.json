{
    "3700": {
        "file_id": 454,
        "content": "            # print(destCoord)\n            # breakpoint()\n            if motion_vector == (0, 0):\n                # print(\"zero motion vector detected. skipping\")\n                # breakpoint()\n                continue\n            # print('destination coords:',destCoord)\n            # print('motion vector:',motion_vector)\n            motion_vectors_dict.update(\n                {destCoord: motion_vectors_dict.get(destCoord, []) + [motion_vector]}\n            )\n            # you know, different frame sources may lead to different results.\n            # these vectors could overlap. which one you want to keep? the smaller ones or the bigger ones?\n            # if destCoord in destCoords:\n            #     print(\"SKIPPING DUPLICATE DESTCOORD:\", destCoord)\n            #     print(\"PREVIOUS MV\",prevMV)\n            #     print(\"CURRENT MV\", mv)\n            #     continue\n            # else:\n            #     destCoords.add(destCoord)\n            # prevMV = mv\n            # try:\n            #     # src_x, src_y may not apply the same rule.",
        "type": "code",
        "location": "/tests/motion_vector_estimation/test.py:209-232"
    },
    "3701": {
        "file_id": 454,
        "content": "This code checks if a motion vector is zero and skips processing if it is. It then updates a dictionary of motion vectors by adding the new motion vector to the destination coordinate, while considering potential overlaps with other motion vectors.",
        "type": "comment"
    },
    "3702": {
        "file_id": 454,
        "content": "            #     # assert src_x % 16 == 8\n            #     # assert src_y % 16 == 8\n            #     assert checkMacroBlock(dst_x) is not None\n            #     assert checkMacroBlock(dst_y) is not None\n            #     # assert dst_x<=res_x # dst_x can go beyond the res_x\n            #     # assert dst_y<=res_y\n            #     # so all rules applied.\n            # except:\n            #     # print('source',src_x, src_y)\n            #     print(\"res\", res_x, res_y)\n            #     print('destionation',dst_x, dst_y)\n            #     print('motion',motion_x, motion_y)\n            #     print(\"scale\",motion_scale)\n        motion_vectors_dict_averaged = {\n            key: averageMotionVectors(motion_vectors_dict[key])\n            for key in motion_vectors_dict.keys()\n        }\n        # assuming no duplicates?\n        weighted_motion_vectors = []\n        weights = []\n        rectangles = []\n        motion_vectors_filtered = []  # for getting data later?\n        for (\n            blockCenterCoordinates,\n            average_motion_vector,",
        "type": "code",
        "location": "/tests/motion_vector_estimation/test.py:233-257"
    },
    "3703": {
        "file_id": 454,
        "content": "Testing macro block placement, asserting non-null checkMacroBlock results for dst_x and dst_y, asserting within res limits (dst_x <= res_x), and handling exceptions with error printing. Averages motion vectors using averageMotionVectors function. Creates weightedMotionVectors list and weights list. Initializing rectangles and motionVectorsFiltered for later use.",
        "type": "comment"
    },
    "3704": {
        "file_id": 454,
        "content": "        ) in motion_vectors_dict_averaged.items():\n            if average_motion_vector == (0, 0):\n                continue\n                # wtf is this? why fucking zero?\n                # print('skipping zero average motion vector')\n                # print(\"destination coords\", key)\n                # print('average motion vector', average_motion_vector)\n            else:\n                m_x, m_y = average_motion_vector\n                motion_vectors_filtered.append(average_motion_vector)\n                rectangle_XYWH = getRectangleXYWHFromBlockCenterCoordinates(\n                    blockCenterCoordinates\n                )\n                rectangles.append(rectangle_XYWH)\n                blockWeight = getBlockWeightFromBlockCenterCoordinates(\n                    blockCenterCoordinates\n                )\n                weights.append(blockWeight)\n                weighted_motion_vectors.append(\n                    (\n                        m_x * blockWeight / frame_common_divisor,\n                        m_y * blockWeight / frame_common_divisor,",
        "type": "code",
        "location": "/tests/motion_vector_estimation/test.py:258-279"
    },
    "3705": {
        "file_id": 454,
        "content": "This code block is filtering and processing motion vectors from a dictionary. It skips any average motion vector that is (0, 0) and then proceeds to calculate the weighted motion vectors by multiplying the motion vector with block weight and dividing it by a frame common divisor. The resulting coordinates are stored in 'weighted_motion_vectors'.",
        "type": "comment"
    },
    "3706": {
        "file_id": 454,
        "content": "                    )\n                )\n        weighted_motion_vectors = np.array(weighted_motion_vectors)\n        sum_weighted_motion_vector = np.sum(weighted_motion_vectors, axis=0)\n        average_global_weighted_motion_vector = (\n            sum_weighted_motion_vector / total_block_weights\n        )\n        sum_weights = sum(weights)\n        average_weighted_motion_vector = sum_weighted_motion_vector / sum_weights\n        motion_area_ratio = sum_weights / total_block_weights\n        # print(motion_vectors.shape)\n        motion_vectors_filtered_cartesian_distance = [\n            cartesianDistance(vector) for vector in motion_vectors_filtered\n        ] + [\n            0\n        ]  # to avoid errors.\n        motion_vectors_filtered_cartesian_distance = np.array(\n            motion_vectors_filtered_cartesian_distance\n        )\n        cartesianWeights = weights + [0]\n        cartesianWeights = np.array(cartesianWeights)\n        cartesianWeightsSum = np.sum(cartesianWeights)\n        weighted_motion_vectors_filtered_cartesian_distance = (",
        "type": "code",
        "location": "/tests/motion_vector_estimation/test.py:280-304"
    },
    "3707": {
        "file_id": 454,
        "content": "This code calculates the weighted average motion vector and the average weighted motion vector for motion vectors in a block. It also determines the motion area ratio, applies cartesian distance to filtered motion vectors, and stores them with corresponding weights.",
        "type": "comment"
    },
    "3708": {
        "file_id": 454,
        "content": "            motion_vectors_filtered_cartesian_distance * cartesianWeights\n        )\n        sum_weighted_motion_vectors_filtered_cartesian_distance = np.sum(\n            weighted_motion_vectors_filtered_cartesian_distance\n        )\n        # print(\"SUM\", sum_weighted_motion_vectors_filtered_cartesian_distance)\n        # breakpoint()\n        average_weighted_motion_vectors_filtered_cartesian_distance = (\n            sum_weighted_motion_vectors_filtered_cartesian_distance / cartesianWeightsSum\n        )\n        average_global_weighted_motion_vectors_filtered_cartesian_distance = (\n            sum_weighted_motion_vectors_filtered_cartesian_distance\n            / total_block_weights # this is a number, not array!\n        )\n        min_cartesian = min(motion_vectors_filtered_cartesian_distance)\n        max_cartesian = max(motion_vectors_filtered_cartesian_distance)\n        motion_area_ratio_array.append(motion_area_ratio)\n        # print()\n        # print(average_weighted_motion_vector)\n        # print(average_global_weighted_motion_vector)",
        "type": "code",
        "location": "/tests/motion_vector_estimation/test.py:305-329"
    },
    "3709": {
        "file_id": 454,
        "content": "This code calculates the average and global-weighted motion vectors for a set of motion vectors, considering their weights and distances. It also finds the minimum and maximum cartesian distance in the list and appends the motion area ratio to an array.",
        "type": "comment"
    },
    "3710": {
        "file_id": 454,
        "content": "        # breakpoint()\n        average_weighted_motion_vector_cartesian=cartesianDistance(average_weighted_motion_vector)\n        average_weighted_motion_vector_cartesian_array.append(average_weighted_motion_vector_cartesian)\n        average_global_weighted_motion_vector_cartesian = cartesianDistance(average_global_weighted_motion_vector)\n        average_global_weighted_motion_vector_cartesian_array.append(\n        average_global_weighted_motion_vector_cartesian\n        )\n        average_weighted_motion_vectors_filtered_cartesian_distance_array.append(\n            average_weighted_motion_vectors_filtered_cartesian_distance\n        )\n        average_global_weighted_motion_vectors_filtered_cartesian_distance_array.append(\n            average_global_weighted_motion_vectors_filtered_cartesian_distance\n        )\n        if motion_vectors_dict_averaged != {}:\n            # breakpoint()\n            if visualize:\n                print(\"motion_area_ratio\", motion_area_ratio)\n                print(\"average_weighted_motion_vector_cartesian\", average_weighted_motion_vector_cartesian)",
        "type": "code",
        "location": "/tests/motion_vector_estimation/test.py:330-348"
    },
    "3711": {
        "file_id": 454,
        "content": "Calculates the average weighted motion vector cartesian distance, appends it to the array and does the same for global vectors. If motion_vectors_dict_averaged is not empty, prints motion_area_ratio and average_weighted_motion_vector_cartesian if visualize is True.",
        "type": "comment"
    },
    "3712": {
        "file_id": 454,
        "content": "                print(\n                    \"average_global_weighted_motion_vecto_cartesianr\",\n                    average_global_weighted_motion_vector_cartesian,\n                )\n                print(\n                    \"average_weighted_motion_vectors_filtered_cartesian_distance\",\n                    average_weighted_motion_vectors_filtered_cartesian_distance,\n                )\n                print(\n                    \"average_global_weighted_motion_vectors_filtered_cartesian_distance\",\n                    average_global_weighted_motion_vectors_filtered_cartesian_distance,\n                )\n                motion_mask = np.zeros(\n                    (motion_render_frame[1], motion_render_frame[0], 1)\n                )\n                for index, (x, y, w, h) in enumerate(rectangles):\n                    pt1, pt2 = XYWHToDiagonal(x, y, w, h)\n                    # print(pt1, pt2)\n                    current_cartesian = motion_vectors_filtered_cartesian_distance[\n                        index\n                    ]",
        "type": "code",
        "location": "/tests/motion_vector_estimation/test.py:349-369"
    },
    "3713": {
        "file_id": 454,
        "content": "Calculates and prints average motion vector metrics. Creates a zeroed motion mask. Iterates through rectangles to calculate the current cartesian distance.",
        "type": "comment"
    },
    "3714": {
        "file_id": 454,
        "content": "                    # print(type(pt1), type(pt1[0]))\n                    relative_motion_cartesian = (current_cartesian - min_cartesian) / (\n                        max_cartesian - min_cartesian\n                    )  # must from 0 to 1 so we can plot this,\n                    # relative_motion_cartesian = 255*((current_cartesian-min_cartesian)/(max_cartesian-min_cartesian))\n                    # relative_motion_cartesian = int(relative_motion_cartesian)\n                    # relative_motion_cartesian = min(255,max(0, relative_motion_cartesian))\n                    # breakpoint()\n                    cv2.rectangle(\n                        motion_mask,\n                        pt1,\n                        pt2,\n                        color=(relative_motion_cartesian,),\n                        thickness=-1,\n                    )\n                # should we gaussian blur, threshold this, do convolution and then apply bounding box on it?\n                # # visualize this.\n                if show_picture:\n                    cv2.imshow(\"motion_mask\", motion_mask)",
        "type": "code",
        "location": "/tests/motion_vector_estimation/test.py:370-388"
    },
    "3715": {
        "file_id": 454,
        "content": "This code calculates the relative motion vectors for a set of points and plots them on an image using OpenCV. It converts the motion vectors to a range of 0-255, representing the pixel intensity values used in the image plotting. The resulting image is then displayed if the \"show_picture\" flag is set.",
        "type": "comment"
    },
    "3716": {
        "file_id": 454,
        "content": "                    cv2.waitKey(100)\n            # may you create bounding box for this? for tracking motion? or not?\n        # breakpoint()\n    else:\n        break\n# print('max_dst_x', max_dst_x)\n# print('max_dst_y', max_dst_y)\nimport matplotlib.pyplot as plt\n# plt.style.use('dark_background')\na, b = 5,1\nfigure, axis = plt.subplots(a, b)\ndata = [\n    motion_area_ratio_array,\n    # average_weighted_motion_vector_array,\n    # average_global_weighted_motion_vector_array,\n    average_weighted_motion_vector_cartesian_array,\n    average_global_weighted_motion_vector_cartesian_array,\n    average_weighted_motion_vectors_filtered_cartesian_distance_array,\n    average_global_weighted_motion_vectors_filtered_cartesian_distance_array,\n]\ntitles = [\n    \"motion_area_ratio\",\n    # \"average_weighted_motion_vector\",\n    # \"average_global_weighted_motion_vector\",\n    \"average_weighted_motion_vector_cartesian\",\n    \"average_global_weighted_motion_vector_cartesian\",\n    \"average_weighted_motion_vectors_filtered_cartesian_distance\",",
        "type": "code",
        "location": "/tests/motion_vector_estimation/test.py:389-419"
    },
    "3717": {
        "file_id": 454,
        "content": "The code imports matplotlib and creates a figure with subplots. It stores various motion vector related arrays in the \"data\" list, presumably for plotting. These arrays are likely different representations of motion vectors at each point. The code then defines titles corresponding to each array's content.",
        "type": "comment"
    },
    "3718": {
        "file_id": 454,
        "content": "    \"average_global_weighted_motion_vectors_filtered_cartesian_distance\",\n]\n# breakpoint()\nassert len(titles) == len(data)\nassert a*b >= len(titles)\nfor _a in range(a):\n    for _b in range(b):\n        index = _a * b + _b\n        if index > len(data) - 1:\n            break\n        if a == 1:\n            if b == 1:\n                axis[0].plot(data[index])\n                axis[0].set_title(titles[index])\n            else:\n                axis[_b].plot(data[index])\n                axis[_b].set_title(titles[index])\n        elif b == 1:\n            axis[_a].plot(data[index])\n            axis[_a].set_title(titles[index])\n        else:\n            axis[_a, _b].plot(data[index])\n            axis[_a, _b].set_title(titles[index])\nplt.show()",
        "type": "code",
        "location": "/tests/motion_vector_estimation/test.py:420-444"
    },
    "3719": {
        "file_id": 454,
        "content": "This code plots the data using matplotlib and sets titles for each plot based on the corresponding title from the provided list. It checks for potential errors like unequal lengths of 'titles' and 'data', and also handles cases when 'a' or 'b' is 1, adjusting the number of axes accordingly.",
        "type": "comment"
    },
    "3720": {
        "file_id": 455,
        "content": "/tests/motion_vector_estimation/run.sh",
        "type": "filepath"
    },
    "3721": {
        "file_id": 455,
        "content": "This code runs a Docker container using lubo1994/mv-extractor image, mounting the current directory to /home/video_cap within the container and allowing X11 forwarding for graphical user interface support.",
        "type": "summary"
    },
    "3722": {
        "file_id": 455,
        "content": "#!/bin/bash\nxhost +\ndocker run \\\n    -it \\\n    --ipc=host \\\n    --env=\"DISPLAY\" \\\n    -v $(pwd):/home/video_cap \\\n    -v /tmp/.X11-unix:/tmp/.X11-unix:rw \\\n    lubo1994/mv-extractor:latest \\\n    \"$@\"",
        "type": "code",
        "location": "/tests/motion_vector_estimation/run.sh:1-12"
    },
    "3723": {
        "file_id": 455,
        "content": "This code runs a Docker container using lubo1994/mv-extractor image, mounting the current directory to /home/video_cap within the container and allowing X11 forwarding for graphical user interface support.",
        "type": "comment"
    },
    "3724": {
        "file_id": 456,
        "content": "/tests/motion_vector_estimation/mpegflow/test.sh",
        "type": "filepath"
    },
    "3725": {
        "file_id": 456,
        "content": "The code sets the VIDEO variable to a specific video file path, runs the mpegflow command on that file and redirects output to output.txt, but the .so file is fake so it doesn't work well. The code also provides an alternative method using Docker or downloading a real .so file from the web, creating an examples/vis_dump directory, and piping the mpegflow command's output into ./vis with the video file path.",
        "type": "summary"
    },
    "3726": {
        "file_id": 456,
        "content": "VIDEO=\"/root/Desktop/works/pyjom/samples/video/cute_cat_gif.mp4\"\n# ./mpegflow $VIDEO > output.txt\n# it does not help because the .so file is fake. you need a real one.\n# you may download it from web, or just use docker\n# mkdir -p examples/vis_dump && ./mpegflow $VIDEO | ./vis $VIDEO examples/vis_dump\n# maybe this shit is not good at all...",
        "type": "code",
        "location": "/tests/motion_vector_estimation/mpegflow/test.sh:1-10"
    },
    "3727": {
        "file_id": 456,
        "content": "The code sets the VIDEO variable to a specific video file path, runs the mpegflow command on that file and redirects output to output.txt, but the .so file is fake so it doesn't work well. The code also provides an alternative method using Docker or downloading a real .so file from the web, creating an examples/vis_dump directory, and piping the mpegflow command's output into ./vis with the video file path.",
        "type": "comment"
    },
    "3728": {
        "file_id": 457,
        "content": "/tests/motion_vector_estimation/mpegflow/init.sh",
        "type": "filepath"
    },
    "3729": {
        "file_id": 457,
        "content": "Downloading 'mpegflow' and 'vis' from the 1.0 release of vadimkantorov/mpegflow repository.",
        "type": "summary"
    },
    "3730": {
        "file_id": 457,
        "content": "curl -L -O https://github.com/vadimkantorov/mpegflow/releases/download/1.0/mpegflow\ncurl -L -O https://github.com/vadimkantorov/mpegflow/releases/download/1.0/vis",
        "type": "code",
        "location": "/tests/motion_vector_estimation/mpegflow/init.sh:1-2"
    },
    "3731": {
        "file_id": 457,
        "content": "Downloading 'mpegflow' and 'vis' from the 1.0 release of vadimkantorov/mpegflow repository.",
        "type": "comment"
    },
    "3732": {
        "file_id": 458,
        "content": "/tests/search_engine_suggestion_based_qa_bot/test_fix.py",
        "type": "filepath"
    },
    "3733": {
        "file_id": 458,
        "content": "This code snippet is from a Python script that uses the BaiduSpider module to search the web for information based on a query. It then prints the results in plain text format. The code also mentions an update needed in the baiduspider package and refers to specific pull requests on GitHub for further details.",
        "type": "summary"
    },
    "3734": {
        "file_id": 458,
        "content": "query = \"python有个问题想请教一下 为什么我这个函数跑不通\"\nfrom baiduspider import BaiduSpider\nspider=BaiduSpider()\nfrom pprint import pprint\nresult = spider.search_web(query, pn= 1)\nprint(result.plain)\n# change the div class name.\n# change 'result-op' into 'result' at line 153\n# file: /usr/local/lib/python3.9/dist-packages/baiduspider/parser/__init__.py:153\n# https://github.com/BaiduSpider/BaiduSpider/pull/151\n# https://github.com/BaiduSpider/BaiduSpider/pull/151/files\n# breakpoint()\n# result.normal[0].url\n# also update the news extraction logic:\n# https://github.com/BaiduSpider/BaiduSpider/pull/127/files\n# 'des', 'origin', 'plain', 'snapshot', 'time', 'title', 'url'",
        "type": "code",
        "location": "/tests/search_engine_suggestion_based_qa_bot/test_fix.py:1-18"
    },
    "3735": {
        "file_id": 458,
        "content": "This code snippet is from a Python script that uses the BaiduSpider module to search the web for information based on a query. It then prints the results in plain text format. The code also mentions an update needed in the baiduspider package and refers to specific pull requests on GitHub for further details.",
        "type": "comment"
    },
    "3736": {
        "file_id": 459,
        "content": "/tests/search_engine_suggestion_based_qa_bot/test.py",
        "type": "filepath"
    },
    "3737": {
        "file_id": 459,
        "content": "The code utilizes BaiduSpider module to find related topics, generating suggestions and messages. It handles no-results scenarios and imports necessary modules, but may have issues with search results and requires implementation of ToAPI and Jina for further processing.",
        "type": "summary"
    },
    "3738": {
        "file_id": 459,
        "content": "# we need suggestion, related topics, also search results.\n# can be used in title generation.\n# title/message as query (-> keyword -> suggested query) -> search results -> extract response/title\n# suggestion, trending topics/keywords\n# black hat seo, https://www.blackhatworld.com/forums/black-hat-seo.28/\n# paste your link 'elsewhere' 自动评论 自动发布信息 私信, submit your link to search engine somehow, visit your link from search engine somehow\n# seo without website\n# write a blog on github?\n# create short links and submit them to search engine\n# get query count, perform n-gram analysis\n# https://www.aeripret.com/ngrams-analysis-seo/\n# https://www.pemavor.com/seo-keyword-clustering-with-python/\n# i have bookmarked links for further use on macbook chrome.\nquery = \"python有个问题想请教一下 为什么我这个函数跑不通\"\nfrom baiduspider import BaiduSpider\nspider=BaiduSpider()\nfrom pprint import pprint\nresult = spider.search_web(query, pn= 1)\n# print(result)\n# nothing returned.\nimport random\n# result.related \nrelated = result.related\nnext_query = random.choice(related)",
        "type": "code",
        "location": "/tests/search_engine_suggestion_based_qa_bot/test.py:1-37"
    },
    "3739": {
        "file_id": 459,
        "content": "This code is using the BaiduSpider module to search for related topics based on a query. The search results are then used to generate suggestions and messages. The code handles cases where no relevant results are found, chooses a random related topic if necessary, and imports required modules.",
        "type": "comment"
    },
    "3740": {
        "file_id": 459,
        "content": "# next_query = 'python'\nprint('next query: %s' % next_query)\nfrom baidusearch.baidusearch import search\n# the abstract is bad\n# use toapi to make website into api.\n# https://github.com/gaojiuli/toapi\nresults = search(next_query, num_results=20)  # returns 20 or less results\n# # next_result = spider.search_web(next_query, pn= 1)\n# # print(next_result)\n# # print(results) #this is working.\n# # breakpoint()\n# import parse\n# use jina? hahaha...\nimport json\nstring = json.dumps(results, ensure_ascii=False, indent=4)\nwith open('result_baidu.json', 'w+') as f:\n    f.write(string)\n# no search result! fuck.\n# what is going on?\n# 'baike', 'blog', 'calc', 'gitee', 'music', 'news', 'normal', 'pages', 'plain', 'related', 'tieba', 'total', 'video'",
        "type": "code",
        "location": "/tests/search_engine_suggestion_based_qa_bot/test.py:38-61"
    },
    "3741": {
        "file_id": 459,
        "content": "This code is using BaiduSearch to search for the next query and saving the results as JSON in a file. It seems there might be some issues with the search results, and it also mentions using ToAPI and Jina for further processing but doesn't appear to have implemented them yet.",
        "type": "comment"
    },
    "3742": {
        "file_id": 460,
        "content": "/tests/search_engine_suggestion_based_qa_bot/search_image_with_keywords.py",
        "type": "filepath"
    },
    "3743": {
        "file_id": 460,
        "content": "This code imports a BaiduSpider class and uses it to search for an image related to the keyword \"绝对领域\". It then prints the result and checks if the 'title', 'url', and 'host' information is available.",
        "type": "summary"
    },
    "3744": {
        "file_id": 460,
        "content": "# not sure if it relates.\nfrom baiduspider import BaiduSpider\nspider=BaiduSpider()\nfrom pprint import pprint\nquery = \"绝对领域\"\nresult = spider.search_pic(query, pn= 1) # are we fucked?\n# yeah we have result.\nprint(result)\nresult.plain\nbreakpoint()\n# 'title', 'url', 'host'\n# can we search for gif?",
        "type": "code",
        "location": "/tests/search_engine_suggestion_based_qa_bot/search_image_with_keywords.py:1-13"
    },
    "3745": {
        "file_id": 460,
        "content": "This code imports a BaiduSpider class and uses it to search for an image related to the keyword \"绝对领域\". It then prints the result and checks if the 'title', 'url', and 'host' information is available.",
        "type": "comment"
    },
    "3746": {
        "file_id": 461,
        "content": "/tests/search_engine_suggestion_based_qa_bot/search_for_picture_embedding.py",
        "type": "filepath"
    },
    "3747": {
        "file_id": 461,
        "content": "This code utilizes BaiDu image search API to find similar images and prints details, implements time delays for safety. It currently uses textrank model for improvements. The code is facing issues with `getBaiduImageSearchAjaxInfoParsed` function from `parse_baidu_search_ajax` module. It handles exceptions, provides URL structure info, and offers debugging support.",
        "type": "summary"
    },
    "3748": {
        "file_id": 461,
        "content": "# actually the clip model does well for this.\n# though you want to use bm25 based textrank\nimage = \"prettyGirl.jpeg\" # girl image\nfrom PicImageSearch.sync import BaiDu\nbaidu = BaiDu()\nresult = baidu.search(file=image)\n# print(result)\n# better not to query 'ajax' unless you want to get banned.\n# breakpoint()\n# you want to use phash, width, height for this.\nimport requests\nSLEEP= 1\nfor elem in result.raw:\n    elem = elem.__dict__\n    # print(elem)\n    # breakpoint()\n    thumbnail = elem.get('thumbnail')\n    simi = elem.get('similarity')\n    title = elem.get('title')\n    # url is not necessary since we almost can't get the picture.\n    ajaxUrl = elem['origin'].get('ajaxUrl')\n    import time\n    print(thumbnail, simi, title)\n    # print(thumbnail, simi, title, ajaxUrl)\n    time.sleep(SLEEP) # wait too long?\n    r = requests.get(ajaxUrl)\n    myJson = r.json()\n    # from lazero.filesystem.io import writeJsonObjectToFile\n    # writeJsonObjectToFile('jq_image_2.json',myJson)\n    # breakpoint()\n    # maybe no need to parse this thing.",
        "type": "code",
        "location": "/tests/search_engine_suggestion_based_qa_bot/search_for_picture_embedding.py:1-34"
    },
    "3749": {
        "file_id": 461,
        "content": "This code uses the BaiDu image search API to find similar images and their details. It prints thumbnail, similarity, title, and AJAX URL for each result. The code also includes time delays to avoid being banned. The clip model is mentioned for potential use in future improvements, but currently, bm25 based textrank is recommended. The code avoids querying 'ajax' to prevent potential bans.",
        "type": "comment"
    },
    "3750": {
        "file_id": 461,
        "content": "    # try: # TODO: skipping this parsing since multiple errors.\n    #     from parse_baidu_search_ajax import getBaiduImageSearchAjaxInfoParsed\n    #     title_some, url_meta_some= getBaiduImageSearchAjaxInfoParsed(myJson, debug=True)\n    #     # changed again?\n    # except:\n    #     import traceback\n    #     traceback.print_exc()\n    #     print(ajaxUrl)\n    #     print('error!')\n    #     breakpoint()\n    # breakpoint()\n# ['origin', 'raw', 'url']\n# result.raw[0].url is the original url. however you won't get the picture.\n# result.raw[0].thumbnail\n# 'origin', 'similarity', 'thumbnail', 'title', 'url'\n# result.raw[0].origin['ajaxUrl'] -> get more similar images of this one",
        "type": "code",
        "location": "/tests/search_engine_suggestion_based_qa_bot/search_for_picture_embedding.py:36-52"
    },
    "3751": {
        "file_id": 461,
        "content": "This code is trying to import the function `getBaiduImageSearchAjaxInfoParsed` from the module `parse_baidu_search_ajax`, but due to some errors, it's skipping this parsing process. It then handles any exceptions that may occur and prints the error message along with the URL. If an exception happens, it also calls a breakpoint to pause the code execution for debugging purposes. The code also provides information about the URL structure and how to access different parts of the URL, such as the original URL, thumbnail, and ajaxUrl to get more similar images.",
        "type": "comment"
    },
    "3752": {
        "file_id": 462,
        "content": "/tests/search_engine_suggestion_based_qa_bot/parse_baidu_title_abstract.py",
        "type": "filepath"
    },
    "3753": {
        "file_id": 462,
        "content": "This code reads a JSON file, cleans text, processes abstracts to generate phrases meeting minimum and maximum length requirements. It parses Baidu search result titles and abstracts for potential question-answering content using \"result_baidu.json\". Text preprocessing is performed, and the top 20 ranked candidate phrases are printed based on BM25 similarity and Chinese character portion in the query.",
        "type": "summary"
    },
    "3754": {
        "file_id": 462,
        "content": "from lazero.filesystem.io import readJsonObjectFromFile\nfrom lazero.utils.mathlib import checkMinMaxDict\ndata = readJsonObjectFromFile(\"result_baidu.json\")\nimport string\nfrom zhon import hanzi\npunctuations = set(list(string.punctuation + hanzi.punctuation))\npermitted = [\" \"]\nfor perm in permitted:\n    if perm in punctuations:\n        punctuations.remove(perm)\ndef removeTimeInfo(phrase):\n    import re\n    timeinfos = re.findall(r\"\\d+年\\d+月\\d+日\", phrase)\n    for timeinfo in timeinfos:\n        phrase = phrase.replace(timeinfo, \"\")\n    return phrase\ndef processQueryResult(abstract, minMaxDict={\"min\": 8, \"max\": 24}):\n    for punc in punctuations:\n        abstract = abstract.replace(punc, \"\\n\")\n    abstract = abstract.split(\"\\n\")\n    for phrase in abstract:\n        phrase = removeTimeInfo(phrase)\n        phrase = phrase.strip()\n        if not checkMinMaxDict(len(phrase), minMaxDict):\n            continue\n        else:\n            yield phrase\ncandidates = []\nquery = \"python有个问题想请教一下 为什么我这个函数跑不通\"\n# use another model please?",
        "type": "code",
        "location": "/tests/search_engine_suggestion_based_qa_bot/parse_baidu_title_abstract.py:1-41"
    },
    "3755": {
        "file_id": 462,
        "content": "This code reads a JSON file, removes time and punctuation information from text, and processes the abstract to yield phrases meeting minimum and maximum length requirements. The purpose is to parse Baidu search result titles and abstracts for potential question-answering content, using the \"result_baidu.json\" file as input. The code also includes a function to remove time information from text and ensures each phrase meets specific length criteria before yielding it. The query variable contains a sample input for testing or using with another model.",
        "type": "comment"
    },
    "3756": {
        "file_id": 462,
        "content": "# haystack?\nfor elem in data:\n    title = elem.get(\"title\")\n    print(\"title: %s\" % title)\n    spliters = [\" - \", \"-\", \"_\", \"－\"]\n    for spliter in spliters:\n        title = title.replace(spliter, \"_\")\n    potentialWebsiteNames = title.split(\"_\")\n    title = potentialWebsiteNames[0].strip()\n    realWebsiteNames = []\n    if len(potentialWebsiteNames) > 1:\n        websiteNames = potentialWebsiteNames[1:]\n        for name in websiteNames:\n            name = name.strip()\n            if len(name) > 0:\n                realWebsiteNames.append(name)\n    abstract = elem.get(\"abstract\")\n    # print(abstract)\n    # breakpoint()\n    for name in realWebsiteNames:\n        abstract = abstract.replace(name, \"\")  # remove website names\n    for phrase in processQueryResult(abstract):\n        if phrase not in candidates and not phrase.endswith(\"\"):  # magic char.\n            candidates.append(phrase)  # what is your query?\nimport jieba\ndef getCuttedWords(phrase):\n    candidates = jieba.lcut(phrase.lower())\n    wordList = []\n    for word in candidates:",
        "type": "code",
        "location": "/tests/search_engine_suggestion_based_qa_bot/parse_baidu_title_abstract.py:42-73"
    },
    "3757": {
        "file_id": 462,
        "content": "This code is iterating over a list of data items, extracting titles and abstracts. It cleans the titles by removing splitting characters like \"-\", \"_\", and \"－\" and then splits them into potential website names. It checks if there are additional website names in the title and removes them from the abstract. Then it cuts the abstract using Jieba's lcut function to generate candidates for further processing.",
        "type": "comment"
    },
    "3758": {
        "file_id": 462,
        "content": "        word = word.strip()\n        if len(word) > 0:\n            wordList.append(word)\n    return wordList\ndef countCommonWords(phrase_1, phrase_2, wordCount=False):\n    words_1 = getCuttedWords(phrase_1)\n    words_2 = getCuttedWords(phrase_2)\n    # count for longest total length?\n    result = list(set(words_1) & set(words_2))\n    if wordCount:\n        return len(result)\n    else:\n        return len(\"\".join(result))\n# candidates = list(set(candidates))\n# https://pypi.org/project/rank-bm25/\n# candidates.sort(key=lambda phrase: -countCommonWords(phrase,query))\n# use bm25?\n# this sorting is wrong.\nfrom rank_bm25 import BM25Okapi\ntokenized_corpus = [getCuttedWords(phrase) for phrase in candidates]\ntokenized_query = getCuttedWords(query)\nbm25 = BM25Okapi(tokenized_corpus)\n# doc_scores = bm25.get_scores(tokenized_query)\ntop_k = 20\nprint(\"TOP\", top_k)\ntopKCandidates = bm25.get_top_n(tokenized_query, candidates, n=top_k)\n# count chinese chars.\n# count for english/chinese portion. (strange hack.)\nimport numpy as np\ndef calculateChinesePortion(phrase):",
        "type": "code",
        "location": "/tests/search_engine_suggestion_based_qa_bot/parse_baidu_title_abstract.py:74-111"
    },
    "3759": {
        "file_id": 462,
        "content": "The code is performing text preprocessing, calculating the similarity between phrases, and ranking candidates using BM25 algorithm. It first tokenizes and cuts the words from the candidate phrases and the query. Then it calculates the common words between two phrases and uses this information to sort and rank the candidates. Finally, it applies the BM25Okapi algorithm to get the scores of each candidate based on their relevance to the query and selects the top 20 ranked candidates. The code also includes a function to calculate the Chinese portion in a phrase.",
        "type": "comment"
    },
    "3760": {
        "file_id": 462,
        "content": "    length = len(phrase)\n    mdata = []\n    isalpha, isascii, isdigit, ischinese = 0, 0, 0, 0\n    for char in phrase:\n        isalpha += int(char.isalpha())\n        isascii += int(char.isascii())\n        isdigit += int(char.isdigit())\n        ischinese += int(not (isalpha or isascii or isdigit))\n    mdata = np.array([isalpha, isascii, isdigit, ischinese]) / length\n    return mdata\nqueryChinesePortion = calculateChinesePortion(query)\nfrom scipy.spatial.distance import cosine\ntopKCandidates.sort(\n    key=lambda phrase: cosine(calculateChinesePortion(phrase), queryChinesePortion)\n)\n# topKCandidates.sort(key=lambda phrase: -len(phrase))\nfor elem in topKCandidates:\n    print(elem.__repr__())",
        "type": "code",
        "location": "/tests/search_engine_suggestion_based_qa_bot/parse_baidu_title_abstract.py:112-132"
    },
    "3761": {
        "file_id": 462,
        "content": "The code calculates the proportion of Chinese characters in a query and uses it to sort a list of candidate phrases. It then prints each candidate phrase, sorted by their similarity to the query based on the Chinese character portion.",
        "type": "comment"
    },
    "3762": {
        "file_id": 463,
        "content": "/tests/search_engine_suggestion_based_qa_bot/parse_baidu_search_ajax.py",
        "type": "filepath"
    },
    "3763": {
        "file_id": 463,
        "content": "This code parses Baidu Image Search results using two functions, extracting title snippets and image similarity, with potential img_sim issue. It retrieves image dimensions and appends to a dataframe before returning two dataframes in debug mode.",
        "type": "summary"
    },
    "3764": {
        "file_id": 463,
        "content": "import pyjq\ndef getBaiduImageSearchAjaxInfoParsed(obj, debug=False):\n    commonFilter = \"select(.extData) | .extData.showInfo | select(. != null) | {titles, snippets, imgs_src, simi} | select (.titles !=null)\"\n    def standardJsonParser(obj):\n        command = \".data.cardData[] | {}\".format(commonFilter)\n        processed_obj = pyjq.first(command, obj)\n        return processed_obj\n    def hiddenJsParser(obj):\n        processed_obj = obj\n        for index in range(3):\n            data = pyjq.first(\".data.commonData.js[{}]\".format(index), obj2)\n            if not ('titles' in data and 'titles_url' in data):\n                continue\n            lines = data.split(\"\\n\")\n            for line in lines:\n                line = line.strip()\n                hint = \"var cardData = \"\n                # print(line)\n                if line.startswith(hint):\n                    import javascript\n                    cardData = javascript.eval_js(line.replace(hint,\"\")).valueOf()\n                    real_data = pyjq.first(commonFilter,cardData)",
        "type": "code",
        "location": "/tests/search_engine_suggestion_based_qa_bot/parse_baidu_search_ajax.py:1-23"
    },
    "3765": {
        "file_id": 463,
        "content": "This code defines two functions: `standardJsonParser` and `hiddenJsParser`. The first function, `standardJsonParser`, processes the data using a common filter and returns the filtered results. The second function, `hiddenJsParser`, extracts data from hidden JavaScript strings and applies the same common filter to return the processed data. This code appears to be parsing Baidu Image Search AJAX information in different formats (standard JSON or hidden JavaScript).",
        "type": "comment"
    },
    "3766": {
        "file_id": 463,
        "content": "                    # import pprint\n                    return real_data\n                    # pprint.pprint(real_data)\n    import pandas as pd\n    processed_obj = None\n    methods = [standardJsonParser,hiddenJsParser]\n    for method in methods:\n        try:\n            processed_obj = method(obj)\n            if processed_obj is not None:\n                break\n        except:\n            ...\n    if processed_obj is None:\n        if debug:\n            print('cannot parse info from obj')\n    # print(processed_obj)\n    # breakpoint()\n    # from pprint import pprint\n    # pprint(processed_obj)\n    title_snippets = pyjq.first(\"{titles, snippets}\", processed_obj)\n    img_sim = pyjq.first(\"(.simi[]|=tonumber )|{imgs_src, simi}\", processed_obj) # TODO: error! what is going on?\n    # img_sim[\"simi\"] = img_sim[\"simi\"] # what is this?\n    # [('titles', 15), ('snippets', 15), ('imgs_src', 43), ('simi', 43)]\n    # 15, 15, 43, 43\n    df_title_snippets = pd.DataFrame(title_snippets)\n    df_img_sim = pd.DataFrame(img_sim)\n    elem = df_img_sim[\"simi\"][0]",
        "type": "code",
        "location": "/tests/search_engine_suggestion_based_qa_bot/parse_baidu_search_ajax.py:24-51"
    },
    "3767": {
        "file_id": 463,
        "content": "This code attempts to parse an object and extract title snippets and image similarity information. It uses various parsing methods, data frames for organization, and the pyjq library for data manipulation. The code also includes error handling and debugging options. However, there is a potential error in the img_sim variable parsing.",
        "type": "comment"
    },
    "3768": {
        "file_id": 463,
        "content": "    if debug:\n        print(df_title_snippets.head())\n        print(df_img_sim.head())\n        print(type(elem), elem)  # str?\n    # breakpoint()\n    from urllib.parse import parse_qs\n    def getWidthHeight(url):\n        qs = url.split(\"?\")[-1]\n        mdict = parse_qs(qs)\n        # print(mdict)\n        # breakpoint()\n        width = int(mdict[\"w\"][0])\n        height = int(mdict[\"h\"][0])\n        area = width * height\n        return width, height, area\n    # pre_qs = df_img_sim['imgs_src'].split(\"?\")\n    width_height = df_img_sim[\"imgs_src\"].apply(\n        lambda v: pd.Series(getWidthHeight(v), index=[\"width\", \"height\", \"area\"])\n    )\n    df_img_sim_width_height = pd.concat([df_img_sim, width_height], axis=1, join=\"inner\")\n    # qs = parse_qs(pre_qs)\n    # print(qs)\n    if debug:\n        print(df_img_sim_width_height.head())\n    return df_title_snippets, df_img_sim_width_height\n# the \"js\" response may contain video info which may help with our reverse video search.\n# but the keyword also helps!\nif __name__ == \"__main__\":",
        "type": "code",
        "location": "/tests/search_engine_suggestion_based_qa_bot/parse_baidu_search_ajax.py:53-85"
    },
    "3769": {
        "file_id": 463,
        "content": "This code snippet is parsing the Baidu search results and retrieving image width, height, and area information. It then appends these values to the dataframe df_img_sim_width_height and returns two dataframes: df_title_snippets and df_img_sim_width_height. The debug mode allows printing of important intermediate data for testing and validation.",
        "type": "comment"
    },
    "3770": {
        "file_id": 463,
        "content": "    from lazero.filesystem.io import readJsonObjectFromFile\n    # obj = readJsonObjectFromFile(\"ajax_baidu.json\")\n    obj2 = readJsonObjectFromFile(\"jq_image_2.json\")\n    getBaiduImageSearchAjaxInfoParsed(obj2, debug=True)",
        "type": "code",
        "location": "/tests/search_engine_suggestion_based_qa_bot/parse_baidu_search_ajax.py:86-89"
    },
    "3771": {
        "file_id": 463,
        "content": "This code imports the readJsonObjectFromFile function and reads two JSON files, \"ajax_baidu.json\" and \"jq_image_2.json\". The function getBaiduImageSearchAjaxInfoParsed is then called with the second file's content (obj2) and debug mode enabled.",
        "type": "comment"
    },
    "3772": {
        "file_id": 464,
        "content": "/tests/nearly_duplicate_frames_detection_removal/test.py",
        "type": "filepath"
    },
    "3773": {
        "file_id": 464,
        "content": "The code imports libraries, checks for still images, and uses scene detection with the scenedetect library. It retrieves video duration, sets adaptive detector, and stores results in an output file. Another code reads a CSV file into a DataFrame, prints first 5 rows, and pauses execution at breakpoint.",
        "type": "summary"
    },
    "3774": {
        "file_id": 464,
        "content": "# source = \"/root/Desktop/works/pyjom/samples/video/nearly_duplicate_frames_detection.gif\"  # this is evil. it defeats my shit.\nsource = \"/root/Desktop/works/pyjom/samples/video/nearly_duplicate_frames_detection_30fps_blend.mp4\"  # this is evil. it defeats my shit.\n# source = \"/root/Desktop/works/pyjom/samples/video/nearly_duplicate_frames_detection_30fps.gif\"  # this is evil. it defeats my shit.\n# is it still image?\n# we can also detect more shits. right?\nimport sys\nimport os\nos.chdir(\"../../\")\nsys.path.append(\".\")\nfrom pyjom.commons import extract_span\nimport scenedetect\nfrom caer.video.frames_and_fps import get_duration\nstats_file_path = \"/media/root/parrot/pyjom/tests/nearly_duplicate_frames_detection_removal/output.csv\"\nduration = get_duration(source)\nprint(\"DURATION:\", duration)\ncuts = scenedetect.detect(\n    video_path=source, stats_file_path=stats_file_path, show_progress=True, \n    # detector=scenedetect.ContentDetector()\n    detector=scenedetect.AdaptiveDetector(),\n) # no fucking cuts???\nimport pandas",
        "type": "code",
        "location": "/tests/nearly_duplicate_frames_detection_removal/test.py:1-28"
    },
    "3775": {
        "file_id": 464,
        "content": "Code imports necessary libraries, checks if the source is a still image, and uses scenedetect library for scene detection. It gets video duration, sets adaptive detector, and stores results in output.csv file. No cuts are found in the video.",
        "type": "comment"
    },
    "3776": {
        "file_id": 464,
        "content": "df = pandas.read_csv(stats_file_path)\nprint(df.head())\nbreakpoint()",
        "type": "code",
        "location": "/tests/nearly_duplicate_frames_detection_removal/test.py:30-32"
    },
    "3777": {
        "file_id": 464,
        "content": "This code reads a CSV file (stats_file_path) into a pandas DataFrame named 'df', then prints the first 5 rows of the DataFrame, and finally pauses execution at this breakpoint.",
        "type": "comment"
    },
    "3778": {
        "file_id": 465,
        "content": "/tests/nearly_duplicate_frames_detection_removal/pyav_effective_fps.py",
        "type": "filepath"
    },
    "3779": {
        "file_id": 465,
        "content": "This code measures the keyframe percentage in a video file using Python and the AV library. It opens a video source, iterates over each frame, appends the keyframes to a list, calculates the percentage of keyframes relative to total frames, and prints the result.",
        "type": "summary"
    },
    "3780": {
        "file_id": 465,
        "content": "import av\n# source = \"/root/Desktop/works/pyjom/samples/video/nearly_duplicate_frames_detection_30fps_blend.mp4\"  # this is evil. it defeats my shit.\n# KEYFRAME PERCENT: 1.36 %\n# source = \"/root/Desktop/works/pyjom/samples/video/dog_with_text.mp4\"  # this is evil. it defeats my shit.\n# KEYFRAME PERCENT: 0.76 %\n# wtf?\n# even smaller.\nsource = \"/root/Desktop/works/pyjom/samples/video/karaoke_effects_source.mp4\"\ncontainer = av.open(source)\nmList = []\nfor frame in container.decode(video=0):\n    mList.append(frame.key_frame)\nprint(\"KEYFRAME PERCENT: {:.2f} %\".format(100*sum(mList)/len(mList)))",
        "type": "code",
        "location": "/tests/nearly_duplicate_frames_detection_removal/pyav_effective_fps.py:1-18"
    },
    "3781": {
        "file_id": 465,
        "content": "This code measures the keyframe percentage in a video file using Python and the AV library. It opens a video source, iterates over each frame, appends the keyframes to a list, calculates the percentage of keyframes relative to total frames, and prints the result.",
        "type": "comment"
    },
    "3782": {
        "file_id": 466,
        "content": "/tests/nearly_duplicate_frames_detection_removal/knn_spatial_similar_color_extraction.py",
        "type": "filepath"
    },
    "3783": {
        "file_id": 466,
        "content": "The code tests centrality thresholds for nearly duplicate frames using OpenCV and numpy, addresses issues like double centers and incorrect percentages, and performs clustering with MiniBatchKMeans.",
        "type": "summary"
    },
    "3784": {
        "file_id": 466,
        "content": "# i'd say i want centrality below 6 percent. what's the catch?\n# we'd like to adjust the shift.\n# another tip: you have forgot the spatial coordinates.\n# fuck!\nsrc = \"/root/Desktop/works/pyjom/samples/image/cute_cat.bmp\"\n# CENTRALITY: 0.00 %\n# single not go beyond 4 percent.\n# total not go beyond 6 percent.\n# is that right? fuck?\n# src = \"/root/Desktop/works/pyjom/samples/image/dog_with_text.png\"\n# and yet this does not look right.\n# NEARBY CENTER PERCENTAGE: 0.84 %\n# CENTRALITY: 2.57 %\n# src = \"/root/Desktop/works/pyjom/samples/image/miku_on_green.png\"\n# for this one we have double centers. fuck.\n# CENTRALITY: 181.80 %\n# it is off the charge!\n# with text. a meme.\n# src = \"/root/Desktop/works/pyjom/samples/image/dog_saturday_night.bmp\"\n# CENTRALITY: 1.26 %\n# src = \"/root/Desktop/works/pyjom/samples/image/similar_color_extraction.bmp\"  # use some filter first, or rather not to?\n# CENTER: [254.62436869 254.63794192 254.79734848]\n# POSITIVE COUNT: 188772\n# SUM: 566316.0 MIN: 0 MAX: 3\n# NEARBY CENTER PERCENTAGE: 81.93 %",
        "type": "code",
        "location": "/tests/nearly_duplicate_frames_detection_removal/knn_spatial_similar_color_extraction.py:1-35"
    },
    "3785": {
        "file_id": 466,
        "content": "The code appears to be testing and adjusting the centrality threshold for detecting nearly duplicate frames. The author is experimenting with different image file sources, and discussing various issues encountered during the process, such as double centers and incorrect centrality percentages. They also mention using filters for certain images.",
        "type": "comment"
    },
    "3786": {
        "file_id": 466,
        "content": "# CENTRALITY: 82.33 %\n# let's try some cats.\n# the filter: removegrain\n# src = \"/root/Desktop/works/pyjom/samples/image/kitty_flash.bmp\"  # use some filter first, or rather not to?\n# CENTER: [1.37254902 2.34313725 9.46078431]\n# POSITIVE COUNT: 2600\n# SUM: 7800.0 MIN: 0 MAX: 3\n# NEARBY CENTER PERCENTAGE: 3.91 %\n# CENTRALITY: 3.91 %\n# now the 八点半配音\n# src = \"/root/Desktop/works/pyjom/samples/image/is_this_duck.bmp\"\n# CENTER: [252.66293811 177.62005966 126.37844892]\n# POSITIVE COUNT: 222893\n# SUM: 668679.0 MIN: 0 MAX: 3\n# NEARBY CENTER PERCENTAGE: 36.49 %\n# CENTRALITY: 36.55 %\n# likely to be the blue.\n# src = \"/root/Desktop/works/pyjom/samples/image/pig_really.bmp\"\n# multiple centers.\n# CENTER: [246.76865924 226.40763256 216.41472476]\n# POSITIVE COUNT: 95497\n# SUM: 286491.0 MIN: 0 MAX: 3\n# NEARBY CENTER PERCENTAGE: 6.74 %\n# CENTRALITY: 7.32 %\nimport numpy as np\nfrom lazero.utils.importers import cv2_custom_build_init\ncv2_custom_build_init()\nuse_spatial=True\nimport cv2\nimage = cv2.imread(src)\nshape = image.shape\nif len(shape) != 3:",
        "type": "code",
        "location": "/tests/nearly_duplicate_frames_detection_removal/knn_spatial_similar_color_extraction.py:36-76"
    },
    "3787": {
        "file_id": 466,
        "content": "This code reads an image from a specified source and checks if it's in the correct format (RGB). It then calculates the centrality and nearby center percentage, likely for duplicate frame detection. The code uses OpenCV to load images and numpy for data manipulation. The code has three different examples with different results: one cat image with high centrality and nearby center percentage, a duck image with very high centrality and nearby center percentage, and a pig image with multiple centers and lower centrality.",
        "type": "comment"
    },
    "3788": {
        "file_id": 466,
        "content": "    print(\"weird shit.\")\nif shape[2] != 3:\n    print(\"depth not right.\")\n# for i in range(3):\n#     image[:,:,i] = i\nif use_spatial:\n    col_0, col_1 = shape[:2]\n    coords = []\n    bias_0 = 2\n    bias_1 = 2\n    for c0 in range(col_0):\n        for c1 in range(col_1):\n            coords.append((bias_0*c0/col_0,bias_1*c1/col_1))\n    coords = np.array(coords)\n# print(image.reshape(-1,3))\nreshapedImage = image.reshape(-1, 3)  # are you sure about this?\nlength, depth = reshapedImage.shape\nsample_size_limit = 5000\nreshapedImageIndexs = np.arange(0, length)\n# so now it is good.\nsampleIndexs = np.random.choice(reshapedImageIndexs, size=min(sample_size_limit, length))\nprint(sampleIndexs)\nprint(sampleIndexs.shape)\nsample_size = len(sampleIndexs)\nsample = reshapedImageIndexs[sampleIndexs]\nsample = reshapedImage[sample, :]\nprint(sample)\nprint(sample.shape)\n# breakpoint()\nif use_spatial:\n    sampleCoords = coords[sampleIndexs]\n    sample = np.hstack([sample, sampleCoords])\n    print(sample)\n    print(sample.shape)\n# breakpoint()\n# warning: OOM?",
        "type": "code",
        "location": "/tests/nearly_duplicate_frames_detection_removal/knn_spatial_similar_color_extraction.py:77-123"
    },
    "3789": {
        "file_id": 466,
        "content": "This code checks if the image depth is correct, then it reshapes and extracts samples from an image for further processing. The code also includes an option to use spatial coordinates, which are added as additional features to the sample data.",
        "type": "comment"
    },
    "3790": {
        "file_id": 466,
        "content": "# now cluster shit shall we?\n# from sklearn.neighbors import NearestNeighbors\n# neigh = NearestNeighbors(n_neighbors=5)\n# X = sample\n# neigh.fit(X)\n# A = neigh.kneighbors_graph(X)\n# A.toarray()\n# print(A)\n# print(A.shape) # sparse matrix? wtf?\nfrom sklearn.cluster import MiniBatchKMeans  # better?\n# from sklearn.cluster import KMeans\nX = sample\nbatch_size = 45\n# kmeans = KMeans(n_clusters=5).fit(X) # not deterministic please?\nn_clusters = 5\nkmeans = MiniBatchKMeans(\n    init=\"k-means++\",\n    n_clusters=n_clusters,\n    batch_size=batch_size,\n    # n_init=10,\n    max_no_improvement=10,\n    verbose=0,\n).fit(X)\n# from lazero.utils import inspectObject\n# inspectObject(kmeans)\n# breakpoint()\nlabels = kmeans.labels_\ncluster_centers = kmeans.cluster_centers_\nprint(labels)\nprint(cluster_centers)\nlabel_percentage = {\n    x: np.count_nonzero(labels == x) / sample_size for x in range(n_clusters)\n}\nflagged_image = image.copy()\nflagged_image[:,:,:] = 1 # every element is 1 now.\nepsilon = 0.01 # shit man.\npercents = []\nshift=2\nfor center5 in cluster_centers:",
        "type": "code",
        "location": "/tests/nearly_duplicate_frames_detection_removal/knn_spatial_similar_color_extraction.py:124-165"
    },
    "3791": {
        "file_id": 466,
        "content": "Code is performing clustering using MiniBatchKMeans from sklearn.cluster, with n_clusters=5 and batch_size=45 to handle larger datasets. After fitting the data, it prints labels and cluster centers. Then, it calculates label percentages based on the labels assigned by KMeans, initializes a flagged image with all elements set to 1, and starts iterating through each cluster center to perform further operations (not shown in code snippet).",
        "type": "comment"
    },
    "3792": {
        "file_id": 466,
        "content": "    # fetch area nearby given center\n    if use_spatial:\n        center = center5[:3]\n    else:\n        center = center5\n    # center_int = center.astype(np.uint8)\n    # i just don't know what the fuck is going on here.\n    upper = center + shift\n    lower = center - shift\n    mask = cv2.inRange(image, lower, upper)\n    # not image.\n    output = cv2.bitwise_and(flagged_image, flagged_image, mask=mask)\n    # print(output)\n    # print(output.shape)\n    mOutput = output.reshape(-1, 3)\n    mOutput = np.sum(mOutput, axis=1)\n    mSum = sum(mOutput)\n    # breakpoint()\n    positive_count = np.count_nonzero(abs(mOutput - 3) < epsilon)\n    percent = positive_count/len(mOutput)\n    # print(mOutput)\n    # print(mOutput.shape)\n    # breakpoint()\n    print(\"CENTER:\",center)\n    print('POSITIVE COUNT:', positive_count)\n    print(\"SUM:\", mSum, \"MIN:\", min(mOutput), 'MAX:', max(mOutput))\n    print(\"NEARBY CENTER PERCENTAGE: {:.2f} %\".format(percent*100))\n    percents.append(percent)\nprint(\"CENTRALITY: {:.2f} %\".format(sum(percents)*100))",
        "type": "code",
        "location": "/tests/nearly_duplicate_frames_detection_removal/knn_spatial_similar_color_extraction.py:166-195"
    },
    "3793": {
        "file_id": 466,
        "content": "The code calculates the centrality of a center by extracting nearby pixel values and checking if they are within a specified epsilon threshold. It uses image processing functions from OpenCV (cv2) and numpy for masking, reshaping, and summing operations. The code then prints various metrics related to the center's centrality, such as positive count, sum of pixel values, minimum and maximum values, and finally calculates the overall centrality percentage.",
        "type": "comment"
    },
    "3794": {
        "file_id": 467,
        "content": "/tests/nearly_duplicate_frames_detection_removal/knn_similar_color_extraction.py",
        "type": "filepath"
    },
    "3795": {
        "file_id": 467,
        "content": "The user is experiencing issues with image centrality and nearby center percentages when using OpenCV (cv2) and MiniBatchKMeans for clustering in numpy. The code extracts similar color frames, calculates percentages of nearby centers, and prints related statistics to calculate overall centrality.",
        "type": "summary"
    },
    "3796": {
        "file_id": 467,
        "content": "# i'd say i want centrality below 6 percent. what's the catch?\n# we'd like to adjust the shift.\n# another tip: you have forgot the spatial coordinates.\n# fuck!\n# src = \"/root/Desktop/works/pyjom/samples/image/cute_cat.bmp\"\n# CENTRALITY: 0.00 %\n# single not go beyond 4 percent.\n# total not go beyond 6 percent.\n# is that right? fuck?\n# src = \"/root/Desktop/works/pyjom/samples/image/dog_with_text.png\"\n# and yet this does not look right.\n# NEARBY CENTER PERCENTAGE: 0.84 %\n# CENTRALITY: 2.57 %\n# src = \"/root/Desktop/works/pyjom/samples/image/miku_on_green.png\"\n# for this one we have double centers. fuck.\n# CENTRALITY: 181.80 %\n# it is off the charge!\n# with text. a meme.\n# src = \"/root/Desktop/works/pyjom/samples/image/dog_saturday_night.bmp\"\n# CENTRALITY: 1.26 %\n# src = \"/root/Desktop/works/pyjom/samples/image/similar_color_extraction.bmp\"  # use some filter first, or rather not to?\n# CENTER: [254.62436869 254.63794192 254.79734848]\n# POSITIVE COUNT: 188772\n# SUM: 566316.0 MIN: 0 MAX: 3\n# NEARBY CENTER PERCENTAGE: 81.93 %",
        "type": "code",
        "location": "/tests/nearly_duplicate_frames_detection_removal/knn_similar_color_extraction.py:1-35"
    },
    "3797": {
        "file_id": 467,
        "content": "The code snippet is displaying image centrality, nearby center percentage, and other related information for several images. The user seems to be adjusting the shift and working with spatial coordinates. However, they are encountering issues like double centers and results that do not look right. They seem to be unsure about some parameters and considering using a filter on an image.",
        "type": "comment"
    },
    "3798": {
        "file_id": 467,
        "content": "# CENTRALITY: 82.33 %\n# let's try some cats.\n# the filter: removegrain\n# src = \"/root/Desktop/works/pyjom/samples/image/kitty_flash.bmp\"  # use some filter first, or rather not to?\n# CENTER: [1.37254902 2.34313725 9.46078431]\n# POSITIVE COUNT: 2600\n# SUM: 7800.0 MIN: 0 MAX: 3\n# NEARBY CENTER PERCENTAGE: 3.91 %\n# CENTRALITY: 3.91 %\n# now the 八点半配音\n# src = \"/root/Desktop/works/pyjom/samples/image/is_this_duck.bmp\"\n# CENTER: [252.66293811 177.62005966 126.37844892]\n# POSITIVE COUNT: 222893\n# SUM: 668679.0 MIN: 0 MAX: 3\n# NEARBY CENTER PERCENTAGE: 36.49 %\n# CENTRALITY: 36.55 %\n# likely to be the blue.\nsrc = \"/root/Desktop/works/pyjom/samples/image/pig_really.bmp\"\n# multiple centers.\n# CENTER: [246.76865924 226.40763256 216.41472476]\n# POSITIVE COUNT: 95497\n# SUM: 286491.0 MIN: 0 MAX: 3\n# NEARBY CENTER PERCENTAGE: 6.74 %\n# CENTRALITY: 7.32 %\nimport numpy as np\nfrom lazero.utils.importers import cv2_custom_build_init\ncv2_custom_build_init()\nimport cv2\nimage = cv2.imread(src)\nshape = image.shape\nif len(shape) != 3:\n    print(\"weird shit.\")",
        "type": "code",
        "location": "/tests/nearly_duplicate_frames_detection_removal/knn_similar_color_extraction.py:36-75"
    },
    "3799": {
        "file_id": 467,
        "content": "This code reads an image from a specific file and applies filters to detect and remove duplicate frames. The results include information about centrality, positive counts, nearby center percentages, and more. The code uses OpenCV (cv2) for image processing and numpy for array manipulation.",
        "type": "comment"
    }
}