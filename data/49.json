{
    "4900": {
        "file_id": 642,
        "content": "/tests/unittest_ffmpegVideoPreProductionFilter.py",
        "type": "filepath"
    },
    "4901": {
        "file_id": 642,
        "content": "This code imports necessary modules, checks ffmpeg, and utilizes MediaInfo for duration. It uses UUID to generate a unique cache file name. The code tests text detection in videos using ffmpeg filters, iterating through videoPaths and applying the filter on each video, while handling false positives and potential None output.",
        "type": "summary"
    },
    "4902": {
        "file_id": 642,
        "content": "from test_commons import *\nfrom pyjom.medialang.processors.dotProcessor import ffmpegVideoPreProductionFilter\nimport tempfile\n# import MediaInfo\nvideoPaths = {\n    \"text\": \"/root/Desktop/works/pyjom/samples/video/dog_with_text.mp4\",\n    \"logo\": \"/root/Desktop/works/pyjom/samples/video/LkS8UkiLL.mp4\",\n    # \"pip\":\"/root/Desktop/works/pyjom/samples/video/LiGlReJ4i.mp4\", # najie\n    \"pip\": \"/root/Desktop/works/pyjom/samples/video/LiEIfnsvn.mp4\",  # double pip\n    # 'complete':\"/root/Desktop/works/pyjom/samples/video/dog_with_text.mp4\"\n}\ntempDir = \"/dev/shm/medialang\"  # anyway we just want something else...\ntest_ffmpeg = True\ntest_text_detector = False\ndef getVideoDuration(filePath):\n    from MediaInfo import MediaInfo\n    info = MediaInfo(filename=videoPath)\n    infoData = info.getInfo()\n    # print(infoData)\n    # breakpoint()\n    defaultWidth = infoData[\"videoWidth\"]\n    defaultHeight = infoData[\"videoHeight\"]\n    # print(infoData)\n    # print(infoData.keys())\n    # breakpoint()\n    start = 0\n    end = float(infoData[\"videoDuration\"])",
        "type": "code",
        "location": "/tests/unittest_ffmpegVideoPreProductionFilter.py:1-34"
    },
    "4903": {
        "file_id": 642,
        "content": "The code imports necessary modules, defines video paths and temporary directory locations, tests ffmpeg functionality, and retrieves the duration of a video using MediaInfo library.",
        "type": "comment"
    },
    "4904": {
        "file_id": 642,
        "content": "    return end\ntestSubject = \"complete\"\nwith tempfile.TemporaryDirectory(prefix=tempDir) as allocatedTmpDir:\n    print(\"Allocated tmpDir:\", allocatedTmpDir)\n    if testSubject == \"logo\":\n        videoPath = videoPaths[\"logo\"]\n        filters = [\"logoRemoval\"]  # how the fuck?\n    elif testSubject == \"text\":\n        videoPath = videoPaths[\"text\"]\n        filters = [\"textRemoval\"]\n    elif testSubject == \"pip\":\n        videoPath = videoPaths[\"pip\"]\n        filters = [\"pipCrop\"]\n    elif testSubject == \"complete\":\n        # videoPath = videoPaths['complete']\n        # filters = ['pipCrop','textRemoval']\n        filters = [\"pipCrop\", \"textRemoval\", \"logoRemoval\"]\n    else:\n        raise Exception(\"Unknown testSubject: %s\" % testSubject)\n    # videoFileName = os.path.basename(videoPath)\n    # # we use the full video here? to check if this shit really works?\n    # # videoFile = os.path.join(allocatedTmpDir,videoFileName)\n    import uuid\n    cacheId = str(uuid.uuid4())\n    fileExtension = \"mp4\"\n    cacheFileName = \".\".join([cacheId, fileExtension])",
        "type": "code",
        "location": "/tests/unittest_ffmpegVideoPreProductionFilter.py:35-64"
    },
    "4905": {
        "file_id": 642,
        "content": "Sets temporary directory, determines filter type based on testSubject, and generates a unique cache file name using UUID.",
        "type": "comment"
    },
    "4906": {
        "file_id": 642,
        "content": "    cachePath = os.path.join(allocatedTmpDir, cacheFileName)\n    # if testSubject == 'pip':\n    #     start=5\n    #     end=10\n    if test_text_detector:\n        from pyjom.medialang.processors.dotProcessor import detectTextRegionOverTime\n        for key, videoPath in videoPaths.items():\n            if testSubject != \"complete\":\n                if key != testSubject:\n                    continue\n            print(\"TESTING: %s\" % key)\n            start = 0\n            end = getVideoDuration(videoPath)\n            # regions = detectTextRegionOverTime(videoPath, start, end)\n            regions = detectTextRegionOverTime(\n                videoPath, 10, 20\n            )  # now we change the start and end.\n            for key, item in regions.items():\n                # could be empty here.\n                print(\"KEY:\", key)\n                print(\"ITEM:\", item)\n            # how to merge continual shits?\n        # pretty much None currently.\n        breakpoint()\n    if test_ffmpeg:\n        # the logoRemoval filter may make the video unwatchable if too many false positive areas were found.",
        "type": "code",
        "location": "/tests/unittest_ffmpegVideoPreProductionFilter.py:65-97"
    },
    "4907": {
        "file_id": 642,
        "content": "This code snippet is testing text detection in videos and ffmpeg filter functionality. It loops through videoPaths, detects text regions over time using `detectTextRegionOverTime` function, and prints the key and item for each detected region. It also handles None output for ffmpeg tests, and mentions potential false positive issues with the logoRemoval filter.",
        "type": "comment"
    },
    "4908": {
        "file_id": 642,
        "content": "        for key, videoPath in videoPaths.items():\n            if testSubject != \"complete\":\n                if key != testSubject:\n                    continue\n            print(\"TESTING: %s\" % key)\n            start = 0\n            end = getVideoDuration(videoPath)\n            output = ffmpegVideoPreProductionFilter(\n                videoPath,\n                cachePath=cachePath,\n                start=start,\n                end=end,\n                filters=filters,\n                preview=True,\n            )  # resolution? make it sufficiently low!\n            print(\"ffmpeg pre production filter processing done.\")\n            print(\"output location:\", output)\n            breakpoint()",
        "type": "code",
        "location": "/tests/unittest_ffmpegVideoPreProductionFilter.py:98-117"
    },
    "4909": {
        "file_id": 642,
        "content": "The code iterates through videoPaths and tests the filter on each video. If testSubject is not \"complete\", it skips the iteration unless the key matches the testSubject. It then applies the ffmpegVideoPreProductionFilter to the video, prints the output location, and breaks the loop.",
        "type": "comment"
    },
    "4910": {
        "file_id": 643,
        "content": "/tests/unittest_ffmpeg_args.py",
        "type": "filepath"
    },
    "4911": {
        "file_id": 643,
        "content": "The code processes video files using FFmpeg for tasks like cropping and scaling, with a specific command to map and filter video/audio streams. This is part of a larger script that uses the subprocess module.",
        "type": "summary"
    },
    "4912": {
        "file_id": 643,
        "content": "command_original = [\n    \"ffmpeg\",\n    \"-y\",\n    \"-ss\",\n    \"0\",\n    \"-to\",\n    \"59.3942553191489\",\n    \"-i\",\n    \"/root/Desktop/works/pyjom/samples/video/LiGlReJ4i.mp4\",\n    \"-ss\",\n    \"59.3942553191489\",\n    \"-to\",\n    \"62.0340000000000\",\n    \"-i\",\n    \"/root/Desktop/works/pyjom/samples/video/LiGlReJ4i.mp4\",\n    \"-ss\",\n    \"0\",\n    \"-to\",\n    \"62.034\",\n    \"-i\",\n    \"/root/Desktop/works/pyjom/samples/video/LiGlReJ4i.mp4\",\n    \"-filter_complex\",\n    \"[0:v]crop=h=1099:w=717:x=1:y=72[s0];[s0]pad=color=black:height=max(ih\\\\, ceil(iw*max(1080/1920\\\\, ih/iw))):width=max(iw\\\\, ceil(ih*max(1920/1080\\\\, iw/ih))):x=floor((max(iw\\\\, ceil(ih*max(1920/1080\\\\, iw/ih)))-iw)/2):y=floor((max(ih\\\\, ceil(iw*max(1080/1920\\\\, ih/iw)))-ih)/2)[s1];[s1]scale=1920:1080[s2];[s2]scale=ceil((iw*0.15555555555555556)/4)*4:ceil((ih*0.15555555555555556)/4)*4[s3];[1:v]pad=color=black:height=max(ih\\\\, ceil(iw*max(1080/1920\\\\, ih/iw))):width=max(iw\\\\, ceil(ih*max(1920/1080\\\\, iw/ih))):x=floor((max(iw\\\\, ceil(ih*max(1920/1080\\\\, iw/ih)))-iw",
        "type": "code",
        "location": "/tests/unittest_ffmpeg_args.py:1-23"
    },
    "4913": {
        "file_id": 643,
        "content": "This code uses FFmpeg to split a video file into segments, applies various filters and transformations to the segments, and finally scales and pads them before saving the final output.",
        "type": "comment"
    },
    "4914": {
        "file_id": 643,
        "content": ")/2):y=floor((max(ih\\\\, ceil(iw*max(1080/1920\\\\, ih/iw)))-ih)/2)[s4];[s4]scale=1920:1080[s5];[s5]scale=ceil((iw*0.15555555555555556)/4)*4:ceil((ih*0.15555555555555556)/4)*4[s6];[s3][s6]concat=n=2[s7]\",\n    \"-map\",\n    \"[s7]\",\n    \"-map\",\n    \"2:a\",\n    \"/dev/shm/2c6b1466-6186-41dd-9ce3-2f757c082c5a.mp4\",\n]\ncommand2 = [\n    \"ffmpeg\",\n    \"-y\",\n    \"-ss\",\n    \"0\",\n    \"-to\",\n    \"59.3942553191489\",\n    \"-i\",\n    \"/root/Desktop/works/pyjom/samples/video/LiGlReJ4i.mp4\",\n    \"-ss\",\n    \"0\",\n    \"-to\",\n    \"62.034\",\n    \"-i\",\n    \"/root/Desktop/works/pyjom/samples/video/LiGlReJ4i.mp4\",\n    \"-filter_complex\",\n    \"[0:v]crop=h=1099:w=717:x=1:y=72[s0];[s0]pad=color=black:height=max(ih\\\\, ceil(iw*max(1080/1920\\\\, ih/iw))):width=max(iw\\\\, ceil(ih*max(1920/1080\\\\, iw/ih))):x=floor((max(iw\\\\, ceil(ih*max(1920/1080\\\\, iw/ih)))-iw)/2):y=floor((max(ih\\\\, ceil(iw*max(1080/1920\\\\, ih/iw)))-ih)/2)[s1];[s1]scale=1920:1080[s2];[s2]scale=ceil((iw*0.15555555555555556)/4)*4:ceil((ih*0.15555555555555556)/4)*4[s3]\",\n    \"-map\",\n    \"[s3]\",",
        "type": "code",
        "location": "/tests/unittest_ffmpeg_args.py:23-48"
    },
    "4915": {
        "file_id": 643,
        "content": "The code is constructing a command for the ffmpeg tool to process and concatenate multiple video inputs. It applies filters such as cropping, padding, scaling, and extracts specific parts of videos before concatenating them into a single output video file. The resulting command is being stored in `command1` and `command2`.",
        "type": "comment"
    },
    "4916": {
        "file_id": 643,
        "content": "    \"-map\",\n    \"1:a\",\n    \"/dev/shm/2c6b1466-6186-41dd-9ce3-2f757c082c5a.mp4\",\n]\ncommand3 = [\n    \"ffmpeg\",\n    \"-y\",\n    \"-ss\",\n    \"59.3942553191489\",\n    \"-to\",\n    \"62.0340000000000\",\n    \"-i\",\n    \"/root/Desktop/works/pyjom/samples/video/LiGlReJ4i.mp4\",\n    \"-ss\",\n    \"0\",\n    \"-to\",\n    \"62.034\",\n    \"-i\",\n    \"/root/Desktop/works/pyjom/samples/video/LiGlReJ4i.mp4\",\n    \"-filter_complex\",\n    \"[0:v]pad=color=black:height=max(ih\\\\, ceil(iw*max(1080/1920\\\\, ih/iw))):width=max(iw\\\\, ceil(ih*max(1920/1080\\\\, iw/ih))):x=floor((max(iw\\\\, ceil(ih*max(1920/1080\\\\, iw/ih)))-iw)/2):y=floor((max(ih\\\\, ceil(iw*max(1080/1920\\\\, ih/iw)))-ih)/2)[s4];[s4]scale=1920:1080[s5];[s5]scale=ceil((iw*0.15555555555555556)/4)*4:ceil((ih*0.15555555555555556)/4)*4[s6]\",\n    \"-map\",\n    \"[s6]\",\n    \"-map\",\n    \"1:a\",\n    \"/dev/shm/2c6b1466-6186-41dd-9ce3-2f757c082c5a.mp4\",\n]\ncommandImprovised = command_original = [\n    \"ffmpeg\",\n    \"-y\",\n    \"-ss\",\n    \"0\",\n    \"-to\",\n    \"59.3942553191489\",\n    \"-i\",\n    \"/root/Desktop/works/pyjom/samples/video/LiGlReJ4i.mp4\",",
        "type": "code",
        "location": "/tests/unittest_ffmpeg_args.py:49-84"
    },
    "4917": {
        "file_id": 643,
        "content": "This code is using FFmpeg command line arguments to perform operations on video files. It's mapping streams, applying filters for scaling and padding, setting start/end times, and specifying output file paths. The code is likely involved in video processing or manipulation tasks.",
        "type": "comment"
    },
    "4918": {
        "file_id": 643,
        "content": "    \"-ss\",\n    \"59.3942553191489\",\n    \"-to\",\n    \"62.0340000000000\",\n    \"-i\",\n    \"/root/Desktop/works/pyjom/samples/video/LiGlReJ4i.mp4\",\n    \"-ss\",\n    \"0\",\n    \"-to\",\n    \"62.034\",\n    \"-i\",\n    \"/root/Desktop/works/pyjom/samples/video/LiGlReJ4i.mp4\",\n    \"-filter_complex\",\n    \"[0:v]crop=h=1099:w=717:x=1:y=72[s0];[s0]pad=color=black:height=max(ih\\\\, ceil(iw*max(1080/1920\\\\, ih/iw))):width=max(iw\\\\, ceil(ih*max(1920/1080\\\\, iw/ih))):x=floor((max(iw\\\\, ceil(ih*max(1920/1080\\\\, iw/ih)))-iw)/2):y=floor((max(ih\\\\, ceil(iw*max(1080/1920\\\\, ih/iw)))-ih)/2)[s1];[s1]scale=1920:1080[s2];[s2]scale=ceil((iw*0.15555555555555556)/4)*4:ceil((ih*0.15555555555555556)/4)*4,setsar=1[s3];[1:v]pad=color=black:height=max(ih\\\\, ceil(iw*max(1080/1920\\\\, ih/iw))):width=max(iw\\\\, ceil(ih*max(1920/1080\\\\, iw/ih))):x=floor((max(iw\\\\, ceil(ih*max(1920/1080\\\\, iw/ih)))-iw)/2):y=floor((max(ih\\\\, ceil(iw*max(1080/1920\\\\, ih/iw)))-ih)/2)[s4];[s4]scale=1920:1080[s5];[s5]scale=ceil((iw*0.15555555555555556)/4)*4:ceil((ih*0.15555555555555556)/4)*4,setsar=1[s6];[s3][s6]concat=n=2[s7]\",",
        "type": "code",
        "location": "/tests/unittest_ffmpeg_args.py:85-98"
    },
    "4919": {
        "file_id": 643,
        "content": "This code is using FFmpeg to crop, scale, and concatenate video streams. It first specifies start and end times for the input video file \"/root/Desktop/works/pyjom/samples/video/LiGlReJ4i.mp4\", then applies a series of filters including cropping, padding, scaling, and setting aspect ratio. Finally, it concatenates the resulting streams for output.",
        "type": "comment"
    },
    "4920": {
        "file_id": 643,
        "content": "    \"-map\",\n    \"[s7]\",\n    \"-map\",\n    \"2:a\",\n    \"/dev/shm/2c6b1466-6186-41dd-9ce3-2f757c082c5a.mp4\",\n]\nimport subprocess\nsubprocess.run(commandImprovised)",
        "type": "code",
        "location": "/tests/unittest_ffmpeg_args.py:99-107"
    },
    "4921": {
        "file_id": 643,
        "content": "This code chunk is part of a larger script that uses the subprocess module to run an FFmpeg command. The command maps video stream from input file \"[s7]\" and audio stream from track 2 to output \"/dev/shm/2c6b1466-6186-41dd-9ce3-2f757c082c5a.mp4\".",
        "type": "comment"
    },
    "4922": {
        "file_id": 644,
        "content": "/tests/unittest_ffmpeg_cropdetect_from_image_parse_log.py",
        "type": "filepath"
    },
    "4923": {
        "file_id": 644,
        "content": "This code uses ffmpeg and OpenCV to detect cropped areas, calculates the cropped area ratio, and decides whether to crop the image based on a threshold. The result depends on the specified threshold value.",
        "type": "summary"
    },
    "4924": {
        "file_id": 644,
        "content": "import ffmpeg\nfrom lazero.utils.importers import cv2_custom_build_init\ncv2_custom_build_init()\n# mediaPath = \"/root/Desktop/works/pyjom/samples/image/dog_blue_sky_split_line.png\"\nmediaPath = \"/root/Desktop/works/pyjom/samples/image/dog_with_black_borders.png\"  # use the image with black background.\n# ffmpeg -loop 1 -i /root/Desktop/works/pyjom/samples/image/dog_blue_sky_split_line.png -t 15 -vf cropdetect -f null -\nimport cv2\nimage = cv2.imread(mediaPath)\nheight, width = image.shape[:2]\ntotal_area = height * width\nareaThreshold = 0\nstdout, stderr = (\n    ffmpeg.input(mediaPath, loop=1, t=15)\n    .filter(\"cropdetect\")\n    .output(\"null\", f=\"null\")\n    .run(capture_stdout=True, capture_stderr=True)\n)\nstdout_decoded = stdout.decode(\"utf-8\")\nstderr_decoded = stderr.decode(\"utf-8\")\n# nothing here.\n# for line in stdout_decoded.split(\"\\n\"):\n#     print(line)\n# breakpoint()\nimport parse\ncropped_area_threshold = 0.1\ncommon_crops = []\nfor line in stderr_decoded.split(\"\\n\"):\n    line = line.replace(\"\\n\", \"\").strip()\n    for",
        "type": "code",
        "location": "/tests/unittest_ffmpeg_cropdetect_from_image_parse_log.py:1-40"
    },
    "4925": {
        "file_id": 644,
        "content": "The code imports necessary libraries and initializes them, sets the media path to an image with a black background, runs ffmpeg on the image with a cropdetect filter, decodes the output and errors, iterates over the stderr output lines to extract cropped areas, and defines a variable for common_crops.",
        "type": "comment"
    },
    "4926": {
        "file_id": 644,
        "content": "matString = \"[{}] x1:{x1:d} x2:{x2:d} y1:{y1:d} y2:{y2:d} w:{w:d} h:{h:d} x:{x:d} y:{y:d} pts:{pts:g} t:{t:g} crop={}:{}:{}:{}\"\n    # print(line)\n    result = parse.parse(formatString, line)\n    if result is not None:\n        # print(result)\n        cropString = \"{}_{}_{}_{}\".format(\n            *[result[key] for key in [\"w\", \"h\", \"x\", \"y\"]]\n        )\n        # print(cropString)\n        # breakpoint()\n        common_crops.append(cropString)\n    # [Parsed_cropdetect_0 @ 0x56246a16cbc0] x1:360 x2:823 y1:0 y2:657 w:464 h:656 x:360 y:2 pts:3 t:0.120000 crop=464:656:360:2\n    # this crop usually will never change. but let's count?\narea = 0\nx, x1, y, y1 = 0, width, 0, height\nif len(common_crops) > 0:\n    common_crops_count_tuple_list = [\n        (cropString, common_crops.count(cropString)) for cropString in set(common_crops)\n    ]\n    common_crops_count_tuple_list.sort(key=lambda x: -x[1])\n    selected_crop_string = common_crops_count_tuple_list[0][0]\n    result = parse.parse(\"{w:d}_{h:d}_{x:d}_{y:d}\", selected_crop_string)",
        "type": "code",
        "location": "/tests/unittest_ffmpeg_cropdetect_from_image_parse_log.py:40-62"
    },
    "4927": {
        "file_id": 644,
        "content": "Code parses a log line, extracts crop information and stores it in common_crops list. It then counts the occurrence of each unique crop string and selects the most frequent one (selected_crop_string). Finally, it parses the selected_crop_string to get the crop dimensions (w, h, x, y) and assigns them to their respective variables.",
        "type": "comment"
    },
    "4928": {
        "file_id": 644,
        "content": "    w, h, x, y = [result[key] for key in [\"w\", \"h\", \"x\", \"y\"]]\n    x1, y1 = min(x + w, width), min(y + h, height)\n    if x < x1 and y < y1:\n        # allow to calculate the area.\n        area = (x1 - x) * (y1 - y)\ncropped_area_ratio = 1 - (area / total_area)  # 0.5652352766414517\n# use 0.1 as threshold?\nprint(\"CROPPED AREA RATIO:\", cropped_area_ratio)\nif cropped_area_ratio > cropped_area_threshold:\n    print(\"we need to crop this. no further processing needed\")\n    image_black_cropped = image[y:y1, x:x1]\n    cv2.imshow(\"CROPPED IMAGE\", image_black_cropped)\n    cv2.waitKey(0)\nelse:\n    print(\"image no need to crop black borders. further processing needed\")",
        "type": "code",
        "location": "/tests/unittest_ffmpeg_cropdetect_from_image_parse_log.py:63-78"
    },
    "4929": {
        "file_id": 644,
        "content": "This code calculates the cropped area ratio of an image and decides whether to crop it or not based on a threshold. If the ratio is greater than the threshold, it crops the image using OpenCV and displays the cropped image. Otherwise, it proceeds with further processing. The result depends on the specified threshold value.",
        "type": "comment"
    },
    "4930": {
        "file_id": 645,
        "content": "/tests/unittest_ffmpeg_delogo_parser.py",
        "type": "filepath"
    },
    "4931": {
        "file_id": 645,
        "content": "This code defines a delogoParser function to parse command strings and processes video streams using the \"delogo\" filter. It checks parameter validity, removes logos from videos, and handles errors for debugging purposes.",
        "type": "summary"
    },
    "4932": {
        "file_id": 645,
        "content": "import parse\nfrom pyjom.videotoolbox import getVideoWidthHeight\nfrom test_commons import *\nimport ffmpeg\ncommandString = \"delogo_0_671_360_6|delogo_144_662_6_4|delogo_355_661_5_7|delogo_117_661_7_5|delogo_68_661_18_5|delogo_182_658_165_9|delogo_252_492_3_1|delogo_214_492_1_2|delogo_200_492_3_1|delogo_74_492_2_1|delogo_170_490_6_4|delogo_145_490_9_4|delogo_129_490_12_4|delogo_107_490_4_3|delogo_91_487_8_6|delogo_72_485_4_3|delogo_147_484_4_3|delogo_178_483_11_11|delogo_219_480_1_1|delogo_53_480_6_2|delogo_268_478_1_1|delogo_164_478_8_4|delogo_128_477_8_4|delogo_295_475_1_1|delogo_105_475_10_4|delogo_61_474_5_4|delogo_274_472_3_2|delogo_196_470_5_2|delogo_209_469_1_1|delogo_143_469_8_5|delogo_75_467_26_6|delogo_0_33_360_25|delogo_0_24_360_6\"\nvideoPath = \"/root/Desktop/works/pyjom/samples/video/LkS8UkiLL.mp4\"\noutputPath = \"/dev/shm/output.mp4\"\ndef delogoParser(command):\n    return parse.parse(\"delogo_{x:d}_{y:d}_{w:d}_{h:d}\", command)\nwidth, height = getVideoWidthHeight(videoPath)\ndef delogoFilter(stream, commandParams):",
        "type": "code",
        "location": "/tests/unittest_ffmpeg_delogo_parser.py:1-20"
    },
    "4933": {
        "file_id": 645,
        "content": "This code defines a delogoParser function that parses a command string into a formatted format using regular expression. It also includes the getVideoWidthHeight function to retrieve the video width and height from a given path, and a delogoFilter function to process video streams with a given command parameter. The commandString contains a list of delogo positions, and the script will process a video at the specified output path.",
        "type": "comment"
    },
    "4934": {
        "file_id": 645,
        "content": "    return stream.filter(\n        \"delogo\",\n        x=commandParams[\"x\"],\n        y=commandParams[\"y\"],\n        w=commandParams[\"w\"],\n        h=commandParams[\"h\"],\n    )\n# minArea = 20\ndef checkXYWH(XYWH, canvas, minArea=20):\n    x, y, w, h = XYWH\n    width, height = canvas\n    if x >= width - 1 or y >= height - 1:\n        return False, None\n    if x == 0:\n        x = 1\n    if y == 0:\n        y = 1\n    if x + w >= width:\n        w = width - x - 1\n        if w <= 2:\n            return False, None\n    if y + h >= height:\n        h = height - y - 1\n        if h <= 2:\n            return False, None\n    if w * h <= minArea:\n        return False, None\n    return True, (x, y, w, h)\nfor command in commandString.split(\"|\"):\n    try:\n        stream = ffmpeg.input(videoPath, ss=0, to=5).video\n        commandArguments = delogoParser(command)\n        x = commandArguments[\"x\"]\n        y = commandArguments[\"y\"]\n        w = commandArguments[\"w\"]\n        h = commandArguments[\"h\"]\n        status, XYWH = checkXYWH((x, y, w, h), (width, height))",
        "type": "code",
        "location": "/tests/unittest_ffmpeg_delogo_parser.py:21-64"
    },
    "4935": {
        "file_id": 645,
        "content": "This code snippet is a part of a larger program that involves video editing using the FFmpeg library. It filters a video stream with the \"delogo\" filter, taking command parameters for x, y, w, and h. Then, it checks if these parameters are valid by calling the checkXYWH function, which returns True or False depending on the input's validity. Finally, it loops through each command in the commandString, splitting them into smaller commands for video editing operations.",
        "type": "comment"
    },
    "4936": {
        "file_id": 645,
        "content": "        if not status:\n            continue\n        x, y, w, h = XYWH\n        commandArguments = {\"x\": x, \"y\": y, \"w\": w, \"h\": h}\n        stream = delogoFilter(stream, commandArguments)\n        ffmpeg.output(stream, outputPath).run(overwrite_output=True)\n    except:\n        import traceback\n        traceback.print_exc()\n        print(\"WIDTH:\", width, \"HEIGHT:\", height)\n        maxX, maxY = (\n            commandArguments[\"x\"] + commandArguments[\"w\"],\n            commandArguments[\"y\"] + commandArguments[\"h\"],\n        )\n        print(\"MAX X:\", maxX, \"MAX Y:\", maxY)\n        print(\"ERROR!\", commandArguments)\n        breakpoint()",
        "type": "code",
        "location": "/tests/unittest_ffmpeg_delogo_parser.py:65-83"
    },
    "4937": {
        "file_id": 645,
        "content": "This code is implementing a delogo filter, which takes input video and removes the logo from it. It checks if the filter was successfully applied, then extracts the position and dimensions of the logo to apply the delogo filter. If any error occurs during this process, it prints out information for debugging and stops the execution.",
        "type": "comment"
    },
    "4938": {
        "file_id": 646,
        "content": "/tests/unittest_ffmpeg_overlay_boxblur.py",
        "type": "filepath"
    },
    "4939": {
        "file_id": 646,
        "content": "The code uses ffmpeg library to apply Gaussian blur filter and scale video stream, then creates a second layer, overlays both layers, and outputs the processed video stream. It sets output dimensions, uses \"scale\" and \"gblur\" or \"boxblur\" filters, scales video stream with aspect ratio preservation, and outputs file to temporary directory.",
        "type": "summary"
    },
    "4940": {
        "file_id": 646,
        "content": "# ffmpeg对视频实现高斯模糊，给视频上下加模糊背景\n# ffmpeg实现视频高斯模糊拓边效果\nimport ffmpeg\nsource = \"/root/Desktop/works/pyjom/samples/video/cute_cat_gif.gif\"\nstream = ffmpeg.input(source)\nvideo_stream = stream.video\n# the damn thing because they are from the same file! fuck!\n# layer_0 = video_stream.filter(\"scale\", w=1080, h=1920).filter(\"boxblur\", 10) # this is default?\n# however, you need to generalize it here.\n# output_width = 1080\n# output_height = 1920\noutput_height = 1080\noutput_width = 1920\nlayer_0 = video_stream.filter(\"scale\", w=output_width, h=output_height).filter(\n    \"gblur\", sigma=9\n)  # this is default?\n# print('layer_0 args', layer_0.get_args())\nlayer_1 = video_stream.filter(\n    \"scale\",\n    w=\"min(floor(iw*{}/ih),{})\".format(output_height, output_width),\n    h=\"min(floor(ih*{}/iw),{})\".format(output_width, output_height),\n)\n# print('layer_1 args', layer_1.get_args())\n## in case you failed to generalize this shit...\noutput_stream = layer_0.overlay(layer_1, x=\"floor((W-w)/2)\", y=\"floor((H-h)/2)\")\n# print('output_stream args', output_stream.get_args())",
        "type": "code",
        "location": "/tests/unittest_ffmpeg_overlay_boxblur.py:1-40"
    },
    "4941": {
        "file_id": 646,
        "content": "The code uses ffmpeg library to apply Gaussian blur filter and scale video stream. It sets output dimensions, applies \"scale\" filter with given width and height, then applies \"gblur\" or \"boxblur\" filter. Then, it creates a second layer by scaling the video stream with aspect ratio preservation and overlays both layers using specific coordinates. Finally, it outputs the processed video stream.",
        "type": "comment"
    },
    "4942": {
        "file_id": 646,
        "content": "from lazero.filesystem import tmpdir\npath = \"/dev/shm/medialang\"\nimport os\nwith tmpdir(path=path) as T:\n    filepath = os.path.join(path, \"output.mp4\")\n    # args = ffmpeg.get_args(output_stream)\n    # print(args)\n    output_args = {\"preset\": \"veryfast\"}  # seems like it won't speed up so much?\n    ffmpeg.output(output_stream, filepath, **output_args).run(overwrite_output=True)\n    print(\"output file location:\", filepath)\n    breakpoint()",
        "type": "code",
        "location": "/tests/unittest_ffmpeg_overlay_boxblur.py:42-54"
    },
    "4943": {
        "file_id": 646,
        "content": "The code sets a temporary directory path, joins it with the file name, creates FFmpeg output arguments with a fast preset, and then runs an FFmpeg command to output the stream to the file in the temporary directory. The output file location is printed.",
        "type": "comment"
    },
    "4944": {
        "file_id": 647,
        "content": "/tests/unittest_full_text_search_peewee_sqlite.py",
        "type": "filepath"
    },
    "4945": {
        "file_id": 647,
        "content": "The code imports modules and sets up a SQLite database for full-text search. It defines a model class, populates the table with data, adds/updates a video, searches \"python world\" using BM25 algorithm, limits results to 2, and prints each result. Debugging breakpoints are included.",
        "type": "summary"
    },
    "4946": {
        "file_id": 647,
        "content": "from peewee import *\nfrom playhouse.sqlite_ext import SqliteExtDatabase, FTSModel, SearchField, RowIDField\ndb_path = \"test_fulltext_search.db\"\ndb = SqliteExtDatabase(\n    db_path, pragmas={\"journal_mode\": \"wal\", \"cache_size\": -1024 * 64}\n)\nclass BilibiliVideoIndex(FTSModel):\n    rowid = RowIDField()  # this does not support\n    title = SearchField()\n    content = SearchField()\n    class Meta:\n        database = None  # that's good.\n        options = {\"tokenize\": \"porter\"}  # you need manually separate some\ndb.create_tables([BilibiliVideoIndex])\nimport uuid\nrandomContent = lambda: str(uuid.uuid4())\nobject, flag = BilibiliVideoIndex.get_and_update_or_create(\n    rowid=1, title=randomContent(), content=randomContent(), _unique_keys=[\"rowid\"]\n)\nBilibiliVideoIndex.get_and_update_or_create(\n    rowid=2,\n    title=\"hello world\",\n    content=\"learn python the hard way\",\n    _unique_keys=[\"rowid\"],\n)\nBilibiliVideoIndex.get_and_update_or_create(\n    rowid=3,\n    title=\"hello world\",\n    content=\"learn python the hard way\",\n    _unique_keys=[\"rowid\"],",
        "type": "code",
        "location": "/tests/unittest_full_text_search_peewee_sqlite.py:1-42"
    },
    "4947": {
        "file_id": 647,
        "content": "This code imports necessary modules and sets up a SQLite database with full-text search capabilities. It defines a model class, BilibiliVideoIndex, and creates its corresponding table in the database. Using the get_and_update_or_create method, it populates the table with data for three records, ensuring uniqueness based on the rowid field.",
        "type": "comment"
    },
    "4948": {
        "file_id": 647,
        "content": ")\nBilibiliVideoIndex.get_and_update_or_create(\n    rowid=4,\n    title=\"hello world\",\n    content=\"learn python the hard way\",\n    _unique_keys=[\"rowid\"],\n)\nprint(object)\nprint(flag)\nprint(object.rowid, object.title, object.content)\n# don't know what magic is inside. whatever.\n# updated. my lord.\n# now search for it.\nterm = \"python world\"\nresults = BilibiliVideoIndex.search_bm25(term).limit(2)  # just how many?\n# breakpoint()\n# it does have the limit.\n# it is ordered.\nfor result in results:\n    print(\"RESULT\", result)\n    breakpoint()",
        "type": "code",
        "location": "/tests/unittest_full_text_search_peewee_sqlite.py:43-68"
    },
    "4949": {
        "file_id": 647,
        "content": "Code adds a video to BilibiliVideoIndex, updates it, and searches for \"python world\" using BM25 algorithm. Limits search results to 2, then prints each result. Breakpoints inserted for debugging.",
        "type": "comment"
    },
    "4950": {
        "file_id": 648,
        "content": "/tests/unittest_get_subtid_name_and_majortid_name.py",
        "type": "filepath"
    },
    "4951": {
        "file_id": 648,
        "content": "The function `getMajorMinorTopicMappings` retrieves major and minor topic IDs and names, storing them in the `majorMinorMappings` dictionary. The code uses this function to get the associated topics for a given tid, formats them into tags, and prints the result along with the tid for topic ID 1.",
        "type": "summary"
    },
    "4952": {
        "file_id": 648,
        "content": "from bilibili_api import search\nBSP = search.bilibiliSearchParams\ndef getMajorMinorTopicMappings(debug: bool = False):\n    majorMinorMappings = {}\n    for key, value in BSP.all.tids.__dict__.items():\n        try:\n            major_tid = value.tid\n            if debug:\n                print(\"MAJOR\", key, major_tid)\n            content = {\"major\": {\"tid\": major_tid, \"name\": key}}\n            majorMinorMappings.update(\n                {major_tid: content, key: content, str(major_tid): content}\n            )\n            for subkey, subvalue in value.__dict__.items():\n                if subkey != \"tid\" and type(subvalue) == int:\n                    if debug:\n                        print(\"MINOR\", subkey, subvalue)\n                    content = {\n                        \"major\": {\"tid\": major_tid, \"name\": key},\n                        \"minor\": {\"tid\": subvalue, \"name\": subkey},\n                    }\n                    majorMinorMappings.update(\n                        {subvalue: content, subkey: content, str(subvalue): content}",
        "type": "code",
        "location": "/tests/unittest_get_subtid_name_and_majortid_name.py:1-26"
    },
    "4953": {
        "file_id": 648,
        "content": "This function `getMajorMinorTopicMappings` retrieves major and minor topic IDs and names from `BSP.all.tids` dictionary, storing them in `majorMinorMappings` dictionary for further use. It also prints the major and minor topics if debug mode is enabled.",
        "type": "comment"
    },
    "4954": {
        "file_id": 648,
        "content": "                    )\n        except:\n            pass\n    return majorMinorMappings\ndef getTagStringFromTid(tid):\n    majorMinorTopicMappings = getMajorMinorTopicMappings()\n    topic = majorMinorTopicMappings.get(tid, None)\n    tags = []\n    if topic:\n        majorTopic = topic.get(\"major\", {}).get(\"name\", None)\n        minorTopic = topic.get(\"minor\", {}).get(\"name\", None)\n        if majorTopic:\n            tags.append(majorTopic)\n            if minorTopic:\n                tags.append(minorTopic)\n    return \",\".join(tags)\ntid = 1\ntagString = getTagStringFromTid(tid)\nprint(tid, tagString)",
        "type": "code",
        "location": "/tests/unittest_get_subtid_name_and_majortid_name.py:27-49"
    },
    "4955": {
        "file_id": 648,
        "content": "This code retrieves the major and minor topics associated with a given topic ID (tid) using the getMajorMinorTopicMappings() function. It then formats these topics into a comma-separated string of tags. If there are both a major and minor topic, they are concatenated in that order, else if only one exists, it is printed alone. Finally, the tid and associated tagString are printed to the console for the given topic ID 1.",
        "type": "comment"
    },
    "4956": {
        "file_id": 649,
        "content": "/tests/unittest_houghline_dog_blur_detection.py",
        "type": "filepath"
    },
    "4957": {
        "file_id": 649,
        "content": "The code imports libraries, reads an image, applies blur detection and edge detection, displays edges with lines based on Hough line detection using OpenCV, waits for a key press to close the window.",
        "type": "summary"
    },
    "4958": {
        "file_id": 649,
        "content": "from lazero.utils.importers import cv2_custom_build_init\ncv2_custom_build_init()\nimport cv2\nimport numpy as np\n# command used for reading an image from the disk, cv2.imread function is used\nimagePath = \"/root/Desktop/works/pyjom/samples/image/dog_blue_sky_split_line.png\"\n# cannot find image without dark/black boundaries.\n# use blur detection, both for blur area removal and motion blur detection for key frame sampling/filtering\n# tool for finding non-blur based black borders:\n# ffmpeg -loop 1 -i /root/Desktop/works/pyjom/samples/image/dog_blue_sky_split_line.png -t 15 -vf cropdetect -f null -\n# maybe you can change the seconds to something shorter.\nimg1 = cv2.imread(imagePath)\n# gray1 = cv2.cvtColor(img1,cv2.COLOR_BGR2GRAY)\n# edges1 = cv2.Canny(gray1,50,150,apertureSize=3)\n# blurred = cv2.GaussianBlur(img1, (5, 5), 0)\nblurred = cv2.bilateralFilter(img1, 15, 75, 75)\nedges1 = cv2.Canny(blurred, 20, 210, apertureSize=3)\ncv2.imshow(\"EDGE\", edges1)\ncv2.waitKey(0)\nlines1 = cv2.HoughLines(edges1, 1, np.pi / 180, 200)  # wtf?",
        "type": "code",
        "location": "/tests/unittest_houghline_dog_blur_detection.py:1-28"
    },
    "4959": {
        "file_id": 649,
        "content": "The code imports necessary libraries, reads an image from disk, applies blur detection using bilateral filtering to remove blur and detect motion blur, converts the image to grayscale, detects edges using Canny edge detection, displays the edges, applies HoughLines to find lines in the image, and then waits for a key press to close the window.",
        "type": "comment"
    },
    "4960": {
        "file_id": 649,
        "content": "for rho, theta in lines1[0]:\n    a = np.cos(theta)\n    b = np.sin(theta)\n    x = a * rho\n    y = b * rho\n    x_1 = int(x + 1000 * (-b))\n    y_1 = int(y + 1000 * (a))\n    x_2 = int(x - 1000 * (-b))\n    y_2 = int(y - 1000 * (a))\n    cv2.line(img1, (x_1, y_1), (x_2, y_2), (0, 0, 255), 2)\n# Creation of a GUI window in order to display the image on the screen\ncv2.imshow(\"line detection\", img1)\n# cv2.waitKey method used for holding the window on screen\ncv2.waitKey(0)\ncv2.destroyAllWindows()",
        "type": "code",
        "location": "/tests/unittest_houghline_dog_blur_detection.py:29-43"
    },
    "4961": {
        "file_id": 649,
        "content": "This code generates lines on an image based on Hough line detection. It iterates over the lines, calculates the coordinates of endpoints, and draws lines using OpenCV. The GUI window displays the image with the drawn lines, holds it open for any key press (cv2.waitKey), then destroys all windows upon closing.",
        "type": "comment"
    },
    "4962": {
        "file_id": 650,
        "content": "/tests/unittest_lazero_external_downloader.py",
        "type": "filepath"
    },
    "4963": {
        "file_id": 650,
        "content": "This code downloads a file from a given URL and saves it to a specified path. It checks if the file already exists at the target location, and if so, deletes it before initiating the download. The success or failure of the download is reported using print statement.",
        "type": "summary"
    },
    "4964": {
        "file_id": 650,
        "content": "from lazero.network.downloader import download\nurl = \"https://media3.giphy.com/media/wTrXRamYhQzsY/giphy.gif?cid=dda24d502m79hkss38jzsxteewhs4e3ocd3iqext2285a3cq&rid=giphy.gif&ct=g\"\npath = \"/dev/shm/medialang/test.gif\"\nimport os\nif os.path.exists(path):\n    os.remove(path)\nreport = download(url, path)\nprint(\"download success?\", report)",
        "type": "code",
        "location": "/tests/unittest_lazero_external_downloader.py:1-14"
    },
    "4965": {
        "file_id": 650,
        "content": "This code downloads a file from a given URL and saves it to a specified path. It checks if the file already exists at the target location, and if so, deletes it before initiating the download. The success or failure of the download is reported using print statement.",
        "type": "comment"
    },
    "4966": {
        "file_id": 651,
        "content": "/tests/unittest_mathlib_ranges_continual.py",
        "type": "filepath"
    },
    "4967": {
        "file_id": 651,
        "content": "This code imports necessary modules and defines input data. It then uses different functions to get results and prints them, with some additional operations like range extraction and rendering. The code seems to be focused on merging ranges of data using a specific library (mathlib) and performing certain operations with the merged data.",
        "type": "summary"
    },
    "4968": {
        "file_id": 651,
        "content": "from test_commons import *\nfrom pyjom.mathlib import *\ninputList = [[(0, 1), (1, 1.1), (2, 3)], [(0.5, 1.5), (1.6, 2.5)]]\nmRangesDict = {\"sample_%s\" % num: inputList[num] for num in range(len(inputList))}\nresult_0 = getContinualNonSympyMergeResult(inputList)\nprint(result_0)\nprint(\"_\" * 20)\n# want to build a language?\nresult_1 = getContinualMappedNonSympyMergeResult(mRangesDict, concatSymbol=\"|\")\nprint(result_1)\nprint(\"_\" * 20)\nresult_2 = getContinualMappedNonSympyMergeResult(\n    mRangesDict, concatSymbol=\"|\", noEmpty=False\n)\nprint(result_2)\nprint(\"_\" * 20)\nstart, end = -1, 4\nresult_3 = getContinualMappedNonSympyMergeResultWithRangedEmpty(\n    mRangesDict, start, end, concatSymbol=\"|\"\n)\nprint(result_3)\nprint(\"_\" * 20)\nrenderList = mergedRangesToSequential(result_3)\nfor renderCommandString, commandTimeSpan in renderList:\n    print(renderCommandString, commandTimeSpan)\nprint(\"_\" * 20)\nfinalCatsMapped = getContinualMappedNonSympyMergeResult({})\nprint(finalCatsMapped)",
        "type": "code",
        "location": "/tests/unittest_mathlib_ranges_continual.py:1-36"
    },
    "4969": {
        "file_id": 651,
        "content": "This code imports necessary modules and defines input data. It then uses different functions to get results and prints them, with some additional operations like range extraction and rendering. The code seems to be focused on merging ranges of data using a specific library (mathlib) and performing certain operations with the merged data.",
        "type": "comment"
    },
    "4970": {
        "file_id": 652,
        "content": "/tests/unittest_music_recognition.py",
        "type": "filepath"
    },
    "4971": {
        "file_id": 652,
        "content": "This code tests the music recognition functionality of different backends (songrec, shazamio, and midomi) by calling the recognizeMusicFromFile function and logging the results. The filepath variable stores the path to the test audio file. The methods list determines which backends will be tested. After each test, there is a 3-second pause before moving on to the next backend.",
        "type": "summary"
    },
    "4972": {
        "file_id": 652,
        "content": "from test_commons import *\nfrom pyjom.musictoolbox import recognizeMusicFromFile\nfrom lazero.utils.logger import sprint\nfilepath = (\n    # \"/root/Desktop/works/pyjom/tests/music_recognization/exciting_bgm_cut_10seconds.mp3\"\n    \"/root/Desktop/works/pyjom/tests/music_analysis/exciting_bgm.mp3\"\n)\n# methods = [\"midomi\"]\nmethods = [\"songrec\", \"shazamio\", \"midomi\"]\nimport time\nfor method in methods:\n    result = recognizeMusicFromFile(filepath, backend=method, debug=True)\n    sprint(\"RESULT:\", result)\n    time.sleep(3)",
        "type": "code",
        "location": "/tests/unittest_music_recognition.py:1-16"
    },
    "4973": {
        "file_id": 652,
        "content": "This code tests the music recognition functionality of different backends (songrec, shazamio, and midomi) by calling the recognizeMusicFromFile function and logging the results. The filepath variable stores the path to the test audio file. The methods list determines which backends will be tested. After each test, there is a 3-second pause before moving on to the next backend.",
        "type": "comment"
    },
    "4974": {
        "file_id": 653,
        "content": "/tests/unittest_musictoolbox_netease_music_lyric.py",
        "type": "filepath"
    },
    "4975": {
        "file_id": 653,
        "content": "This code tests the functionality of getting music and lyrics using NeteaseMusic API from the pyjom library. It checks for similarity in keywords and prints the result, but encounters an issue when no lyrics are found. It then plans to test a specific music ID for lyrics retrieval and shows preparedness in case it fails.",
        "type": "summary"
    },
    "4976": {
        "file_id": 653,
        "content": "from test_commons import *\nfrom pyjom.musictoolbox import neteaseMusic\nNMClient = neteaseMusic()\n# import random\nquery = \"linkin park numb\"\nfor sim in [False, True]:\n    result = NMClient.getMusicAndLyricWithKeywords(query, similar=sim, debug=True)\n    print(\"similar?\", sim)\n    # no lyrics! wtf??\n    breakpoint()\n# now let's test something surely will get lyrics.\n# music_id = 497572729\n# lyric_string = NMClient.getMusicLyricFromNetease(music_id)\n# print(\"LYRIC STRING:\",lyric_string)\n# in case we don't get the lyric, you should be prepared.\n# it works.",
        "type": "code",
        "location": "/tests/unittest_musictoolbox_netease_music_lyric.py:1-17"
    },
    "4977": {
        "file_id": 653,
        "content": "This code tests the functionality of getting music and lyrics using NeteaseMusic API from the pyjom library. It checks for similarity in keywords and prints the result, but encounters an issue when no lyrics are found. It then plans to test a specific music ID for lyrics retrieval and shows preparedness in case it fails.",
        "type": "comment"
    },
    "4978": {
        "file_id": 654,
        "content": "/tests/unittest_nsfw_video_score.py",
        "type": "filepath"
    },
    "4979": {
        "file_id": 654,
        "content": "The code utilizes a trained model to detect NSFW content in videos and images, ensuring compliance by posting non-sexual content through an API. It stores classification probabilities and handles exceptions for unknown test_flags. However, only GIFs can be posted currently with caution about picture stretching.",
        "type": "summary"
    },
    "4980": {
        "file_id": 654,
        "content": "# we take max for the concerned ones, and take mean for the unconcerned ones.\nfrom test_commons import *\nimport requests\nfrom lazero.network.checker import waitForServerUp\nfrom pyjom.videotoolbox import getVideoFrameIteratorWithFPS\nfrom typing import Literal\ngateway = \"http://localhost:8511/\"\nfrom pyjom.mathlib import superMean, superMax\nfrom lazero.utils.importers import cv2_custom_build_init\ncv2_custom_build_init()\nimport cv2\n# suggest you not to use this shit.\n# import math\nfrom pyjom.imagetoolbox import resizeImageWithPadding, scanImageWithWindowSizeAutoResize\nfrom lazero.filesystem import tmpdir, tmpfile\ntmpdirPath = \"/dev/shm/medialang/nsfw\"\nimport uuid\nwaitForServerUp(8511, \"nsfw nodejs server\")\nimport os\ntest_flag = \"nsfw_video\"\n# test_flag = \"nsfw_image\"\n# test_flag = \"scanning\"\n# test_flag = \"paddinging\"\nsource = \"/root/Desktop/works/pyjom/samples/video/cute_cat_gif.gif\"\nimport numpy as np\ndef processNSFWServerImageReply(reply):\n    mDict = {}\n    for elem in reply:\n        className, probability = elem[\"className\"], elem[\"probability\"]",
        "type": "code",
        "location": "/tests/unittest_nsfw_video_score.py:1-44"
    },
    "4981": {
        "file_id": 654,
        "content": "The code imports necessary libraries, initializes certain functions and variables, and defines the processNSFWServerImageReply function which processes image classification reply from the server. It is for testing NSFW detection in videos or images, with options to test different aspects such as scanning, padding, etc. Note that it may not be recommended to use some parts of the code.",
        "type": "comment"
    },
    "4982": {
        "file_id": 654,
        "content": "        mDict.update({className: probability})\n    return mDict\ndef processNSFWReportArray(\n    NSFWReportArray,\n    average_classes=[\"Neutral\"],\n    get_max_classes=[\"Drawing\", \"Porn\", \"Sexy\", \"Hentai\"],\n):\n    assert set(average_classes).intersection(set(get_max_classes)) == set()\n    NSFWReport = {}\n    for element in NSFWReportArray:\n        for key in element.keys():\n            NSFWReport[key] = NSFWReport.get(key, []) + [element[key]]\n    for average_class in average_classes:\n        NSFWReport[average_class] = superMean(NSFWReport.get(average_class, [0]))\n    for get_max_class in get_max_classes:\n        NSFWReport[get_max_class] = superMax(NSFWReport.get(get_max_class, [0]))\n    return NSFWReport\nfrom pyjom.commons import checkMinMaxDict\n# you can reuse this, really.\ndef NSFWFilter(\n    NSFWReport,\n    filter_dict={\n        \"Neutral\": {\"min\": 0.5},\n        \"Sexy\": {\"max\": 0.5},\n        \"Porn\": {\"max\": 0.5},\n        \"Hentai\": {\"max\": 0.5},\n        \"Drawing\": {\"max\": 0.5},\n    },\n    debug=False,\n):\n    for key in filter_dict:",
        "type": "code",
        "location": "/tests/unittest_nsfw_video_score.py:45-80"
    },
    "4983": {
        "file_id": 654,
        "content": "This code processes an NSFW report array and returns a filtered dictionary. It updates the dictionary with class names as keys and their corresponding probabilities. Then, it calculates the average and maximum scores for certain classes. Lastly, it applies filters to the resulting dictionary based on specified minimum or maximum threshold values for each class.",
        "type": "comment"
    },
    "4984": {
        "file_id": 654,
        "content": "        value = NSFWReport.get(key, 0)\n        key_filter = filter_dict[key]\n        result = checkMinMaxDict(value, key_filter)\n        if not result:\n            if debug:\n                print(\"not passing NSFW filter: %s\" % key)\n                print(\"value: %s\" % value)\n                print(\"filter: %s\" % str(key_filter))\n            return False\n    return True\nif test_flag == \"padding\":\n    for frame in getVideoFrameIteratorWithFPS(source, -1, -1, fps=1):\n        image = resizeImageWithPadding(frame, 1280, 720, border_type=\"replicate\")\n        # i'd like to view this.\n        cv2.imshow(\"PADDED\", image)\n        cv2.waitKey(0)\nelif test_flag == \"scanning\":\n    for frame in getVideoFrameIteratorWithFPS(source, -1, -1, fps=1):\n        scanned_array = scanImageWithWindowSizeAutoResize(\n            frame, 1280, 720, threshold=0.3\n        )\n        for index, image in enumerate(scanned_array):\n            cv2.imshow(\"SCANNED %d\" % index, image)\n            cv2.waitKey(0)\nelif test_flag == \"nsfw_video\":\n    # use another source?",
        "type": "code",
        "location": "/tests/unittest_nsfw_video_score.py:81-108"
    },
    "4985": {
        "file_id": 654,
        "content": "The code snippet checks if a video passes the NSFW filter based on certain key values, and then displays the video frames in different scenarios: when testing for padding, it shows each frame with padding; when testing for scanning, it displays each frame after scanning with a specified threshold; and if test_flag is set to \"nsfw_video\", it processes another source.",
        "type": "comment"
    },
    "4986": {
        "file_id": 654,
        "content": "    with tmpdir(path=tmpdirPath) as T:\n        responses = []\n        for frame in getVideoFrameIteratorWithFPS(source, -1, -1, fps=1):\n            padded_resized_frame = resizeImageWithPadding(\n                frame, 224, 224, border_type=\"replicate\"\n            )\n            # i'd like to view this.\n            basename = \"{}.jpg\".format(uuid.uuid4())\n            jpg_path = os.path.join(tmpdirPath, basename)\n            with tmpfile(path=jpg_path) as TF:\n                cv2.imwrite(jpg_path, padded_resized_frame)\n                files = {\"image\": (basename, open(jpg_path, \"rb\"), \"image/jpeg\")}\n                r = requests.post(\n                    gateway + \"nsfw\", files=files\n                )  # post gif? or just jpg?\n                try:\n                    response_json = r.json()\n                    response_json = processNSFWServerImageReply(response_json)\n                    # breakpoint()\n                    # print(\"RESPONSE:\", response_json)\n                    responses.append(\n                        response_json  # it contain 'messages'",
        "type": "code",
        "location": "/tests/unittest_nsfw_video_score.py:109-130"
    },
    "4987": {
        "file_id": 654,
        "content": "This code is looping through video frames, resizing and saving them as JPEGs in a temporary directory. It then posts each image to an API endpoint for NSFW content classification and appends the response JSON to a list of responses. The breakpoint and print statement are optional for debugging purposes.",
        "type": "comment"
    },
    "4988": {
        "file_id": 654,
        "content": "                    )  # there must be at least one response, i suppose?\n                except:\n                    import traceback\n                    traceback.print_exc()\n                    print(\"error when processing NSFW server response\")\n        NSFWReport = processNSFWReportArray(responses)\n        # print(NSFWReport)\n        # breakpoint()\n        result = NSFWFilter(NSFWReport)\n        if result:\n            print(\"NSFW test passed.\")\n            print(\"source %s\" % source)\n# we don't want drawing dogs.\n# [{'className': 'Neutral', 'probability': 0.9995943903923035}, {'className': 'Drawing', 'probability': 0.00019544694805517793}, {'className': 'Porn', 'probability': 0.00013213469355832785}, {'className': 'Sexy', 'probability': 6.839347042841837e-05}, {'className': 'Hentai', 'probability': 9.632151886762585e-06}]\nelif test_flag == \"nsfw_image\":\n    source = \"/root/Desktop/works/pyjom/samples/image/kitty_flash.bmp\"\n    # RESPONSE: [{'className': 'Neutral', 'probability': 0.9997681975364685}",
        "type": "code",
        "location": "/tests/unittest_nsfw_video_score.py:131-149"
    },
    "4989": {
        "file_id": 654,
        "content": "The code processes a server response for NSFW content classification and checks if the test passed. It uses the processNSFWReportArray function to analyze the responses and stores the result in the variable NSFWReport. If there's at least one response, it proceeds with the NSFWFilter function to evaluate the report. If the result is true, it prints \"NSFW test passed\" and source information. The code includes a case for the NSFW_IMAGE test flag and specifies a source file path.",
        "type": "comment"
    },
    "4990": {
        "file_id": 654,
        "content": ", {'className': 'Drawing', 'probability': 0.0002115015813615173}, {'className': 'Porn', 'probability': 1.3146535820851568e-05}, {'className': 'Hentai', 'probability': 4.075543984072283e-06}, {'className': 'Sexy', 'probability': 3.15313491228153e-06}]\n    # source = '/root/Desktop/works/pyjom/samples/image/pig_really.bmp'\n    # RESPONSE: [{'className': 'Neutral', 'probability': 0.9634107351303101}, {'className': 'Porn', 'probability': 0.0244674663990736}, {'className': 'Drawing', 'probability': 0.006115634460002184}, {'className': 'Hentai', 'probability': 0.003590137232095003}, {'className': 'Sexy', 'probability': 0.002416097791865468}]\n    # source = \"/root/Desktop/works/pyjom/samples/image/dog_with_text.bmp\"\n    # source = '/root/Desktop/works/pyjom/samples/image/dick2.jpeg'\n    # [{'className': 'Porn', 'probability': 0.7400921583175659}, {'className': 'Hentai', 'probability': 0.2109236866235733}, {'className': 'Sexy', 'probability': 0.04403943940997124}, {'className': 'Neutral', 'probability': 0.0034419416915625334}, {'className': 'Drawing', 'probability': 0.0015027812914922833}]",
        "type": "code",
        "location": "/tests/unittest_nsfw_video_score.py:149-154"
    },
    "4991": {
        "file_id": 654,
        "content": "This code demonstrates the results of a classification model for detecting different content categories in images. The provided examples show how the model predicts various probabilities for classes like 'Porn', 'Drawing', 'Hentai', and others, given specific image sources.",
        "type": "comment"
    },
    "4992": {
        "file_id": 654,
        "content": "    # source = '/root/Desktop/works/pyjom/samples/image/dick4.jpeg'\n    # RESPONSE: [{'className': 'Porn', 'probability': 0.8319052457809448}, {'className': 'Hentai', 'probability': 0.16578854620456696}, {'className': 'Sexy', 'probability': 0.002254955470561981}, {'className': 'Neutral', 'probability': 3.2827374525368214e-05}, {'className': 'Drawing', 'probability': 1.8473130694474094e-05}]\n    # source = '/root/Desktop/works/pyjom/samples/image/porn_shemale.jpeg'\n    # no good for this one. this is definitely some unacceptable shit, with just cloth wearing.\n    # RESPONSE: [{'className': 'Neutral', 'probability': 0.6256022453308105}, {'className': 'Hentai', 'probability': 0.1276213526725769}, {'className': 'Porn', 'probability': 0.09777139872312546}, {'className': 'Sexy', 'probability': 0.09318379312753677}, {'className': 'Drawing', 'probability': 0.05582122132182121}]\n    # source ='/root/Desktop/works/pyjom/samples/image/dick3.jpeg'\n    # [{'className': 'Porn', 'probability': 0.9784200787",
        "type": "code",
        "location": "/tests/unittest_nsfw_video_score.py:155-161"
    },
    "4993": {
        "file_id": 654,
        "content": "This code is testing the classification accuracy of an image classification model for NSFW content. The comments describe three test cases with different images and the corresponding classifications provided by the model, highlighting the need for improving the model's ability to accurately identify NSFW content.",
        "type": "comment"
    },
    "4994": {
        "file_id": 654,
        "content": "54425}, {'className': 'Hentai', 'probability': 0.01346961222589016}, {'className': 'Sexy', 'probability': 0.006554164923727512}, {'className': 'Neutral', 'probability': 0.0015426197787746787}, {'className': 'Drawing', 'probability': 1.354961841570912e-05}]\n    # a known source causing unwanted shits.\n    image = cv2.imread(source)\n    basename = \"{}.jpg\".format(uuid.uuid4())\n    jpg_path = os.path.join(tmpdirPath, basename)\n    with tmpfile(path=jpg_path) as TF:\n        # black padding will lower the probability of being porn.\n        padded_resized_frame = resizeImageWithPadding(image, 224, 224)\n        # RESPONSE: [{'className': 'Neutral', 'probability': 0.6441782116889954}, {'className': 'Porn', 'probability': 0.3301379978656769}, {'className': 'Sexy', 'probability': 0.010329035110771656}, {'className': 'Hentai', 'probability': 0.010134727694094181}, {'className': 'Drawing', 'probability': 0.005219993181526661}]\n        # padded_resized_frame = resizeImageWithPadding(image, 224, 224,border_type='replicate')",
        "type": "code",
        "location": "/tests/unittest_nsfw_video_score.py:161-170"
    },
    "4995": {
        "file_id": 654,
        "content": "This code reads an image from a known source, generates a unique filename, saves it temporarily, pads and resizes the image for classification, and then passes the processed image to the model for probability prediction. The goal is to lower the probability of being classified as porn by adding black padding around the image before processing.",
        "type": "comment"
    },
    "4996": {
        "file_id": 654,
        "content": "        # RESPONSE: [{'className': 'Neutral', 'probability': 0.6340386867523193}, {'className': 'Porn', 'probability': 0.3443007171154022}, {'className': 'Sexy', 'probability': 0.011606302112340927}, {'className': 'Hentai', 'probability': 0.006618513725697994}, {'className': 'Drawing', 'probability': 0.0034359097480773926}]\n        # neutral again? try porn!\n        cv2.imwrite(jpg_path, padded_resized_frame)\n        files = {\"image\": (basename, open(jpg_path, \"rb\"), \"image/jpeg\")}\n        r = requests.post(gateway + \"nsfw\", files=files)  # post gif? or just jpg?\n        print(\"RESPONSE:\", r.json())\nelse:\n    raise Exception(\"unknown test_flag: %s\" % test_flag)\n# you can only post gif now, or you want to post some other formats?\n# if you post shit, you know it will strentch your picture and produce unwanted shits.",
        "type": "code",
        "location": "/tests/unittest_nsfw_video_score.py:171-180"
    },
    "4997": {
        "file_id": 654,
        "content": "Code snippet is performing the following actions: \n1. Storing response from API containing classification probabilities for video.\n2. Writing frame to JPG format and posting it to gateway as non-sexual content using requests.\n3. If unknown test_flag, raising exception.\n4. Note mentions that only GIF can be posted now and caution about stretching pictures.",
        "type": "comment"
    },
    "4998": {
        "file_id": 655,
        "content": "/tests/unittest_ocr_filter_large_area_of_text.py",
        "type": "filepath"
    },
    "4999": {
        "file_id": 655,
        "content": "This code sets up libraries and variables for processing image or video files, detects text within frames using EasyOCRReader, calculates text area percentage, draws rectangles, and displays the result.",
        "type": "summary"
    }
}