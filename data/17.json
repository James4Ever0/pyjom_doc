{
    "1700": {
        "file_id": 153,
        "content": "from test_commons import *\nfrom pyjom.primitives import *  # this is capitalized.\nwbRev = FilesystemContentReviewer(dirpath=\"./samples/video/\")\nwbRev.main()",
        "type": "code",
        "location": "/tests/test_local_reviewer.py:1-5"
    },
    "1701": {
        "file_id": 153,
        "content": "This code imports necessary modules, initializes a FilesystemContentReviewer object with a directory path, and calls its main() method to perform content review on the specified directory.",
        "type": "comment"
    },
    "1702": {
        "file_id": 154,
        "content": "/tests/test_iterator_generator_wrapper_lazero_utils.py",
        "type": "filepath"
    },
    "1703": {
        "file_id": 154,
        "content": "This code tests the functionality of lazero's iteratorWrapper with different parameters such as init_repeat, repeat, and max_iter. It compares the generated results to predefined objective lists for validation.",
        "type": "summary"
    },
    "1704": {
        "file_id": 154,
        "content": "from lazero.utils.tools import iteratorWrapper, flattenUnhashableList\nsequence = [i for i in range(10)]\nINIT_REPEAT = 3\nobjective_init_repeat = [sequence[0]] * INIT_REPEAT + sequence\nREPEAT = 2\nobjective_repeat = [sequence[0]] * INIT_REPEAT + flattenUnhashableList(\n    list(zip(*([sequence] * (1 + REPEAT))))\n)\nMAX_ITER = 4\nobjective_max_iter = [sequence[0]] * INIT_REPEAT + flattenUnhashableList(\n    list(zip(*([sequence[:MAX_ITER]] * (1 + REPEAT))))\n)\ndef test_init_repeat():\n    result = list(iteratorWrapper((s for s in sequence), init_repeat=INIT_REPEAT))\n    assert result == objective_init_repeat\ndef test_repeat():\n    result = list(\n        iteratorWrapper((s for s in sequence), init_repeat=INIT_REPEAT, repeat=REPEAT)\n    )\n    assert result == objective_repeat\ndef test_max_iter():\n    result = list(\n        iteratorWrapper(\n            (s for s in sequence),\n            init_repeat=INIT_REPEAT,\n            repeat=REPEAT,\n            max_iter=MAX_ITER,\n        )\n    )\n    assert result == objective_max_iter",
        "type": "code",
        "location": "/tests/test_iterator_generator_wrapper_lazero_utils.py:1-42"
    },
    "1705": {
        "file_id": 154,
        "content": "This code tests the functionality of lazero's iteratorWrapper with different parameters such as init_repeat, repeat, and max_iter. It compares the generated results to predefined objective lists for validation.",
        "type": "comment"
    },
    "1706": {
        "file_id": 155,
        "content": "/tests/test_medialang.py",
        "type": "filepath"
    },
    "1707": {
        "file_id": 155,
        "content": "This code imports necessary modules, defines test paths, and iterates through each path. It creates a Medialang object with the specified script path and prettifies it in-place.",
        "type": "summary"
    },
    "1708": {
        "file_id": 155,
        "content": "from test_commons import *\nfrom pyjom.medialang.core import *\nimport os\ntestpaths = [\n    \"processor_demo.mdl\",\n    \"processor_multi.mdl\",\n    \"recipe.mdl\",\n    \"audiolang.mdl\",\n    \"videolang.mdl\",\n]\n# testcontent = open(testpath,\"r\").read()\nfor path in testpaths:\n    testpath = os.path.join(\"/root/Desktop/works/pyjom/test/\", path)\n    mdl = Medialang(script_path=testpath)  # will be parsed.\n    mdl.prettify(inplace=True)",
        "type": "code",
        "location": "/tests/test_medialang.py:1-18"
    },
    "1709": {
        "file_id": 155,
        "content": "This code imports necessary modules, defines test paths, and iterates through each path. It creates a Medialang object with the specified script path and prettifies it in-place.",
        "type": "comment"
    },
    "1710": {
        "file_id": 156,
        "content": "/tests/test_dummy.sh",
        "type": "filepath"
    },
    "1711": {
        "file_id": 156,
        "content": "This code is executing a Python script named \"test_dummy.py\" using the default installed Python3 interpreter. It's likely being run in a Unix-like environment as it uses \"python3\" instead of \"python\". The purpose of running this script might be for testing, debugging or execution of the code within \"test_dummy.py\".",
        "type": "summary"
    },
    "1712": {
        "file_id": 156,
        "content": "python3 test_dummy.py",
        "type": "code",
        "location": "/tests/test_dummy.sh:1-1"
    },
    "1713": {
        "file_id": 156,
        "content": "This code is executing a Python script named \"test_dummy.py\" using the default installed Python3 interpreter. It's likely being run in a Unix-like environment as it uses \"python3\" instead of \"python\". The purpose of running this script might be for testing, debugging or execution of the code within \"test_dummy.py\".",
        "type": "comment"
    },
    "1714": {
        "file_id": 157,
        "content": "/tests/test_dummy.py",
        "type": "filepath"
    },
    "1715": {
        "file_id": 157,
        "content": "This code imports necessary modules, initializes a ContentProducer and ContentReviewer objects, runs their main methods, and prints their identifier data. It tests content production and reviewing functionality.",
        "type": "summary"
    },
    "1716": {
        "file_id": 157,
        "content": "from test_commons import *\nfrom pyjom.main import *\nproducer = ContentProducer()\nproducer.main()\nprint(producer.identifier.data)\nreviewer = ContentReviewer()\nreviewer.main()\nprint(reviewer.identifier.data)",
        "type": "code",
        "location": "/tests/test_dummy.py:1-10"
    },
    "1717": {
        "file_id": 157,
        "content": "This code imports necessary modules, initializes a ContentProducer and ContentReviewer objects, runs their main methods, and prints their identifier data. It tests content production and reviewing functionality.",
        "type": "comment"
    },
    "1718": {
        "file_id": 158,
        "content": "/tests/unittest_ffmpeg_cropdetect_from_image_parse_log.py",
        "type": "filepath"
    },
    "1719": {
        "file_id": 158,
        "content": "This code uses ffmpeg and OpenCV to detect cropped areas, calculates the cropped area ratio, and decides whether to crop the image based on a threshold. The result depends on the specified threshold value.",
        "type": "summary"
    },
    "1720": {
        "file_id": 158,
        "content": "import ffmpeg\nfrom lazero.utils.importers import cv2_custom_build_init\ncv2_custom_build_init()\n# mediaPath = \"/root/Desktop/works/pyjom/samples/image/dog_blue_sky_split_line.png\"\nmediaPath = \"/root/Desktop/works/pyjom/samples/image/dog_with_black_borders.png\"  # use the image with black background.\n# ffmpeg -loop 1 -i /root/Desktop/works/pyjom/samples/image/dog_blue_sky_split_line.png -t 15 -vf cropdetect -f null -\nimport cv2\nimage = cv2.imread(mediaPath)\nheight, width = image.shape[:2]\ntotal_area = height * width\nareaThreshold = 0\nstdout, stderr = (\n    ffmpeg.input(mediaPath, loop=1, t=15)\n    .filter(\"cropdetect\")\n    .output(\"null\", f=\"null\")\n    .run(capture_stdout=True, capture_stderr=True)\n)\nstdout_decoded = stdout.decode(\"utf-8\")\nstderr_decoded = stderr.decode(\"utf-8\")\n# nothing here.\n# for line in stdout_decoded.split(\"\\n\"):\n#     print(line)\n# breakpoint()\nimport parse\ncropped_area_threshold = 0.1\ncommon_crops = []\nfor line in stderr_decoded.split(\"\\n\"):\n    line = line.replace(\"\\n\", \"\").strip()\n    for",
        "type": "code",
        "location": "/tests/unittest_ffmpeg_cropdetect_from_image_parse_log.py:1-40"
    },
    "1721": {
        "file_id": 158,
        "content": "The code imports necessary libraries and initializes them, sets the media path to an image with a black background, runs ffmpeg on the image with a cropdetect filter, decodes the output and errors, iterates over the stderr output lines to extract cropped areas, and defines a variable for common_crops.",
        "type": "comment"
    },
    "1722": {
        "file_id": 158,
        "content": "matString = \"[{}] x1:{x1:d} x2:{x2:d} y1:{y1:d} y2:{y2:d} w:{w:d} h:{h:d} x:{x:d} y:{y:d} pts:{pts:g} t:{t:g} crop={}:{}:{}:{}\"\n    # print(line)\n    result = parse.parse(formatString, line)\n    if result is not None:\n        # print(result)\n        cropString = \"{}_{}_{}_{}\".format(\n            *[result[key] for key in [\"w\", \"h\", \"x\", \"y\"]]\n        )\n        # print(cropString)\n        # breakpoint()\n        common_crops.append(cropString)\n    # [Parsed_cropdetect_0 @ 0x56246a16cbc0] x1:360 x2:823 y1:0 y2:657 w:464 h:656 x:360 y:2 pts:3 t:0.120000 crop=464:656:360:2\n    # this crop usually will never change. but let's count?\narea = 0\nx, x1, y, y1 = 0, width, 0, height\nif len(common_crops) > 0:\n    common_crops_count_tuple_list = [\n        (cropString, common_crops.count(cropString)) for cropString in set(common_crops)\n    ]\n    common_crops_count_tuple_list.sort(key=lambda x: -x[1])\n    selected_crop_string = common_crops_count_tuple_list[0][0]\n    result = parse.parse(\"{w:d}_{h:d}_{x:d}_{y:d}\", selected_crop_string)",
        "type": "code",
        "location": "/tests/unittest_ffmpeg_cropdetect_from_image_parse_log.py:40-62"
    },
    "1723": {
        "file_id": 158,
        "content": "Code parses a log line, extracts crop information and stores it in common_crops list. It then counts the occurrence of each unique crop string and selects the most frequent one (selected_crop_string). Finally, it parses the selected_crop_string to get the crop dimensions (w, h, x, y) and assigns them to their respective variables.",
        "type": "comment"
    },
    "1724": {
        "file_id": 158,
        "content": "    w, h, x, y = [result[key] for key in [\"w\", \"h\", \"x\", \"y\"]]\n    x1, y1 = min(x + w, width), min(y + h, height)\n    if x < x1 and y < y1:\n        # allow to calculate the area.\n        area = (x1 - x) * (y1 - y)\ncropped_area_ratio = 1 - (area / total_area)  # 0.5652352766414517\n# use 0.1 as threshold?\nprint(\"CROPPED AREA RATIO:\", cropped_area_ratio)\nif cropped_area_ratio > cropped_area_threshold:\n    print(\"we need to crop this. no further processing needed\")\n    image_black_cropped = image[y:y1, x:x1]\n    cv2.imshow(\"CROPPED IMAGE\", image_black_cropped)\n    cv2.waitKey(0)\nelse:\n    print(\"image no need to crop black borders. further processing needed\")",
        "type": "code",
        "location": "/tests/unittest_ffmpeg_cropdetect_from_image_parse_log.py:63-78"
    },
    "1725": {
        "file_id": 158,
        "content": "This code calculates the cropped area ratio of an image and decides whether to crop it or not based on a threshold. If the ratio is greater than the threshold, it crops the image using OpenCV and displays the cropped image. Otherwise, it proceeds with further processing. The result depends on the specified threshold value.",
        "type": "comment"
    },
    "1726": {
        "file_id": 159,
        "content": "/tests/test_bilibili_resolve_tid.py",
        "type": "filepath"
    },
    "1727": {
        "file_id": 159,
        "content": "This code imports necessary modules, sets tid to 217, and calls resolveSubTidsFromTid function with the tid. The returned result is then printed. It tests resolving subTids from a given tid in Bilibili platform's database.",
        "type": "summary"
    },
    "1728": {
        "file_id": 159,
        "content": "from test_commons import *\nfrom pyjom.platforms.bilibili.database import resolveSubTidsFromTid\ntid = 217\nresult = resolveSubTidsFromTid(tid)\nprint(\"RESULT?\", result)",
        "type": "code",
        "location": "/tests/test_bilibili_resolve_tid.py:1-6"
    },
    "1729": {
        "file_id": 159,
        "content": "This code imports necessary modules, sets tid to 217, and calls resolveSubTidsFromTid function with the tid. The returned result is then printed. It tests resolving subTids from a given tid in Bilibili platform's database.",
        "type": "comment"
    },
    "1730": {
        "file_id": 160,
        "content": "/tests/unittest_ffmpeg_args.py",
        "type": "filepath"
    },
    "1731": {
        "file_id": 160,
        "content": "The code processes video files using FFmpeg for tasks like cropping and scaling, with a specific command to map and filter video/audio streams. This is part of a larger script that uses the subprocess module.",
        "type": "summary"
    },
    "1732": {
        "file_id": 160,
        "content": "command_original = [\n    \"ffmpeg\",\n    \"-y\",\n    \"-ss\",\n    \"0\",\n    \"-to\",\n    \"59.3942553191489\",\n    \"-i\",\n    \"/root/Desktop/works/pyjom/samples/video/LiGlReJ4i.mp4\",\n    \"-ss\",\n    \"59.3942553191489\",\n    \"-to\",\n    \"62.0340000000000\",\n    \"-i\",\n    \"/root/Desktop/works/pyjom/samples/video/LiGlReJ4i.mp4\",\n    \"-ss\",\n    \"0\",\n    \"-to\",\n    \"62.034\",\n    \"-i\",\n    \"/root/Desktop/works/pyjom/samples/video/LiGlReJ4i.mp4\",\n    \"-filter_complex\",\n    \"[0:v]crop=h=1099:w=717:x=1:y=72[s0];[s0]pad=color=black:height=max(ih\\\\, ceil(iw*max(1080/1920\\\\, ih/iw))):width=max(iw\\\\, ceil(ih*max(1920/1080\\\\, iw/ih))):x=floor((max(iw\\\\, ceil(ih*max(1920/1080\\\\, iw/ih)))-iw)/2):y=floor((max(ih\\\\, ceil(iw*max(1080/1920\\\\, ih/iw)))-ih)/2)[s1];[s1]scale=1920:1080[s2];[s2]scale=ceil((iw*0.15555555555555556)/4)*4:ceil((ih*0.15555555555555556)/4)*4[s3];[1:v]pad=color=black:height=max(ih\\\\, ceil(iw*max(1080/1920\\\\, ih/iw))):width=max(iw\\\\, ceil(ih*max(1920/1080\\\\, iw/ih))):x=floor((max(iw\\\\, ceil(ih*max(1920/1080\\\\, iw/ih)))-iw",
        "type": "code",
        "location": "/tests/unittest_ffmpeg_args.py:1-23"
    },
    "1733": {
        "file_id": 160,
        "content": "This code uses FFmpeg to split a video file into segments, applies various filters and transformations to the segments, and finally scales and pads them before saving the final output.",
        "type": "comment"
    },
    "1734": {
        "file_id": 160,
        "content": ")/2):y=floor((max(ih\\\\, ceil(iw*max(1080/1920\\\\, ih/iw)))-ih)/2)[s4];[s4]scale=1920:1080[s5];[s5]scale=ceil((iw*0.15555555555555556)/4)*4:ceil((ih*0.15555555555555556)/4)*4[s6];[s3][s6]concat=n=2[s7]\",\n    \"-map\",\n    \"[s7]\",\n    \"-map\",\n    \"2:a\",\n    \"/dev/shm/2c6b1466-6186-41dd-9ce3-2f757c082c5a.mp4\",\n]\ncommand2 = [\n    \"ffmpeg\",\n    \"-y\",\n    \"-ss\",\n    \"0\",\n    \"-to\",\n    \"59.3942553191489\",\n    \"-i\",\n    \"/root/Desktop/works/pyjom/samples/video/LiGlReJ4i.mp4\",\n    \"-ss\",\n    \"0\",\n    \"-to\",\n    \"62.034\",\n    \"-i\",\n    \"/root/Desktop/works/pyjom/samples/video/LiGlReJ4i.mp4\",\n    \"-filter_complex\",\n    \"[0:v]crop=h=1099:w=717:x=1:y=72[s0];[s0]pad=color=black:height=max(ih\\\\, ceil(iw*max(1080/1920\\\\, ih/iw))):width=max(iw\\\\, ceil(ih*max(1920/1080\\\\, iw/ih))):x=floor((max(iw\\\\, ceil(ih*max(1920/1080\\\\, iw/ih)))-iw)/2):y=floor((max(ih\\\\, ceil(iw*max(1080/1920\\\\, ih/iw)))-ih)/2)[s1];[s1]scale=1920:1080[s2];[s2]scale=ceil((iw*0.15555555555555556)/4)*4:ceil((ih*0.15555555555555556)/4)*4[s3]\",\n    \"-map\",\n    \"[s3]\",",
        "type": "code",
        "location": "/tests/unittest_ffmpeg_args.py:23-48"
    },
    "1735": {
        "file_id": 160,
        "content": "The code is constructing a command for the ffmpeg tool to process and concatenate multiple video inputs. It applies filters such as cropping, padding, scaling, and extracts specific parts of videos before concatenating them into a single output video file. The resulting command is being stored in `command1` and `command2`.",
        "type": "comment"
    },
    "1736": {
        "file_id": 160,
        "content": "    \"-map\",\n    \"1:a\",\n    \"/dev/shm/2c6b1466-6186-41dd-9ce3-2f757c082c5a.mp4\",\n]\ncommand3 = [\n    \"ffmpeg\",\n    \"-y\",\n    \"-ss\",\n    \"59.3942553191489\",\n    \"-to\",\n    \"62.0340000000000\",\n    \"-i\",\n    \"/root/Desktop/works/pyjom/samples/video/LiGlReJ4i.mp4\",\n    \"-ss\",\n    \"0\",\n    \"-to\",\n    \"62.034\",\n    \"-i\",\n    \"/root/Desktop/works/pyjom/samples/video/LiGlReJ4i.mp4\",\n    \"-filter_complex\",\n    \"[0:v]pad=color=black:height=max(ih\\\\, ceil(iw*max(1080/1920\\\\, ih/iw))):width=max(iw\\\\, ceil(ih*max(1920/1080\\\\, iw/ih))):x=floor((max(iw\\\\, ceil(ih*max(1920/1080\\\\, iw/ih)))-iw)/2):y=floor((max(ih\\\\, ceil(iw*max(1080/1920\\\\, ih/iw)))-ih)/2)[s4];[s4]scale=1920:1080[s5];[s5]scale=ceil((iw*0.15555555555555556)/4)*4:ceil((ih*0.15555555555555556)/4)*4[s6]\",\n    \"-map\",\n    \"[s6]\",\n    \"-map\",\n    \"1:a\",\n    \"/dev/shm/2c6b1466-6186-41dd-9ce3-2f757c082c5a.mp4\",\n]\ncommandImprovised = command_original = [\n    \"ffmpeg\",\n    \"-y\",\n    \"-ss\",\n    \"0\",\n    \"-to\",\n    \"59.3942553191489\",\n    \"-i\",\n    \"/root/Desktop/works/pyjom/samples/video/LiGlReJ4i.mp4\",",
        "type": "code",
        "location": "/tests/unittest_ffmpeg_args.py:49-84"
    },
    "1737": {
        "file_id": 160,
        "content": "This code is using FFmpeg command line arguments to perform operations on video files. It's mapping streams, applying filters for scaling and padding, setting start/end times, and specifying output file paths. The code is likely involved in video processing or manipulation tasks.",
        "type": "comment"
    },
    "1738": {
        "file_id": 160,
        "content": "    \"-ss\",\n    \"59.3942553191489\",\n    \"-to\",\n    \"62.0340000000000\",\n    \"-i\",\n    \"/root/Desktop/works/pyjom/samples/video/LiGlReJ4i.mp4\",\n    \"-ss\",\n    \"0\",\n    \"-to\",\n    \"62.034\",\n    \"-i\",\n    \"/root/Desktop/works/pyjom/samples/video/LiGlReJ4i.mp4\",\n    \"-filter_complex\",\n    \"[0:v]crop=h=1099:w=717:x=1:y=72[s0];[s0]pad=color=black:height=max(ih\\\\, ceil(iw*max(1080/1920\\\\, ih/iw))):width=max(iw\\\\, ceil(ih*max(1920/1080\\\\, iw/ih))):x=floor((max(iw\\\\, ceil(ih*max(1920/1080\\\\, iw/ih)))-iw)/2):y=floor((max(ih\\\\, ceil(iw*max(1080/1920\\\\, ih/iw)))-ih)/2)[s1];[s1]scale=1920:1080[s2];[s2]scale=ceil((iw*0.15555555555555556)/4)*4:ceil((ih*0.15555555555555556)/4)*4,setsar=1[s3];[1:v]pad=color=black:height=max(ih\\\\, ceil(iw*max(1080/1920\\\\, ih/iw))):width=max(iw\\\\, ceil(ih*max(1920/1080\\\\, iw/ih))):x=floor((max(iw\\\\, ceil(ih*max(1920/1080\\\\, iw/ih)))-iw)/2):y=floor((max(ih\\\\, ceil(iw*max(1080/1920\\\\, ih/iw)))-ih)/2)[s4];[s4]scale=1920:1080[s5];[s5]scale=ceil((iw*0.15555555555555556)/4)*4:ceil((ih*0.15555555555555556)/4)*4,setsar=1[s6];[s3][s6]concat=n=2[s7]\",",
        "type": "code",
        "location": "/tests/unittest_ffmpeg_args.py:85-98"
    },
    "1739": {
        "file_id": 160,
        "content": "This code is using FFmpeg to crop, scale, and concatenate video streams. It first specifies start and end times for the input video file \"/root/Desktop/works/pyjom/samples/video/LiGlReJ4i.mp4\", then applies a series of filters including cropping, padding, scaling, and setting aspect ratio. Finally, it concatenates the resulting streams for output.",
        "type": "comment"
    },
    "1740": {
        "file_id": 160,
        "content": "    \"-map\",\n    \"[s7]\",\n    \"-map\",\n    \"2:a\",\n    \"/dev/shm/2c6b1466-6186-41dd-9ce3-2f757c082c5a.mp4\",\n]\nimport subprocess\nsubprocess.run(commandImprovised)",
        "type": "code",
        "location": "/tests/unittest_ffmpeg_args.py:99-107"
    },
    "1741": {
        "file_id": 160,
        "content": "This code chunk is part of a larger script that uses the subprocess module to run an FFmpeg command. The command maps video stream from input file \"[s7]\" and audio stream from track 2 to output \"/dev/shm/2c6b1466-6186-41dd-9ce3-2f757c082c5a.mp4\".",
        "type": "comment"
    },
    "1742": {
        "file_id": 161,
        "content": "/tests/test_bilibili_register_video.py",
        "type": "filepath"
    },
    "1743": {
        "file_id": 161,
        "content": "This code imports necessary modules, sets bilibili video ID and user ID, calls the \"registerBilibiliUserVideo\" function to register the video with given parameters, and prints the success status of registration.",
        "type": "summary"
    },
    "1744": {
        "file_id": 161,
        "content": "dedeuserid = str(397424026)\nbvid = \"BV1Gd4y1j7ht\"\nfrom test_commons import *\nfrom pyjom.modules.contentPosting.bilibiliPoster import registerBilibiliUserVideo\nsuccess = registerBilibiliUserVideo(bvid, dedeuserid)\nprint(\"SUCCESS?\", success)",
        "type": "code",
        "location": "/tests/test_bilibili_register_video.py:1-7"
    },
    "1745": {
        "file_id": 161,
        "content": "This code imports necessary modules, sets bilibili video ID and user ID, calls the \"registerBilibiliUserVideo\" function to register the video with given parameters, and prints the success status of registration.",
        "type": "comment"
    },
    "1746": {
        "file_id": 162,
        "content": "/tests/unittest_full_text_search_peewee_sqlite.py",
        "type": "filepath"
    },
    "1747": {
        "file_id": 162,
        "content": "The code imports modules and sets up a SQLite database for full-text search. It defines a model class, populates the table with data, adds/updates a video, searches \"python world\" using BM25 algorithm, limits results to 2, and prints each result. Debugging breakpoints are included.",
        "type": "summary"
    },
    "1748": {
        "file_id": 162,
        "content": "from peewee import *\nfrom playhouse.sqlite_ext import SqliteExtDatabase, FTSModel, SearchField, RowIDField\ndb_path = \"test_fulltext_search.db\"\ndb = SqliteExtDatabase(\n    db_path, pragmas={\"journal_mode\": \"wal\", \"cache_size\": -1024 * 64}\n)\nclass BilibiliVideoIndex(FTSModel):\n    rowid = RowIDField()  # this does not support\n    title = SearchField()\n    content = SearchField()\n    class Meta:\n        database = None  # that's good.\n        options = {\"tokenize\": \"porter\"}  # you need manually separate some\ndb.create_tables([BilibiliVideoIndex])\nimport uuid\nrandomContent = lambda: str(uuid.uuid4())\nobject, flag = BilibiliVideoIndex.get_and_update_or_create(\n    rowid=1, title=randomContent(), content=randomContent(), _unique_keys=[\"rowid\"]\n)\nBilibiliVideoIndex.get_and_update_or_create(\n    rowid=2,\n    title=\"hello world\",\n    content=\"learn python the hard way\",\n    _unique_keys=[\"rowid\"],\n)\nBilibiliVideoIndex.get_and_update_or_create(\n    rowid=3,\n    title=\"hello world\",\n    content=\"learn python the hard way\",\n    _unique_keys=[\"rowid\"],",
        "type": "code",
        "location": "/tests/unittest_full_text_search_peewee_sqlite.py:1-42"
    },
    "1749": {
        "file_id": 162,
        "content": "This code imports necessary modules and sets up a SQLite database with full-text search capabilities. It defines a model class, BilibiliVideoIndex, and creates its corresponding table in the database. Using the get_and_update_or_create method, it populates the table with data for three records, ensuring uniqueness based on the rowid field.",
        "type": "comment"
    },
    "1750": {
        "file_id": 162,
        "content": ")\nBilibiliVideoIndex.get_and_update_or_create(\n    rowid=4,\n    title=\"hello world\",\n    content=\"learn python the hard way\",\n    _unique_keys=[\"rowid\"],\n)\nprint(object)\nprint(flag)\nprint(object.rowid, object.title, object.content)\n# don't know what magic is inside. whatever.\n# updated. my lord.\n# now search for it.\nterm = \"python world\"\nresults = BilibiliVideoIndex.search_bm25(term).limit(2)  # just how many?\n# breakpoint()\n# it does have the limit.\n# it is ordered.\nfor result in results:\n    print(\"RESULT\", result)\n    breakpoint()",
        "type": "code",
        "location": "/tests/unittest_full_text_search_peewee_sqlite.py:43-68"
    },
    "1751": {
        "file_id": 162,
        "content": "Code adds a video to BilibiliVideoIndex, updates it, and searches for \"python world\" using BM25 algorithm. Limits search results to 2, then prints each result. Breakpoints inserted for debugging.",
        "type": "comment"
    },
    "1752": {
        "file_id": 163,
        "content": "/tests/unittest_ffmpeg_overlay_boxblur.py",
        "type": "filepath"
    },
    "1753": {
        "file_id": 163,
        "content": "The code uses ffmpeg library to apply Gaussian blur filter and scale video stream, then creates a second layer, overlays both layers, and outputs the processed video stream. It sets output dimensions, uses \"scale\" and \"gblur\" or \"boxblur\" filters, scales video stream with aspect ratio preservation, and outputs file to temporary directory.",
        "type": "summary"
    },
    "1754": {
        "file_id": 163,
        "content": "# ffmpeg对视频实现高斯模糊，给视频上下加模糊背景\n# ffmpeg实现视频高斯模糊拓边效果\nimport ffmpeg\nsource = \"/root/Desktop/works/pyjom/samples/video/cute_cat_gif.gif\"\nstream = ffmpeg.input(source)\nvideo_stream = stream.video\n# the damn thing because they are from the same file! fuck!\n# layer_0 = video_stream.filter(\"scale\", w=1080, h=1920).filter(\"boxblur\", 10) # this is default?\n# however, you need to generalize it here.\n# output_width = 1080\n# output_height = 1920\noutput_height = 1080\noutput_width = 1920\nlayer_0 = video_stream.filter(\"scale\", w=output_width, h=output_height).filter(\n    \"gblur\", sigma=9\n)  # this is default?\n# print('layer_0 args', layer_0.get_args())\nlayer_1 = video_stream.filter(\n    \"scale\",\n    w=\"min(floor(iw*{}/ih),{})\".format(output_height, output_width),\n    h=\"min(floor(ih*{}/iw),{})\".format(output_width, output_height),\n)\n# print('layer_1 args', layer_1.get_args())\n## in case you failed to generalize this shit...\noutput_stream = layer_0.overlay(layer_1, x=\"floor((W-w)/2)\", y=\"floor((H-h)/2)\")\n# print('output_stream args', output_stream.get_args())",
        "type": "code",
        "location": "/tests/unittest_ffmpeg_overlay_boxblur.py:1-40"
    },
    "1755": {
        "file_id": 163,
        "content": "The code uses ffmpeg library to apply Gaussian blur filter and scale video stream. It sets output dimensions, applies \"scale\" filter with given width and height, then applies \"gblur\" or \"boxblur\" filter. Then, it creates a second layer by scaling the video stream with aspect ratio preservation and overlays both layers using specific coordinates. Finally, it outputs the processed video stream.",
        "type": "comment"
    },
    "1756": {
        "file_id": 163,
        "content": "from lazero.filesystem import tmpdir\npath = \"/dev/shm/medialang\"\nimport os\nwith tmpdir(path=path) as T:\n    filepath = os.path.join(path, \"output.mp4\")\n    # args = ffmpeg.get_args(output_stream)\n    # print(args)\n    output_args = {\"preset\": \"veryfast\"}  # seems like it won't speed up so much?\n    ffmpeg.output(output_stream, filepath, **output_args).run(overwrite_output=True)\n    print(\"output file location:\", filepath)\n    breakpoint()",
        "type": "code",
        "location": "/tests/unittest_ffmpeg_overlay_boxblur.py:42-54"
    },
    "1757": {
        "file_id": 163,
        "content": "The code sets a temporary directory path, joins it with the file name, creates FFmpeg output arguments with a fast preset, and then runs an FFmpeg command to output the stream to the file in the temporary directory. The output file location is printed.",
        "type": "comment"
    },
    "1758": {
        "file_id": 164,
        "content": "/tests/test_auto_local_reviewer.py",
        "type": "filepath"
    },
    "1759": {
        "file_id": 164,
        "content": "The code imports modules, defines detectors' dictionaries, sets template names, and initializes an object wbRev from Main class with specific arguments. The main() method is then called with these arguments for execution.",
        "type": "summary"
    },
    "1760": {
        "file_id": 164,
        "content": "from test_commons import *\nfrom pyjom.primitives import *  # this is capitalized.\n# autoArgs = {\"subtitle_detector\": {\"timestep\": 0.2}} # not work for boundary works.\n# autoArgs = {\"subtitle_detector\": {\"timestep\": 0.2},\"yolov5_detector\":{\"model\":\"yolov5x\"}}\n# template_names = [\"subtitle_detector.mdl.j2\"] # test ocr entities first.\n# template_names = [\"yolov5_detector.mdl.j2\"]\nautoArgs = {\n    \"frameborder_detector\": {\n        \"model\": \"huffline_horizontal_vertical\",\n        \"config\": {\"includeBoundaryLines\": True},\n    }\n}\n# autoArgs={\"frameborder_detector\":{\"model\":\"framedifference_talib\",\"config\":{}}}\ntemplate_names = [\"frameborder_detector.mdl.j2\"]\n# template_names = [\"framediff_detector.mdl.j2\"]\n# seems cudnn is causing trouble?\n# CuDNN Version 降到7.6试试，这个问题是环境问题引起的\n# https://pypi.tuna.tsinghua.edu.cn/packages/a4/1f/56dddeb4794137e3f824476ead29806d60a5d5fc20adba9f4d7ca5899900/paddlepaddle_gpu-2.2.2-cp39-cp39-manylinux1_x86_64.whl\n# from pip._internal.cli.main\n# we have modified the pip downloader.\nwbRev = FilesystemAutoContentReviewer(",
        "type": "code",
        "location": "/tests/test_auto_local_reviewer.py:1-24"
    },
    "1761": {
        "file_id": 164,
        "content": "This code is importing necessary modules, defining the autoArgs dictionary for various detectors, and setting template names for testing. It mentions potential issues with CuDNN version and a modified pip downloader.",
        "type": "comment"
    },
    "1762": {
        "file_id": 164,
        "content": "    dirpath=\"./samples/video/\",\n    dummy_auto=False,\n    args=autoArgs,\n    template_names=template_names,\n    semiauto=False,  # i do not want to comment shit.\n)\nwbRev.main()",
        "type": "code",
        "location": "/tests/test_auto_local_reviewer.py:25-32"
    },
    "1763": {
        "file_id": 164,
        "content": "This code initializes an object, wbRev, of the main function from the class Main in the module main.py, and calls its main() method with specific arguments: dirpath set to \"./samples/video/\", dummy_auto as False, args as autoArgs, template_names not mentioned, and semiauto as False. The main() method is then executed.",
        "type": "comment"
    },
    "1764": {
        "file_id": 165,
        "content": "/tests/unittest_convolution_bilibili_translate_text_detect.py",
        "type": "filepath"
    },
    "1765": {
        "file_id": 165,
        "content": "This code imports libraries, defines image and video processing functions, reads a JSON file, applies these functions to create the final image, processes bounding boxes, creates rectangles, blurs, visualizes, and displays images while waiting for key presses.",
        "type": "summary"
    },
    "1766": {
        "file_id": 165,
        "content": "import json\nfrom test_commons import *\nfrom pyjom.commons import *\nimport cv2\ndef getVideoPixels(videoPath):\n    from MediaInfo import MediaInfo\n    info = MediaInfo(filename=videoPath)\n    infoData = info.getInfo()\n    # print(infoData)\n    # breakpoint()\n    defaultWidth = infoData[\"videoWidth\"]\n    defaultHeight = infoData[\"videoHeight\"]\n    return defaultWidth, defaultHeight\n# easy gig, you said.\n# basePath = \"/Users/jamesbrown/desktop/works/pyjom_remote\"\nbasePath = \"/root/Desktop/works/pyjom\"\ntargetFile = (\n    basePath + \"/tests/bilibili_practices/bilibili_video_translate/japan_day.json\"\n)\noriginalFile = (\n    basePath + \"/tests/bilibili_practices/bilibili_video_translate/japan_day.webm\"\n)\n# visualization can only be done here?\n# where is the original file?\nmJson = json.loads(open(targetFile, \"r\", encoding=\"utf-8\").read())\nimport numpy as np\nwidth, height = getVideoPixels(originalFile)\ndef getBlackPicture(width, height):\n    blackPicture = np.zeros((height, width, 1), dtype=\"uint8\")  # this is grayscale.\n    return blackPicture",
        "type": "code",
        "location": "/tests/unittest_convolution_bilibili_translate_text_detect.py:1-41"
    },
    "1767": {
        "file_id": 165,
        "content": "The code is importing necessary libraries and defining a function `getVideoPixels` to retrieve the default video width and height from a given video file. It then sets the base path, target file, and original file paths. The code reads the target JSON file, loads it into a variable `mJson`, and retrieves the video dimensions using the `getVideoPixels` function. Finally, it defines a function `getBlackPicture` to create a black grayscale image with the specified width and height.",
        "type": "comment"
    },
    "1768": {
        "file_id": 165,
        "content": "mKeys = list(mJson.keys())\nmIntKeys = [int(x) for x in mKeys]\nminKey, maxKey = min(mIntKeys), max(mIntKeys)\n# imutils is created by pyimagesearch.\nfrom imutils.object_detection import non_max_suppression\ndef getConvBlurredCurrentShot(blurredSpan, span=5):\n    # honor the most the latest one.\n    mImage = None\n    for index, blurredImage in enumerate(blurredSpan):\n        ratio = index / span\n        if mImage is None:\n            mImage = blurredImage * ratio\n        else:\n            mImage += blurredImage * ratio\n    # print(mImage.shape)\n    # breakpoint()\n    # change this mImage.\n    mImage = mImage > 128\n    mImage = mImage.astype(np.uint8)\n    mImage = mImage * 255\n    return mImage\n    # return 256*((mImage>128).astype(np.uint8))\nconvolutionSpan = 20\nconvolutionBoundingBoxSpan = []\nconvolutionBlurredSpan = []\nfor intKey in range(minKey, maxKey + 1):\n    strKey = str(intKey)\n    target = mJson[strKey]\n    boundingBoxes = []\n    for item in target:\n        location = item[0]\n        text, confidence = item[1]\n        # print(\"location\",location) # four points. do not know if there is any rotation here.",
        "type": "code",
        "location": "/tests/unittest_convolution_bilibili_translate_text_detect.py:44-86"
    },
    "1769": {
        "file_id": 165,
        "content": "This code defines a function `getConvBlurredCurrentShot` that averages multiple blurred images to create a final image. It also initializes variables for convolution bounding boxes and blurred spans based on a range of keys in `mJson`. The resulting image is then thresholded and converted to 8-bit format before being returned.",
        "type": "comment"
    },
    "1770": {
        "file_id": 165,
        "content": "        if confidence > 0.7:\n            npLocation = np.array(location)\n            xlocs = npLocation[:, 0]\n            ylocs = npLocation[:, 1]\n            # print(xlocs)\n            # print(ylocs)\n            # breakpoint()\n            minX, maxX = min(xlocs), max(xlocs)\n            minY, maxY = min(ylocs), max(ylocs)\n            boundingBox = [minX, minY, maxX, maxY]\n            boundingBoxes.append(boundingBox.copy())\n            # breakpoint()\n        # print(\"text\", text)\n        # print(\"confidence\", confidence)\n    convolutionBoundingBoxSpan.append(boundingBoxes.copy())\n    if len(convolutionBoundingBoxSpan) > convolutionSpan:\n        convolutionBoundingBoxSpan.pop(0)\n    # do your calculation!\n    flatSpan = [y for x in convolutionBoundingBoxSpan for y in x]\n    flatSpan = np.array(flatSpan)\n    currentNonOverlappingBoxes = non_max_suppression(flatSpan)\n    # print(intKey,target)\n    # this time we do not care about the text inside.\n    blackPicture = getBlackPicture(width, height)\n    for rectangle in flatSpan:",
        "type": "code",
        "location": "/tests/unittest_convolution_bilibili_translate_text_detect.py:87-111"
    },
    "1771": {
        "file_id": 165,
        "content": "The code processes bounding boxes from a convolution operation, filters them based on confidence score, and performs non-maximum suppression to eliminate overlapping boxes. It then creates an array of non-overlapping bounding boxes and generates a black picture with the same width and height as the original image.",
        "type": "comment"
    },
    "1772": {
        "file_id": 165,
        "content": "        # make it all int.\n        x0, y0, x1, y1 = [int(num) for num in rectangle]\n        loc0 = (x0, y0)\n        loc1 = (x1, y1)\n        cv2.rectangle(\n            blackPicture, loc0, loc1, 255, cv2.FILLED\n        )  # we fill so we can merge shits.\n    blackPictureBlurred = cv2.GaussianBlur(blackPicture, (33, 33), 0)\n    convolutionBlurredSpan.append(blackPictureBlurred.copy())\n    if len(convolutionBlurredSpan) > convolutionSpan:\n        convolutionBlurredSpan.pop(0)\n    currentBlackPictureBlurred = getConvBlurredCurrentShot(\n        convolutionBlurredSpan, span=convolutionSpan\n    )\n    # print(currentBlackPictureBlurred.shape)\n    print(\"boundingBoxes:\", len(flatSpan))\n    if len(flatSpan) == 0:\n        continue\n    contours = cv2.findContours(\n        currentBlackPictureBlurred, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n    )\n    contours = contours[0] if len(contours) == 2 else contours[1]\n    currentBoundingBoxesVisualize = getBlackPicture(width, height)\n    for i in contours:\n        x, y, w, h = cv2.boundingRect(i)",
        "type": "code",
        "location": "/tests/unittest_convolution_bilibili_translate_text_detect.py:112-142"
    },
    "1773": {
        "file_id": 165,
        "content": "The code creates a rectangle from input, fills it in the black picture, blurs the filled image, appends it to a list if length is less than convolutionSpan, pops oldest if length exceeds convolutionSpan, gets the current blurred image from the list, prints the bounding boxes count, and if no elements in flatSpan, continues. It then finds contours in the current blurred image and creates a new image for visualization of bounding rectangles.",
        "type": "comment"
    },
    "1774": {
        "file_id": 165,
        "content": "        cv2.rectangle(currentBoundingBoxesVisualize, (x, y), (x + w, y + h), 255, 4)\n    cv2.imshow(\"IMAGE\", currentBoundingBoxesVisualize)\n    cv2.waitKey(10)\n    print(\"showing image:\", intKey)\n    # print\n    # cv2.waitKey(1000)\n    # print(\"NON OVERLAPPING BOXES:\")\n    # print(currentNonOverlappingBoxes)\n    # we need to visualize this shit.\n    # breakpoint()\ncv2.destroyAllWindows()\nprint(\"THE END\")",
        "type": "code",
        "location": "/tests/unittest_convolution_bilibili_translate_text_detect.py:143-156"
    },
    "1775": {
        "file_id": 165,
        "content": "This code snippet is responsible for visualizing bounding boxes, displaying an image, and waiting for a key press. It prints the non-overlapping boxes but may require visualization. The code will close all windows at the end with a final message \"THE END\".",
        "type": "comment"
    },
    "1776": {
        "file_id": 166,
        "content": "/tests/test_auto_dog_video_giphy_online_producer.sh",
        "type": "filepath"
    },
    "1777": {
        "file_id": 166,
        "content": "This code sets up an environment and runs tests for a video producer script. It first kills the existing test session, loads a configuration file, and then checks the media language render result. The Python script is used to perform full testing, and there's mention of potentially improving time duration using Gaussian.",
        "type": "summary"
    },
    "1778": {
        "file_id": 166,
        "content": "# env LD_LIBRARY_PATH=/usr/local/lib python3 test_auto_dog_video_giphy_online_producer.py \n#### PHASE 1 ####\n# FULL TEST\nulimit -n 1048576 # to avoid NOF issues.\ntmux kill-session -t online_dog_cat_generator_test && echo \"killed session: online_dog_cat_generator_test\"\ntmuxp load test_auto_dog_video_giphy_online_producer.yaml\n#### PHASE 2 ####\n# check medialang render result.\n# python3 test_auto_dog_video_giphy_online_producer.py -p\n# seems all good. but the time duration is not so good. maybe gaussian will help? set breakpoint after main list is created.",
        "type": "code",
        "location": "/tests/test_auto_dog_video_giphy_online_producer.sh:1-12"
    },
    "1779": {
        "file_id": 166,
        "content": "This code sets up an environment and runs tests for a video producer script. It first kills the existing test session, loads a configuration file, and then checks the media language render result. The Python script is used to perform full testing, and there's mention of potentially improving time duration using Gaussian.",
        "type": "comment"
    },
    "1780": {
        "file_id": 167,
        "content": "/tests/test_auto_local_producer.py",
        "type": "filepath"
    },
    "1781": {
        "file_id": 167,
        "content": "The code imports modules, sets environment variables, and installs local producers for OCR testing. It creates video processing configurations with completeTest() and partialMedialangRenderTest(). It handles temporary directories, cleans them, and prints save paths for debugging.",
        "type": "summary"
    },
    "1782": {
        "file_id": 167,
        "content": "import os\nos.environ[\"LD_LIBRARY_PATH\"] = \"/usr/local/lib\"\nfrom test_commons import *\nfrom pyjom.primitives import *  # this is capitalized.\n# let's hack the gl!\n# os.environ[\"DISPLAY\"] = \":1\"\n# os.environ[\"XAUTHORITY\"] = \"/root/.Xauthority\"\n# undefined symbol? wtf? how about use xvfb-run directly?\nautoArgs = {\n    \"subtitle_detector\": {\"timestep\": 0.2}\n}  # what is this? should't you detect all before production?\n# autoArgs = {\"subtitle_detector\": {\"timestep\": 0.2},\"yolov5_detector\":{\"model\":\"yolov5x\"}}\ntemplate_names = [\"subtitle_detector.mdl.j2\"]  # test ocr entities first.\n# template_names = [\"yolov5_detector.mdl.j2\"]\n# template_names = [\"framediff_detector.mdl.j2\"]\n# seems cudnn is causing trouble?\n# CuDNN Version 降到7.6试试，这个问题是环境问题引起的\n# https://pypi.tuna.tsinghua.edu.cn/packages/a4/1f/56dddeb4794137e3f824476ead29806d60a5d5fc20adba9f4d7ca5899900/paddlepaddle_gpu-2.2.2-cp39-cp39-manylinux1_x86_64.whl\n# from pip._internal.cli.main\n# we have modified the pip downloader.\nwbRev = FilesystemAutoContentProducer(",
        "type": "code",
        "location": "/tests/test_auto_local_producer.py:1-27"
    },
    "1783": {
        "file_id": 167,
        "content": "This code is importing necessary modules and setting environment variables for a specific purpose. It seems to be testing OCR entities first by using a subtitle detector and possibly other detectors later. The comment mentions potential issues with CUDA libraries and suggests downgrading the CuDNN version to resolve them. Additionally, it notes that the pip downloader has been modified.",
        "type": "comment"
    },
    "1784": {
        "file_id": 167,
        "content": "    dirpath=\"./samples/video/\",\n    reviewerLogs=[\n        \"/root/Desktop/works/pyjom/logs/local/1648576077_705094.log\",  # this is the paddleocr result.\n        \"/root/Desktop/works/pyjom/logs/local/1652502047_091761.json\",  # yolov5\n        \"/root/Desktop/works/pyjom/logs/local/1652856912_480332.json\",  # framedifference_talib\n    ],\n    producer_filters={\n        \"yolov5\": {\"objects\": [\"dog\", \"cat\"], \"min_time\": 2},\n        \"meta\": {\n            \"type\": \"video\",\n            \"timelimit\": {\n                \"min\": 1,\n            },\n        },\n    },\n    path_replacers=[\n        [\n            [\n                \"/media/root/help/pyjom/samples/\",\n                \"/media/root/parrot/pyjom/samples/\",\n                \"/media/root/parrot1/pyjom/samples/\",  # new location of sample media files.\n                \"/root/Desktop/works/pyjom/src/samples/\",\n                \"/media/root/help1/pyjom/samples/\",\n            ],\n            \"/root/Desktop/works/pyjom/samples/\",\n        ]\n    ],\n    template=\"pets_with_music\",\n    template_config={",
        "type": "code",
        "location": "/tests/test_auto_local_producer.py:28-56"
    },
    "1785": {
        "file_id": 167,
        "content": "This code sets up a local producer for video files, specifying the directory path, reviewer logs to be considered, and filters based on objects detected and minimum time. It also includes path replacers for sample media file locations and defines a template for the output.",
        "type": "comment"
    },
    "1786": {
        "file_id": 167,
        "content": "        \"music\": {\n            \"filepath\": \"/root/Desktop/works/pyjom/tests/music_analysis/exciting_bgm.mp3\",  # these things were not right.\n            \"lyric_path\": \"/root/Desktop/works/pyjom/tests/music_analysis/exciting_bgm.lrc\",\n        },\n        \"font\": \"/root/.local/share/fonts/simhei.ttf\",\n        # \"font\": \"/root/.local/share/fonts/simyou.ttf\", # 幼圆可能打不出来\n        \"policy\": {},\n        \"maxtime\": 4,\n        \"mintime\": 2,\n        \"fast\": True,  # pass this flag to medialang export engine\n    },\n    processor_filters={\n        \"yolov5\": [\"dog\", \"cat\"],\n        \"labels\": [\"dog\", \"cat\"],\n        \"framedifference_talib_detector\": 30,\n        \"ensure\": [\"yolov5\"],\n    }\n    # you can also translate funny videos from youtube.\n    # dummy_auto=False,\n    # args=autoArgs,\n    # semiauto=False # i do not want to comment shit.\n)\ndef completeTest():\n    wbRev.main()\ndef partialMedialangRenderTest(medialangScript, verbose=True):\n    # copy that script to my dear clipboard please?\n    medialangObject = Medialang(script=medialangScript, verbose=verbose)",
        "type": "code",
        "location": "/tests/test_auto_local_producer.py:57-87"
    },
    "1787": {
        "file_id": 167,
        "content": "Code is creating a configuration for video processing, specifying file paths, fonts, policy, time parameters, and processor filters. It also mentions that you can translate funny videos from YouTube and includes functions completeTest() and partialMedialangRenderTest().",
        "type": "comment"
    },
    "1788": {
        "file_id": 167,
        "content": "    result = medialangObject.execute()\n    return result\ndef PMRT_0(scriptFilePath=\"\", verbose=True):\n    with open(scriptFilePath, \"r\") as f:\n        medialangScript = f.read()\n    return partialMedialangRenderTest(medialangScript, verbose=verbose)\nfrom contextlib import AbstractContextManager\nclass tmpdir(AbstractContextManager):\n    \"\"\"Context manager to suppress specified exceptions\n    After the exception is suppressed, execution proceeds with the next\n    statement following the with statement.\n         with suppress(FileNotFoundError):\n             os.remove(somefile)\n         # Execution still resumes here if the file was already removed\n    \"\"\"\n    def __init__(self, path=None):\n        assert os.path.isabs(path)\n        self._tmpdir = path\n    def __enter__(self):\n        print(\"temporary directory: %s\" % self._tmpdir)\n        if os.path.exists(self._tmpdir):\n            shutil.rmtree(self._tmpdir)\n        os.makedirs(self._tmpdir)\n        return self._tmpdir\n    def __exit__(self, exctype, excinst, exctb):",
        "type": "code",
        "location": "/tests/test_auto_local_producer.py:88-123"
    },
    "1789": {
        "file_id": 167,
        "content": "The code defines a context manager class `tmpdir` that creates and manages temporary directories. It also includes a function `PMRT_0` which takes a script file path and verbose flag as input, reads the script content, and returns the result of partialMedialangRenderTest function. The main function is `execute()` which executes the code within the context manager and returns the result.",
        "type": "comment"
    },
    "1790": {
        "file_id": 167,
        "content": "        # try not to handle exceptions?\n        tempdir = self._tmpdir\n        print(\"cleaning tempdir: %s\" % tempdir)\n        shutil.rmtree(tempdir)\n        return False\nif __name__ == \"__main__\":\n    COMPLETE_TEST = False\n    if COMPLETE_TEST:\n        completeTest()\n    # so we don't have to run it all the time. really?\n    else:\n        scriptFilePath = \"/root/Desktop/works/pyjom/tests/medialang_tests/aef2ab90-6414-4b55-a40e-63014e5648a8.mdl\"  # add random flips, picture enhancement, super resolution and minterpolate\n        # a special hack\n        # import tempfile\n        with tmpdir(path=\"/dev/shm/medialang\") as medialangTmpDir:\n            print(\"MEDIALANG SUPER TMPDIR:\", medialangTmpDir)\n            result = PMRT_0(scriptFilePath, verbose=False)\n            editly_outputPath, medialang_item_list = result  # this just return none!\n            # data -> editly json\n            # this output path is modified. we shall change this.\n            outPath = editly_outputPath  # WE SHALL MUTE IT!\n            # print(editly_json.keys())",
        "type": "code",
        "location": "/tests/test_auto_local_producer.py:124-147"
    },
    "1791": {
        "file_id": 167,
        "content": "The code is attempting to clean a temporary directory, but it's trying not to handle exceptions. It then checks if a variable COMPLETE_TEST is True or False and executes the corresponding code block. The script path is specified, and a special hack using tmpdir is used within a with statement to create a medialangTmpDir. The code prints the medialangTmpDir and calls the PMRT_0 function with the scriptFilePath and verbose=False. It stores editly_outputPath and medialang_item_list in variables result, modifies outPath, and ends.",
        "type": "comment"
    },
    "1792": {
        "file_id": 167,
        "content": "            print(\"MEDIA SAVE PATH (MAYBE YOU CAN PLAY IT?):\", outPath)\n            # where is the damn save path???\n            breakpoint()  # HERE IS THE DAMN BREAKPOINT\n            # import json\n            # data_array -> input of dot processor? check it out.\n            # breakpoint() # what is this?",
        "type": "code",
        "location": "/tests/test_auto_local_producer.py:149-154"
    },
    "1793": {
        "file_id": 167,
        "content": "The code is trying to display the save path and then using a breakpoint for debugging purposes. The comments are pointing out the location of the save path and mentioning that a breakpoint has been set for debugging.",
        "type": "comment"
    },
    "1794": {
        "file_id": 168,
        "content": "/tests/unittest_cv2_rectangle.py",
        "type": "filepath"
    },
    "1795": {
        "file_id": 168,
        "content": "This code imports necessary libraries, defines a function to create a black image of given dimensions, creates a black image, draws a rectangle on it with white color, displays the image, and waits for any key press before exiting.",
        "type": "summary"
    },
    "1796": {
        "file_id": 168,
        "content": "from test_commons import *\nfrom pyjom.commons import *\nimport cv2\nimport numpy as np\ndef getBlackPicture(width, height):\n    blackPicture = np.zeros((height, width, 3), dtype=\"uint8\")  # this is grayscale.\n    return blackPicture\nblackPicture = getBlackPicture(500, 500)\ncv2.rectangle(blackPicture, (200, 200), (300, 300), (255, 255, 255), 3)\ncv2.imshow(\"image\", blackPicture)\ncv2.waitKey(0)",
        "type": "code",
        "location": "/tests/unittest_cv2_rectangle.py:1-16"
    },
    "1797": {
        "file_id": 168,
        "content": "This code imports necessary libraries, defines a function to create a black image of given dimensions, creates a black image, draws a rectangle on it with white color, displays the image, and waits for any key press before exiting.",
        "type": "comment"
    },
    "1798": {
        "file_id": 169,
        "content": "/tests/test_commons.py",
        "type": "filepath"
    },
    "1799": {
        "file_id": 169,
        "content": "The code changes the current working directory, adds the current directory to Python's module search path, and removes the proxy environment variables to ignore global proxies during testing.",
        "type": "summary"
    }
}