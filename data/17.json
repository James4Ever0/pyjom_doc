{
    "1700": {
        "file_id": 152,
        "content": "    except KeyboardInterrupt:\n        AppHelper.stopEventLoop()\nif __name__ == \"__main__\":\n    app = NSApplication.sharedApplication()\n    delegate = AppDelegate.alloc().init()\n    NSApp().setDelegate_(delegate)\n    AppHelper.runEventLoop()",
        "type": "code",
        "location": "/tests/adb_phone_control_termux_network_broadcast_scrcpy_appium_airtest/pkl_nowriting.py:75-83"
    },
    "1701": {
        "file_id": 152,
        "content": "The code sets up an event loop and handles interrupts, ensuring that the application properly terminates when needed.",
        "type": "comment"
    },
    "1702": {
        "file_id": 153,
        "content": "/tests/adb_phone_control_termux_network_broadcast_scrcpy_appium_airtest/unlock_phone_on_given_ip.py",
        "type": "filepath"
    },
    "1703": {
        "file_id": 153,
        "content": "Device address is set to connect to the phone on a specific IP and port for further interactions.",
        "type": "summary"
    },
    "1704": {
        "file_id": 153,
        "content": "# first, check phone status.\ndevice_address = \"192.168.10.3:5555\"",
        "type": "code",
        "location": "/tests/adb_phone_control_termux_network_broadcast_scrcpy_appium_airtest/unlock_phone_on_given_ip.py:1-2"
    },
    "1705": {
        "file_id": 153,
        "content": "Device address is set to connect to the phone on a specific IP and port for further interactions.",
        "type": "comment"
    },
    "1706": {
        "file_id": 154,
        "content": "/tests/aiohttp_python_clash_delay_proxy_set_proxy/download_from_multiple_websites_at_once.py",
        "type": "filepath"
    },
    "1707": {
        "file_id": 154,
        "content": "This code imports the \"concurrentGet\" function from the \"lzero.network.asyncio\" module, which allows for making concurrent HTTP GET requests asynchronously.",
        "type": "summary"
    },
    "1708": {
        "file_id": 154,
        "content": "from lazero.network.asyncio import concurrentGet",
        "type": "code",
        "location": "/tests/aiohttp_python_clash_delay_proxy_set_proxy/download_from_multiple_websites_at_once.py:1-1"
    },
    "1709": {
        "file_id": 154,
        "content": "This code imports the \"concurrentGet\" function from the \"lzero.network.asyncio\" module, which allows for making concurrent HTTP GET requests asynchronously.",
        "type": "comment"
    },
    "1710": {
        "file_id": 155,
        "content": "/tests/aiohttp_python_clash_delay_proxy_set_proxy/test.py",
        "type": "filepath"
    },
    "1711": {
        "file_id": 155,
        "content": "This code fetches and tests proxies, sets up a connection gateway, makes a GET request to \"https://deepl.com\" using the valid proxy, prints first 100 bytes and status code, and displays \"deepl response\".",
        "type": "summary"
    },
    "1712": {
        "file_id": 155,
        "content": "# from download_from_multiple_websites_at_once import concurrentGet\nfrom lazero.network.proxy.clash import (\n    getProxyList,\n    testProxyList,\n    getConnectionGateway,\n    setProxyConfig,\n    setProxyWithSelector,\n)\nimport requests\nif __name__ == \"__main__\":\n    # validProxyDelayList = []\n    proxyList = getProxyList(debug=True)\n    # pprint.pprint(result)\n    validProxyDelayList = testProxyList(proxyList, timeout=5000)\n    #     pprint(gateway)\n    #     {'allow-lan': True,\n    #  'authentication': [],\n    #  'bind-address': '*',\n    #  'ipv6': False,\n    #  'log-level': 'info',\n    #  'mixed-port': 0,\n    #  'mode': 'rule',\n    #  'port': 8381,\n    #  'redir-port': 0,\n    #  'socks-port': 0,\n    #  'tproxy-port': 0}\n    gateway = getConnectionGateway()\n    print(\"valid proxies:\", len(validProxyDelayList))\n    validProxyName = validProxyDelayList[0][\"name\"]\n    # if no valid proxy, better do another run.\n    setProxyConfig(mode=\"Global\")\n    # you can switch to 'Rule' if you want the baidu translation\n    setProxyWithSelector(validProxyName, debug=True)",
        "type": "code",
        "location": "/tests/aiohttp_python_clash_delay_proxy_set_proxy/test.py:1-34"
    },
    "1713": {
        "file_id": 155,
        "content": "This code fetches the proxy list from Clash, tests the proxies for validity, sets up a connection gateway, and configures the global proxy using Clash's functions. It prints the number of valid proxies found and sets a specific valid proxy for further use.",
        "type": "comment"
    },
    "1714": {
        "file_id": 155,
        "content": "    # now use the proxy!\n    r = requests.get(\"https://deepl.com\", proxies={\"http\": gateway, \"https\": gateway})\n    print()\n    print(r.content[:100])\n    print(r.status_code)\n    print(\"deepl response\")",
        "type": "code",
        "location": "/tests/aiohttp_python_clash_delay_proxy_set_proxy/test.py:35-40"
    },
    "1715": {
        "file_id": 155,
        "content": "Using the proxy, make a GET request to \"https://deepl.com\", print the first 100 bytes of response content and status code, then display \"deepl response\".",
        "type": "comment"
    },
    "1716": {
        "file_id": 156,
        "content": "/tests/anime1_me_video_download/README.md",
        "type": "filepath"
    },
    "1717": {
        "file_id": 156,
        "content": "This code instructs to extract data from the video API, send it to a specific URL, use the resulting cookie for downloading the video from another URL to avoid errors.",
        "type": "summary"
    },
    "1718": {
        "file_id": 156,
        "content": "the data is hide in the video data-api. unquote it and we will get the info. \npost data to https://v.anime1.me/api, then use responded cookie p,h with the original e(timestamp) for download.\ndownload video from https://shiro.v.anime1.me/(or elsewhere) or somehow we will get it wrong.",
        "type": "code",
        "location": "/tests/anime1_me_video_download/README.md:1-5"
    },
    "1719": {
        "file_id": 156,
        "content": "This code instructs to extract data from the video API, send it to a specific URL, use the resulting cookie for downloading the video from another URL to avoid errors.",
        "type": "comment"
    },
    "1720": {
        "file_id": 157,
        "content": "/tests/anime1_me_video_download/api_curl.sh",
        "type": "filepath"
    },
    "1721": {
        "file_id": 157,
        "content": "The code sends a POST request to anime1.me API using cURL, containing video ID, episode number, timestamp, and secret key, likely for interacting with anime videos. It also includes the \"--compressed\" flag for file compression during download, saving storage space and time.",
        "type": "summary"
    },
    "1722": {
        "file_id": 157,
        "content": "curl 'https://v.anime1.me/api' \\\n  -H 'authority: v.anime1.me' \\\n  -H 'accept: */*' \\\n  -H 'accept-language: en-US,en;q=0.9' \\\n  -H 'content-type: application/x-www-form-urlencoded' \\\n  -H 'origin: https://anime1.me' \\\n  -H 'referer: https://anime1.me/' \\\n  --data-raw 'd=%7B%22c%22%3A%221019%22%2C%22e%22%3A%226b%22%2C%22t%22%3A1652431596%2C%22p%22%3A0%2C%22s%22%3A%221bc65800b44a935eb4e5287655c64cb2%22%7D' \\\n  --compressed\n# curl 'https://v.anime1.me/api' \\\n#   -H 'authority: v.anime1.me' \\\n#   -H 'accept: */*' \\\n#   -H 'accept-language: en-US,en;q=0.9' \\\n#   -H 'content-type: application/x-www-form-urlencoded' \\\n#   -H 'origin: https://anime1.me' \\\n#   -H 'referer: https://anime1.me/' \\\n#   -H 'sec-ch-ua: \" Not A;Brand\";v=\"99\", \"Chromium\";v=\"101\"' \\\n#   -H 'sec-ch-ua-mobile: ?1' \\\n#   -H 'sec-ch-ua-platform: \"Android\"' \\\n#   -H 'sec-fetch-dest: empty' \\\n#   -H 'sec-fetch-mode: cors' \\\n#   -H 'sec-fetch-site: same-site' \\\n#   -H 'user-agent: Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.4951.41 Mobile Safari/537.36' \\",
        "type": "code",
        "location": "/tests/anime1_me_video_download/api_curl.sh:1-24"
    },
    "1723": {
        "file_id": 157,
        "content": "This code sends a POST request to 'https://v.anime1.me/api' using cURL, with specified headers and data in the request body. The request includes an API key for authentication and retrieves data from the anime1.me website.",
        "type": "comment"
    },
    "1724": {
        "file_id": 157,
        "content": "#   --data-raw 'd=%7B%22c%22%3A%221019%22%2C%22e%22%3A%226b%22%2C%22t%22%3A1652431596%2C%22p%22%3A0%2C%22s%22%3A%221bc65800b44a935eb4e5287655c64cb2%22%7D' \\\n#   --compressed\n# curl 'https://v.anime1.me/api' \\\n#   -H 'authority: v.anime1.me' \\\n#   -H 'accept: */*' \\\n#   -H 'accept-language: en-US,en;q=0.9' \\\n#   -H 'content-type: application/x-www-form-urlencoded' \\\n#   -H 'cookie: _ga=GA1.2.354375679.1652431604; _gid=GA1.2.1847563412.1652431604; _gat=1' \\\n#   -H 'origin: https://anime1.me' \\\n#   -H 'referer: https://anime1.me/' \\\n#   -H 'sec-ch-ua: \" Not A;Brand\";v=\"99\", \"Chromium\";v=\"101\"' \\\n#   -H 'sec-ch-ua-mobile: ?1' \\\n#   -H 'sec-ch-ua-platform: \"Android\"' \\\n#   -H 'sec-fetch-dest: empty' \\\n#   -H 'sec-fetch-mode: cors' \\\n#   -H 'sec-fetch-site: same-site' \\\n#   -H 'user-agent: Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.4951.41 Mobile Safari/537.36' \\\n#   --data-raw 'd=%7B%22c%22%3A%221019%22%2C%22e%22%3A%226b%22%2C%22t%22%3A1652431596%2C%22p%22%3A0%2C%22s%22%3A%221bc65800b44a935eb4e5287655c64cb2%22%7D' \\",
        "type": "code",
        "location": "/tests/anime1_me_video_download/api_curl.sh:25-43"
    },
    "1725": {
        "file_id": 157,
        "content": "This code is making an API request to 'https://v.anime1.me/api' using curl command with various headers and a data payload in JSON format. The payload contains information such as video ID, episode number, timestamp, and secret key. It seems to be fetching information or performing an action related to an anime video from the anime1.me website.",
        "type": "comment"
    },
    "1726": {
        "file_id": 157,
        "content": "#   --compressed",
        "type": "code",
        "location": "/tests/anime1_me_video_download/api_curl.sh:44-44"
    },
    "1727": {
        "file_id": 157,
        "content": "The code snippet \"--compressed\" is used to compress the file during download, which can save storage space and reduce transfer time.",
        "type": "comment"
    },
    "1728": {
        "file_id": 158,
        "content": "/tests/anime1_me_video_download/get_best_edm.sh",
        "type": "filepath"
    },
    "1729": {
        "file_id": 158,
        "content": "The code uses FFmpeg to extract segments from the \"edm_super_summit.m4a\" audio file, saving them as \"best_edm_split.mp3\" and \"best_edm_split2.mp3\". The -ss option specifies the start time and -to the end time for each segment.",
        "type": "summary"
    },
    "1730": {
        "file_id": 158,
        "content": "# ffmpeg -y -i edm_super_summit.m4a -ss 00:00:50 -to 00:01:05 best_edm_split.mp3\nffmpeg -y -i edm_super_summit.m4a -ss 00:01:38 -to 00:01:49 best_edm_split2.mp3",
        "type": "code",
        "location": "/tests/anime1_me_video_download/get_best_edm.sh:1-2"
    },
    "1731": {
        "file_id": 158,
        "content": "The code uses FFmpeg to extract segments from the \"edm_super_summit.m4a\" audio file, saving them as \"best_edm_split.mp3\" and \"best_edm_split2.mp3\". The -ss option specifies the start time and -to the end time for each segment.",
        "type": "comment"
    },
    "1732": {
        "file_id": 159,
        "content": "/tests/anime1_me_video_download/get_cookie_sample.py",
        "type": "filepath"
    },
    "1733": {
        "file_id": 159,
        "content": "This code downloads a file, displays progress in real-time, uses chunked data for memory efficiency, and sets cookies from response headers.",
        "type": "summary"
    },
    "1734": {
        "file_id": 159,
        "content": "import requests\nimport json\nimport urllib.parse as up\nimport sys\n# import multithread\nfrom fake_useragent import UserAgent\nua = UserAgent()\nuser_agent =ua.random\nurl = \"https://v.anime1.me/api\"\n# data = '{\"c\":\"1019\",\"e\":\"6b\",\"t\":1652428857,\"p\":0,\"s\":\"ec9042ac177510fd67dd508f4d974074\"}'\n# data = '%7B%22c%22%3A%221019%22%2C%22e%22%3A%222b%22%2C%22t%22%3A1652429744%2C%22p%22%3A0%2C%22s%22%3A%225a78c05bd07077f05278ed6b44897878%22%7D'\ndata = \"%7B%22c%22%3A%221019%22%2C%22e%22%3A%225b%22%2C%22t%22%3A1652429744%2C%22p%22%3A0%2C%22s%22%3A%222d424b87559a56d7f761c436bca72502%22%7D\"\ndata_unquote = up.unquote(data)\ndata_json = json.loads(data_unquote)\n# url0 = \"https://anime1.me/category/2022%e5%b9%b4%e6%98%a5%e5%ad%a3/%e5%8b%87%e8%80%85%e8%be%ad%e8%81%b7%e4%b8%8d%e5%b9%b9%e4%ba%86\"\ns = requests.Session()\ns.headers.update({\"User-Agent\":user_agent}) # no freaking drama.\n# s.get(url0)\n# r = requests.post(url,body=data)\nmdata = \"d={}\".format(data)\nmheaders = {'authority': 'v.anime1.me'\n  ,'accept': '*/*' \n  ,'accept-language': 'en-US,en;q=0.9' ",
        "type": "code",
        "location": "/tests/anime1_me_video_download/get_cookie_sample.py:1-28"
    },
    "1735": {
        "file_id": 159,
        "content": "Code imports necessary libraries, sets a random user agent, defines the URL and data for API request, creates a session with the user agent as header, and formats the data for the API call.",
        "type": "comment"
    },
    "1736": {
        "file_id": 159,
        "content": "  ,'content-type': 'application/x-www-form-urlencoded' \n  ,'origin': 'https://anime1.me' \n  ,'referer': 'https://anime1.me/'}\nrpost = s.post(url,data=mdata,headers=mheaders)\n# print(dir(rpost))\nmjson2 = rpost.json()\ndownload_url = mjson2['s']['src']\ndownload_url = \"https:\"+download_url\ndownload_name = \"sample\"\ndownload_name = \"{}.{}\".format(download_name,download_url.split(\".\")[-1])\n# '{\"success\":false,\"errors\":[\"Signature invalid.\"]}' <- shit.\n# breakpoint()\n# print(rpost.text) # good. then where is the cookie?\n# print(s.cookies)\nfilename = download_name\n# print(\"downloading target file:\",filename)\n# download_object = multithread.Downloader(download_url, filename,aiohttp_args= {\"headers\":mheaders_session}) # ther e is no 'Content-Length'\n# download_object.start()\nwith open(filename, 'wb') as f:\n    # response = requests.get(url, stream=True)\n    response = s.get(download_url,stream = True)\n    total = response.headers.get('content-length')\n    if total is None:\n        f.write(response.content)\n    else:\n        downloaded = 0",
        "type": "code",
        "location": "/tests/anime1_me_video_download/get_cookie_sample.py:29-60"
    },
    "1737": {
        "file_id": 159,
        "content": "This code downloads a video from anime1.me and saves it in the specified format. It uses requests library to handle HTTP requests, extracts download URL from JSON response, sets headers for post and get requests, opens file in write mode for downloading the video, checks content length of the video, and downloads it if content length is available.",
        "type": "comment"
    },
    "1738": {
        "file_id": 159,
        "content": "        total = int(total)\n        for data in response.iter_content(chunk_size=max(int(total/1000), 1024*1024)):\n            downloaded += len(data)\n            f.write(data)\n            done = int(50*downloaded/total)\n            sys.stdout.write('\\r[{}{}]'.format('█' * done, '.' * (50-done)))\n            sys.stdout.flush()\nsys.stdout.write('\\n')\n# print(download_content.headers)\n# now you have the freaking cookie.\n# <RequestsCookieJar[<Cookie e=1652444144 for .v.anime1.me/1019/2b.mp4>, <Cookie h=oRLPqsTE0KXMFmVWJD669g for .v.anime1.me/1019/2b.mp4>, <Cookie p=eyJpc3MiOiJhbmltZTEubWUiLCJleHAiOjE2NTI0NDQxNDQwMDAsImlhdCI6MTY1MjQzNDEzNzAwMCwic3ViIjoiLzEwMTkvMmIubXA0In0 for .v.anime1.me/1019/2b.mp4>]>\n# get set-cookie header.",
        "type": "code",
        "location": "/tests/anime1_me_video_download/get_cookie_sample.py:61-72"
    },
    "1739": {
        "file_id": 159,
        "content": "This code is downloading a file and displaying the progress in real-time. It uses chunked data to manage memory efficiently and sets cookies from the response headers.",
        "type": "comment"
    },
    "1740": {
        "file_id": 160,
        "content": "/tests/anime1_me_video_download/parse_static.py",
        "type": "filepath"
    },
    "1741": {
        "file_id": 160,
        "content": "Code is parsing HTML data from 'sample.html' using BeautifulSoup and finding video tags. It then extracts channel and episode information from JSON data, formats download link, and prints the episode ID and download link. The code likely automates the process of downloading videos from a specific website.",
        "type": "summary"
    },
    "1742": {
        "file_id": 160,
        "content": "source = \"sample.html\"\n# curl -L -o sample.html \"https://anime1.me/category/2022%e5%b9%b4%e6%98%a5%e5%ad%a3/%e5%8b%87%e8%80%85%e8%be%ad%e8%81%b7%e4%b8%8d%e5%b9%b9%e4%ba%86\"\nfrom bs4 import BeautifulSoup\ndata = open(source,\"r\",encoding=\"utf-8\").read()\ndom = BeautifulSoup(data)\n# dom = BeautifulSoup(data,features='lxml')\nimport urllib.parse as up\nimport json\nimport re\nvideos = dom.find_all(\"video\")\nformat_download_link = lambda c,e: \"https://shiro.v.anime1.me/{}/{}.mp4\".format(c,e)\nfor video in videos:\n    # print(dir(video))\n    data_src = \"data-apireq\"\n    json_obj = video[data_src]\n    json_obj = up.unquote(json_obj)\n    json_obj = json.loads(json_obj)\n    channel, episode = json_obj[\"c\"], json_obj[\"e\"]\n    link = format_download_link(channel, episode)\n    episode_id = re.findall(r\"\\d+\",episode)[0]\n    print(\"EPISODE:\",episode_id)\n    print(\"DOWNLOAD LINK:\",link)\n    # breakpoint()",
        "type": "code",
        "location": "/tests/anime1_me_video_download/parse_static.py:1-28"
    },
    "1743": {
        "file_id": 160,
        "content": "Code is parsing HTML data from 'sample.html' using BeautifulSoup and finding video tags. It then extracts channel and episode information from JSON data, formats download link, and prints the episode ID and download link. The code likely automates the process of downloading videos from a specific website.",
        "type": "comment"
    },
    "1744": {
        "file_id": 161,
        "content": "/tests/anime1_me_video_download/test_download.sh",
        "type": "filepath"
    },
    "1745": {
        "file_id": 161,
        "content": "This script downloads \"crossdressing.mp4\" from the URL using various cookies until successful, involving timestamp, header, and Google Analytics parameters.",
        "type": "summary"
    },
    "1746": {
        "file_id": 161,
        "content": "# curl -L -o crossdressing.mp4 --cookie \"e=1652443257\" https://shiro.v.anime1.me/1019/6b.mp4\n# curl -L -o crossdressing.mp4 --cookie \"e=1652443257; p=eyJpc3MiOiJhbmltZTEubWUiLCJleHAiOjE2NTI0NDMyNTcwMDAsImlhdCI6MTY1MjQyODk1NTAwMCwic3ViIjoiLzEwMTkvNmIubXA0In0\" https://shiro.v.anime1.me/1019/6b.mp4\n# curl -L -o crossdressing.mp4 --cookie \"h=i6CylEHO-BiMkCPCqFDk_A\" https://shiro.v.anime1.me/1019/6b.mp4\n# curl -L -o crossdressing.mp4 --cookie \"e=1652443257; p=eyJpc3MiOiJhbmltZTEubWUiLCJleHAiOjE2NTI0NDMyNTcwMDAsImlhdCI6MTY1MjQyODk1NTAwMCwic3ViIjoiLzEwMTkvNmIubXA0In0; h=i6CylEHO-BiMkCPCqFDk_A\" https://shiro.v.anime1.me/1019/6b.mp4\ncurl -L -o crossdressing.mp4 --cookie \"e=1652443257; p=eyJpc3MiOiJhbmltZTEubWUiLCJleHAiOjE2NTI0NDMyNTcwMDAsImlhdCI6MTY1MjQyODk1NTAwMCwic3ViIjoiLzEwMTkvNmIubXA0In0; h=i6CylEHO-BiMkCPCqFDk_A\" https://shiro.v.anime1.me/1019/6b.mp4 # the only way to be.\n# curl -L -o crossdressing.mp4 --cookie \"e=1652443257; p=eyJpc3MiOiJhbmltZTEubWUiLCJleHAiOjE2NTI0NDMyNTcwMDAsImlhdCI6MTY1Mj",
        "type": "code",
        "location": "/tests/anime1_me_video_download/test_download.sh:1-6"
    },
    "1747": {
        "file_id": 161,
        "content": "This script is downloading a video file named \"crossdressing.mp4\" from the URL \"https://shiro.v.anime1.me/1019/6b.mp4\", using different combinations of cookies to access and save the file, with each attempt providing additional cookie values until the final combination successfully downloads the video.",
        "type": "comment"
    },
    "1748": {
        "file_id": 161,
        "content": "QyODk1NTAwMCwic3ViIjoiLzEwMTkvNmIubXA0In0; h=i6CylEHO-BiMkCPCqFDk_A; _ga=GA1.2.1032429949.1652428850; _gid=GA1.2.244096696.1652428850\" https://shiro.v.anime1.me/1019/6b.mp4",
        "type": "code",
        "location": "/tests/anime1_me_video_download/test_download.sh:6-6"
    },
    "1749": {
        "file_id": 161,
        "content": "The code appears to be a string containing a series of parameters and URL for downloading an MP4 file. The specific parameters include a timestamp, header value, Google Analytics IDs, and the video URL.",
        "type": "comment"
    },
    "1750": {
        "file_id": 162,
        "content": "/tests/anime_highlight_cuts/bittorrent_downloader/download_given_file_to_given_name.sh",
        "type": "filepath"
    },
    "1751": {
        "file_id": 162,
        "content": "This script downloads a torrent file using aria2c and removes temporary files once finished. The script sets the base path, torrent name, and file ID for the download. It also includes two different command variations for stopping the download after completion with timeout options. The commands use kill and grep to end the aria2c process with a signal and remove temporary files.",
        "type": "summary"
    },
    "1752": {
        "file_id": 162,
        "content": "# how to end downloading when finished?\n# using some command?\nBASE_PATH=\"/Users/jamesbrown/Downloads/anime_download\"\n# DOWNLOAD_FILE_PATH=\"$BASE_PATH/sample.webp\"\nTORRENT_NAME=\"[Kamigami&VCB-Studio] Yahari Ore no Seishun Lovecome wa Machigatte Iru. [Ma10p_1080p]\"\n# torrent name might be different.\nTORRENT_PATH=\"$BASE_PATH/$TORRENT_NAME.torrent\"\n# echo \"ps aux | grep '$TORRENT_NAME' | grep -v grep | awk '{print \\$1}' | xargs -Iabc kill -s INT abc\" > kill_aria2c.sh\nFILE_ID=\"117\"\n# timeout set to what?\n# rm \"$DOWNLOAD_FILE_PATH\"\nrm -rf \"$TORRENT_NAME\"\nrm -rf \"$TORRENT_NAME.aria2\"\n# this will be ignored.\n# change directory to our temp directory.\n# this speed shall be precalculated.\n# \n# you may check integrity.\n# just count seeders.\n# aria2c -x 16 --select-file=\"$FILE_ID\" --seed-time=0 --file-allocation=none \"$TORRENT_PATH\"\n# aria2c -x 16 --select-file=\"$FILE_ID\" --seed-time=0 --file-allocation=none --lowest-speed-limit=300K --bt-stop-timeout=60 \"$TORRENT_PATH\"",
        "type": "code",
        "location": "/tests/anime_highlight_cuts/bittorrent_downloader/download_given_file_to_given_name.sh:1-27"
    },
    "1753": {
        "file_id": 162,
        "content": "This script downloads a torrent file using aria2c and removes temporary files once finished. The script sets the base path, torrent name, and file ID for the download. It also includes two different command variations for stopping the download after completion with timeout options. The commands use kill and grep to end the aria2c process with a signal and remove temporary files.",
        "type": "comment"
    },
    "1754": {
        "file_id": 163,
        "content": "/tests/anime_highlight_cuts/bittorrent_downloader/dynamic_import.mjs",
        "type": "filepath"
    },
    "1755": {
        "file_id": 163,
        "content": "Code imports and initializes two modules, FfmpegCommand and WebTorrent, using dynamic import. The code checks the data types of the imported functions, with FfmpegCommand being a function and WebTorrent appearing as a class in the console despite its data type being \"function\".",
        "type": "summary"
    },
    "1756": {
        "file_id": 163,
        "content": "const FfmpegCommand = (await import(`${process.env.NODE_PATH}/fluent-ffmpeg/index.js`)).default \nconst WebTorrent = (await import(`${process.env.NODE_PATH}/webtorrent/index.js`)).default \n// promise!\n// shit this ESM can directly use await statements.\nconsole.log(FfmpegCommand)\nconsole.log(typeof(FfmpegCommand)) // \"function\", with default name.\nconsole.log(WebTorrent)\nconsole.log(typeof(WebTorrent)) // \"function\"? why i see \"class\" in console.log?\n// this syntax is not recommended. autocompletion will not work.",
        "type": "code",
        "location": "/tests/anime_highlight_cuts/bittorrent_downloader/dynamic_import.mjs:1-12"
    },
    "1757": {
        "file_id": 163,
        "content": "Code imports and initializes two modules, FfmpegCommand and WebTorrent, using dynamic import. The code checks the data types of the imported functions, with FfmpegCommand being a function and WebTorrent appearing as a class in the console despite its data type being \"function\".",
        "type": "comment"
    },
    "1758": {
        "file_id": 164,
        "content": "/tests/anime_highlight_cuts/bittorrent_downloader/kill_aria2c.sh",
        "type": "filepath"
    },
    "1759": {
        "file_id": 164,
        "content": "This command is killing the aria2c process with a SIGINT signal, specifically targeting the specified anime episode titled 'Yahari Ore no Seishun Lovecome wa Machigatte Iru. [Ma10p_1080p]' from the Kamigami & VCB-Studio group.",
        "type": "summary"
    },
    "1760": {
        "file_id": 164,
        "content": "ps aux | grep '[Kamigami&VCB-Studio] Yahari Ore no Seishun Lovecome wa Machigatte Iru. [Ma10p_1080p]' | grep -v grep | awk '{print $1}' | xargs -Iabc kill -s INT abc",
        "type": "code",
        "location": "/tests/anime_highlight_cuts/bittorrent_downloader/kill_aria2c.sh:1-1"
    },
    "1761": {
        "file_id": 164,
        "content": "This command is killing the aria2c process with a SIGINT signal, specifically targeting the specified anime episode titled 'Yahari Ore no Seishun Lovecome wa Machigatte Iru. [Ma10p_1080p]' from the Kamigami & VCB-Studio group.",
        "type": "comment"
    },
    "1762": {
        "file_id": 165,
        "content": "/tests/anime_highlight_cuts/bittorrent_downloader/make_node_symlink.sh",
        "type": "filepath"
    },
    "1763": {
        "file_id": 165,
        "content": "This script creates a symbolic link named \"node_modules\" pointing to the $NODE_PATH, presumably to resolve or fix an issue with file locations.",
        "type": "summary"
    },
    "1764": {
        "file_id": 165,
        "content": "ln -s $NODE_PATH node_modules # to fix shit.",
        "type": "code",
        "location": "/tests/anime_highlight_cuts/bittorrent_downloader/make_node_symlink.sh:1-1"
    },
    "1765": {
        "file_id": 165,
        "content": "This script creates a symbolic link named \"node_modules\" pointing to the $NODE_PATH, presumably to resolve or fix an issue with file locations.",
        "type": "comment"
    },
    "1766": {
        "file_id": 166,
        "content": "/tests/anime_highlight_cuts/bittorrent_downloader/name_fuzzy.py",
        "type": "filepath"
    },
    "1767": {
        "file_id": 166,
        "content": "This code initializes two empty lists, 'filenames' and 'bangumi_names'. 'bangumi_names' contains two string values representing anime titles.",
        "type": "summary"
    },
    "1768": {
        "file_id": 166,
        "content": "filenames = []\nbangumi_names= [\"Yahari Ore no Seishun Lovecome wa Machigatte Iru.\",\"Yahari Ore no Seishun Love Come wa Machigatteiru.\"]",
        "type": "code",
        "location": "/tests/anime_highlight_cuts/bittorrent_downloader/name_fuzzy.py:1-3"
    },
    "1769": {
        "file_id": 166,
        "content": "This code initializes two empty lists, 'filenames' and 'bangumi_names'. 'bangumi_names' contains two string values representing anime titles.",
        "type": "comment"
    },
    "1770": {
        "file_id": 167,
        "content": "/tests/anime_highlight_cuts/bittorrent_downloader/name_resolution_parsing_chapter_recognition.py",
        "type": "filepath"
    },
    "1771": {
        "file_id": 167,
        "content": "The code utilizes ffmpeg to extract subtitles, sets anime series constants, filters series names, and reads filenames from a JSON. It checks for bangume names, identifies episode index location, compares with expected position, and prints the episode index or displays \"EPISODE?\" if not recognized.",
        "type": "summary"
    },
    "1772": {
        "file_id": 167,
        "content": "subtitle_types = [\"ass\", \"srt\"]\nvideo_types = [\n    \"mkv\",\n    \"mov\",\n    \"mp4\",\n    \"flv\",\n    \"avi\",\n    \"ogv\",\n    \"webm\",\n    \"ts\",\n    \"wmv\",\n    \"webm\",\n    \"m4v\",\n    \"3gp\",\n]\n# use ffmpeg for subtitle extraction?\nfiletypes = {\"subtitle\": subtitle_types, \"video\": video_types}\nBangumi_Name = \"Yahari Ore no Seishun Lovecome wa Machigatte Iru.\".strip()\nepisodeIndex = 3\nchinese_simplified_sub_types = [\"chs\", \"简体\", \"简日\"]\nchinese_traditional_sub_types = [\"繁日\", \"繁体\", \"繁體\", \"cht\"]\nimport json\n# replace non-alphanumeric charcters.\nepisode_formatter = lambda episode_index: str(episode_index).zfill(2)\nimport re\n# also replace all double spaces.\ndef double_space_replacer(chars: str):\n    if \"  \" in chars:\n        chars = chars.replace(\"  \", \" \")\n        return double_space_replacer(chars)\n    else:\n        return chars\nalphanumeric_filter = lambda chars: double_space_replacer(\n    re.sub(r\"[^a-z0-9]\", \" \", chars)\n)\nbangume_name_lower_alphanumeric = alphanumeric_filter(Bangumi_Name.lower())\nwith open(\"test_filenames.json\", \"r\") as f:",
        "type": "code",
        "location": "/tests/anime_highlight_cuts/bittorrent_downloader/name_resolution_parsing_chapter_recognition.py:1-46"
    },
    "1773": {
        "file_id": 167,
        "content": "This code defines file types for subtitles and videos, uses ffmpeg for subtitle extraction, defines constants related to a specific anime series, applies an alphanumeric filter to the anime name, and reads filenames from a JSON file.",
        "type": "comment"
    },
    "1774": {
        "file_id": 167,
        "content": "    fnames = json.loads(f.read())\nfor fname in fnames:\n    fname_lower = fname.lower()\n    fname_lower_alphanumeric = alphanumeric_filter(fname_lower)\n    file_extension = fname_lower.split(\".\")[-1]\n    current_file_type = \"unknown\"\n    for filetype, file_extensions in filetypes.items():\n        if file_extension in file_extensions:\n            current_file_type = filetype\n            break\n    print(f\"<{current_file_type}> {fname}\")\n    print(fname_lower_alphanumeric)\n    substring_location_start = fname_lower_alphanumeric.find(\n        bangume_name_lower_alphanumeric\n    )\n    if substring_location_start!=-1:\n        substring_location_end = substring_location_start + len(\n        bangume_name_lower_alphanumeric\n    )\n        assert fname_lower_alphanumeric[substring_location_start: substring_location_end] == bangume_name_lower_alphanumeric\n        episodeIndexLocation = fname_lower_alphanumeric.find(f\" {episode_formatter(episodeIndex)} \")\n        if episodeIndexLocation!=-1:\n            if episodeIndexLocation+1>=substring_location_end:",
        "type": "code",
        "location": "/tests/anime_highlight_cuts/bittorrent_downloader/name_resolution_parsing_chapter_recognition.py:47-71"
    },
    "1775": {
        "file_id": 167,
        "content": "Reading file names from a JSON, filtering, and determining their types. Checking if the bangume name substring is present in the filename. Identifying the episode index location and comparing it with the expected position.",
        "type": "comment"
    },
    "1776": {
        "file_id": 167,
        "content": "                print(\"EPISODE?\") # this is the index we want\n                print(episodeIndex)",
        "type": "code",
        "location": "/tests/anime_highlight_cuts/bittorrent_downloader/name_resolution_parsing_chapter_recognition.py:72-73"
    },
    "1777": {
        "file_id": 167,
        "content": "Code snippet checks the episode index and prints it. If the desired index is not recognized, it displays \"EPISODE?\" for clarification.",
        "type": "comment"
    },
    "1778": {
        "file_id": 168,
        "content": "/tests/anime_highlight_cuts/bittorrent_downloader/nyaa_api_connector.py",
        "type": "filepath"
    },
    "1779": {
        "file_id": 168,
        "content": "The Python script uses requests and BeautifulSoup to search the Nyaa torrent site for anime with 7+ seeders, retrieves results, stores in \"output.html\", and checks if more pages exist using a template and NyaaPy library for torrent handling.",
        "type": "summary"
    },
    "1780": {
        "file_id": 168,
        "content": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\nimport requests\nurl = \"https://nyaa.si\" # change this to mirror sites.\nMIN_SEEDERS=7 # must be greater than this.\nquery = \"oniichan wa oshimai! 01\"\nsort_term = \"seeders\"\nanime_categories = {\n    \"Anime\": \"1_0\",\n    \"Anime - Anime Music Video\": \"1_1\",\n    \"Anime - English-translated\": \"1_2\",\n    \"Anime - Non-English-translated\": \"1_3\",\n    \"Anime - Raw\": \"1_4\",\n}\ncategory_code = anime_categories[\"Anime\"]  # anime\npage = 1  # start page: 1\nend_of_page = False\n# better not to use rss version since it will not sort terms.\nparams = dict(f=0, c=category_code, q=query, s=sort_term, o=\"desc\", p=page)\n# better parse it yourself first huh?\n# r = requests.get(url, params=params)\n# assert r.code == 200\n# text = r.text\nwith open(\"output.html\", \"r\") as f:\n    text = f.read()\nfrom bs4 import BeautifulSoup\n# with open(\"output.html\",'w+') as f:\n#    f.write(text)\nsoup = BeautifulSoup(text, \"html.parser\")\n# breakpoint()\nimport parse\ntemplate = \"Displaying results {start:d}-{end:d} out of {total:d} results.\"",
        "type": "code",
        "location": "/tests/anime_highlight_cuts/bittorrent_downloader/nyaa_api_connector.py:1-48"
    },
    "1781": {
        "file_id": 168,
        "content": "The code is a Python script that uses the requests library to make an API request to the Nyaa torrent site. It searches for a specific anime with 7 or more seeders, retrieves the results, and stores them in a file named \"output.html\". The BeautifulSoup library is used to parse the HTML response, and the parse module seems to be utilized for further processing.",
        "type": "comment"
    },
    "1782": {
        "file_id": 168,
        "content": "banner = soup.find(\"div\", class_=\"pagination-page-info\").text\npagination_info = banner.split(\"\\n\")[0]\npagination_info_result = parse.parse(template, pagination_info)\nif pagination_info_result:\n    if pagination_info_result[\"total\"] == pagination_info_result[\"end\"]:\n        print(\"Reached end of page.\")\n        end_of_page = True\nfrom NyaaPy import utils\nSITE = utils.TorrentSite.NYAASI\njson_info = utils.parse_nyaa(request_text=text, limit=None, site=SITE)\nimport rich\nrich.print(json_info)\n# breakpoint()\nfor videoInfo in json_info:\n    seeders = int(videoInfo['seeders'])\n    seeders_enough = seeders>=MIN_SEEDERS\n    print('seeders?',seeders)\n    print(\"seeders enough?\", seeders_enough)\n    # videoInfo['id'] -> \"https://nyaa.si/view/{}\"\n# you can also download torrent file for only file info.",
        "type": "code",
        "location": "/tests/anime_highlight_cuts/bittorrent_downloader/nyaa_api_connector.py:50-77"
    },
    "1783": {
        "file_id": 168,
        "content": "The code retrieves the banner from a webpage, extracts pagination information, and checks if it has reached the end of the page. It then parses the response using a template and determines if there are enough seeders for each video info. The code prints the number of seeders and whether they are enough based on a minimum seeders threshold. The code uses the NyaaPy library for site-specific torrent handling.",
        "type": "comment"
    },
    "1784": {
        "file_id": 169,
        "content": "/tests/anime_highlight_cuts/bittorrent_downloader/nyaa_torrent_file_list.py",
        "type": "filepath"
    },
    "1785": {
        "file_id": 169,
        "content": "This code fetches data from the Nyaa.si website using requests library, parses it using NyaaPy's utils and torrent modules, and prints the parsed JSON data and the corresponding data class object.",
        "type": "summary"
    },
    "1786": {
        "file_id": 169,
        "content": "url = \"https://nyaa.si/view/1627038\"\nimport requests\nfrom NyaaPy import utils, torrent\nr = requests.get(url)\nSITE = utils.TorrentSite.NYAASI\njson_data = utils.parse_single(request_text=r.text, site=SITE)\ndata_class = torrent.json_to_class(json_data)\nimport rich\n# json_data['seeders']\n# json_data['title']\n# json_data['files']\nrich.print(json_data)\nprint()\nprint(\"_\"*20)\nprint()\nrich.print(data_class)\nbreakpoint()",
        "type": "code",
        "location": "/tests/anime_highlight_cuts/bittorrent_downloader/nyaa_torrent_file_list.py:1-25"
    },
    "1787": {
        "file_id": 169,
        "content": "This code fetches data from the Nyaa.si website using requests library, parses it using NyaaPy's utils and torrent modules, and prints the parsed JSON data and the corresponding data class object.",
        "type": "comment"
    },
    "1788": {
        "file_id": 170,
        "content": "/tests/anime_highlight_cuts/bittorrent_downloader/subtitle_extractor.py",
        "type": "filepath"
    },
    "1789": {
        "file_id": 170,
        "content": "This code references three different tools for extracting subtitles from a video file: mkvextract, ffmpeg, and optical character recognition (OCR). It suggests using the appropriate tool based on the source being processed, with the assumption that it's easier to extract subtitles from fixed locations in Bangumi videos.",
        "type": "summary"
    },
    "1790": {
        "file_id": 170,
        "content": "# use mkvextract:\n# https://github.com/jorti/extract-subs/blob/master/extract-subs.py\n# use ffmpeg:\n# https://github.com/fdenivac/ffextract-subtitles/blob/master/ffextract-subtitles.py\n# use ocr to extract subtitles. since this is bangumi, it is easy to extract from fixed location.",
        "type": "code",
        "location": "/tests/anime_highlight_cuts/bittorrent_downloader/subtitle_extractor.py:1-7"
    },
    "1791": {
        "file_id": 170,
        "content": "This code references three different tools for extracting subtitles from a video file: mkvextract, ffmpeg, and optical character recognition (OCR). It suggests using the appropriate tool based on the source being processed, with the assumption that it's easier to extract subtitles from fixed locations in Bangumi videos.",
        "type": "comment"
    },
    "1792": {
        "file_id": 171,
        "content": "/tests/anime_highlight_cuts/bittorrent_downloader/torrent_analyzer.py",
        "type": "filepath"
    },
    "1793": {
        "file_id": 171,
        "content": "The code snippet imports libraries, defines paths and analyzes a torrent file using torrent_parser. It prints the data, checks for multiple files, stores their names and lengths (if applicable), formats size, prints file details, writes filenames to JSON, and prepares the file for further processing.",
        "type": "summary"
    },
    "1794": {
        "file_id": 171,
        "content": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\nimport os\n# single file.\n# torrent_path = \"[桜都字幕組] 不當哥哥了！ _ Onii-chan wa Oshimai! [01][1080p][繁體內嵌].torrent\"\ntorrent_path = \"[Kamigami&VCB-Studio] Yahari Ore no Seishun Lovecome wa Machigatte Iru. [Ma10p_1080p].torrent\"\nbasepath = \"/Users/jamesbrown/Downloads/anime_download\"\ntorrent_path = os.path.join(basepath, torrent_path)\n# analyze this torrent file.\nimport torrent_parser as tp\ndata = tp.parse_torrent_file(torrent_path)\nimport rich\nrich.print(data)\n# will be complete name later?\nsingle_file = not('files' in data['info'].keys())\n# data['info']['name'] \n# length will be total length?\n# data['info']['length']\n# breakpoint()\n# does it preserve the order?\n# import humanize\n# well.\nfnames=[]\nimport json\nfrom humanfriendly import format_size\nif not single_file:\n    for index, fileInfo in enumerate(data['info']['files']):\n        aria2c_index = index+1\n        length = fileInfo['length']\n        path = fileInfo['path'] # multiple strings in a list\n        joined_path = \"/\".join(path)",
        "type": "code",
        "location": "/tests/anime_highlight_cuts/bittorrent_downloader/torrent_analyzer.py:1-41"
    },
    "1795": {
        "file_id": 171,
        "content": "Code snippet imports necessary libraries, defines a torrent file path and basepath for downloads, joins the paths, analyzes the torrent file using torrent_parser, prints the parsed data, checks if the torrent contains multiple files or not, stores the name and length of each file (if applicable), converts file paths to single string format, and finally prepares the file for further processing.",
        "type": "comment"
    },
    "1796": {
        "file_id": 171,
        "content": "        filesize_human_readable = format_size(length)\n        print(f\"[{aria2c_index}] ** [{filesize_human_readable}] ** {path[-1]}\")\n        # the index is right.\n        fnames.append(path[-1])\n        print(f\"FULLPATH: {joined_path}\")\nwith open(\"test_filenames.json\",'w+') as f:\n    f.write(json.dumps(fnames))",
        "type": "code",
        "location": "/tests/anime_highlight_cuts/bittorrent_downloader/torrent_analyzer.py:42-49"
    },
    "1797": {
        "file_id": 171,
        "content": "This code snippet formats the file size in human-readable format and prints it along with the aria2c index, file name, and full path. It then stores the filenames in a list and writes them to a JSON file named \"test_filenames.json\".",
        "type": "comment"
    },
    "1798": {
        "file_id": 172,
        "content": "/tests/anime_highlight_cuts/bittorrent_downloader/webtorrent_streaming_test_cut_partial_download.mjs",
        "type": "filepath"
    },
    "1799": {
        "file_id": 172,
        "content": "The code downloads videos using Webtorrent, handles temporary directories and exceptions but has string concatenation issues. It suggests Unix domain sockets for performance improvement, uses FFmpeg to download segments, handles progress/errors and terminates upon torrent completion, though unpipe is unused and readstream may show progress issues.",
        "type": "summary"
    }
}