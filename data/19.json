{
    "1900": {
        "file_id": 187,
        "content": "import json\nfrom test_commons import *\nfrom pyjom.commons import *\nimport cv2\ndef getVideoPixels(videoPath):\n    from MediaInfo import MediaInfo\n    info = MediaInfo(filename=videoPath)\n    infoData = info.getInfo()\n    # print(infoData)\n    # breakpoint()\n    defaultWidth = infoData[\"videoWidth\"]\n    defaultHeight = infoData[\"videoHeight\"]\n    return defaultWidth, defaultHeight\n# easy gig, you said.\n# basePath = \"/Users/jamesbrown/desktop/works/pyjom_remote\"\nbasePath = \"/root/Desktop/works/pyjom\"\ntargetFile = (\n    basePath + \"/tests/bilibili_practices/bilibili_video_translate/japan_day.json\"\n)\noriginalFile = (\n    basePath + \"/tests/bilibili_practices/bilibili_video_translate/japan_day.webm\"\n)\n# visualization can only be done here?\n# where is the original file?\nmJson = json.loads(open(targetFile, \"r\", encoding=\"utf-8\").read())\nimport numpy as np\nwidth, height = getVideoPixels(originalFile)\ndef getBlackPicture(width, height):\n    blackPicture = np.zeros((height, width, 1), dtype=\"uint8\")  # this is grayscale.\n    return blackPicture",
        "type": "code",
        "location": "/tests/unittest_convolution_bilibili_translate_text_detect.py:1-41"
    },
    "1901": {
        "file_id": 187,
        "content": "The code is importing necessary libraries and defining a function `getVideoPixels` to retrieve the default video width and height from a given video file. It then sets the base path, target file, and original file paths. The code reads the target JSON file, loads it into a variable `mJson`, and retrieves the video dimensions using the `getVideoPixels` function. Finally, it defines a function `getBlackPicture` to create a black grayscale image with the specified width and height.",
        "type": "comment"
    },
    "1902": {
        "file_id": 187,
        "content": "mKeys = list(mJson.keys())\nmIntKeys = [int(x) for x in mKeys]\nminKey, maxKey = min(mIntKeys), max(mIntKeys)\n# imutils is created by pyimagesearch.\nfrom imutils.object_detection import non_max_suppression\ndef getConvBlurredCurrentShot(blurredSpan, span=5):\n    # honor the most the latest one.\n    mImage = None\n    for index, blurredImage in enumerate(blurredSpan):\n        ratio = index / span\n        if mImage is None:\n            mImage = blurredImage * ratio\n        else:\n            mImage += blurredImage * ratio\n    # print(mImage.shape)\n    # breakpoint()\n    # change this mImage.\n    mImage = mImage > 128\n    mImage = mImage.astype(np.uint8)\n    mImage = mImage * 255\n    return mImage\n    # return 256*((mImage>128).astype(np.uint8))\nconvolutionSpan = 20\nconvolutionBoundingBoxSpan = []\nconvolutionBlurredSpan = []\nfor intKey in range(minKey, maxKey + 1):\n    strKey = str(intKey)\n    target = mJson[strKey]\n    boundingBoxes = []\n    for item in target:\n        location = item[0]\n        text, confidence = item[1]\n        # print(\"location\",location) # four points. do not know if there is any rotation here.",
        "type": "code",
        "location": "/tests/unittest_convolution_bilibili_translate_text_detect.py:44-86"
    },
    "1903": {
        "file_id": 187,
        "content": "This code defines a function `getConvBlurredCurrentShot` that averages multiple blurred images to create a final image. It also initializes variables for convolution bounding boxes and blurred spans based on a range of keys in `mJson`. The resulting image is then thresholded and converted to 8-bit format before being returned.",
        "type": "comment"
    },
    "1904": {
        "file_id": 187,
        "content": "        if confidence > 0.7:\n            npLocation = np.array(location)\n            xlocs = npLocation[:, 0]\n            ylocs = npLocation[:, 1]\n            # print(xlocs)\n            # print(ylocs)\n            # breakpoint()\n            minX, maxX = min(xlocs), max(xlocs)\n            minY, maxY = min(ylocs), max(ylocs)\n            boundingBox = [minX, minY, maxX, maxY]\n            boundingBoxes.append(boundingBox.copy())\n            # breakpoint()\n        # print(\"text\", text)\n        # print(\"confidence\", confidence)\n    convolutionBoundingBoxSpan.append(boundingBoxes.copy())\n    if len(convolutionBoundingBoxSpan) > convolutionSpan:\n        convolutionBoundingBoxSpan.pop(0)\n    # do your calculation!\n    flatSpan = [y for x in convolutionBoundingBoxSpan for y in x]\n    flatSpan = np.array(flatSpan)\n    currentNonOverlappingBoxes = non_max_suppression(flatSpan)\n    # print(intKey,target)\n    # this time we do not care about the text inside.\n    blackPicture = getBlackPicture(width, height)\n    for rectangle in flatSpan:",
        "type": "code",
        "location": "/tests/unittest_convolution_bilibili_translate_text_detect.py:87-111"
    },
    "1905": {
        "file_id": 187,
        "content": "The code processes bounding boxes from a convolution operation, filters them based on confidence score, and performs non-maximum suppression to eliminate overlapping boxes. It then creates an array of non-overlapping bounding boxes and generates a black picture with the same width and height as the original image.",
        "type": "comment"
    },
    "1906": {
        "file_id": 187,
        "content": "        # make it all int.\n        x0, y0, x1, y1 = [int(num) for num in rectangle]\n        loc0 = (x0, y0)\n        loc1 = (x1, y1)\n        cv2.rectangle(\n            blackPicture, loc0, loc1, 255, cv2.FILLED\n        )  # we fill so we can merge shits.\n    blackPictureBlurred = cv2.GaussianBlur(blackPicture, (33, 33), 0)\n    convolutionBlurredSpan.append(blackPictureBlurred.copy())\n    if len(convolutionBlurredSpan) > convolutionSpan:\n        convolutionBlurredSpan.pop(0)\n    currentBlackPictureBlurred = getConvBlurredCurrentShot(\n        convolutionBlurredSpan, span=convolutionSpan\n    )\n    # print(currentBlackPictureBlurred.shape)\n    print(\"boundingBoxes:\", len(flatSpan))\n    if len(flatSpan) == 0:\n        continue\n    contours = cv2.findContours(\n        currentBlackPictureBlurred, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n    )\n    contours = contours[0] if len(contours) == 2 else contours[1]\n    currentBoundingBoxesVisualize = getBlackPicture(width, height)\n    for i in contours:\n        x, y, w, h = cv2.boundingRect(i)",
        "type": "code",
        "location": "/tests/unittest_convolution_bilibili_translate_text_detect.py:112-142"
    },
    "1907": {
        "file_id": 187,
        "content": "The code creates a rectangle from input, fills it in the black picture, blurs the filled image, appends it to a list if length is less than convolutionSpan, pops oldest if length exceeds convolutionSpan, gets the current blurred image from the list, prints the bounding boxes count, and if no elements in flatSpan, continues. It then finds contours in the current blurred image and creates a new image for visualization of bounding rectangles.",
        "type": "comment"
    },
    "1908": {
        "file_id": 187,
        "content": "        cv2.rectangle(currentBoundingBoxesVisualize, (x, y), (x + w, y + h), 255, 4)\n    cv2.imshow(\"IMAGE\", currentBoundingBoxesVisualize)\n    cv2.waitKey(10)\n    print(\"showing image:\", intKey)\n    # print\n    # cv2.waitKey(1000)\n    # print(\"NON OVERLAPPING BOXES:\")\n    # print(currentNonOverlappingBoxes)\n    # we need to visualize this shit.\n    # breakpoint()\ncv2.destroyAllWindows()\nprint(\"THE END\")",
        "type": "code",
        "location": "/tests/unittest_convolution_bilibili_translate_text_detect.py:143-156"
    },
    "1909": {
        "file_id": 187,
        "content": "This code snippet is responsible for visualizing bounding boxes, displaying an image, and waiting for a key press. It prints the non-overlapping boxes but may require visualization. The code will close all windows at the end with a final message \"THE END\".",
        "type": "comment"
    },
    "1910": {
        "file_id": 188,
        "content": "/tests/unittest_ffmpeg_args.py",
        "type": "filepath"
    },
    "1911": {
        "file_id": 188,
        "content": "The code processes video files using FFmpeg for tasks like cropping and scaling, with a specific command to map and filter video/audio streams. This is part of a larger script that uses the subprocess module.",
        "type": "summary"
    },
    "1912": {
        "file_id": 188,
        "content": "command_original = [\n    \"ffmpeg\",\n    \"-y\",\n    \"-ss\",\n    \"0\",\n    \"-to\",\n    \"59.3942553191489\",\n    \"-i\",\n    \"/root/Desktop/works/pyjom/samples/video/LiGlReJ4i.mp4\",\n    \"-ss\",\n    \"59.3942553191489\",\n    \"-to\",\n    \"62.0340000000000\",\n    \"-i\",\n    \"/root/Desktop/works/pyjom/samples/video/LiGlReJ4i.mp4\",\n    \"-ss\",\n    \"0\",\n    \"-to\",\n    \"62.034\",\n    \"-i\",\n    \"/root/Desktop/works/pyjom/samples/video/LiGlReJ4i.mp4\",\n    \"-filter_complex\",\n    \"[0:v]crop=h=1099:w=717:x=1:y=72[s0];[s0]pad=color=black:height=max(ih\\\\, ceil(iw*max(1080/1920\\\\, ih/iw))):width=max(iw\\\\, ceil(ih*max(1920/1080\\\\, iw/ih))):x=floor((max(iw\\\\, ceil(ih*max(1920/1080\\\\, iw/ih)))-iw)/2):y=floor((max(ih\\\\, ceil(iw*max(1080/1920\\\\, ih/iw)))-ih)/2)[s1];[s1]scale=1920:1080[s2];[s2]scale=ceil((iw*0.15555555555555556)/4)*4:ceil((ih*0.15555555555555556)/4)*4[s3];[1:v]pad=color=black:height=max(ih\\\\, ceil(iw*max(1080/1920\\\\, ih/iw))):width=max(iw\\\\, ceil(ih*max(1920/1080\\\\, iw/ih))):x=floor((max(iw\\\\, ceil(ih*max(1920/1080\\\\, iw/ih)))-iw",
        "type": "code",
        "location": "/tests/unittest_ffmpeg_args.py:1-23"
    },
    "1913": {
        "file_id": 188,
        "content": "This code uses FFmpeg to split a video file into segments, applies various filters and transformations to the segments, and finally scales and pads them before saving the final output.",
        "type": "comment"
    },
    "1914": {
        "file_id": 188,
        "content": ")/2):y=floor((max(ih\\\\, ceil(iw*max(1080/1920\\\\, ih/iw)))-ih)/2)[s4];[s4]scale=1920:1080[s5];[s5]scale=ceil((iw*0.15555555555555556)/4)*4:ceil((ih*0.15555555555555556)/4)*4[s6];[s3][s6]concat=n=2[s7]\",\n    \"-map\",\n    \"[s7]\",\n    \"-map\",\n    \"2:a\",\n    \"/dev/shm/2c6b1466-6186-41dd-9ce3-2f757c082c5a.mp4\",\n]\ncommand2 = [\n    \"ffmpeg\",\n    \"-y\",\n    \"-ss\",\n    \"0\",\n    \"-to\",\n    \"59.3942553191489\",\n    \"-i\",\n    \"/root/Desktop/works/pyjom/samples/video/LiGlReJ4i.mp4\",\n    \"-ss\",\n    \"0\",\n    \"-to\",\n    \"62.034\",\n    \"-i\",\n    \"/root/Desktop/works/pyjom/samples/video/LiGlReJ4i.mp4\",\n    \"-filter_complex\",\n    \"[0:v]crop=h=1099:w=717:x=1:y=72[s0];[s0]pad=color=black:height=max(ih\\\\, ceil(iw*max(1080/1920\\\\, ih/iw))):width=max(iw\\\\, ceil(ih*max(1920/1080\\\\, iw/ih))):x=floor((max(iw\\\\, ceil(ih*max(1920/1080\\\\, iw/ih)))-iw)/2):y=floor((max(ih\\\\, ceil(iw*max(1080/1920\\\\, ih/iw)))-ih)/2)[s1];[s1]scale=1920:1080[s2];[s2]scale=ceil((iw*0.15555555555555556)/4)*4:ceil((ih*0.15555555555555556)/4)*4[s3]\",\n    \"-map\",\n    \"[s3]\",",
        "type": "code",
        "location": "/tests/unittest_ffmpeg_args.py:23-48"
    },
    "1915": {
        "file_id": 188,
        "content": "The code is constructing a command for the ffmpeg tool to process and concatenate multiple video inputs. It applies filters such as cropping, padding, scaling, and extracts specific parts of videos before concatenating them into a single output video file. The resulting command is being stored in `command1` and `command2`.",
        "type": "comment"
    },
    "1916": {
        "file_id": 188,
        "content": "    \"-map\",\n    \"1:a\",\n    \"/dev/shm/2c6b1466-6186-41dd-9ce3-2f757c082c5a.mp4\",\n]\ncommand3 = [\n    \"ffmpeg\",\n    \"-y\",\n    \"-ss\",\n    \"59.3942553191489\",\n    \"-to\",\n    \"62.0340000000000\",\n    \"-i\",\n    \"/root/Desktop/works/pyjom/samples/video/LiGlReJ4i.mp4\",\n    \"-ss\",\n    \"0\",\n    \"-to\",\n    \"62.034\",\n    \"-i\",\n    \"/root/Desktop/works/pyjom/samples/video/LiGlReJ4i.mp4\",\n    \"-filter_complex\",\n    \"[0:v]pad=color=black:height=max(ih\\\\, ceil(iw*max(1080/1920\\\\, ih/iw))):width=max(iw\\\\, ceil(ih*max(1920/1080\\\\, iw/ih))):x=floor((max(iw\\\\, ceil(ih*max(1920/1080\\\\, iw/ih)))-iw)/2):y=floor((max(ih\\\\, ceil(iw*max(1080/1920\\\\, ih/iw)))-ih)/2)[s4];[s4]scale=1920:1080[s5];[s5]scale=ceil((iw*0.15555555555555556)/4)*4:ceil((ih*0.15555555555555556)/4)*4[s6]\",\n    \"-map\",\n    \"[s6]\",\n    \"-map\",\n    \"1:a\",\n    \"/dev/shm/2c6b1466-6186-41dd-9ce3-2f757c082c5a.mp4\",\n]\ncommandImprovised = command_original = [\n    \"ffmpeg\",\n    \"-y\",\n    \"-ss\",\n    \"0\",\n    \"-to\",\n    \"59.3942553191489\",\n    \"-i\",\n    \"/root/Desktop/works/pyjom/samples/video/LiGlReJ4i.mp4\",",
        "type": "code",
        "location": "/tests/unittest_ffmpeg_args.py:49-84"
    },
    "1917": {
        "file_id": 188,
        "content": "This code is using FFmpeg command line arguments to perform operations on video files. It's mapping streams, applying filters for scaling and padding, setting start/end times, and specifying output file paths. The code is likely involved in video processing or manipulation tasks.",
        "type": "comment"
    },
    "1918": {
        "file_id": 188,
        "content": "    \"-ss\",\n    \"59.3942553191489\",\n    \"-to\",\n    \"62.0340000000000\",\n    \"-i\",\n    \"/root/Desktop/works/pyjom/samples/video/LiGlReJ4i.mp4\",\n    \"-ss\",\n    \"0\",\n    \"-to\",\n    \"62.034\",\n    \"-i\",\n    \"/root/Desktop/works/pyjom/samples/video/LiGlReJ4i.mp4\",\n    \"-filter_complex\",\n    \"[0:v]crop=h=1099:w=717:x=1:y=72[s0];[s0]pad=color=black:height=max(ih\\\\, ceil(iw*max(1080/1920\\\\, ih/iw))):width=max(iw\\\\, ceil(ih*max(1920/1080\\\\, iw/ih))):x=floor((max(iw\\\\, ceil(ih*max(1920/1080\\\\, iw/ih)))-iw)/2):y=floor((max(ih\\\\, ceil(iw*max(1080/1920\\\\, ih/iw)))-ih)/2)[s1];[s1]scale=1920:1080[s2];[s2]scale=ceil((iw*0.15555555555555556)/4)*4:ceil((ih*0.15555555555555556)/4)*4,setsar=1[s3];[1:v]pad=color=black:height=max(ih\\\\, ceil(iw*max(1080/1920\\\\, ih/iw))):width=max(iw\\\\, ceil(ih*max(1920/1080\\\\, iw/ih))):x=floor((max(iw\\\\, ceil(ih*max(1920/1080\\\\, iw/ih)))-iw)/2):y=floor((max(ih\\\\, ceil(iw*max(1080/1920\\\\, ih/iw)))-ih)/2)[s4];[s4]scale=1920:1080[s5];[s5]scale=ceil((iw*0.15555555555555556)/4)*4:ceil((ih*0.15555555555555556)/4)*4,setsar=1[s6];[s3][s6]concat=n=2[s7]\",",
        "type": "code",
        "location": "/tests/unittest_ffmpeg_args.py:85-98"
    },
    "1919": {
        "file_id": 188,
        "content": "This code is using FFmpeg to crop, scale, and concatenate video streams. It first specifies start and end times for the input video file \"/root/Desktop/works/pyjom/samples/video/LiGlReJ4i.mp4\", then applies a series of filters including cropping, padding, scaling, and setting aspect ratio. Finally, it concatenates the resulting streams for output.",
        "type": "comment"
    },
    "1920": {
        "file_id": 188,
        "content": "    \"-map\",\n    \"[s7]\",\n    \"-map\",\n    \"2:a\",\n    \"/dev/shm/2c6b1466-6186-41dd-9ce3-2f757c082c5a.mp4\",\n]\nimport subprocess\nsubprocess.run(commandImprovised)",
        "type": "code",
        "location": "/tests/unittest_ffmpeg_args.py:99-107"
    },
    "1921": {
        "file_id": 188,
        "content": "This code chunk is part of a larger script that uses the subprocess module to run an FFmpeg command. The command maps video stream from input file \"[s7]\" and audio stream from track 2 to output \"/dev/shm/2c6b1466-6186-41dd-9ce3-2f757c082c5a.mp4\".",
        "type": "comment"
    },
    "1922": {
        "file_id": 189,
        "content": "/tests/unittest_ffmpeg_delogo_parser.py",
        "type": "filepath"
    },
    "1923": {
        "file_id": 189,
        "content": "This code defines a delogoParser function to parse command strings and processes video streams using the \"delogo\" filter. It checks parameter validity, removes logos from videos, and handles errors for debugging purposes.",
        "type": "summary"
    },
    "1924": {
        "file_id": 189,
        "content": "import parse\nfrom pyjom.videotoolbox import getVideoWidthHeight\nfrom test_commons import *\nimport ffmpeg\ncommandString = \"delogo_0_671_360_6|delogo_144_662_6_4|delogo_355_661_5_7|delogo_117_661_7_5|delogo_68_661_18_5|delogo_182_658_165_9|delogo_252_492_3_1|delogo_214_492_1_2|delogo_200_492_3_1|delogo_74_492_2_1|delogo_170_490_6_4|delogo_145_490_9_4|delogo_129_490_12_4|delogo_107_490_4_3|delogo_91_487_8_6|delogo_72_485_4_3|delogo_147_484_4_3|delogo_178_483_11_11|delogo_219_480_1_1|delogo_53_480_6_2|delogo_268_478_1_1|delogo_164_478_8_4|delogo_128_477_8_4|delogo_295_475_1_1|delogo_105_475_10_4|delogo_61_474_5_4|delogo_274_472_3_2|delogo_196_470_5_2|delogo_209_469_1_1|delogo_143_469_8_5|delogo_75_467_26_6|delogo_0_33_360_25|delogo_0_24_360_6\"\nvideoPath = \"/root/Desktop/works/pyjom/samples/video/LkS8UkiLL.mp4\"\noutputPath = \"/dev/shm/output.mp4\"\ndef delogoParser(command):\n    return parse.parse(\"delogo_{x:d}_{y:d}_{w:d}_{h:d}\", command)\nwidth, height = getVideoWidthHeight(videoPath)\ndef delogoFilter(stream, commandParams):",
        "type": "code",
        "location": "/tests/unittest_ffmpeg_delogo_parser.py:1-20"
    },
    "1925": {
        "file_id": 189,
        "content": "This code defines a delogoParser function that parses a command string into a formatted format using regular expression. It also includes the getVideoWidthHeight function to retrieve the video width and height from a given path, and a delogoFilter function to process video streams with a given command parameter. The commandString contains a list of delogo positions, and the script will process a video at the specified output path.",
        "type": "comment"
    },
    "1926": {
        "file_id": 189,
        "content": "    return stream.filter(\n        \"delogo\",\n        x=commandParams[\"x\"],\n        y=commandParams[\"y\"],\n        w=commandParams[\"w\"],\n        h=commandParams[\"h\"],\n    )\n# minArea = 20\ndef checkXYWH(XYWH, canvas, minArea=20):\n    x, y, w, h = XYWH\n    width, height = canvas\n    if x >= width - 1 or y >= height - 1:\n        return False, None\n    if x == 0:\n        x = 1\n    if y == 0:\n        y = 1\n    if x + w >= width:\n        w = width - x - 1\n        if w <= 2:\n            return False, None\n    if y + h >= height:\n        h = height - y - 1\n        if h <= 2:\n            return False, None\n    if w * h <= minArea:\n        return False, None\n    return True, (x, y, w, h)\nfor command in commandString.split(\"|\"):\n    try:\n        stream = ffmpeg.input(videoPath, ss=0, to=5).video\n        commandArguments = delogoParser(command)\n        x = commandArguments[\"x\"]\n        y = commandArguments[\"y\"]\n        w = commandArguments[\"w\"]\n        h = commandArguments[\"h\"]\n        status, XYWH = checkXYWH((x, y, w, h), (width, height))",
        "type": "code",
        "location": "/tests/unittest_ffmpeg_delogo_parser.py:21-64"
    },
    "1927": {
        "file_id": 189,
        "content": "This code snippet is a part of a larger program that involves video editing using the FFmpeg library. It filters a video stream with the \"delogo\" filter, taking command parameters for x, y, w, and h. Then, it checks if these parameters are valid by calling the checkXYWH function, which returns True or False depending on the input's validity. Finally, it loops through each command in the commandString, splitting them into smaller commands for video editing operations.",
        "type": "comment"
    },
    "1928": {
        "file_id": 189,
        "content": "        if not status:\n            continue\n        x, y, w, h = XYWH\n        commandArguments = {\"x\": x, \"y\": y, \"w\": w, \"h\": h}\n        stream = delogoFilter(stream, commandArguments)\n        ffmpeg.output(stream, outputPath).run(overwrite_output=True)\n    except:\n        import traceback\n        traceback.print_exc()\n        print(\"WIDTH:\", width, \"HEIGHT:\", height)\n        maxX, maxY = (\n            commandArguments[\"x\"] + commandArguments[\"w\"],\n            commandArguments[\"y\"] + commandArguments[\"h\"],\n        )\n        print(\"MAX X:\", maxX, \"MAX Y:\", maxY)\n        print(\"ERROR!\", commandArguments)\n        breakpoint()",
        "type": "code",
        "location": "/tests/unittest_ffmpeg_delogo_parser.py:65-83"
    },
    "1929": {
        "file_id": 189,
        "content": "This code is implementing a delogo filter, which takes input video and removes the logo from it. It checks if the filter was successfully applied, then extracts the position and dimensions of the logo to apply the delogo filter. If any error occurs during this process, it prints out information for debugging and stops the execution.",
        "type": "comment"
    },
    "1930": {
        "file_id": 190,
        "content": "/tests/unittest_ffmpeg_cropdetect_from_image_parse_log.py",
        "type": "filepath"
    },
    "1931": {
        "file_id": 190,
        "content": "This code uses ffmpeg and OpenCV to detect cropped areas, calculates the cropped area ratio, and decides whether to crop the image based on a threshold. The result depends on the specified threshold value.",
        "type": "summary"
    },
    "1932": {
        "file_id": 190,
        "content": "import ffmpeg\nfrom lazero.utils.importers import cv2_custom_build_init\ncv2_custom_build_init()\n# mediaPath = \"/root/Desktop/works/pyjom/samples/image/dog_blue_sky_split_line.png\"\nmediaPath = \"/root/Desktop/works/pyjom/samples/image/dog_with_black_borders.png\"  # use the image with black background.\n# ffmpeg -loop 1 -i /root/Desktop/works/pyjom/samples/image/dog_blue_sky_split_line.png -t 15 -vf cropdetect -f null -\nimport cv2\nimage = cv2.imread(mediaPath)\nheight, width = image.shape[:2]\ntotal_area = height * width\nareaThreshold = 0\nstdout, stderr = (\n    ffmpeg.input(mediaPath, loop=1, t=15)\n    .filter(\"cropdetect\")\n    .output(\"null\", f=\"null\")\n    .run(capture_stdout=True, capture_stderr=True)\n)\nstdout_decoded = stdout.decode(\"utf-8\")\nstderr_decoded = stderr.decode(\"utf-8\")\n# nothing here.\n# for line in stdout_decoded.split(\"\\n\"):\n#     print(line)\n# breakpoint()\nimport parse\ncropped_area_threshold = 0.1\ncommon_crops = []\nfor line in stderr_decoded.split(\"\\n\"):\n    line = line.replace(\"\\n\", \"\").strip()\n    for",
        "type": "code",
        "location": "/tests/unittest_ffmpeg_cropdetect_from_image_parse_log.py:1-40"
    },
    "1933": {
        "file_id": 190,
        "content": "The code imports necessary libraries and initializes them, sets the media path to an image with a black background, runs ffmpeg on the image with a cropdetect filter, decodes the output and errors, iterates over the stderr output lines to extract cropped areas, and defines a variable for common_crops.",
        "type": "comment"
    },
    "1934": {
        "file_id": 190,
        "content": "matString = \"[{}] x1:{x1:d} x2:{x2:d} y1:{y1:d} y2:{y2:d} w:{w:d} h:{h:d} x:{x:d} y:{y:d} pts:{pts:g} t:{t:g} crop={}:{}:{}:{}\"\n    # print(line)\n    result = parse.parse(formatString, line)\n    if result is not None:\n        # print(result)\n        cropString = \"{}_{}_{}_{}\".format(\n            *[result[key] for key in [\"w\", \"h\", \"x\", \"y\"]]\n        )\n        # print(cropString)\n        # breakpoint()\n        common_crops.append(cropString)\n    # [Parsed_cropdetect_0 @ 0x56246a16cbc0] x1:360 x2:823 y1:0 y2:657 w:464 h:656 x:360 y:2 pts:3 t:0.120000 crop=464:656:360:2\n    # this crop usually will never change. but let's count?\narea = 0\nx, x1, y, y1 = 0, width, 0, height\nif len(common_crops) > 0:\n    common_crops_count_tuple_list = [\n        (cropString, common_crops.count(cropString)) for cropString in set(common_crops)\n    ]\n    common_crops_count_tuple_list.sort(key=lambda x: -x[1])\n    selected_crop_string = common_crops_count_tuple_list[0][0]\n    result = parse.parse(\"{w:d}_{h:d}_{x:d}_{y:d}\", selected_crop_string)",
        "type": "code",
        "location": "/tests/unittest_ffmpeg_cropdetect_from_image_parse_log.py:40-62"
    },
    "1935": {
        "file_id": 190,
        "content": "Code parses a log line, extracts crop information and stores it in common_crops list. It then counts the occurrence of each unique crop string and selects the most frequent one (selected_crop_string). Finally, it parses the selected_crop_string to get the crop dimensions (w, h, x, y) and assigns them to their respective variables.",
        "type": "comment"
    },
    "1936": {
        "file_id": 190,
        "content": "    w, h, x, y = [result[key] for key in [\"w\", \"h\", \"x\", \"y\"]]\n    x1, y1 = min(x + w, width), min(y + h, height)\n    if x < x1 and y < y1:\n        # allow to calculate the area.\n        area = (x1 - x) * (y1 - y)\ncropped_area_ratio = 1 - (area / total_area)  # 0.5652352766414517\n# use 0.1 as threshold?\nprint(\"CROPPED AREA RATIO:\", cropped_area_ratio)\nif cropped_area_ratio > cropped_area_threshold:\n    print(\"we need to crop this. no further processing needed\")\n    image_black_cropped = image[y:y1, x:x1]\n    cv2.imshow(\"CROPPED IMAGE\", image_black_cropped)\n    cv2.waitKey(0)\nelse:\n    print(\"image no need to crop black borders. further processing needed\")",
        "type": "code",
        "location": "/tests/unittest_ffmpeg_cropdetect_from_image_parse_log.py:63-78"
    },
    "1937": {
        "file_id": 190,
        "content": "This code calculates the cropped area ratio of an image and decides whether to crop it or not based on a threshold. If the ratio is greater than the threshold, it crops the image using OpenCV and displays the cropped image. Otherwise, it proceeds with further processing. The result depends on the specified threshold value.",
        "type": "comment"
    },
    "1938": {
        "file_id": 191,
        "content": "/tests/unittest_mathlib_ranges_continual.py",
        "type": "filepath"
    },
    "1939": {
        "file_id": 191,
        "content": "This code imports necessary modules and defines input data. It then uses different functions to get results and prints them, with some additional operations like range extraction and rendering. The code seems to be focused on merging ranges of data using a specific library (mathlib) and performing certain operations with the merged data.",
        "type": "summary"
    },
    "1940": {
        "file_id": 191,
        "content": "from test_commons import *\nfrom pyjom.mathlib import *\ninputList = [[(0, 1), (1, 1.1), (2, 3)], [(0.5, 1.5), (1.6, 2.5)]]\nmRangesDict = {\"sample_%s\" % num: inputList[num] for num in range(len(inputList))}\nresult_0 = getContinualNonSympyMergeResult(inputList)\nprint(result_0)\nprint(\"_\" * 20)\n# want to build a language?\nresult_1 = getContinualMappedNonSympyMergeResult(mRangesDict, concatSymbol=\"|\")\nprint(result_1)\nprint(\"_\" * 20)\nresult_2 = getContinualMappedNonSympyMergeResult(\n    mRangesDict, concatSymbol=\"|\", noEmpty=False\n)\nprint(result_2)\nprint(\"_\" * 20)\nstart, end = -1, 4\nresult_3 = getContinualMappedNonSympyMergeResultWithRangedEmpty(\n    mRangesDict, start, end, concatSymbol=\"|\"\n)\nprint(result_3)\nprint(\"_\" * 20)\nrenderList = mergedRangesToSequential(result_3)\nfor renderCommandString, commandTimeSpan in renderList:\n    print(renderCommandString, commandTimeSpan)\nprint(\"_\" * 20)\nfinalCatsMapped = getContinualMappedNonSympyMergeResult({})\nprint(finalCatsMapped)",
        "type": "code",
        "location": "/tests/unittest_mathlib_ranges_continual.py:1-36"
    },
    "1941": {
        "file_id": 191,
        "content": "This code imports necessary modules and defines input data. It then uses different functions to get results and prints them, with some additional operations like range extraction and rendering. The code seems to be focused on merging ranges of data using a specific library (mathlib) and performing certain operations with the merged data.",
        "type": "comment"
    },
    "1942": {
        "file_id": 192,
        "content": "/tests/unittest_ffmpegVideoPreProductionFilter.py",
        "type": "filepath"
    },
    "1943": {
        "file_id": 192,
        "content": "This code imports necessary modules, checks ffmpeg, and utilizes MediaInfo for duration. It uses UUID to generate a unique cache file name. The code tests text detection in videos using ffmpeg filters, iterating through videoPaths and applying the filter on each video, while handling false positives and potential None output.",
        "type": "summary"
    },
    "1944": {
        "file_id": 192,
        "content": "from test_commons import *\nfrom pyjom.medialang.processors.dotProcessor import ffmpegVideoPreProductionFilter\nimport tempfile\n# import MediaInfo\nvideoPaths = {\n    \"text\": \"/root/Desktop/works/pyjom/samples/video/dog_with_text.mp4\",\n    \"logo\": \"/root/Desktop/works/pyjom/samples/video/LkS8UkiLL.mp4\",\n    # \"pip\":\"/root/Desktop/works/pyjom/samples/video/LiGlReJ4i.mp4\", # najie\n    \"pip\": \"/root/Desktop/works/pyjom/samples/video/LiEIfnsvn.mp4\",  # double pip\n    # 'complete':\"/root/Desktop/works/pyjom/samples/video/dog_with_text.mp4\"\n}\ntempDir = \"/dev/shm/medialang\"  # anyway we just want something else...\ntest_ffmpeg = True\ntest_text_detector = False\ndef getVideoDuration(filePath):\n    from MediaInfo import MediaInfo\n    info = MediaInfo(filename=videoPath)\n    infoData = info.getInfo()\n    # print(infoData)\n    # breakpoint()\n    defaultWidth = infoData[\"videoWidth\"]\n    defaultHeight = infoData[\"videoHeight\"]\n    # print(infoData)\n    # print(infoData.keys())\n    # breakpoint()\n    start = 0\n    end = float(infoData[\"videoDuration\"])",
        "type": "code",
        "location": "/tests/unittest_ffmpegVideoPreProductionFilter.py:1-34"
    },
    "1945": {
        "file_id": 192,
        "content": "The code imports necessary modules, defines video paths and temporary directory locations, tests ffmpeg functionality, and retrieves the duration of a video using MediaInfo library.",
        "type": "comment"
    },
    "1946": {
        "file_id": 192,
        "content": "    return end\ntestSubject = \"complete\"\nwith tempfile.TemporaryDirectory(prefix=tempDir) as allocatedTmpDir:\n    print(\"Allocated tmpDir:\", allocatedTmpDir)\n    if testSubject == \"logo\":\n        videoPath = videoPaths[\"logo\"]\n        filters = [\"logoRemoval\"]  # how the fuck?\n    elif testSubject == \"text\":\n        videoPath = videoPaths[\"text\"]\n        filters = [\"textRemoval\"]\n    elif testSubject == \"pip\":\n        videoPath = videoPaths[\"pip\"]\n        filters = [\"pipCrop\"]\n    elif testSubject == \"complete\":\n        # videoPath = videoPaths['complete']\n        # filters = ['pipCrop','textRemoval']\n        filters = [\"pipCrop\", \"textRemoval\", \"logoRemoval\"]\n    else:\n        raise Exception(\"Unknown testSubject: %s\" % testSubject)\n    # videoFileName = os.path.basename(videoPath)\n    # # we use the full video here? to check if this shit really works?\n    # # videoFile = os.path.join(allocatedTmpDir,videoFileName)\n    import uuid\n    cacheId = str(uuid.uuid4())\n    fileExtension = \"mp4\"\n    cacheFileName = \".\".join([cacheId, fileExtension])",
        "type": "code",
        "location": "/tests/unittest_ffmpegVideoPreProductionFilter.py:35-64"
    },
    "1947": {
        "file_id": 192,
        "content": "Sets temporary directory, determines filter type based on testSubject, and generates a unique cache file name using UUID.",
        "type": "comment"
    },
    "1948": {
        "file_id": 192,
        "content": "    cachePath = os.path.join(allocatedTmpDir, cacheFileName)\n    # if testSubject == 'pip':\n    #     start=5\n    #     end=10\n    if test_text_detector:\n        from pyjom.medialang.processors.dotProcessor import detectTextRegionOverTime\n        for key, videoPath in videoPaths.items():\n            if testSubject != \"complete\":\n                if key != testSubject:\n                    continue\n            print(\"TESTING: %s\" % key)\n            start = 0\n            end = getVideoDuration(videoPath)\n            # regions = detectTextRegionOverTime(videoPath, start, end)\n            regions = detectTextRegionOverTime(\n                videoPath, 10, 20\n            )  # now we change the start and end.\n            for key, item in regions.items():\n                # could be empty here.\n                print(\"KEY:\", key)\n                print(\"ITEM:\", item)\n            # how to merge continual shits?\n        # pretty much None currently.\n        breakpoint()\n    if test_ffmpeg:\n        # the logoRemoval filter may make the video unwatchable if too many false positive areas were found.",
        "type": "code",
        "location": "/tests/unittest_ffmpegVideoPreProductionFilter.py:65-97"
    },
    "1949": {
        "file_id": 192,
        "content": "This code snippet is testing text detection in videos and ffmpeg filter functionality. It loops through videoPaths, detects text regions over time using `detectTextRegionOverTime` function, and prints the key and item for each detected region. It also handles None output for ffmpeg tests, and mentions potential false positive issues with the logoRemoval filter.",
        "type": "comment"
    },
    "1950": {
        "file_id": 192,
        "content": "        for key, videoPath in videoPaths.items():\n            if testSubject != \"complete\":\n                if key != testSubject:\n                    continue\n            print(\"TESTING: %s\" % key)\n            start = 0\n            end = getVideoDuration(videoPath)\n            output = ffmpegVideoPreProductionFilter(\n                videoPath,\n                cachePath=cachePath,\n                start=start,\n                end=end,\n                filters=filters,\n                preview=True,\n            )  # resolution? make it sufficiently low!\n            print(\"ffmpeg pre production filter processing done.\")\n            print(\"output location:\", output)\n            breakpoint()",
        "type": "code",
        "location": "/tests/unittest_ffmpegVideoPreProductionFilter.py:98-117"
    },
    "1951": {
        "file_id": 192,
        "content": "The code iterates through videoPaths and tests the filter on each video. If testSubject is not \"complete\", it skips the iteration unless the key matches the testSubject. It then applies the ffmpegVideoPreProductionFilter to the video, prints the output location, and breaks the loop.",
        "type": "comment"
    },
    "1952": {
        "file_id": 193,
        "content": "/tests/unittest_houghline_dog_blur_detection.py",
        "type": "filepath"
    },
    "1953": {
        "file_id": 193,
        "content": "The code imports libraries, reads an image, applies blur detection and edge detection, displays edges with lines based on Hough line detection using OpenCV, waits for a key press to close the window.",
        "type": "summary"
    },
    "1954": {
        "file_id": 193,
        "content": "from lazero.utils.importers import cv2_custom_build_init\ncv2_custom_build_init()\nimport cv2\nimport numpy as np\n# command used for reading an image from the disk, cv2.imread function is used\nimagePath = \"/root/Desktop/works/pyjom/samples/image/dog_blue_sky_split_line.png\"\n# cannot find image without dark/black boundaries.\n# use blur detection, both for blur area removal and motion blur detection for key frame sampling/filtering\n# tool for finding non-blur based black borders:\n# ffmpeg -loop 1 -i /root/Desktop/works/pyjom/samples/image/dog_blue_sky_split_line.png -t 15 -vf cropdetect -f null -\n# maybe you can change the seconds to something shorter.\nimg1 = cv2.imread(imagePath)\n# gray1 = cv2.cvtColor(img1,cv2.COLOR_BGR2GRAY)\n# edges1 = cv2.Canny(gray1,50,150,apertureSize=3)\n# blurred = cv2.GaussianBlur(img1, (5, 5), 0)\nblurred = cv2.bilateralFilter(img1, 15, 75, 75)\nedges1 = cv2.Canny(blurred, 20, 210, apertureSize=3)\ncv2.imshow(\"EDGE\", edges1)\ncv2.waitKey(0)\nlines1 = cv2.HoughLines(edges1, 1, np.pi / 180, 200)  # wtf?",
        "type": "code",
        "location": "/tests/unittest_houghline_dog_blur_detection.py:1-28"
    },
    "1955": {
        "file_id": 193,
        "content": "The code imports necessary libraries, reads an image from disk, applies blur detection using bilateral filtering to remove blur and detect motion blur, converts the image to grayscale, detects edges using Canny edge detection, displays the edges, applies HoughLines to find lines in the image, and then waits for a key press to close the window.",
        "type": "comment"
    },
    "1956": {
        "file_id": 193,
        "content": "for rho, theta in lines1[0]:\n    a = np.cos(theta)\n    b = np.sin(theta)\n    x = a * rho\n    y = b * rho\n    x_1 = int(x + 1000 * (-b))\n    y_1 = int(y + 1000 * (a))\n    x_2 = int(x - 1000 * (-b))\n    y_2 = int(y - 1000 * (a))\n    cv2.line(img1, (x_1, y_1), (x_2, y_2), (0, 0, 255), 2)\n# Creation of a GUI window in order to display the image on the screen\ncv2.imshow(\"line detection\", img1)\n# cv2.waitKey method used for holding the window on screen\ncv2.waitKey(0)\ncv2.destroyAllWindows()",
        "type": "code",
        "location": "/tests/unittest_houghline_dog_blur_detection.py:29-43"
    },
    "1957": {
        "file_id": 193,
        "content": "This code generates lines on an image based on Hough line detection. It iterates over the lines, calculates the coordinates of endpoints, and draws lines using OpenCV. The GUI window displays the image with the drawn lines, holds it open for any key press (cv2.waitKey), then destroys all windows upon closing.",
        "type": "comment"
    },
    "1958": {
        "file_id": 194,
        "content": "/tests/unittest_get_subtid_name_and_majortid_name.py",
        "type": "filepath"
    },
    "1959": {
        "file_id": 194,
        "content": "The function `getMajorMinorTopicMappings` retrieves major and minor topic IDs and names, storing them in the `majorMinorMappings` dictionary. The code uses this function to get the associated topics for a given tid, formats them into tags, and prints the result along with the tid for topic ID 1.",
        "type": "summary"
    },
    "1960": {
        "file_id": 194,
        "content": "from bilibili_api import search\nBSP = search.bilibiliSearchParams\ndef getMajorMinorTopicMappings(debug: bool = False):\n    majorMinorMappings = {}\n    for key, value in BSP.all.tids.__dict__.items():\n        try:\n            major_tid = value.tid\n            if debug:\n                print(\"MAJOR\", key, major_tid)\n            content = {\"major\": {\"tid\": major_tid, \"name\": key}}\n            majorMinorMappings.update(\n                {major_tid: content, key: content, str(major_tid): content}\n            )\n            for subkey, subvalue in value.__dict__.items():\n                if subkey != \"tid\" and type(subvalue) == int:\n                    if debug:\n                        print(\"MINOR\", subkey, subvalue)\n                    content = {\n                        \"major\": {\"tid\": major_tid, \"name\": key},\n                        \"minor\": {\"tid\": subvalue, \"name\": subkey},\n                    }\n                    majorMinorMappings.update(\n                        {subvalue: content, subkey: content, str(subvalue): content}",
        "type": "code",
        "location": "/tests/unittest_get_subtid_name_and_majortid_name.py:1-26"
    },
    "1961": {
        "file_id": 194,
        "content": "This function `getMajorMinorTopicMappings` retrieves major and minor topic IDs and names from `BSP.all.tids` dictionary, storing them in `majorMinorMappings` dictionary for further use. It also prints the major and minor topics if debug mode is enabled.",
        "type": "comment"
    },
    "1962": {
        "file_id": 194,
        "content": "                    )\n        except:\n            pass\n    return majorMinorMappings\ndef getTagStringFromTid(tid):\n    majorMinorTopicMappings = getMajorMinorTopicMappings()\n    topic = majorMinorTopicMappings.get(tid, None)\n    tags = []\n    if topic:\n        majorTopic = topic.get(\"major\", {}).get(\"name\", None)\n        minorTopic = topic.get(\"minor\", {}).get(\"name\", None)\n        if majorTopic:\n            tags.append(majorTopic)\n            if minorTopic:\n                tags.append(minorTopic)\n    return \",\".join(tags)\ntid = 1\ntagString = getTagStringFromTid(tid)\nprint(tid, tagString)",
        "type": "code",
        "location": "/tests/unittest_get_subtid_name_and_majortid_name.py:27-49"
    },
    "1963": {
        "file_id": 194,
        "content": "This code retrieves the major and minor topics associated with a given topic ID (tid) using the getMajorMinorTopicMappings() function. It then formats these topics into a comma-separated string of tags. If there are both a major and minor topic, they are concatenated in that order, else if only one exists, it is printed alone. Finally, the tid and associated tagString are printed to the console for the given topic ID 1.",
        "type": "comment"
    },
    "1964": {
        "file_id": 195,
        "content": "/tests/unittest_musictoolbox_netease_music_lyric.py",
        "type": "filepath"
    },
    "1965": {
        "file_id": 195,
        "content": "This code tests the functionality of getting music and lyrics using NeteaseMusic API from the pyjom library. It checks for similarity in keywords and prints the result, but encounters an issue when no lyrics are found. It then plans to test a specific music ID for lyrics retrieval and shows preparedness in case it fails.",
        "type": "summary"
    },
    "1966": {
        "file_id": 195,
        "content": "from test_commons import *\nfrom pyjom.musictoolbox import neteaseMusic\nNMClient = neteaseMusic()\n# import random\nquery = \"linkin park numb\"\nfor sim in [False, True]:\n    result = NMClient.getMusicAndLyricWithKeywords(query, similar=sim, debug=True)\n    print(\"similar?\", sim)\n    # no lyrics! wtf??\n    breakpoint()\n# now let's test something surely will get lyrics.\n# music_id = 497572729\n# lyric_string = NMClient.getMusicLyricFromNetease(music_id)\n# print(\"LYRIC STRING:\",lyric_string)\n# in case we don't get the lyric, you should be prepared.\n# it works.",
        "type": "code",
        "location": "/tests/unittest_musictoolbox_netease_music_lyric.py:1-17"
    },
    "1967": {
        "file_id": 195,
        "content": "This code tests the functionality of getting music and lyrics using NeteaseMusic API from the pyjom library. It checks for similarity in keywords and prints the result, but encounters an issue when no lyrics are found. It then plans to test a specific music ID for lyrics retrieval and shows preparedness in case it fails.",
        "type": "comment"
    },
    "1968": {
        "file_id": 196,
        "content": "/tests/unittest_lazero_external_downloader.py",
        "type": "filepath"
    },
    "1969": {
        "file_id": 196,
        "content": "This code downloads a file from a given URL and saves it to a specified path. It checks if the file already exists at the target location, and if so, deletes it before initiating the download. The success or failure of the download is reported using print statement.",
        "type": "summary"
    },
    "1970": {
        "file_id": 196,
        "content": "from lazero.network.downloader import download\nurl = \"https://media3.giphy.com/media/wTrXRamYhQzsY/giphy.gif?cid=dda24d502m79hkss38jzsxteewhs4e3ocd3iqext2285a3cq&rid=giphy.gif&ct=g\"\npath = \"/dev/shm/medialang/test.gif\"\nimport os\nif os.path.exists(path):\n    os.remove(path)\nreport = download(url, path)\nprint(\"download success?\", report)",
        "type": "code",
        "location": "/tests/unittest_lazero_external_downloader.py:1-14"
    },
    "1971": {
        "file_id": 196,
        "content": "This code downloads a file from a given URL and saves it to a specified path. It checks if the file already exists at the target location, and if so, deletes it before initiating the download. The success or failure of the download is reported using print statement.",
        "type": "comment"
    },
    "1972": {
        "file_id": 197,
        "content": "/tests/unittest_ocr_filter_large_area_of_text.py",
        "type": "filepath"
    },
    "1973": {
        "file_id": 197,
        "content": "This code sets up libraries and variables for processing image or video files, detects text within frames using EasyOCRReader, calculates text area percentage, draws rectangles, and displays the result.",
        "type": "summary"
    },
    "1974": {
        "file_id": 197,
        "content": "from test_commons import *\n# import pytesseract\n# from pytesseract import Output\nfrom lazero.utils.importers import cv2_custom_build_init\ncv2_custom_build_init()\nimport cv2\n# img = cv2.imread('image.jpg')\nfrom pyjom.videotoolbox import getVideoFrameIteratorWithFPS\ndetectionList = []\nfrom pyjom.imagetoolbox import getEasyOCRReader, LRTBToDiagonal\nreader = getEasyOCRReader((\"en\",))\nimport numpy as np\ntest_subject = \"image\"\nif test_subject == \"video\":\n    videoPath = \"/root/Desktop/works/pyjom/samples/video/dog_with_large_text.gif\"\n    iterator = getVideoFrameIteratorWithFPS(videoPath, start=-1, end=-1, fps=10)\nelif test_subject == \"image\":\n    imagePath = \"/root/Desktop/works/pyjom/samples/image/dog_saturday_night.bmp\"\n    iterator = [cv2.imread(imagePath)]\nelse:\n    raise Exception(\"unknown test_subject:\", test_subject)\n# threshold: {'max':0.3}\nfor frame in iterator:\n    height, width = frame.shape[:2]\n    img = np.zeros((height, width, 3))\n    detection, recognition = reader.detect(frame)  # not very sure.\n    if detection == [[]]:",
        "type": "code",
        "location": "/tests/unittest_ocr_filter_large_area_of_text.py:1-37"
    },
    "1975": {
        "file_id": 197,
        "content": "Code imports necessary libraries and sets up variables for working with an image or video file. It initializes OpenCV, EasyOCRReader, and numpy, then determines the test subject (image or video) to be used. The code creates an iterator based on the test subject and sets a threshold for detection. It loops through each frame in the iterator, creating a blank image, and detects text within the frame using EasyOCRReader.",
        "type": "comment"
    },
    "1976": {
        "file_id": 197,
        "content": "        diagonalRects = []\n    else:\n        diagonalRects = [LRTBToDiagonal(x) for x in detection[0]]\n    for x1, y1, x2, y2 in diagonalRects:\n        w, h = x2 - x1, y2 - y1\n        x, y = x1, y1\n        cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), -1)\n    # calculate the portion of the text area.\n    textArea = np.sum(img)\n    textAreaRatio = (textArea / 255) / (width * height)\n    print(\"text area: {:.2f} %\".format(textAreaRatio))\n    cv2.imshow(\"img\", img)\n    cv2.waitKey(0)",
        "type": "code",
        "location": "/tests/unittest_ocr_filter_large_area_of_text.py:38-50"
    },
    "1977": {
        "file_id": 197,
        "content": "This code calculates the text area percentage of an image and displays it. It first determines diagonal rectangles from detection data, then draws rectangles around the detected text areas using OpenCV's rectangle function. The total text area is calculated by summing pixel values in the image, which is then normalized to a percentage of the image's total area. Finally, the image with drawn rectangles and text area ratio is displayed.",
        "type": "comment"
    },
    "1978": {
        "file_id": 198,
        "content": "/tests/unittest_music_recognition.py",
        "type": "filepath"
    },
    "1979": {
        "file_id": 198,
        "content": "This code tests the music recognition functionality of different backends (songrec, shazamio, and midomi) by calling the recognizeMusicFromFile function and logging the results. The filepath variable stores the path to the test audio file. The methods list determines which backends will be tested. After each test, there is a 3-second pause before moving on to the next backend.",
        "type": "summary"
    },
    "1980": {
        "file_id": 198,
        "content": "from test_commons import *\nfrom pyjom.musictoolbox import recognizeMusicFromFile\nfrom lazero.utils.logger import sprint\nfilepath = (\n    # \"/root/Desktop/works/pyjom/tests/music_recognization/exciting_bgm_cut_10seconds.mp3\"\n    \"/root/Desktop/works/pyjom/tests/music_analysis/exciting_bgm.mp3\"\n)\n# methods = [\"midomi\"]\nmethods = [\"songrec\", \"shazamio\", \"midomi\"]\nimport time\nfor method in methods:\n    result = recognizeMusicFromFile(filepath, backend=method, debug=True)\n    sprint(\"RESULT:\", result)\n    time.sleep(3)",
        "type": "code",
        "location": "/tests/unittest_music_recognition.py:1-16"
    },
    "1981": {
        "file_id": 198,
        "content": "This code tests the music recognition functionality of different backends (songrec, shazamio, and midomi) by calling the recognizeMusicFromFile function and logging the results. The filepath variable stores the path to the test audio file. The methods list determines which backends will be tested. After each test, there is a 3-second pause before moving on to the next backend.",
        "type": "comment"
    },
    "1982": {
        "file_id": 199,
        "content": "/tests/unittest_paddlehub_animal_resnet.py",
        "type": "filepath"
    },
    "1983": {
        "file_id": 199,
        "content": "This code uses PaddleHub's Animal ResNet model to classify dogs and cats from video frames, post-processing the results to test accuracy with various images including non-animal ones.",
        "type": "summary"
    },
    "1984": {
        "file_id": 199,
        "content": "# source = \"/root/Desktop/works/pyjom/samples/video/kitty_flash_15fps.gif\"  # check that kitty video!\nsource = \"/root/Desktop/works/pyjom/samples/video/cute_cat_gif.gif\"  # another kitty!\nfrom test_commons import *\nfrom pyjom.videotoolbox import getVideoFrameIteratorWithFPS\nfrom pyjom.imagetoolbox import resizeImageWithPadding\nimport paddlehub as hub\nimport cv2\ndef labelFileReader(filename):\n    with open(filename, \"r\") as f:\n        content = f.read()\n        content = content.split(\"\\n\")\n        content = [elem.replace(\"\\n\", \"\").strip() for elem in content]\n        content = [elem for elem in content if len(elem) > 0]\n    return content\ndog_suffixs = [\"\", \"\", \"\"]\ncat_suffixs = [\"\"]  # ends with this, and not containing forbidden words.\ndog_labels = labelFileReader(\n    \"/root/Desktop/works/pyjom/tests/animals_paddlehub_classification_resnet/dogs.txt\"\n)\ncat_labels = labelFileReader(\n    \"/root/Desktop/works/pyjom/tests/animals_paddlehub_classification_resnet/cats.txt\"\n)\nforbidden_words = [\n    \"\",\n    \"\",",
        "type": "code",
        "location": "/tests/unittest_paddlehub_animal_resnet.py:1-32"
    },
    "1985": {
        "file_id": 199,
        "content": "This code imports necessary libraries and defines constants for labels and forbidden words. It reads the dog and cat labels from separate text files, and later on, it will use these labels to classify animals in a video. The forbidden words are likely used to exclude certain categories of animals that may appear similar to cats or dogs but should not be confused with them.",
        "type": "comment"
    },
    "1986": {
        "file_id": 199,
        "content": "    \"\",\n    \"\",\n    \"\",\n    \"\",\n    \"\",\n    \"\",\n    \"\",\n    \"\",\n    \"\",\n]\ndef dog_cat_name_recognizer(name):\n    if name in dog_labels:\n        return \"dog\"\n    elif name in cat_labels:\n        return \"cat\"\n    elif name not in forbidden_words:\n        for dog_suffix in dog_suffixs:\n            if name.endswith(dog_suffix):\n                return \"dog\"\n        for cat_suffix in cat_suffixs:\n            if name.endswith(cat_suffix):\n                return \"cat\"\n    return None\nfrom lazero.utils.logger import sprint\nclassifier = hub.Module(name=\"resnet50_vd_animals\")\n# 'ResNet50vdAnimals' object has no attribute 'gpu_predictor'\n# no gpu? really?\n# test_flag = \"video\"\ntest_flag = \"image\"\ndef paddleAnimalDetectionResultToList(result):\n    resultDict = result[0]\n    resultList = [(key, value) for key, value in resultDict.items()]\n    resultList.sort(key=lambda item: -item[1])\n    return resultList\ndef translateResultListToDogCatList(resultList):\n    final_result_list = []\n    for name, confidence in resultList:",
        "type": "code",
        "location": "/tests/unittest_paddlehub_animal_resnet.py:33-78"
    },
    "1987": {
        "file_id": 199,
        "content": "Function dog_cat_name_recognizer identifies if a given name is of a dog or cat by checking it against pre-defined labels. If the name does not fit into these categories, it further checks for common suffixes to classify as either a dog or cat. The code imports necessary modules and sets up variables for testing purposes before defining functions for post-processing the result and translating it into a dog/cat list format.",
        "type": "comment"
    },
    "1988": {
        "file_id": 199,
        "content": "        new_name = dog_cat_name_recognizer(name)\n        final_result_list.append((new_name, confidence))\n    return final_result_list\nif test_flag == \"video\":\n    for frame in getVideoFrameIteratorWithFPS(source, -1, -1, fps=1):\n        padded_resized_frame = resizeImageWithPadding(\n            frame, 224, 224, border_type=\"replicate\"\n        )  # pass the test only if three of these containing 'cats'\n        result = classifier.classification(\n            images=[padded_resized_frame], top_k=3, use_gpu=False\n        )  # check it?\n        resultList = paddleAnimalDetectionResultToList(result)\n        final_result_list = translateResultListToDogCatList(resultList)\n        sprint(\"RESULT LIST:\", final_result_list)\n        # RESULT: [{'': 0.23492032289505005, '': 0.14728288352489471, '': 0.13097935914993286}]\n        # so what is the major categories?\n        # thanks to chinese, we are never confused.\n        # check the labels, shall we?\n        # what about samoyed?\n        # sprint(\"RESULT:\", result)",
        "type": "code",
        "location": "/tests/unittest_paddlehub_animal_resnet.py:79-100"
    },
    "1989": {
        "file_id": 199,
        "content": "This code is using PaddleHub's Animal ResNet model to classify frames from a video source, identifying either dogs or cats. The final results are translated to a list of dog and cat names along with their respective confidences. This code checks the major categories in the final result and verifies if \"samoyed\" is included.",
        "type": "comment"
    },
    "1990": {
        "file_id": 199,
        "content": "        breakpoint()\nelif test_flag == \"image\":\n    # source = \"/root/Desktop/works/pyjom/samples/image/samoyed.jpeg\"\n    # [('dog', 0.8835851550102234), ('dog', 0.08754527568817139), ('dog', 0.008648859336972237)]\n    # source = \"/root/Desktop/works/pyjom/samples/image/dog_saturday_night.jpg\"\n    #  [(None, 0.33663231134414673), ('dog', 0.32254937291145325), ('dog', 0.0494903139770031)]\n    # not animal? wtf?\n    # source = \"/root/Desktop/works/pyjom/samples/image/porn_shemale.jpeg\" # definitely not animal\n    # [(None, 0.9894463419914246), ('dog', 1.564090962347109e-05), ('dog', 1.3550661606132053e-05)]\n    # source = \"/root/Desktop/works/pyjom/samples/image/is_this_duck.bmp\"\n    # [(None, 0.9864748120307922), ('dog', 1.2670795513258781e-05), (None, 9.569253961672075e-06)]\n    # source = \"/root/Desktop/works/pyjom/samples/image/pig_really.bmp\" # it's really a dog\n    # [(None, 0.35919442772865295), ('dog', 0.16199783980846405), ('dog', 0.07987158000469208)]\n    # source = \"/root/Desktop/works/pyjom/samples/image/miku_on_green.png\"",
        "type": "code",
        "location": "/tests/unittest_paddlehub_animal_resnet.py:101-114"
    },
    "1991": {
        "file_id": 199,
        "content": "The code includes various image source paths and the corresponding classification outputs. The script seems to be testing the accuracy of an animal recognition model by inputting different images, including some non-animal images for reference. Some images are misclassified or not classified at all, highlighting potential areas for improvement in the model.",
        "type": "comment"
    },
    "1992": {
        "file_id": 199,
        "content": "    # besides calculating \"DOG\" or \"CAT\" we are also concerned about \"NONE\"\n    # [(None, 0.9998186230659485), (None, 1.7534730432089418e-06), (None, 7.280816021193459e-07)]\n    # source = \"/root/Desktop/works/pyjom/samples/image/dog_with_text.jpg\" # no dog\n    #  [(None, 0.9998675584793091), ('dog', 2.565316492564307e-07), (None, 1.562129767762599e-07)]\n    source = \"/root/Desktop/works/pyjom/samples/image/dog_with_text2.png\"  # has dog\n    #  [(None, 0.8876796960830688), ('dog', 0.0498274527490139), ('dog', 0.02175540290772915)]\n    # a little, but not focused.\n    frame = cv2.imread(source)\n    padded_resized_frame = resizeImageWithPadding(\n        frame, 224, 224, border_type=\"replicate\"\n    )\n    result = classifier.classification(\n        images=[padded_resized_frame], top_k=3, use_gpu=False\n    )\n    resultList = paddleAnimalDetectionResultToList(result)\n    final_result_list = translateResultListToDogCatList(resultList)\n    sprint(\"FINAL RESULT LIST:\", final_result_list)\n    breakpoint()\nelse:\n    raise Exception(\"unknown test flag: %s\" % test_flag)",
        "type": "code",
        "location": "/tests/unittest_paddlehub_animal_resnet.py:115-134"
    },
    "1993": {
        "file_id": 199,
        "content": "This code is testing the accuracy of an image classifier for detecting \"dog\" or \"cat\", while also considering the possibility of \"none\". The code reads an image, resizes and pads it, then uses a classifier to predict its categories. It translates the results into a list of \"dog\" or \"cat\" and displays the final result.",
        "type": "comment"
    },
    "1994": {
        "file_id": 200,
        "content": "/tests/unittest_nsfw_video_score.py",
        "type": "filepath"
    },
    "1995": {
        "file_id": 200,
        "content": "The code utilizes a trained model to detect NSFW content in videos and images, ensuring compliance by posting non-sexual content through an API. It stores classification probabilities and handles exceptions for unknown test_flags. However, only GIFs can be posted currently with caution about picture stretching.",
        "type": "summary"
    },
    "1996": {
        "file_id": 200,
        "content": "# we take max for the concerned ones, and take mean for the unconcerned ones.\nfrom test_commons import *\nimport requests\nfrom lazero.network.checker import waitForServerUp\nfrom pyjom.videotoolbox import getVideoFrameIteratorWithFPS\nfrom typing import Literal\ngateway = \"http://localhost:8511/\"\nfrom pyjom.mathlib import superMean, superMax\nfrom lazero.utils.importers import cv2_custom_build_init\ncv2_custom_build_init()\nimport cv2\n# suggest you not to use this shit.\n# import math\nfrom pyjom.imagetoolbox import resizeImageWithPadding, scanImageWithWindowSizeAutoResize\nfrom lazero.filesystem import tmpdir, tmpfile\ntmpdirPath = \"/dev/shm/medialang/nsfw\"\nimport uuid\nwaitForServerUp(8511, \"nsfw nodejs server\")\nimport os\ntest_flag = \"nsfw_video\"\n# test_flag = \"nsfw_image\"\n# test_flag = \"scanning\"\n# test_flag = \"paddinging\"\nsource = \"/root/Desktop/works/pyjom/samples/video/cute_cat_gif.gif\"\nimport numpy as np\ndef processNSFWServerImageReply(reply):\n    mDict = {}\n    for elem in reply:\n        className, probability = elem[\"className\"], elem[\"probability\"]",
        "type": "code",
        "location": "/tests/unittest_nsfw_video_score.py:1-44"
    },
    "1997": {
        "file_id": 200,
        "content": "The code imports necessary libraries, initializes certain functions and variables, and defines the processNSFWServerImageReply function which processes image classification reply from the server. It is for testing NSFW detection in videos or images, with options to test different aspects such as scanning, padding, etc. Note that it may not be recommended to use some parts of the code.",
        "type": "comment"
    },
    "1998": {
        "file_id": 200,
        "content": "        mDict.update({className: probability})\n    return mDict\ndef processNSFWReportArray(\n    NSFWReportArray,\n    average_classes=[\"Neutral\"],\n    get_max_classes=[\"Drawing\", \"Porn\", \"Sexy\", \"Hentai\"],\n):\n    assert set(average_classes).intersection(set(get_max_classes)) == set()\n    NSFWReport = {}\n    for element in NSFWReportArray:\n        for key in element.keys():\n            NSFWReport[key] = NSFWReport.get(key, []) + [element[key]]\n    for average_class in average_classes:\n        NSFWReport[average_class] = superMean(NSFWReport.get(average_class, [0]))\n    for get_max_class in get_max_classes:\n        NSFWReport[get_max_class] = superMax(NSFWReport.get(get_max_class, [0]))\n    return NSFWReport\nfrom pyjom.commons import checkMinMaxDict\n# you can reuse this, really.\ndef NSFWFilter(\n    NSFWReport,\n    filter_dict={\n        \"Neutral\": {\"min\": 0.5},\n        \"Sexy\": {\"max\": 0.5},\n        \"Porn\": {\"max\": 0.5},\n        \"Hentai\": {\"max\": 0.5},\n        \"Drawing\": {\"max\": 0.5},\n    },\n    debug=False,\n):\n    for key in filter_dict:",
        "type": "code",
        "location": "/tests/unittest_nsfw_video_score.py:45-80"
    },
    "1999": {
        "file_id": 200,
        "content": "This code processes an NSFW report array and returns a filtered dictionary. It updates the dictionary with class names as keys and their corresponding probabilities. Then, it calculates the average and maximum scores for certain classes. Lastly, it applies filters to the resulting dictionary based on specified minimum or maximum threshold values for each class.",
        "type": "comment"
    }
}