{
    "1900": {
        "file_id": 182,
        "content": ")  # this margin is used randomly. we can make it 0 or as is.\ntextOrigin = (-30, 30)\nfontScale = 1\nfont = cv2.FONT_HERSHEY_SIMPLEX\nfontThickness = 2\ngetRadius = lambda: random.randint(1, 30)\nimageIndex = (\n    sorted(\n        [int(fpath.split(\".\")[0]) for fpath in os.listdir(train_path_relative)],\n        key=lambda index: -index,\n    )[0]\n    + 1\n)  # shall be increased on demand.\nprint(\"START MARKING PICTURES WITH INDEX:\", imageIndex)\nMAX_COCO_PIP_IMAGE_COUNT = 10000  # well, super huge. is it?\n# don't insert 20000 cause it will break shit.\nalphabets = \"abcdefghijklmnopqrstuvwxyz\"\nALPHABETS = alphabets.upper()\nnumbers = \"0123456789\"\ncharacterList = list(alphabets + ALPHABETS + numbers + punctuation + \" \")\ngetRandomCharacter = lambda: random.choice(characterList)\ngetRandomCharacters = lambda charCount: \"\".join(\n    [getRandomCharacter() for _ in range(charCount)]\n)\ngetRandomLinesOfCharacters = lambda lineCount, charCount: \"\\r\".join(\n    [getRandomCharacters(charCount) for _ in range(lineCount)]\n)\nimageFormats = [1, 2, 4]",
        "type": "code",
        "location": "/tests/anime_highlight_cuts/theme_collector/create_coco_pip_dataset_standalone.py:38-73"
    },
    "1901": {
        "file_id": 182,
        "content": "This code initializes variables for creating a COCO PIP dataset. It sets text origin, font scale, and font type. It defines functions to get random character or characters, and handles image formats. The image index is incremented and the maximum allowed image count is set.",
        "type": "comment"
    },
    "1902": {
        "file_id": 182,
        "content": "textFormats = [\"up\", \"down\", \"none\"]\nbackgroundFormats = [\"solidColor\", \"horizontalStripes\", \"verticalStripes\", \"gradients\"]\ncolors = [\n    (0, 0, 0),\n    (255, 255, 255),\n    (0, 0, 192),\n    (255, 255, 64),\n    (0, 255, 0),\n    (0, 0, 255),\n    (255, 0, 0),\n]\ncolorsNumpyArray = [np.array(color) for color in colors]\ncolorsWithIndex = [(index, color) for index, color in enumerate(colors)]\n# we are not doing this while testing.\n# imageFormat = random.choice(imageFormats)\n# textFormat = random.choice(textFormats)\n# backgroundFormat = random.choice(backgroundFormats)\ndef get_gradient_2d(start, stop, width, height, is_horizontal):\n    if is_horizontal:\n        return np.tile(np.linspace(start, stop, width), (height, 1))\n    else:\n        return np.tile(np.linspace(start, stop, height), (width, 1)).T\ndef get_gradient_3d(width, height, start_list, stop_list, is_horizontal_list):\n    result = np.zeros((height, width, len(start_list)), dtype=np.float64)\n    for i, (start, stop, is_horizontal) in enumerate(\n        zip(start_list, stop_list, is_horizontal_list)",
        "type": "code",
        "location": "/tests/anime_highlight_cuts/theme_collector/create_coco_pip_dataset_standalone.py:74-104"
    },
    "1903": {
        "file_id": 182,
        "content": "This code generates a COCO-style dataset for object detection using random color combinations. It includes lists of text formats, background formats, and colors, which are then converted to numpy arrays. The code also defines functions to generate 2D and 3D gradients for backgrounds. However, image format, text format, and background format selections are commented out while testing.",
        "type": "comment"
    },
    "1904": {
        "file_id": 182,
        "content": "    ):\n        result[:, :, i] = get_gradient_2d(start, stop, width, height, is_horizontal)\n    return result.astype(np.uint8)\n# for imageFormat, textFormat, backgroundFormat in itertools.product(\n#     imageFormats, textFormats, backgroundFormats\n# ):  # you can use these things to get test output picture names.\nprint(\"creating coco pip dataset:\")\nimport progressbar\nfor _i in progressbar.progressbar(range(MAX_COCO_PIP_IMAGE_COUNT)):\n    imageFormat = random.choice(imageFormats)\n    textFormat = random.choice(textFormats)\n    backgroundFormat = random.choice(backgroundFormats)\n    colorDistances = {}\n    selectedImages = [\n        cv2.imread(os.path.join(imageBasePath, imagePath), cv2.IMREAD_COLOR)\n        for imagePath in random.sample(imagePaths, k=imageFormat)\n    ]\n    for image in selectedImages:\n        averageColor = np.average(image.reshape((-1, 3)), axis=0)\n        for index, colorNumpyArray in enumerate(colorsNumpyArray):\n            colorDistances[index] = colorDistances.get(index, []) + [\n                np.sum(np.abs(averageColor - colorNumpyArray))",
        "type": "code",
        "location": "/tests/anime_highlight_cuts/theme_collector/create_coco_pip_dataset_standalone.py:105-130"
    },
    "1905": {
        "file_id": 182,
        "content": "Creating a COCO PIP dataset: randomly selects image, text, and background formats to generate test output pictures. Chooses multiple images for each format, averages their colors, compares them with color arrays, and adds distances to the dictionary.",
        "type": "comment"
    },
    "1906": {
        "file_id": 182,
        "content": "            ]\n    sortedColorsWithIndex = sorted(\n        colorsWithIndex, key=lambda element: -np.sum(colorDistances[element[0]])\n    )  # the further the better.\n    # sortedColors = [color for _, color in sortedColorsWithIndex]\n    ## create background first.\n    imageCanvasHeight = half_width if imageFormat == 2 else width\n    textCanvasHeight = 0 if textFormat == \"none\" else textTotalHeight\n    backgroundShape = (imageCanvasHeight + textCanvasHeight, width, 3)  # height, width\n    _, color_main = sortedColorsWithIndex[0]\n    if backgroundFormat in [\"horizontalStripes\", \"verticalStripes\", \"gradients\"]:\n        # fill background with color_main first.\n        _, color_sub = sortedColorsWithIndex[1]\n        if backgroundFormat in [\"horizontalStripes\", \"verticalStripes\"]:\n            backgroundImage = np.zeros(backgroundShape, dtype=np.uint8)\n            backgroundImage[:, :, 0] = color_main[0]\n            backgroundImage[:, :, 1] = color_main[1]\n            backgroundImage[:, :, 2] = color_main[2]\n            stripeCount = random.randint(2, 5)",
        "type": "code",
        "location": "/tests/anime_highlight_cuts/theme_collector/create_coco_pip_dataset_standalone.py:131-156"
    },
    "1907": {
        "file_id": 182,
        "content": "Code creates a background for an image based on the furthest color from the given colors. It sorts the colors by distance from their average and chooses the furthest one. If the background format is \"horizontalStripes\", \"verticalStripes\" or \"gradients\", it fills the background with this color and another color chosen from the sorted list, then generates a random stripe count for the background image.",
        "type": "comment"
    },
    "1908": {
        "file_id": 182,
        "content": "            if backgroundFormat == \"verticalStripes\":  # slice width\n                arr = np.linspace(0, backgroundShape[1], stripeCount + 1)\n                for width_start, width_end in [\n                    (int(arr[i]), int(arr[i + 1]))\n                    for i in range(stripeCount)\n                    if i % 2 == 1\n                ]:\n                    backgroundImage[:, width_start:width_end, 0] = color_sub[0]\n                    backgroundImage[:, width_start:width_end, 1] = color_sub[1]\n                    backgroundImage[:, width_start:width_end, 2] = color_sub[2]\n            else:  # horizontal. slice height.\n                arr = np.linspace(0, backgroundShape[0], stripeCount + 1)\n                for height_start, height_end in [\n                    (int(arr[i]), int(arr[i + 1]))\n                    for i in range(stripeCount)\n                    if i % 2 == 1\n                ]:\n                    backgroundImage[height_start:height_end, :, 0] = color_sub[0]\n                    backgroundImage[height_start:height_end, :, 1] = color_sub[1]",
        "type": "code",
        "location": "/tests/anime_highlight_cuts/theme_collector/create_coco_pip_dataset_standalone.py:157-176"
    },
    "1909": {
        "file_id": 182,
        "content": "Code snippet generates a background image with vertical or horizontal stripes based on the `backgroundFormat`. For the \"verticalStripes\" format, it creates an array of strip widths and sets the corresponding pixel values for each strip. Otherwise, for the \"horizontal\" format, it creates an array of strip heights and sets the corresponding pixel values for each strip.",
        "type": "comment"
    },
    "1910": {
        "file_id": 182,
        "content": "                    backgroundImage[height_start:height_end, :, 2] = color_sub[2]\n        else:  # gradient!\n            is_horizontal = [False, False, False]\n            is_horizontal[random.randint(0, 2)] = True\n            backgroundImage = get_gradient_3d(\n                backgroundShape[1],\n                backgroundShape[0],\n                color_main,\n                color_sub,\n                is_horizontal,\n            )\n    else:  # pure color.\n        backgroundImage = np.zeros(backgroundShape, dtype=np.uint8)\n        backgroundImage[:, :, 0] = color_main[0]\n        backgroundImage[:, :, 1] = color_main[1]\n        backgroundImage[:, :, 2] = color_main[2]\n    ## next, paint text!\n    if textFormat != \"none\":\n        ## only calculate text color when needed.\n        backgroundAverageColor = np.average(backgroundImage.reshape((-1, 3)), axis=0)\n        textColorNumpyArray = sorted(\n            colorsNumpyArray,\n            key=lambda colorNumpyArray: -np.sum(\n                np.abs(backgroundAverageColor - np.array(colorNumpyArray))",
        "type": "code",
        "location": "/tests/anime_highlight_cuts/theme_collector/create_coco_pip_dataset_standalone.py:177-201"
    },
    "1911": {
        "file_id": 182,
        "content": "This code generates background images based on the given parameters: pure color, gradient, or a combination of both. It also calculates the text color by comparing the background image's average color with a list of colors to find the best match.",
        "type": "comment"
    },
    "1912": {
        "file_id": 182,
        "content": "            ),\n        )[0]\n        textColor = textColorNumpyArray.tolist()\n        # let's paint it all over the place!\n        textShift = 40\n        # TODO: check if string is **just enough** to fill the background.\n        for textLineIndex in range(\n            int((backgroundShape[0] / (textTotalHeight + width)) * 27)\n        ):\n            baseNumber = 50\n            baseNumber2 = random.randint(1, baseNumber)\n            textContent = random.choice(\n                [\n                    \"\",\n                    (\" \" * baseNumber2)\n                    + getRandomCharacters(random.randint(0, baseNumber - baseNumber2)),\n                ]\n            )\n            backgroundImage = cv2.putText(\n                backgroundImage,\n                textContent,\n                (textOrigin[0], textOrigin[1] + textShift * textLineIndex),\n                font,\n                fontScale,\n                textColor,\n                fontThickness,\n                cv2.LINE_AA,\n            )\n    ## put pictures!\n    imageCanvasShape = (imageCanvasHeight, width, 3)",
        "type": "code",
        "location": "/tests/anime_highlight_cuts/theme_collector/create_coco_pip_dataset_standalone.py:202-232"
    },
    "1913": {
        "file_id": 182,
        "content": "This code generates a random text overlay on an image using OpenCV's putText function. It randomly selects and modifies a character string, then applies it to the image at various locations. The background shape is used to determine the number of overlays generated.",
        "type": "comment"
    },
    "1914": {
        "file_id": 182,
        "content": "    imageMask = Image.new(\n        \"RGB\", (imageCanvasShape[1], imageCanvasShape[0]), \"black\"\n    )  # width, height?\n    draw = ImageDraw.Draw(imageMask)\n    imageCanvas = np.zeros(imageCanvasShape, dtype=np.uint8)\n    imageCoordinates = []\n    if imageFormat == 1:\n        image = selectedImages[0]\n        imageShape = image.shape\n        margin = getMarginRatio()\n        base = width * (1 - margin * 2)\n        imageHeight, imageWidth = imageShape[:2]\n        if imageHeight > imageWidth:\n            imageShape = (int(base * (imageWidth / imageHeight)), int(base))\n        else:\n            imageShape = (int(base), int(base * (imageHeight / imageWidth)))\n        # print(image.shape)\n        image = cv2.resize(image, imageShape)\n        x0 = int((width - imageShape[0]) / 2)\n        x1 = x0 + imageShape[0]\n        y0 = int((width - imageShape[1]) / 2)\n        y1 = y0 + imageShape[1]\n        if random.random() > 0.5:\n            draw.rectangle((x0, y0, x1, y1), fill=\"white\")\n        else:\n            draw.rounded_rectangle(",
        "type": "code",
        "location": "/tests/anime_highlight_cuts/theme_collector/create_coco_pip_dataset_standalone.py:234-263"
    },
    "1915": {
        "file_id": 182,
        "content": "Creates a mask image for given shape, draws on it based on the image format and random selection, then resizes the selected image according to the new shape. It also determines the coordinates for drawing rounded rectangles on the mask.",
        "type": "comment"
    },
    "1916": {
        "file_id": 182,
        "content": "                (x0, y0, x1, y1),\n                fill=\"white\",\n                radius=min(int(x1 - x0) / 2, int(y1 - y0) / 2, getRadius()),\n            )\n        # print(\"___\")\n        # print(imageShape)\n        # print(imageCanvas.shape)\n        # print(image.shape)\n        # print(x0,x1,x1-x0)\n        # print(y0,y1,y1-y0)\n        # print(\"___\")\n        # cv2.imshow(\"mask\", np.array(imageMask))\n        # cv2.waitKey(0)\n        imageCanvas[y0 : image.shape[0] + y0, x0 : image.shape[1] + x0, :] = image\n        imageCoordinates.append(\n            (\n                x0 + image.shape[1] / 2,\n                y0 + image.shape[0] / 2,\n                image.shape[1],\n                image.shape[0],\n            )\n        )  # x_center, y_center, width, height\n    else:\n        basePoints = [\n            (x * half_width, y * half_width)\n            for x, y in [(0, 0), (1, 0), (1, 1), (0, 1)]\n        ]  # width, height\n        for index, image in enumerate(selectedImages):\n            imageShape = image.shape\n            margin = getMarginRatio()",
        "type": "code",
        "location": "/tests/anime_highlight_cuts/theme_collector/create_coco_pip_dataset_standalone.py:264-296"
    },
    "1917": {
        "file_id": 182,
        "content": "This code is creating a mask for an image, adjusting the radius for the ellipse shape, and then composites it with other images if needed. It also calculates the base points for a rectangle, and gets the image shapes of each selected image in the list. The \"else\" part suggests that there is a condition being checked before this code block is executed. The function getMarginRatio() and getRadius() are used to calculate the margins and radius of the ellipse respectively.",
        "type": "comment"
    },
    "1918": {
        "file_id": 182,
        "content": "            base = half_width * (1 - margin * 2)\n            imageHeight, imageWidth = imageShape[:2]\n            if imageHeight > imageWidth:\n                imageShape = (int(base * (imageWidth / imageHeight)), int(base))\n            else:\n                imageShape = (int(base), int(base * (imageHeight / imageWidth)))\n            image = cv2.resize(image, imageShape)\n            x0 = int((half_width - imageShape[0]) / 2) + basePoints[index][0]\n            x1 = x0 + imageShape[0]\n            y0 = int((half_width - imageShape[1]) / 2) + basePoints[index][1]\n            y1 = y0 + imageShape[1]\n            if random.random() > 0.5:\n                draw.rectangle((x0, y0, x1, y1), fill=\"white\")\n            else:\n                draw.rounded_rectangle(\n                    (x0, y0, x1, y1),\n                    fill=\"white\",\n                    radius=min(int(x1 - x0) / 2, int(y1 - y0) / 2, getRadius()),\n                )\n            imageCanvas[y0 : image.shape[0] + y0, x0 : image.shape[1] + x0, :] = image\n            imageCoordinates.append(",
        "type": "code",
        "location": "/tests/anime_highlight_cuts/theme_collector/create_coco_pip_dataset_standalone.py:297-321"
    },
    "1919": {
        "file_id": 182,
        "content": "Resizing image to fit within specified bounds and applying rectangle or rounded rectangle based on random chance, then combining with canvas image.",
        "type": "comment"
    },
    "1920": {
        "file_id": 182,
        "content": "                (\n                    x0 + image.shape[1] / 2,\n                    y0 + image.shape[0] / 2,\n                    image.shape[1],\n                    image.shape[0],\n                )\n            )  # x_center, y_center, width, height\n    ## mix images with mask\n    imageMaskNumpyArray = np.array(imageMask) / 255  # float64\n    imageMaskNumpyArrayInverted = 1 - imageMaskNumpyArray\n    x0 = 0\n    y0 = textTotalHeight if textFormat == \"up\" else 0\n    backgroundImage[y0 : y0 + imageCanvasShape[0], x0 : x0 + imageCanvasShape[1], :] = (\n        backgroundImage[y0 : y0 + imageCanvasShape[0], x0 : x0 + imageCanvasShape[1], :]\n        * imageMaskNumpyArrayInverted\n    ).astype(np.uint8) + (imageCanvas * imageMaskNumpyArray).astype(np.uint8)\n    # print()\n    ## get labels which will be exported to txt\n    contents = []\n    for coord in imageCoordinates:\n        x_center_relative, y_center_relative, imWidth, imHeight = coord\n        x_center, y_center = x_center_relative + x0, y_center_relative + y0\n        dataPoints = [",
        "type": "code",
        "location": "/tests/anime_highlight_cuts/theme_collector/create_coco_pip_dataset_standalone.py:322-349"
    },
    "1921": {
        "file_id": 182,
        "content": "Code snippet combines images with a mask, creates a new image by multiplying original image with mask and inverting the result. This modified image is then added to the background image, creating an overlay effect. The code also collects data points for labels that will be exported to txt files.",
        "type": "comment"
    },
    "1922": {
        "file_id": 182,
        "content": "            x_center / backgroundShape[1],\n            y_center / backgroundShape[0],\n            imWidth / backgroundShape[1],\n            imHeight / backgroundShape[0],\n        ]\n        labelString = \" \".join(([\"0\"] + [f\"{number:.3f}\" for number in dataPoints]))\n        contents.append(labelString)\n        # print(\"LABELSTRING?\", labelString)\n    ## preview\n    # previewImageName = f\"{imageFormat}_{textFormat}_{backgroundFormat}.png\"\n    realIndex = imageIndex + _i\n    cv2.imwrite(\n        os.path.join(train_path_relative, f\"{str(realIndex).zfill(12)}.png\"),\n        backgroundImage,\n    )\n    with open(\n        os.path.join(train_label_path_relative, f\"{str(realIndex).zfill(12)}.txt\"), \"w+\"\n    ) as f:\n        f.write(\"\\n\".join(contents))\n    # cv2.imshow(previewImageName, backgroundImage)\n    # cv2.waitKey(0)\nprint(\"coco pip dataset created!\")",
        "type": "code",
        "location": "/tests/anime_highlight_cuts/theme_collector/create_coco_pip_dataset_standalone.py:350-374"
    },
    "1923": {
        "file_id": 182,
        "content": "Creates COCO-style pip dataset standalone by iterating over images and labels, writing them to train_path_relative and train_label_path_relative, with real indexing. It also prints a confirmation message upon completion.",
        "type": "comment"
    },
    "1924": {
        "file_id": 183,
        "content": "/tests/anime_highlight_cuts/theme_collector/create_dataset.py",
        "type": "filepath"
    },
    "1925": {
        "file_id": 183,
        "content": "This code creates a new directory, reads video frames, extracts data points, labels them, and saves images as JPEGs in YAML format. The process ends with \"dataset created.\" message after the loop.",
        "type": "summary"
    },
    "1926": {
        "file_id": 183,
        "content": "import yaml\n# why you are taking so much RAM?\n## suggest that you label some (many) still image and mark out the picture-in-picture parts from it? about 2000 images?\n## man just make sure these pictures are not \"pip\" so we can put borders and arrange them randomly to create our super dataset. use MSCOCO/coco128?\ntrain_path = \"images/train\"\ntest_path = \"images/test\"\ntrain_label_path = \"labels/train\"\ntest_label_path = \"labels/test\"\nbasepath = \"pip_dataset\"\ndata = {\n    \"path\": f\"../{basepath}\",  # dataset root dir\n    \"train\": train_path,  # train images (relative to 'path')\n    \"val\": train_path,  # val images (relative to 'path')\n    \"test\": test_path,\n    \"names\": {0: \"active_frame\"},\n}\nimport os\nos.system(f\"rm -rf {basepath}\")\nindex = 1\nos.makedirs(os.path.join(basepath, train_path), exist_ok=True)\nos.makedirs(os.path.join(basepath, test_path), exist_ok=True)\nos.makedirs(os.path.join(basepath, train_label_path), exist_ok=True)\nos.makedirs(os.path.join(basepath, test_label_path), exist_ok=True)\nwith open(\"pip_dataset/pip_dataset.yaml\", \"w+\") as f:",
        "type": "code",
        "location": "/tests/anime_highlight_cuts/theme_collector/create_dataset.py:1-39"
    },
    "1927": {
        "file_id": 183,
        "content": "This code creates a new dataset by copying and renaming files from existing \"train\" and \"test\" directories into a new directory named \"pip_dataset\". It also creates label files for the copied images in the same manner. The resulting dataset is stored in a YAML file named \"pip_dataset/pip_dataset.yaml\".",
        "type": "comment"
    },
    "1928": {
        "file_id": 183,
        "content": "    f.write(yaml.dump(data, default_flow_style=False))\nimport cv2\nimport pandas\ncsvNames = [fpath for fpath in os.listdir(\".\") if fpath.endswith(\".csv\")]\nimport progressbar\nremainder = 7 # changed? heck?\nfor csvName in csvNames:\n    dataframe = pandas.read_csv(csvName)\n    videoFileName = f'{csvName.split(\".\")[0]}.mp4'\n    #\n    frameIndex = 0\n    cap = cv2.VideoCapture(videoFileName)\n    myIterator = progressbar.progressbar(dataframe.iterrows())\n    frame_height, frame_width = cap.get(cv2.CAP_PROP_FRAME_HEIGHT), cap.get(\n        cv2.CAP_PROP_FRAME_WIDTH\n    )\n    while True:\n        succ, image = cap.read()\n        nextRow = next(myIterator, None)\n        if nextRow is None:\n            break\n        if succ:\n            frameIndex += 1\n            if frameIndex % remainder != 0:\n                continue\n            _, _, min_x, min_y, w, h = nextRow[1].tolist()\n            if (min_x, min_y, w, h) == (0, 0, 0, 0) or w == 0 or h == 0:\n                continue\n            index += 1\n            imageName = f'{f\"{index}\".zfill(12)}.png'",
        "type": "code",
        "location": "/tests/anime_highlight_cuts/theme_collector/create_dataset.py:40-73"
    },
    "1929": {
        "file_id": 183,
        "content": "Reading CSV files, extracting relevant data and frame index, creating image names based on the index, writing dataset to YAML format.",
        "type": "comment"
    },
    "1930": {
        "file_id": 183,
        "content": "            labelName = f'{f\"{index}\".zfill(12)}.txt'\n            dataPoints = [\n                (min_x + w / 2) / frame_width,\n                (min_y + h / 2) / frame_height,\n                w / frame_width,\n                h / frame_height,\n            ]\n            with open(os.path.join(basepath, train_label_path, labelName), \"w+\") as f:\n                content = \" \".join(([\"0\"] + [f\"{number:.3f}\" for number in dataPoints]))\n                f.write(content)\n            cv2.imwrite(os.path.join(basepath, train_path, imageName), image)\n            del image\n        else:\n            break\n    cap.release()\n    del cap\n    del dataframe\ntestVideo = \"output.mp4\"\nw, h = 1152, 648\nmin_x, min_y = 384, 216\nprint(\"creating 4min pip dataset\")\ncap = cv2.VideoCapture(testVideo)\nframe_height, frame_width = cap.get(cv2.CAP_PROP_FRAME_HEIGHT), cap.get(\n    cv2.CAP_PROP_FRAME_WIDTH\n)\ndataPoints = [\n    (min_x + w / 2) / frame_width,\n    (min_y + h / 2) / frame_height,\n    w / frame_width,\n    h / frame_height,\n]\nframeCounter = 0",
        "type": "code",
        "location": "/tests/anime_highlight_cuts/theme_collector/create_dataset.py:74-110"
    },
    "1931": {
        "file_id": 183,
        "content": "The code reads an input video file, extracts and saves data points for each frame, and stores the labeled data in a text file. It also writes each frame to an image file.",
        "type": "comment"
    },
    "1932": {
        "file_id": 183,
        "content": "while True:\n    succ, image = cap.read()\n    if succ:\n        frameCounter += 1\n        if frameCounter % remainder != 0:\n            continue\n        index += 1\n        imageName = f'{f\"{index}\".zfill(12)}.png'\n        labelName = f'{f\"{index}\".zfill(12)}.txt'\n        with open(os.path.join(basepath, train_label_path, labelName), \"w+\") as f:\n            content = \" \".join(([\"0\"] + [f\"{number:.3f}\" for number in dataPoints]))\n            f.write(content)\n        cv2.imwrite(os.path.join(basepath, train_path, imageName), image)\n        del image\n    else:\n        break\ncap.release()\ndel cap\nprint(\"creating reference dataset\")\ntestVideo = \"output_1.mp4\"\ncap = cv2.VideoCapture(testVideo)\nframe_height, frame_width = cap.get(cv2.CAP_PROP_FRAME_HEIGHT), cap.get(\n    cv2.CAP_PROP_FRAME_WIDTH\n)\ndataPoints = [0.5, 0.5, 1, 1]\nframeCounter = 0\nwhile True:\n    succ, image = cap.read()\n    if succ:\n        frameCounter += 1\n        if frameCounter % remainder != 0:\n            continue\n        index += 1\n        imageName = f'{f\"{index}\".zfill(12)}.png'",
        "type": "code",
        "location": "/tests/anime_highlight_cuts/theme_collector/create_dataset.py:111-152"
    },
    "1933": {
        "file_id": 183,
        "content": "Code snippet reads frames from a video, creates label files for each frame containing data points, and saves corresponding images. The loop iterates until the end of the video is reached, skipping non-remainder frames. It uses OpenCV to read and write image data and handles file writing.",
        "type": "comment"
    },
    "1934": {
        "file_id": 183,
        "content": "        labelName = f'{f\"{index}\".zfill(12)}.txt'\n        with open(os.path.join(basepath, train_label_path, labelName), \"w+\") as f:\n            content = \" \".join(([\"0\"] + [f\"{number:.3f}\" for number in dataPoints]))\n            f.write(content)\n        cv2.imwrite(os.path.join(basepath, train_path, imageName), image)\n        del image\n    else:\n        break\ncap.release()\ndel cap\nprint(\"dataset created.\")",
        "type": "code",
        "location": "/tests/anime_highlight_cuts/theme_collector/create_dataset.py:153-166"
    },
    "1935": {
        "file_id": 183,
        "content": "This code creates a dataset by iterating over data points and saving them as labels in text files. It also saves corresponding images as JPEGs. Finally, it prints \"dataset created.\" after the loop ends.",
        "type": "comment"
    },
    "1936": {
        "file_id": 184,
        "content": "/tests/anime_highlight_cuts/theme_collector/create_rounded_rectangle.py",
        "type": "filepath"
    },
    "1937": {
        "file_id": 184,
        "content": "The code defines a function called \"rectangle\" that creates an 800x400 black image, draws a regular rectangle and a rounded rectangle on it using PIL, converts the image to numpy array, prints its shape, data type, and maximum value, displays the image using OpenCV, and then waits for a key press.",
        "type": "summary"
    },
    "1938": {
        "file_id": 184,
        "content": "from PIL import Image, ImageDraw\nimport cv2\nimport numpy as np\ndef rectangle():\n    image = Image.new(\"RGB\", (800, 400), \"black\")  # width, height?\n    draw = ImageDraw.Draw(image)\n    # Draw a regular rectangle\n    draw.rectangle((200, 100, 300, 200), fill=\"white\")\n    # Draw a rounded rectangle\n    draw.rounded_rectangle((50, 50, 150, 150), fill=\"white\", radius=20)\n    npArray = np.array(image)  # /255\n    # uint8? then float64? great.\n    print(npArray)\n    print(npArray.shape, npArray.dtype, npArray.max())  # 255?\n    cv2.imshow(\"mask\", npArray)\n    # maybe we just want \"1\" instead of \"255\"\n    # divide by 255 then.\n    cv2.waitKey(0)\nrectangle()",
        "type": "code",
        "location": "/tests/anime_highlight_cuts/theme_collector/create_rounded_rectangle.py:1-23"
    },
    "1939": {
        "file_id": 184,
        "content": "The code defines a function called \"rectangle\" that creates an 800x400 black image, draws a regular rectangle and a rounded rectangle on it using PIL, converts the image to numpy array, prints its shape, data type, and maximum value, displays the image using OpenCV, and then waits for a key press.",
        "type": "comment"
    },
    "1940": {
        "file_id": 185,
        "content": "/tests/anime_highlight_cuts/theme_collector/keyboard_listener.py",
        "type": "filepath"
    },
    "1941": {
        "file_id": 185,
        "content": "This code uses the pynput library to listen for keyboard events. It defines two functions, `on_press` and `on_release`, to handle key presses and releases respectively. The listener object is created with these functions assigned as event handlers, and then the program enters a loop where it continuously listens for keystrokes until the listener is stopped or terminated.",
        "type": "summary"
    },
    "1942": {
        "file_id": 185,
        "content": "from pynput.keyboard import Listener\ndef on_press(key):\n    try:\n        print(\"alphanumeric key {0} pressed\".format(key.char))\n    except AttributeError:\n        print(\"special key {0} pressed\".format(key))\ndef on_release(key):\n    print(\"{0} released\".format(key))\nlistener = Listener(on_press=on_press, on_release=on_release)\n# listener.start()\nwith listener:\n    listener.join()",
        "type": "code",
        "location": "/tests/anime_highlight_cuts/theme_collector/keyboard_listener.py:1-18"
    },
    "1943": {
        "file_id": 185,
        "content": "This code uses the pynput library to listen for keyboard events. It defines two functions, `on_press` and `on_release`, to handle key presses and releases respectively. The listener object is created with these functions assigned as event handlers, and then the program enters a loop where it continuously listens for keystrokes until the listener is stopped or terminated.",
        "type": "comment"
    },
    "1944": {
        "file_id": 186,
        "content": "/tests/anime_highlight_cuts/theme_collector/make_picture_in_picture_challange.py",
        "type": "filepath"
    },
    "1945": {
        "file_id": 186,
        "content": "This code sets the base path for video files, imports os module, and uses os.path.join() to create file paths for source_video and background_video. It also sets a placeholder value for video duration (10) and comments on using ffplay and saving metadata in filenames.",
        "type": "summary"
    },
    "1946": {
        "file_id": 186,
        "content": "basepath = \"/Users/jamesbrown/Downloads/anime_download\"\nimport os\nsource_video = os.path.join(\n    basepath, \"[Sakurato] Onii-chan wa Oshimai! [未删减][02][AVC-8bit 1080p AAC][CHT].mp4\"\n)\nbackground_video = os.path.join(\n    basepath, \"[MLU-S] Onii-chan wa Oshimai! - 03 [1080p][Multi Subs].mkv\"\n)\nvideo_duration = 10  # just for test.\n# use ffplay?\n# better save metadata in the filename.",
        "type": "code",
        "location": "/tests/anime_highlight_cuts/theme_collector/make_picture_in_picture_challange.py:1-16"
    },
    "1947": {
        "file_id": 186,
        "content": "This code sets the base path for video files, imports os module, and uses os.path.join() to create file paths for source_video and background_video. It also sets a placeholder value for video duration (10) and comments on using ffplay and saving metadata in filenames.",
        "type": "comment"
    },
    "1948": {
        "file_id": 187,
        "content": "/tests/anime_highlight_cuts/theme_collector/pack_source_dataset.sh",
        "type": "filepath"
    },
    "1949": {
        "file_id": 187,
        "content": "This command uses 7-Zip (7z) to create a compressed archive named \"pip_source_dataset.7z\" that includes all .mp4 and .csv files in the current directory, likely for efficient storage or transfer.",
        "type": "summary"
    },
    "1950": {
        "file_id": 187,
        "content": "7z a pip_source_dataset.7z *.mp4 *.csv",
        "type": "code",
        "location": "/tests/anime_highlight_cuts/theme_collector/pack_source_dataset.sh:1-1"
    },
    "1951": {
        "file_id": 187,
        "content": "This command uses 7-Zip (7z) to create a compressed archive named \"pip_source_dataset.7z\" that includes all .mp4 and .csv files in the current directory, likely for efficient storage or transfer.",
        "type": "comment"
    },
    "1952": {
        "file_id": 188,
        "content": "/tests/anime_highlight_cuts/theme_collector/screenshot_saucenao.py",
        "type": "filepath"
    },
    "1953": {
        "file_id": 188,
        "content": "This code uses the SauceNao API to identify an anime source from a given image file, displaying results such as similarity and URLs, and extracting relevant data for anime cuts including part, title, estimated time, and IDs from various platforms.",
        "type": "summary"
    },
    "1954": {
        "file_id": 188,
        "content": "# saucenao (if fail, use trace.moe)\n# use proxies, since we are using free tiers.\nimport os\nSAUCENAO_API_KEY=os.environ.get('SAUCENAO_API_KEY') # how to run this without api key?\nprint(\"API KEY?\", SAUCENAO_API_KEY)\n# sauce = SauceNao(api_key=SAUCENAO_API_KEY) # shit. not working!\nfilepath = \"/Users/jamesbrown/Downloads/anime_download/dress_test_pictures/女装0.jpeg\"\n# import asyncio\n# loop = asyncio.get_event_loop()\n# results = loop.run_until_complete(sauce.from_file(filepath))\n# results = await sauce.from_url('https://i.imgur.com/QaKpV3s.png')\n# no api key. fuck.\nfrom saucenao_api import SauceNao\nsauce = SauceNao(SAUCENAO_API_KEY)\nwith open(filepath,'rb') as f:\n    results = sauce.from_file(f)\n    long_remaining = results.long_remaining # wait till next day? wtf?\n    short_remaining = results.short_remaining\n    result_results = len(results)\n    print(results)\n    best = results[0]\n    similarity = best.similarity\n    # just trust anilist.\n    urls = best.urls # https://anilist.co/anime/ https://anidb.net/anime/ https://myanimelist.net/anime/",
        "type": "code",
        "location": "/tests/anime_highlight_cuts/theme_collector/screenshot_saucenao.py:1-25"
    },
    "1955": {
        "file_id": 188,
        "content": "The code aims to use the SauceNao API to identify an anime source from a given image file. It first checks if the SAUCENAO_API_KEY is set in the environment variables and prints it out. Then, it creates a SauceNao object with the API key and tries to find similar images using the image file path. The code then displays the remaining time before the results become available (long_remaining and short_remaining), the number of results found (result_results), the best match's similarity, and the URLs associated with the best match. If no API key is provided, the code indicates that it will not be able to use SauceNao API.",
        "type": "comment"
    },
    "1956": {
        "file_id": 188,
        "content": "    best_data =  best.raw.get('data',{})\n    part = best_data.get('part', None) # not always.\n    title = best.title\n    est_time =best_data.get('est_time',None) # be like: '00:16:21 / 00:25:12'\n    if est_time:\n        start_end = [timestamp.strip() for timestamp in est_time.split(\"/\")]\n        start_time, end_time = start_end\n    # these ids must be the same across different images.\n    anidb_aid = best_data.get('anidb_aid',None)\n    mal_id = best_data.get('mal_id',None)\n    anilist_id = best_data.get('anilist_id',None)\n    breakpoint()",
        "type": "code",
        "location": "/tests/anime_highlight_cuts/theme_collector/screenshot_saucenao.py:26-37"
    },
    "1957": {
        "file_id": 188,
        "content": "Extracts relevant data for an anime cut: part (may not always be available), title, estimated time (in the format \"start / end\"), and IDs from different platforms - AniDB (anidb_aid), MyAnimeList (mal_id), and AniList (anilist_id).",
        "type": "comment"
    },
    "1958": {
        "file_id": 189,
        "content": "/tests/anime_highlight_cuts/theme_collector/screenshot_tracemoe.py",
        "type": "filepath"
    },
    "1959": {
        "file_id": 189,
        "content": "This code sends a JPEG image to the trace.moe API for anime character recognition and stores the results in the 'data' variable. It then prints the data using rich library, handles potential errors, and retrieves necessary information from the results, including anilist ID, filename, episode (if available), start and end timestamps, and similarity rating.",
        "type": "summary"
    },
    "1960": {
        "file_id": 189,
        "content": "# anilist has typo on \"Yahari Ore no Seishun Lovecome wa Machigatte Iru.\" which might be harmful.\n# imagePath = \"/Users/jamesbrown/Downloads/anime_download/dress_test_pictures/女装0.jpeg\"\nimagePath = \"/Users/jamesbrown/Downloads/gay_anime_shot.jpeg\"\nimport requests\ndata =requests.post(\"https://api.trace.moe/search\",\n  data=open(imagePath, \"rb\"), # since this is smallest\n  headers={\"Content-Type\": \"image/jpeg\"}\n).json() # remember you must change your ip later.\nimport rich\nrich.print(data) # the anime character recognition website is not running so well.\nerror = data['error']\nassert error == \"\"\nresults = data.get('result',[])\nfor result in results: # already sorted.\n    anilist_id = result['anilist'] # well. we only got one.\n    filename = result['filename'] # need parsing right?\n    episode = result.get('episode', None) # really we don't have episode here.\n    start, end = result['from'], result['to']\n    similarity = result['similarity']\nbreakpoint()",
        "type": "code",
        "location": "/tests/anime_highlight_cuts/theme_collector/screenshot_tracemoe.py:1-25"
    },
    "1961": {
        "file_id": 189,
        "content": "This code sends a JPEG image to the trace.moe API for anime character recognition and stores the results in the 'data' variable. It then prints the data using rich library, handles potential errors, and retrieves necessary information from the results, including anilist ID, filename, episode (if available), start and end timestamps, and similarity rating.",
        "type": "comment"
    },
    "1962": {
        "file_id": 190,
        "content": "/tests/anime_highlight_cuts/theme_collector/strip_optimizer_from_trained_best_model.py",
        "type": "filepath"
    },
    "1963": {
        "file_id": 190,
        "content": "This code imports the \"strip_optimizer\" function from the \"torch_utils\" module, then it specifies the original model path (\"general_ver1_with_optimizer.pt\") and the exported model path (\"general_ver1.pt\"). The \"strip_optimizer\" function is called with these paths to remove any optimizers associated with the original model while saving a new model without them at the specified export path.",
        "type": "summary"
    },
    "1964": {
        "file_id": 190,
        "content": "from ultralytics.yolo.utils.torch_utils import strip_optimizer\nmodel_path = \"general_ver1_with_optimizer.pt\"\nexport_path = \"general_ver1.pt\"\nstrip_optimizer(f=model_path, s=export_path)",
        "type": "code",
        "location": "/tests/anime_highlight_cuts/theme_collector/strip_optimizer_from_trained_best_model.py:1-6"
    },
    "1965": {
        "file_id": 190,
        "content": "This code imports the \"strip_optimizer\" function from the \"torch_utils\" module, then it specifies the original model path (\"general_ver1_with_optimizer.pt\") and the exported model path (\"general_ver1.pt\"). The \"strip_optimizer\" function is called with these paths to remove any optimizers associated with the original model while saving a new model without them at the specified export path.",
        "type": "comment"
    },
    "1966": {
        "file_id": 191,
        "content": "/tests/anime_highlight_cuts/theme_collector/test_video_overlay.sh",
        "type": "filepath"
    },
    "1967": {
        "file_id": 191,
        "content": "This script combines two video files, scales the first one to specific dimensions, and overlays them. It is used for creating a video highlight with theme overlay. The basepath variable contains the downloaded videos' location. The resulting video is saved as output.mp4 if both inputs are present, or output_1.mp4 if only the second input video is provided.",
        "type": "summary"
    },
    "1968": {
        "file_id": 191,
        "content": "video_0=\"[Sakurato] Onii-chan wa Oshimai! [未删减][02][AVC-8bit 1080p AAC][CHT].mp4\"\nvideo_1=\"[MLU-S] Onii-chan wa Oshimai! - 03 [1080p][Multi Subs].mkv\"\nbasepath=\"/Users/jamesbrown/Downloads/anime_download\"\nvideo_2=\"[Sakurato] Onii-chan wa Oshimai! [01][AVC-8bit 1080p AAC][CHT].mp4\"\n# ffmpeg -y -t 0:04:00 -i \"$basepath/$video_0\" -t 0:04:00 -i \"$basepath/$video_1\" -filter_complex \"[0:v]scale=1152:648[v0];[1:v][v0]overlay=384:216\" output.mp4\nffmpeg -y -t 0:04:00 -i \"$basepath/$video_2\" output_1.mp4",
        "type": "code",
        "location": "/tests/anime_highlight_cuts/theme_collector/test_video_overlay.sh:1-9"
    },
    "1969": {
        "file_id": 191,
        "content": "This script combines two video files, scales the first one to specific dimensions, and overlays them. It is used for creating a video highlight with theme overlay. The basepath variable contains the downloaded videos' location. The resulting video is saved as output.mp4 if both inputs are present, or output_1.mp4 if only the second input video is provided.",
        "type": "comment"
    },
    "1970": {
        "file_id": 192,
        "content": "/tests/anime_highlight_cuts/theme_collector/view_boundingbox.py",
        "type": "filepath"
    },
    "1971": {
        "file_id": 192,
        "content": "This code reads a bounding box coordinates, calculates the minimum x and y values, and then uses OpenCV to draw a rectangle on an image at these coordinates. The color of the rectangle is green (0, 255, 0) and the thickness is 3 pixels. Finally, it displays the image with the rectangle drawn in a window named \"PIP\".",
        "type": "summary"
    },
    "1972": {
        "file_id": 192,
        "content": "x, y, w, h = [1118.5, 545.5, 1585, 1069]\nmin_x, min_y = int(x - (w / 2)), int(y - (h / 2))\nimport cv2\nimagePath = \"\"\nimage = cv2.imread(imagePath)\np0, p1 = (min_x, min_y), (min_x + w, min_y + h)\ncv2.rectangle(image, p0, p1, (0, 255, 0), 3)\ncv2.imshow(\"PIP\", image)",
        "type": "code",
        "location": "/tests/anime_highlight_cuts/theme_collector/view_boundingbox.py:1-11"
    },
    "1973": {
        "file_id": 192,
        "content": "This code reads a bounding box coordinates, calculates the minimum x and y values, and then uses OpenCV to draw a rectangle on an image at these coordinates. The color of the rectangle is green (0, 255, 0) and the thickness is 3 pixels. Finally, it displays the image with the rectangle drawn in a window named \"PIP\".",
        "type": "comment"
    },
    "1974": {
        "file_id": 193,
        "content": "/tests/anime_highlight_cuts/theme_collector/yolov8_test.py",
        "type": "filepath"
    },
    "1975": {
        "file_id": 193,
        "content": "This code imports and processes images using the YOLO model, filters frames based on criteria, selects candidates for main frame detection, draws a rectangle around the detected frame, and displays an image with the PIP frame.",
        "type": "summary"
    },
    "1976": {
        "file_id": 193,
        "content": "from ultralytics import YOLO\n## yolov8 tracking needs special ultralytics version. it is been updated too damn often. you need to downgrade.\n## https://github.com/mikel-brostrom/yolov8_tracking\n## this might add unwanted overheads. warning!\n# no one will miss `genesis.pt`, right?\nmodel = YOLO(\"general_ver1.pt\")\n## TODO: create dataset to prevent detection of pure color/gradient borders\n# model = YOLO(\"ver3.pt\")\n# find trained weights on huggingface:\n# https://huggingface.co/James4Ever0/yolov8_pip_ultralytics\n# imagePaths = [\n#     \"000000003099.png\",\n#     \"simple_pip.png\",\n#     \"no_border_0.jpg\",\n#     \"has_border_0.jpg\",\n#     \"has_border_1.jpg\",\n#     \"has_border_2.jpg\",\n# ]\nimport os\nimagePaths = [\n    fpath\n    for fpath in os.listdir(\".\")\n    if fpath.split(\".\")[-1].lower() in (\"jpg\", \"jpeg\", \"png\")\n]\nimport cv2\nframeRatioFilters = [(16 / 9, 0.2, \"landscape\")]\nframeAreaThreshold = 0.15\nfor imagePath in imagePaths:\n    image = cv2.imread(imagePath)\n    output = model(image)\n    height, width, _ = image.shape\n    center = (width / 2, height / 2)",
        "type": "code",
        "location": "/tests/anime_highlight_cuts/theme_collector/yolov8_test.py:1-43"
    },
    "1977": {
        "file_id": 193,
        "content": "This code imports the YOLO model from ultralytics, loads a specific model file, and defines image paths. It also retrieves all image files in the current directory, filters frames based on aspect ratio and area threshold, and processes each image using the loaded YOLO model. This might involve downgrading the ultralytics version due to frequent updates and creating a dataset to prevent detection of pure color/gradient borders as TODO tasks.",
        "type": "comment"
    },
    "1978": {
        "file_id": 193,
        "content": "    # print(\"CENTER:\",center)\n    candidates = []\n    for xyxy in output[0].boxes.xyxy.numpy().astype(int).tolist():\n        x0, y0, x1, y1 = xyxy\n        currentFrameWidth = x1 - x0\n        currentFrameHeight = y1 - y0\n        currentFrameArea = currentFrameWidth * currentFrameHeight\n        # area filter? a must.\n        if currentFrameArea / (height * width) < frameAreaThreshold:\n            continue\n        else:\n            # filter out malformed frames? just for anime?\n            currentFrameRatio = currentFrameWidth / currentFrameHeight\n            if all(\n                [\n                    (\n                        currentFrameRatio < frameRatioStandard - frameRatioMargin\n                        or currentFrameRatio > frameRatioStandard + frameRatioMargin\n                    )\n                    for frameRatioStandard, frameRatioMargin, _ in frameRatioFilters\n                ]\n            ):\n                continue\n            candidates.append((x0, y0, x1, y1))\n    # sort it by area, then by centrality?",
        "type": "code",
        "location": "/tests/anime_highlight_cuts/theme_collector/yolov8_test.py:44-69"
    },
    "1979": {
        "file_id": 193,
        "content": "This code filters out detected frames based on area, frame aspect ratio, and possibly malformed frames. It appends valid frames to the 'candidates' list, which may be sorted by area and centrality later in the script.",
        "type": "comment"
    },
    "1980": {
        "file_id": 193,
        "content": "    candidates.sort(\n        key=lambda points: -(points[2] - points[0]) * (points[3] - points[1])\n    )\n    # print(\"SORT_AREA:\", [(points[2] - points[0]) * (points[3] - points[1]) for points in candidates])\n    candidates = candidates[:2]\n    candidates.sort(\n        key=lambda points: (((points[2] + points[0]) / 2) - center[0]) ** 2\n        + (((points[3] + points[1]) / 2) - center[1]) ** 2\n    )\n    # print(\"SORT_CENTRALITY:\", [(((points[2] + points[0]) / 2) - center[0]) ** 2\n    # + (((points[3] + points[1]) / 2) - center[1]) ** 2 for points in candidates])\n    if len(candidates) > 0:\n        print(\"main frame found.\")\n        x0, y0, x1, y1 = candidates[0]\n        cv2.rectangle(image, (x0, y0), (x1, y1), (0, 0, 255), thickness=10)\n    else:\n        print(\"no main frame found.\")\n    cv2.imshow(\"PIP\", image)\n    cv2.waitKey(0)",
        "type": "code",
        "location": "/tests/anime_highlight_cuts/theme_collector/yolov8_test.py:71-89"
    },
    "1981": {
        "file_id": 193,
        "content": "The code sorts the candidates by area and centrality, selects two candidates, and if a main frame is found, it draws a rectangle around it. If no main frame is found, it displays a message. Finally, it shows the image with the PIP frame.",
        "type": "comment"
    },
    "1982": {
        "file_id": 194,
        "content": "/tests/anime_highlight_cuts/theme_collector/yolov8_train_save_test.py",
        "type": "filepath"
    },
    "1983": {
        "file_id": 194,
        "content": "The code is training a YOLO object detection model, validating its performance, and then exporting it to be used later. It uses the \"yolov8n.pt\" pre-trained model, trains it for 3 epochs with the provided dataset, evaluates its validation accuracy, displays the results, and finally exports the trained model as \"pip_detector.pth\".",
        "type": "summary"
    },
    "1984": {
        "file_id": 194,
        "content": "from ultralytics import YOLO\n# pip install opencv-python==4.5.5.64\n# shit?\n# https://github.com/asweigart/pyautogui/issues/706\nmodel = YOLO(\"yolov8n.pt\")\n# print(model)\n# model.to('mps')\n# The operator 'aten::_slow_conv2d_forward' is not current implemented for the MPS device.\n# fuck.\n# breakpoint()\nimport rich\ntrain_result = model.train(epochs=3, data=\"./pip_dataset/pip_dataset.yaml\")\nprint(\"TRAIN RESULT?\")\nrich.print(train_result)\nval_result = model.val()\nprint(\"VALIDATION RESULT?\")\nrich.print(val_result)\ntest_result = model(\"./pip_dataset/images/test/000000003099.png\")\ntest_boxes = test_result[0].boxes\ntest_classes, test_xywh, test_confidence = (\n    test_boxes.cls.numpy(),\n    test_boxes.xywh.numpy(), # the xy in this xywh means the center of the bounding box.\n    test_boxes.conf.numpy(),\n)\nprint(\"XYWH?\", test_xywh)\nprint(\"CLASSES?\", test_classes)\nprint(\"CONFIDENCE?\", test_confidence)\n# model.export(format=\"pytorch\", path=\"./pip_detector.pth\")",
        "type": "code",
        "location": "/tests/anime_highlight_cuts/theme_collector/yolov8_train_save_test.py:1-39"
    },
    "1985": {
        "file_id": 194,
        "content": "The code is training a YOLO object detection model, validating its performance, and then exporting it to be used later. It uses the \"yolov8n.pt\" pre-trained model, trains it for 3 epochs with the provided dataset, evaluates its validation accuracy, displays the results, and finally exports the trained model as \"pip_detector.pth\".",
        "type": "comment"
    },
    "1986": {
        "file_id": 195,
        "content": "/tests/apple_prores_encoding_play/test.sh",
        "type": "filepath"
    },
    "1987": {
        "file_id": 195,
        "content": "This script encodes a video file using FFmpeg with the prores_aw encoder and saves it in an Apple ProRes format. The code uses the vulkan hardware acceleration and sets the output container format to .mkv. It also provides alternative options for encoding in ProRes 422 or 4444 HQ, specifying profile, vendor, bits per MB, and pixel format. It mentions allowed container formats for ProRes as .mov, .mkv, and .mxf.",
        "type": "summary"
    },
    "1988": {
        "file_id": 195,
        "content": "videoPath=\"/root/Desktop/works/pyjom/samples/video/cute_cat_gif.mp4\"\n# prores_aw\nffmpeg -hwaccel vulkan -i $videoPath -c:v prores_ks  output.mkv\n# https://ottverse.com/ffmpeg-convert-to-apple-prores-422-4444-hq/#:~:text=FFmpeg%20contains%20two%20ProRes%20encoders%2C%20the%20prores-aw%20and,option%20to%20choose%20the%20ProRes%20profile%20to%20encode.\n# videoPath=\"/Users/jamesbrown/Desktop/works/pyjom_remote/samples/video/cute_cat_gif.mp4\"\n# ffmpeg -hwaccel videotoolbox -i $videoPath -c:v prores_ks  \\\n# -profile:v 4 \\\n# -vendor apl0 \\\n# -bits_per_mb 8000 \\\n# -pix_fmt yuva444p10le \\ \n# output.mov\n# Do remember to store the output in either of these three formats that are allowed as containers for the ProRes format.\n# .mov (QuickTime)\n# .mkv (Matroska)\n# .mxf (Material eXchange Format)",
        "type": "code",
        "location": "/tests/apple_prores_encoding_play/test.sh:1-16"
    },
    "1989": {
        "file_id": 195,
        "content": "This script encodes a video file using FFmpeg with the prores_aw encoder and saves it in an Apple ProRes format. The code uses the vulkan hardware acceleration and sets the output container format to .mkv. It also provides alternative options for encoding in ProRes 422 or 4444 HQ, specifying profile, vendor, bits per MB, and pixel format. It mentions allowed container formats for ProRes as .mov, .mkv, and .mxf.",
        "type": "comment"
    },
    "1990": {
        "file_id": 196,
        "content": "/tests/audio_volume_meter/test_volume_meter.py",
        "type": "filepath"
    },
    "1991": {
        "file_id": 196,
        "content": "This code calculates audio parameters, generates vocal slices, and clusters segments using KMeans for labeling. It merges adjacent segments with similar labels and stores the updated labels.",
        "type": "summary"
    },
    "1992": {
        "file_id": 196,
        "content": "# usually yelling is not always funny. but we can do speech to text. taking longer time though... pinpoint the cue time.\n# often some exclamation attempts like repetation or louder sounds.\naudio_src = \"/media/root/help/pyjom/samples/audio/dog_with_text/vocals.wav\"\n# heard of dog woooling.\n# import audioop\nimport pydub\ntimestep = 0.1  # my time setting.\naudiofile = pydub.AudioSegment.from_wav(audio_src)\nframe_rate = audiofile.frame_rate\nseconds = audiofile.duration_seconds\nprint(frame_rate)  # 44100.\nprint(seconds)  # sample length\nimport math\nimport numpy as np\nfrom talib import stream\n# frame_rate2 = frame_rate *timestep\nmilistep = 1000 * timestep\nma_step = 10  # one second of buffer size. or more. timeperiod=ma_step\nstd_arr, maxval_arr, abs_nonzero_arr = [], [], []\ndef getPaddingMovingAverage(myarray, timeperiod=10):\n    lt = math.ceil(timeperiod / 2)\n    rt = timeperiod - lt\n    len_myarray = len(myarray)\n    max_index = len_myarray - 1\n    result_array = []\n    for i in range(len_myarray):\n        start_index = i - lt",
        "type": "code",
        "location": "/tests/audio_volume_meter/test_volume_meter.py:1-37"
    },
    "1993": {
        "file_id": 196,
        "content": "Code imports PyDub, sets timestep and frame rate variables from audio file duration and frame rate. Imports math, numpy and talib.stream. Defines function getPaddingMovingAverage to calculate moving average with padding, taking an array and time period as parameters. Initializes std_arr, maxval_arr and abs_nonzero_arr lists for further calculations.",
        "type": "comment"
    },
    "1994": {
        "file_id": 196,
        "content": "        start_index = max(0, start_index)\n        end_index = i + rt\n        end_index = min(end_index, max_index)\n        array_slice = myarray[start_index:end_index]\n        arr_slice_length = end_index - start_index\n        val = sum(array_slice) / arr_slice_length\n        # val = np.median(array_slice)\n        result_array.append(val)\n    return result_array\nmsteps = math.ceil(seconds / timestep)\nfor i in range(msteps):\n    # print(frame_rate2)\n    # probably in miliseconds.\n    segment = audiofile[i * milistep : (i + 1) * milistep]\n    data = segment.get_array_of_samples()\n    # containes two channels. 4410*2\n    darray = np.array(data)\n    print(darray.shape)\n    std = np.std(darray)\n    abs_darray = abs(darray)\n    maxval = np.max(abs_darray)\n    abs_nonzero = np.average(abs_darray)\n    print(\"STD:{} MAX:{} AVG:{}\".format(std, maxval, abs_nonzero))\n    std_arr.append(std)\n    # ma_std = stream.SMA(np.array(std_arr[-ma_step:]).astype(np.float64))\n    maxval_arr.append(maxval)\n    # ma_maxval = stream.SMA(np.array(maxval_arr[-ma_step:]).astype(np.float64))",
        "type": "code",
        "location": "/tests/audio_volume_meter/test_volume_meter.py:38-67"
    },
    "1995": {
        "file_id": 196,
        "content": "This code calculates the standard deviation, maximum value, and average of absolute values for a given audio segment. It appends the calculated values to respective lists and potentially calculates moving averages. The code utilizes numpy functions for array processing and the SMA function from the stream module (possibly) for calculating moving averages.",
        "type": "comment"
    },
    "1996": {
        "file_id": 196,
        "content": "    abs_nonzero_arr.append(abs_nonzero)\n    # ma_abs_nonzero = stream.SMA(np.array(abs_nonzero_arr[-ma_step:]).astype(np.float64))\n    # breakpoint()\n    # print(\"MA_STD:{} MA_MAX:{} MA_AVG:{}\".format(ma_std,ma_maxval,ma_abs_nonzero))\n    # print(data)\n    # breakpoint()\n    # maxAudioValue =audioop.max(data,2)\n    # print(\"STEP:\",i,\"VOLUME:\",maxAudioValue)\nstd_arr0 = getPaddingMovingAverage(std_arr, timeperiod=20)\nmaxval_arr0 = getPaddingMovingAverage(maxval_arr, timeperiod=20)\nabs_nonzero_arr0 = getPaddingMovingAverage(abs_nonzero_arr, timeperiod=20)\nma_std_arr = getPaddingMovingAverage(std_arr, timeperiod=60)\nma_maxval_arr = getPaddingMovingAverage(maxval_arr, timeperiod=60)\nma_abs_nonzero_arr = getPaddingMovingAverage(abs_nonzero_arr, timeperiod=60)\n# just use one freaking example as my conclusion.\nstatus = \"end\"\nvocal_slices = []\nvocal_slice = []\nfinal_index = msteps - 1\n# could you use clustering.\n# like time versus duration.\navg_std = []\nfor i in range(msteps):\n    a, b, c = std_arr0[i], maxval_arr0[i], abs_nonzero_arr0[i]",
        "type": "code",
        "location": "/tests/audio_volume_meter/test_volume_meter.py:68-92"
    },
    "1997": {
        "file_id": 196,
        "content": "This code calculates the moving average for various audio parameters (std_arr, maxval_arr, and abs_nonzero_arr) over different time periods. It then generates a vocal slice based on these moving averages for each step in the range of msteps. The final index is set to be one less than the total number of steps, and an average list is created.",
        "type": "comment"
    },
    "1998": {
        "file_id": 196,
        "content": "    a0, b0, c0 = ma_std_arr[i], ma_maxval_arr[i], ma_abs_nonzero_arr[i]\n    if status == \"end\":\n        # startpoint = a0 < a\n        startpoint = a0 < a or b0 < b or c0 < c\n        if startpoint:\n            vocal_slice.append(i)\n            avg_std.append(a)\n            status = \"start\"\n    else:\n        avg_std.append(a)\n        # endpoint = a0 > a\n        endpoint = a0 > a and b0 > b and c0 > c\n        if endpoint:\n            vocal_slice.append(i)\n            # vocal_slice[1] = i\n            status = \"end\"\n            vocal_slices.append([vocal_slice, np.average(avg_std)])\n            vocal_slice = []\n            avg_std = []\nif len(vocal_slice) == 1:\n    vocal_slice.append(final_index)\n    vocal_slices.append([vocal_slice, np.average(avg_std)])\ntime_rate = timestep\ntimed_vocal_slices = [\n    [[x[0][0] * time_rate, x[0][1] * time_rate], x[1]] for x in vocal_slices\n]\nd2_data = []\nd1_data = []\nfor slice_vocal in timed_vocal_slices:\n    print(slice_vocal)  # it could be two dimentional. both for length and volume?",
        "type": "code",
        "location": "/tests/audio_volume_meter/test_volume_meter.py:93-123"
    },
    "1999": {
        "file_id": 196,
        "content": "The code is iterating through an array of data and dividing it into segments based on threshold values for average, maximum, and absolute non-zero values. These segments are classified as either \"start\" or \"end\", and the indices of the start and end points are stored in separate lists. If a segment only has one point, it is added to the list of vocal slices along with the average of the threshold values. The code then calculates the time rate and creates two-dimensional lists of timed vocal slices (segment start and end times), and data for d1 and d2. Finally, the code prints the timed vocal slices, which could be in a two-dimensional format representing length and volume.",
        "type": "comment"
    }
}