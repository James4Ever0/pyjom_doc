{
    "1900": {
        "file_id": 186,
        "content": "from test_commons import *\nfrom pyjom.commons import *\nimport cv2\nimport numpy as np\ndef getBlackPicture(width, height):\n    blackPicture = np.zeros((height, width, 3), dtype=\"uint8\")  # this is grayscale.\n    return blackPicture\nblackPicture = getBlackPicture(500, 500)\ncv2.rectangle(blackPicture, (200, 200), (300, 300), (255, 255, 255), 3)\ncv2.imshow(\"image\", blackPicture)\ncv2.waitKey(0)",
        "type": "code",
        "location": "/tests/unittest_cv2_rectangle.py:1-16"
    },
    "1901": {
        "file_id": 186,
        "content": "This code imports necessary libraries, defines a function to create a black image of given dimensions, creates a black image, draws a rectangle on it with white color, displays the image, and waits for any key press before exiting.",
        "type": "comment"
    },
    "1902": {
        "file_id": 187,
        "content": "/tests/unittest_ffmpeg_delogo_parser.py",
        "type": "filepath"
    },
    "1903": {
        "file_id": 187,
        "content": "This code defines a delogoParser function to parse command strings and processes video streams using the \"delogo\" filter. It checks parameter validity, removes logos from videos, and handles errors for debugging purposes.",
        "type": "summary"
    },
    "1904": {
        "file_id": 187,
        "content": "import parse\nfrom pyjom.videotoolbox import getVideoWidthHeight\nfrom test_commons import *\nimport ffmpeg\ncommandString = \"delogo_0_671_360_6|delogo_144_662_6_4|delogo_355_661_5_7|delogo_117_661_7_5|delogo_68_661_18_5|delogo_182_658_165_9|delogo_252_492_3_1|delogo_214_492_1_2|delogo_200_492_3_1|delogo_74_492_2_1|delogo_170_490_6_4|delogo_145_490_9_4|delogo_129_490_12_4|delogo_107_490_4_3|delogo_91_487_8_6|delogo_72_485_4_3|delogo_147_484_4_3|delogo_178_483_11_11|delogo_219_480_1_1|delogo_53_480_6_2|delogo_268_478_1_1|delogo_164_478_8_4|delogo_128_477_8_4|delogo_295_475_1_1|delogo_105_475_10_4|delogo_61_474_5_4|delogo_274_472_3_2|delogo_196_470_5_2|delogo_209_469_1_1|delogo_143_469_8_5|delogo_75_467_26_6|delogo_0_33_360_25|delogo_0_24_360_6\"\nvideoPath = \"/root/Desktop/works/pyjom/samples/video/LkS8UkiLL.mp4\"\noutputPath = \"/dev/shm/output.mp4\"\ndef delogoParser(command):\n    return parse.parse(\"delogo_{x:d}_{y:d}_{w:d}_{h:d}\", command)\nwidth, height = getVideoWidthHeight(videoPath)\ndef delogoFilter(stream, commandParams):",
        "type": "code",
        "location": "/tests/unittest_ffmpeg_delogo_parser.py:1-20"
    },
    "1905": {
        "file_id": 187,
        "content": "This code defines a delogoParser function that parses a command string into a formatted format using regular expression. It also includes the getVideoWidthHeight function to retrieve the video width and height from a given path, and a delogoFilter function to process video streams with a given command parameter. The commandString contains a list of delogo positions, and the script will process a video at the specified output path.",
        "type": "comment"
    },
    "1906": {
        "file_id": 187,
        "content": "    return stream.filter(\n        \"delogo\",\n        x=commandParams[\"x\"],\n        y=commandParams[\"y\"],\n        w=commandParams[\"w\"],\n        h=commandParams[\"h\"],\n    )\n# minArea = 20\ndef checkXYWH(XYWH, canvas, minArea=20):\n    x, y, w, h = XYWH\n    width, height = canvas\n    if x >= width - 1 or y >= height - 1:\n        return False, None\n    if x == 0:\n        x = 1\n    if y == 0:\n        y = 1\n    if x + w >= width:\n        w = width - x - 1\n        if w <= 2:\n            return False, None\n    if y + h >= height:\n        h = height - y - 1\n        if h <= 2:\n            return False, None\n    if w * h <= minArea:\n        return False, None\n    return True, (x, y, w, h)\nfor command in commandString.split(\"|\"):\n    try:\n        stream = ffmpeg.input(videoPath, ss=0, to=5).video\n        commandArguments = delogoParser(command)\n        x = commandArguments[\"x\"]\n        y = commandArguments[\"y\"]\n        w = commandArguments[\"w\"]\n        h = commandArguments[\"h\"]\n        status, XYWH = checkXYWH((x, y, w, h), (width, height))",
        "type": "code",
        "location": "/tests/unittest_ffmpeg_delogo_parser.py:21-64"
    },
    "1907": {
        "file_id": 187,
        "content": "This code snippet is a part of a larger program that involves video editing using the FFmpeg library. It filters a video stream with the \"delogo\" filter, taking command parameters for x, y, w, and h. Then, it checks if these parameters are valid by calling the checkXYWH function, which returns True or False depending on the input's validity. Finally, it loops through each command in the commandString, splitting them into smaller commands for video editing operations.",
        "type": "comment"
    },
    "1908": {
        "file_id": 187,
        "content": "        if not status:\n            continue\n        x, y, w, h = XYWH\n        commandArguments = {\"x\": x, \"y\": y, \"w\": w, \"h\": h}\n        stream = delogoFilter(stream, commandArguments)\n        ffmpeg.output(stream, outputPath).run(overwrite_output=True)\n    except:\n        import traceback\n        traceback.print_exc()\n        print(\"WIDTH:\", width, \"HEIGHT:\", height)\n        maxX, maxY = (\n            commandArguments[\"x\"] + commandArguments[\"w\"],\n            commandArguments[\"y\"] + commandArguments[\"h\"],\n        )\n        print(\"MAX X:\", maxX, \"MAX Y:\", maxY)\n        print(\"ERROR!\", commandArguments)\n        breakpoint()",
        "type": "code",
        "location": "/tests/unittest_ffmpeg_delogo_parser.py:65-83"
    },
    "1909": {
        "file_id": 187,
        "content": "This code is implementing a delogo filter, which takes input video and removes the logo from it. It checks if the filter was successfully applied, then extracts the position and dimensions of the logo to apply the delogo filter. If any error occurs during this process, it prints out information for debugging and stops the execution.",
        "type": "comment"
    },
    "1910": {
        "file_id": 188,
        "content": "/tests/unittest_extract_cat_cover_from_video.py",
        "type": "filepath"
    },
    "1911": {
        "file_id": 188,
        "content": "This code downloads Bilibili videos, extracts covers for pet videos, and checks frames to display the cover. It uses yt_dlp, image processing libraries, and OpenCV's imshow function. If a clear frame is found, it breaks the loop and waits for a key press before proceeding.",
        "type": "summary"
    },
    "1912": {
        "file_id": 188,
        "content": "videoLink = \"https://www.bilibili.com/video/BV1Cb4y1s7em\"  # this is a dog.\n# videoLink = \"https://www.bilibili.com/video/BV1Lx411B7X6\"  # multipart download\n# from lazero.filesystem.temp import tmpfile\nimport yt_dlp\n# import pyidm\npath = \"/dev/shm/testVideo.mp4\"\nfrom test_commons import *\nfrom lazero.utils.importers import cv2_custom_build_init\ncv2_custom_build_init()\nimport cv2\nfrom pyjom.videotoolbox import getVideoFrameSampler\nfrom pyjom.imagetoolbox import imageDogCatCoverCropAdvanced\n# from pyjom.imagetoolbox import (\n#     bezierPaddleHubResnet50ImageDogCatDetector,\n#     # we deprecate this thing to make it somehow better.\n#     getImageTextAreaRatio,\n#     imageFourCornersInpainting,\n#     imageCropoutBlackArea,\n#     imageCropoutBlurArea,\n#     imageDogCatDetectionForCoverExtraction,\n#     imageLoader,\n# )\nfrom pyjom.commons import checkMinMaxDict\nimport os\n# with tmpfile(path=path, replace=True) as TF:\nif os.path.exists(path):\n    os.remove(path)\nx = yt_dlp.YoutubeDL(\n    {\n        \"outtmpl\": path,  # seems only video p1 is downloaded.",
        "type": "code",
        "location": "/tests/unittest_extract_cat_cover_from_video.py:1-42"
    },
    "1913": {
        "file_id": 188,
        "content": "This code is downloading a video from Bilibili using yt_dlp library and saving it to the path \"/dev/shm/testVideo.mp4\". It also imports various libraries for image processing and video analysis. The commented out section suggests an alternative download method, possibly for multipart downloads.",
        "type": "comment"
    },
    "1914": {
        "file_id": 188,
        "content": "    }\n)\ny = x.download([videoLink])\n# shall you use frame sampler instead of iterator? cause this is dumb.\n# breakpoint()\nfrom pyjom.videotoolbox import corruptVideoFilter\nvideo_fine = corruptVideoFilter(path)\nif not video_fine:\n    print(\"VIDEO FILE CORRUPTED\")\n    exit()\nfrom caer.video.frames_and_fps import get_duration\nduration = get_duration(path)\nmSampleSize = int(duration / 2)  # fps = 0.5 or something?\nprocessed_frame = None\ndog_or_cat = \"dog\"\nfor frame in getVideoFrameSampler(path, -1, -1, sample_size=mSampleSize, iterate=True):\n    # animalCropDiagonalRect = imageDogCatDetectionForCoverExtraction(\n    #     frame,\n    #     dog_or_cat=dog_or_cat,\n    #     confidence_threshold=confidence_threshold,\n    #     crop=False,\n    # )  # you must use gpu this time.\n    # if animalCropDiagonalRect is not None:  # of course this is not None.\n    # we need to identify this shit.\n    # if checkMinMaxDict(text_area_ratio, text_area_threshold):\n    processed_frame = imageDogCatCoverCropAdvanced(frame, dog_or_cat=dog_or_cat)",
        "type": "code",
        "location": "/tests/unittest_extract_cat_cover_from_video.py:43-74"
    },
    "1915": {
        "file_id": 188,
        "content": "This code downloads a video file, checks for corruption, calculates the duration, sets the sample size based on duration, iterates through video frames, and applies an image processing algorithm to extract a cover for either dog or cat videos. The code might benefit from using a frame sampler instead of an iterator as the current implementation is considered inefficient.",
        "type": "comment"
    },
    "1916": {
        "file_id": 188,
        "content": "    if processed_frame is not None:\n        # blurValue = imageCropoutBlurArea(processed_frame, value=True)\n        # print(\"BLUR VALUE:\", blurValue)\n        # if not checkMinMaxDict(blurValue, blurValue_threshold):\n        #     # will skip this one since it is not so clear.\n        #     continue\n        break\nif processed_frame is not None:\n    print(\"COVER IMAGE FOUND!\")\n    processed_frame_show = cv2.resize(processed_frame, (int(1920 / 2), int(1080 / 2)))\n    cv2.imshow(\"image\", processed_frame_show)\n    cv2.waitKey(0)\nelse:\n    print(\"COVER NOT FOUND FOR %s\" % videoLink)",
        "type": "code",
        "location": "/tests/unittest_extract_cat_cover_from_video.py:75-88"
    },
    "1917": {
        "file_id": 188,
        "content": "This code checks for a clear frame in a video and if found, displays it; otherwise, it indicates that the cover was not found. If a clear frame is detected (processed_frame), it will break out of the loop. The processed frame is resized and displayed using OpenCV's imshow function, then the program waits for any key press before proceeding.",
        "type": "comment"
    },
    "1918": {
        "file_id": 189,
        "content": "/tests/unittest_ffmpeg_cropdetect_from_image_parse_log.py",
        "type": "filepath"
    },
    "1919": {
        "file_id": 189,
        "content": "This code uses ffmpeg and OpenCV to detect cropped areas, calculates the cropped area ratio, and decides whether to crop the image based on a threshold. The result depends on the specified threshold value.",
        "type": "summary"
    },
    "1920": {
        "file_id": 189,
        "content": "import ffmpeg\nfrom lazero.utils.importers import cv2_custom_build_init\ncv2_custom_build_init()\n# mediaPath = \"/root/Desktop/works/pyjom/samples/image/dog_blue_sky_split_line.png\"\nmediaPath = \"/root/Desktop/works/pyjom/samples/image/dog_with_black_borders.png\"  # use the image with black background.\n# ffmpeg -loop 1 -i /root/Desktop/works/pyjom/samples/image/dog_blue_sky_split_line.png -t 15 -vf cropdetect -f null -\nimport cv2\nimage = cv2.imread(mediaPath)\nheight, width = image.shape[:2]\ntotal_area = height * width\nareaThreshold = 0\nstdout, stderr = (\n    ffmpeg.input(mediaPath, loop=1, t=15)\n    .filter(\"cropdetect\")\n    .output(\"null\", f=\"null\")\n    .run(capture_stdout=True, capture_stderr=True)\n)\nstdout_decoded = stdout.decode(\"utf-8\")\nstderr_decoded = stderr.decode(\"utf-8\")\n# nothing here.\n# for line in stdout_decoded.split(\"\\n\"):\n#     print(line)\n# breakpoint()\nimport parse\ncropped_area_threshold = 0.1\ncommon_crops = []\nfor line in stderr_decoded.split(\"\\n\"):\n    line = line.replace(\"\\n\", \"\").strip()\n    for",
        "type": "code",
        "location": "/tests/unittest_ffmpeg_cropdetect_from_image_parse_log.py:1-40"
    },
    "1921": {
        "file_id": 189,
        "content": "The code imports necessary libraries and initializes them, sets the media path to an image with a black background, runs ffmpeg on the image with a cropdetect filter, decodes the output and errors, iterates over the stderr output lines to extract cropped areas, and defines a variable for common_crops.",
        "type": "comment"
    },
    "1922": {
        "file_id": 189,
        "content": "matString = \"[{}] x1:{x1:d} x2:{x2:d} y1:{y1:d} y2:{y2:d} w:{w:d} h:{h:d} x:{x:d} y:{y:d} pts:{pts:g} t:{t:g} crop={}:{}:{}:{}\"\n    # print(line)\n    result = parse.parse(formatString, line)\n    if result is not None:\n        # print(result)\n        cropString = \"{}_{}_{}_{}\".format(\n            *[result[key] for key in [\"w\", \"h\", \"x\", \"y\"]]\n        )\n        # print(cropString)\n        # breakpoint()\n        common_crops.append(cropString)\n    # [Parsed_cropdetect_0 @ 0x56246a16cbc0] x1:360 x2:823 y1:0 y2:657 w:464 h:656 x:360 y:2 pts:3 t:0.120000 crop=464:656:360:2\n    # this crop usually will never change. but let's count?\narea = 0\nx, x1, y, y1 = 0, width, 0, height\nif len(common_crops) > 0:\n    common_crops_count_tuple_list = [\n        (cropString, common_crops.count(cropString)) for cropString in set(common_crops)\n    ]\n    common_crops_count_tuple_list.sort(key=lambda x: -x[1])\n    selected_crop_string = common_crops_count_tuple_list[0][0]\n    result = parse.parse(\"{w:d}_{h:d}_{x:d}_{y:d}\", selected_crop_string)",
        "type": "code",
        "location": "/tests/unittest_ffmpeg_cropdetect_from_image_parse_log.py:40-62"
    },
    "1923": {
        "file_id": 189,
        "content": "Code parses a log line, extracts crop information and stores it in common_crops list. It then counts the occurrence of each unique crop string and selects the most frequent one (selected_crop_string). Finally, it parses the selected_crop_string to get the crop dimensions (w, h, x, y) and assigns them to their respective variables.",
        "type": "comment"
    },
    "1924": {
        "file_id": 189,
        "content": "    w, h, x, y = [result[key] for key in [\"w\", \"h\", \"x\", \"y\"]]\n    x1, y1 = min(x + w, width), min(y + h, height)\n    if x < x1 and y < y1:\n        # allow to calculate the area.\n        area = (x1 - x) * (y1 - y)\ncropped_area_ratio = 1 - (area / total_area)  # 0.5652352766414517\n# use 0.1 as threshold?\nprint(\"CROPPED AREA RATIO:\", cropped_area_ratio)\nif cropped_area_ratio > cropped_area_threshold:\n    print(\"we need to crop this. no further processing needed\")\n    image_black_cropped = image[y:y1, x:x1]\n    cv2.imshow(\"CROPPED IMAGE\", image_black_cropped)\n    cv2.waitKey(0)\nelse:\n    print(\"image no need to crop black borders. further processing needed\")",
        "type": "code",
        "location": "/tests/unittest_ffmpeg_cropdetect_from_image_parse_log.py:63-78"
    },
    "1925": {
        "file_id": 189,
        "content": "This code calculates the cropped area ratio of an image and decides whether to crop it or not based on a threshold. If the ratio is greater than the threshold, it crops the image using OpenCV and displays the cropped image. Otherwise, it proceeds with further processing. The result depends on the specified threshold value.",
        "type": "comment"
    },
    "1926": {
        "file_id": 190,
        "content": "/tests/unittest_check_video_corrput.py",
        "type": "filepath"
    },
    "1927": {
        "file_id": 190,
        "content": "The code tests a video file for corruption by using ffmpeg to input the video, output it to null format, and then checks if any error or failure messages appear in the stderr. If such messages are found, the video is considered corrupted.",
        "type": "summary"
    },
    "1928": {
        "file_id": 190,
        "content": "import ffmpeg\nnot_nice = [\"invalid\", \"failed\", \"error\"]\nvideoPath = \"/root/Desktop/works/pyjom/samples/video/dog_with_large_text.gif\"\n# videoPath = \"/root/Desktop/works/pyjom/samples/video/cute_cat_gif.gif\"\n# videoPath = \"/root/Desktop/works/pyjom/samples/video/corrupt_video.gif\"\ncorrupted = False\ntry:\n    stdout, stderr = (\n        ffmpeg.input(videoPath)\n        .output(\"null\", f=\"null\")\n        .run(capture_stdout=True, capture_stderr=True)\n    )\n    stderr_lower = stderr.decode(\"utf-8\").lower()\n    for word in not_nice:\n        if word in stderr_lower:\n            print(\"video is corrupted\")\n            corrupted = True\n            break\nexcept:\n    import traceback\n    traceback.print_exc()\n    corrupted = True\n    print(\"corrupt video\")\nif not corrupted:\n    print(\"video is fine\")",
        "type": "code",
        "location": "/tests/unittest_check_video_corrput.py:1-28"
    },
    "1929": {
        "file_id": 190,
        "content": "The code tests a video file for corruption by using ffmpeg to input the video, output it to null format, and then checks if any error or failure messages appear in the stderr. If such messages are found, the video is considered corrupted.",
        "type": "comment"
    },
    "1930": {
        "file_id": 191,
        "content": "/tests/unittest_clean_lrc.py",
        "type": "filepath"
    },
    "1931": {
        "file_id": 191,
        "content": "The code checks lyrics' adherence to line requirements, processes a list of lyrics, extracts flags, and formats the lyrics into an LRC string.",
        "type": "summary"
    },
    "1932": {
        "file_id": 191,
        "content": "lyric_string = \"\"\"[00:00.000] 作词 : 苏喜多/挡风玻璃\\n[00:01.000] 作曲 : 苏喜多/陈恒冠\\n[00:02.000] 编曲 : 陈恒冠/陈恒家\\n[00:31.154]你戴上帽子遮住眼睛 轻轻地绕着我 总洋溢着暖\\n[00:44.404]我…我只能唱\\n[00:54.902]你像气泡水直接淘气 爱和星星眨眼睛 轻易抓住我\\n[01:07.903]我…我只能唱\\n[01:16.651]有一个岛屿 在北极冰川\\n[01:23.403]那儿没有花朵 也没有失落\\n[01:30.159]在那个岛屿 洒满了繁星\\n[01:36.904]拥有我和你 再没有失落\\n[02:15.903]你邀请流浪期待欢喜 惹我专心好奇 我看见了光\\n[02:29.153]我…我只能唱\\n[02:39.403]难免坏天气闪电暴雨 练就肩膀和勇气 只为你拥抱我\\n[02:54.156]我…我只能唱\\n[03:01.659]有一个岛屿 在北极冰川\\n[03:08.154]那儿没有花朵 也没有失落\\n[03:14.906]在那个岛屿 洒满了繁星\\n[03:21.651]拥有我和你 再没有失落\\n[03:28.656]有一个岛屿 在北极冰川\\n[03:35.152]\n那儿没有花朵 也没有失落\\n[03:41.904]在那个岛屿 洒满了繁星\\n[03:48.661]拥有我和你 再没有失落\\n[03:59.159]有一个岛屿 在北极冰川\\n[04:05.654]那儿没有花朵 也没有失落\\n[04:12.659]在那个岛屿 洒满了繁星\\n[04:19.152]拥有我和你 再没有失落\\n[04:26.405]有一个岛屿\n在北极冰川\\n[04:33.658]那儿没有花朵 也没有失落…\\n[04:40.401]吉他：陈恒家\\n[04:42.654]钢琴：陈恒冠\\n[04:47.407]混音：陈恒家\\n[04:49.907]母带：陈恒家\\n[04:53.907]监制：1991与她\\n\"\"\"\n# assume song duration here!\nsong_duration = 5 * 60\nimport pylrc\n# you'd better inspect the thing. what is really special about the lyric, which can never appear?",
        "type": "code",
        "location": "/tests/unittest_clean_lrc.py:1-10"
    },
    "1933": {
        "file_id": 191,
        "content": "Lyric string contains time-stamped song lyrics and metadata, assumed song duration is set to 5 minutes, and pylrc module is imported.",
        "type": "comment"
    },
    "1934": {
        "file_id": 191,
        "content": "min_lines_of_lyrics = 5\nmin_total_lines_of_lyrics = 10\npotential_forbidden_chars = [\"[\", \"]\", \"【\", \"】\", \"「\", \"」\", \"《\", \"》\", \"/\", \"(\", \")\"]\ncore_forbidden_chars = [\":\", \"：\", \"@\"]\ndef checkLyricText(text, core_only=False):\n    if core_only:\n        forbidden_chars = core_forbidden_chars\n    else:\n        forbidden_chars = core_forbidden_chars + potential_forbidden_chars\n    return not any([char in text for char in forbidden_chars])\n# also get the total time covered by lyric.\n# the time must be long enough, compared to the total time of the song.\nlrc_parsed = pylrc.parse(lyric_string)\nlrc_parsed_list = [line for line in lrc_parsed]\nlrc_parsed_list.sort(key=lambda line: line.time)\nbegin = False\n# end = False\nline_counter = 0\nnew_lines = []\n# lrc_parsed: pylrc.classes.Lyrics\nflags = []\nfor line in lrc_parsed_list:\n    # print(line)\n    text = line.text.strip()\n    startTime = line.time\n    if not begin:\n        flag = checkLyricText(text, core_only=False)\n        if not flag:\n            begin = True\n    else:\n        flag = checkLyricText(text, core_only=True)",
        "type": "code",
        "location": "/tests/unittest_clean_lrc.py:12-46"
    },
    "1935": {
        "file_id": 191,
        "content": "This code checks if a Lyrics object from pylrc meets certain criteria. It defines minimum line requirements for lyrics and forbidden characters. The function checkLyricText() determines whether a line contains any forbidden characters. The code then processes the lrc_parsed list to get the total time covered by lyrics, ensuring it's long enough compared to the song's total time. It creates new_lines with valid lines and flags for each line based on the presence of forbidden characters.",
        "type": "comment"
    },
    "1936": {
        "file_id": 191,
        "content": "        if flag:\n            begin = False\n    flags.append(flag)\n    # breakpoint()\n# select consecutive spans.\nfrom test_commons import *\nfrom pyjom.mathlib import extract_span\nint_flags = [int(flag) for flag in flags]\nmySpans = extract_span(int_flags, target=1)\nprint(mySpans)  # this will work.\n# this span is for the range function. no need to add one to the end.\ntotal_length = 0\nnew_lyric_list = []\nfor mstart, mend in mySpans:\n    length = mend - mstart\n    total_length += length\n    if length >= min_lines_of_lyrics:\n        # process these lines.\n        for index in range(mstart, mend):\n            line_start_time = lrc_parsed_list[index].time\n            line_text = lrc_parsed_list[index].text\n            if line_start_time <= song_duration:\n                line_end_time = song_duration\n                if index + 1 < len(lrc_parsed_list):\n                    line_end_time = lrc_parsed_list[index + 1].time\n                    if line_end_time > song_duration:\n                        line_end_time = song_duration",
        "type": "code",
        "location": "/tests/unittest_clean_lrc.py:47-78"
    },
    "1937": {
        "file_id": 191,
        "content": "Checks if a flag is set, appends it to the flags list. Filters and extracts consecutive spans from the flags list. Calculates total length of spans. Iterates over the spans, retrieves line start time and text from lrc_parsed_list, checks if line end time is within song duration.",
        "type": "comment"
    },
    "1938": {
        "file_id": 191,
        "content": "                new_lyric_list.append((line_text, line_start_time))\n                if index == mend - 1:\n                    # append one more thing.\n                    new_lyric_list.append((\"\", line_end_time))\n            else:\n                continue\n# for elem in new_lyric_list:\n#     print(elem)\n# exit()\nif total_length >= min_total_lines_of_lyrics:\n    print(\"LYRIC ACCEPTED.\")\n    new_lrc = pylrc.classes.Lyrics()\n    for text, myTime in new_lyric_list:\n        timecode_min, timecode_sec = divmod(myTime, 60)\n        timecode = \"[{:d}:{:.3f}]\".format(int(timecode_min), timecode_sec)\n        myLine = pylrc.classes.LyricLine(timecode, text)\n        new_lrc.append(myLine)\n    new_lrc_string = new_lrc.toLRC()\n    print(new_lrc_string)",
        "type": "code",
        "location": "/tests/unittest_clean_lrc.py:79-98"
    },
    "1939": {
        "file_id": 191,
        "content": "The code processes a list of lyrics and checks if it meets the minimum requirements for length. If so, it formats the lyrics into an LRC string and prints it.",
        "type": "comment"
    },
    "1940": {
        "file_id": 192,
        "content": "/tests/unittest_ffmpeg_args.py",
        "type": "filepath"
    },
    "1941": {
        "file_id": 192,
        "content": "The code processes video files using FFmpeg for tasks like cropping and scaling, with a specific command to map and filter video/audio streams. This is part of a larger script that uses the subprocess module.",
        "type": "summary"
    },
    "1942": {
        "file_id": 192,
        "content": "command_original = [\n    \"ffmpeg\",\n    \"-y\",\n    \"-ss\",\n    \"0\",\n    \"-to\",\n    \"59.3942553191489\",\n    \"-i\",\n    \"/root/Desktop/works/pyjom/samples/video/LiGlReJ4i.mp4\",\n    \"-ss\",\n    \"59.3942553191489\",\n    \"-to\",\n    \"62.0340000000000\",\n    \"-i\",\n    \"/root/Desktop/works/pyjom/samples/video/LiGlReJ4i.mp4\",\n    \"-ss\",\n    \"0\",\n    \"-to\",\n    \"62.034\",\n    \"-i\",\n    \"/root/Desktop/works/pyjom/samples/video/LiGlReJ4i.mp4\",\n    \"-filter_complex\",\n    \"[0:v]crop=h=1099:w=717:x=1:y=72[s0];[s0]pad=color=black:height=max(ih\\\\, ceil(iw*max(1080/1920\\\\, ih/iw))):width=max(iw\\\\, ceil(ih*max(1920/1080\\\\, iw/ih))):x=floor((max(iw\\\\, ceil(ih*max(1920/1080\\\\, iw/ih)))-iw)/2):y=floor((max(ih\\\\, ceil(iw*max(1080/1920\\\\, ih/iw)))-ih)/2)[s1];[s1]scale=1920:1080[s2];[s2]scale=ceil((iw*0.15555555555555556)/4)*4:ceil((ih*0.15555555555555556)/4)*4[s3];[1:v]pad=color=black:height=max(ih\\\\, ceil(iw*max(1080/1920\\\\, ih/iw))):width=max(iw\\\\, ceil(ih*max(1920/1080\\\\, iw/ih))):x=floor((max(iw\\\\, ceil(ih*max(1920/1080\\\\, iw/ih)))-iw",
        "type": "code",
        "location": "/tests/unittest_ffmpeg_args.py:1-23"
    },
    "1943": {
        "file_id": 192,
        "content": "This code uses FFmpeg to split a video file into segments, applies various filters and transformations to the segments, and finally scales and pads them before saving the final output.",
        "type": "comment"
    },
    "1944": {
        "file_id": 192,
        "content": ")/2):y=floor((max(ih\\\\, ceil(iw*max(1080/1920\\\\, ih/iw)))-ih)/2)[s4];[s4]scale=1920:1080[s5];[s5]scale=ceil((iw*0.15555555555555556)/4)*4:ceil((ih*0.15555555555555556)/4)*4[s6];[s3][s6]concat=n=2[s7]\",\n    \"-map\",\n    \"[s7]\",\n    \"-map\",\n    \"2:a\",\n    \"/dev/shm/2c6b1466-6186-41dd-9ce3-2f757c082c5a.mp4\",\n]\ncommand2 = [\n    \"ffmpeg\",\n    \"-y\",\n    \"-ss\",\n    \"0\",\n    \"-to\",\n    \"59.3942553191489\",\n    \"-i\",\n    \"/root/Desktop/works/pyjom/samples/video/LiGlReJ4i.mp4\",\n    \"-ss\",\n    \"0\",\n    \"-to\",\n    \"62.034\",\n    \"-i\",\n    \"/root/Desktop/works/pyjom/samples/video/LiGlReJ4i.mp4\",\n    \"-filter_complex\",\n    \"[0:v]crop=h=1099:w=717:x=1:y=72[s0];[s0]pad=color=black:height=max(ih\\\\, ceil(iw*max(1080/1920\\\\, ih/iw))):width=max(iw\\\\, ceil(ih*max(1920/1080\\\\, iw/ih))):x=floor((max(iw\\\\, ceil(ih*max(1920/1080\\\\, iw/ih)))-iw)/2):y=floor((max(ih\\\\, ceil(iw*max(1080/1920\\\\, ih/iw)))-ih)/2)[s1];[s1]scale=1920:1080[s2];[s2]scale=ceil((iw*0.15555555555555556)/4)*4:ceil((ih*0.15555555555555556)/4)*4[s3]\",\n    \"-map\",\n    \"[s3]\",",
        "type": "code",
        "location": "/tests/unittest_ffmpeg_args.py:23-48"
    },
    "1945": {
        "file_id": 192,
        "content": "The code is constructing a command for the ffmpeg tool to process and concatenate multiple video inputs. It applies filters such as cropping, padding, scaling, and extracts specific parts of videos before concatenating them into a single output video file. The resulting command is being stored in `command1` and `command2`.",
        "type": "comment"
    },
    "1946": {
        "file_id": 192,
        "content": "    \"-map\",\n    \"1:a\",\n    \"/dev/shm/2c6b1466-6186-41dd-9ce3-2f757c082c5a.mp4\",\n]\ncommand3 = [\n    \"ffmpeg\",\n    \"-y\",\n    \"-ss\",\n    \"59.3942553191489\",\n    \"-to\",\n    \"62.0340000000000\",\n    \"-i\",\n    \"/root/Desktop/works/pyjom/samples/video/LiGlReJ4i.mp4\",\n    \"-ss\",\n    \"0\",\n    \"-to\",\n    \"62.034\",\n    \"-i\",\n    \"/root/Desktop/works/pyjom/samples/video/LiGlReJ4i.mp4\",\n    \"-filter_complex\",\n    \"[0:v]pad=color=black:height=max(ih\\\\, ceil(iw*max(1080/1920\\\\, ih/iw))):width=max(iw\\\\, ceil(ih*max(1920/1080\\\\, iw/ih))):x=floor((max(iw\\\\, ceil(ih*max(1920/1080\\\\, iw/ih)))-iw)/2):y=floor((max(ih\\\\, ceil(iw*max(1080/1920\\\\, ih/iw)))-ih)/2)[s4];[s4]scale=1920:1080[s5];[s5]scale=ceil((iw*0.15555555555555556)/4)*4:ceil((ih*0.15555555555555556)/4)*4[s6]\",\n    \"-map\",\n    \"[s6]\",\n    \"-map\",\n    \"1:a\",\n    \"/dev/shm/2c6b1466-6186-41dd-9ce3-2f757c082c5a.mp4\",\n]\ncommandImprovised = command_original = [\n    \"ffmpeg\",\n    \"-y\",\n    \"-ss\",\n    \"0\",\n    \"-to\",\n    \"59.3942553191489\",\n    \"-i\",\n    \"/root/Desktop/works/pyjom/samples/video/LiGlReJ4i.mp4\",",
        "type": "code",
        "location": "/tests/unittest_ffmpeg_args.py:49-84"
    },
    "1947": {
        "file_id": 192,
        "content": "This code is using FFmpeg command line arguments to perform operations on video files. It's mapping streams, applying filters for scaling and padding, setting start/end times, and specifying output file paths. The code is likely involved in video processing or manipulation tasks.",
        "type": "comment"
    },
    "1948": {
        "file_id": 192,
        "content": "    \"-ss\",\n    \"59.3942553191489\",\n    \"-to\",\n    \"62.0340000000000\",\n    \"-i\",\n    \"/root/Desktop/works/pyjom/samples/video/LiGlReJ4i.mp4\",\n    \"-ss\",\n    \"0\",\n    \"-to\",\n    \"62.034\",\n    \"-i\",\n    \"/root/Desktop/works/pyjom/samples/video/LiGlReJ4i.mp4\",\n    \"-filter_complex\",\n    \"[0:v]crop=h=1099:w=717:x=1:y=72[s0];[s0]pad=color=black:height=max(ih\\\\, ceil(iw*max(1080/1920\\\\, ih/iw))):width=max(iw\\\\, ceil(ih*max(1920/1080\\\\, iw/ih))):x=floor((max(iw\\\\, ceil(ih*max(1920/1080\\\\, iw/ih)))-iw)/2):y=floor((max(ih\\\\, ceil(iw*max(1080/1920\\\\, ih/iw)))-ih)/2)[s1];[s1]scale=1920:1080[s2];[s2]scale=ceil((iw*0.15555555555555556)/4)*4:ceil((ih*0.15555555555555556)/4)*4,setsar=1[s3];[1:v]pad=color=black:height=max(ih\\\\, ceil(iw*max(1080/1920\\\\, ih/iw))):width=max(iw\\\\, ceil(ih*max(1920/1080\\\\, iw/ih))):x=floor((max(iw\\\\, ceil(ih*max(1920/1080\\\\, iw/ih)))-iw)/2):y=floor((max(ih\\\\, ceil(iw*max(1080/1920\\\\, ih/iw)))-ih)/2)[s4];[s4]scale=1920:1080[s5];[s5]scale=ceil((iw*0.15555555555555556)/4)*4:ceil((ih*0.15555555555555556)/4)*4,setsar=1[s6];[s3][s6]concat=n=2[s7]\",",
        "type": "code",
        "location": "/tests/unittest_ffmpeg_args.py:85-98"
    },
    "1949": {
        "file_id": 192,
        "content": "This code is using FFmpeg to crop, scale, and concatenate video streams. It first specifies start and end times for the input video file \"/root/Desktop/works/pyjom/samples/video/LiGlReJ4i.mp4\", then applies a series of filters including cropping, padding, scaling, and setting aspect ratio. Finally, it concatenates the resulting streams for output.",
        "type": "comment"
    },
    "1950": {
        "file_id": 192,
        "content": "    \"-map\",\n    \"[s7]\",\n    \"-map\",\n    \"2:a\",\n    \"/dev/shm/2c6b1466-6186-41dd-9ce3-2f757c082c5a.mp4\",\n]\nimport subprocess\nsubprocess.run(commandImprovised)",
        "type": "code",
        "location": "/tests/unittest_ffmpeg_args.py:99-107"
    },
    "1951": {
        "file_id": 192,
        "content": "This code chunk is part of a larger script that uses the subprocess module to run an FFmpeg command. The command maps video stream from input file \"[s7]\" and audio stream from track 2 to output \"/dev/shm/2c6b1466-6186-41dd-9ce3-2f757c082c5a.mp4\".",
        "type": "comment"
    },
    "1952": {
        "file_id": 193,
        "content": "/tests/unittest_full_text_search_peewee_sqlite.py",
        "type": "filepath"
    },
    "1953": {
        "file_id": 193,
        "content": "The code imports modules and sets up a SQLite database for full-text search. It defines a model class, populates the table with data, adds/updates a video, searches \"python world\" using BM25 algorithm, limits results to 2, and prints each result. Debugging breakpoints are included.",
        "type": "summary"
    },
    "1954": {
        "file_id": 193,
        "content": "from peewee import *\nfrom playhouse.sqlite_ext import SqliteExtDatabase, FTSModel, SearchField, RowIDField\ndb_path = \"test_fulltext_search.db\"\ndb = SqliteExtDatabase(\n    db_path, pragmas={\"journal_mode\": \"wal\", \"cache_size\": -1024 * 64}\n)\nclass BilibiliVideoIndex(FTSModel):\n    rowid = RowIDField()  # this does not support\n    title = SearchField()\n    content = SearchField()\n    class Meta:\n        database = None  # that's good.\n        options = {\"tokenize\": \"porter\"}  # you need manually separate some\ndb.create_tables([BilibiliVideoIndex])\nimport uuid\nrandomContent = lambda: str(uuid.uuid4())\nobject, flag = BilibiliVideoIndex.get_and_update_or_create(\n    rowid=1, title=randomContent(), content=randomContent(), _unique_keys=[\"rowid\"]\n)\nBilibiliVideoIndex.get_and_update_or_create(\n    rowid=2,\n    title=\"hello world\",\n    content=\"learn python the hard way\",\n    _unique_keys=[\"rowid\"],\n)\nBilibiliVideoIndex.get_and_update_or_create(\n    rowid=3,\n    title=\"hello world\",\n    content=\"learn python the hard way\",\n    _unique_keys=[\"rowid\"],",
        "type": "code",
        "location": "/tests/unittest_full_text_search_peewee_sqlite.py:1-42"
    },
    "1955": {
        "file_id": 193,
        "content": "This code imports necessary modules and sets up a SQLite database with full-text search capabilities. It defines a model class, BilibiliVideoIndex, and creates its corresponding table in the database. Using the get_and_update_or_create method, it populates the table with data for three records, ensuring uniqueness based on the rowid field.",
        "type": "comment"
    },
    "1956": {
        "file_id": 193,
        "content": ")\nBilibiliVideoIndex.get_and_update_or_create(\n    rowid=4,\n    title=\"hello world\",\n    content=\"learn python the hard way\",\n    _unique_keys=[\"rowid\"],\n)\nprint(object)\nprint(flag)\nprint(object.rowid, object.title, object.content)\n# don't know what magic is inside. whatever.\n# updated. my lord.\n# now search for it.\nterm = \"python world\"\nresults = BilibiliVideoIndex.search_bm25(term).limit(2)  # just how many?\n# breakpoint()\n# it does have the limit.\n# it is ordered.\nfor result in results:\n    print(\"RESULT\", result)\n    breakpoint()",
        "type": "code",
        "location": "/tests/unittest_full_text_search_peewee_sqlite.py:43-68"
    },
    "1957": {
        "file_id": 193,
        "content": "Code adds a video to BilibiliVideoIndex, updates it, and searches for \"python world\" using BM25 algorithm. Limits search results to 2, then prints each result. Breakpoints inserted for debugging.",
        "type": "comment"
    },
    "1958": {
        "file_id": 194,
        "content": "/tests/unittest_update_peewee_while_get.py",
        "type": "filepath"
    },
    "1959": {
        "file_id": 194,
        "content": "Creating a Peewee database, defining a BilibiliUser model, and updating a specific user's username.",
        "type": "summary"
    },
    "1960": {
        "file_id": 194,
        "content": "dbpath = \"test.db\"\nfrom peewee import *\nclass BilibiliUser(Model):\n    username = CharField()\n    user_id = IntegerField(unique=True)\n    is_mine = BooleanField(default=False)\n    followers = IntegerField(\n        null=True\n    )  # how to get that? every time you get some video you do this shit? will get you blocked.\n    # well you can check it later.\n    avatar = CharField(null=True)  # warning! charfield max length is 255\ndb = SqliteDatabase(dbpath)\ndb.create_tables([BilibiliUser])\nimport uuid\nusername = str(uuid.uuid4())\n# u, _ = BilibiliUser.get_and_update_or_create(username=username, user_id=1)\nBilibiliUser.update(username=username).where(BilibiliUser.user_id == 1).execute()\n# why don't you update? need i delete it manually?\nu = BilibiliUser.get(user_id=1)\nprint(\"current username:\", username)\nprint(\"fetched username:\", u.username)",
        "type": "code",
        "location": "/tests/unittest_update_peewee_while_get.py:1-30"
    },
    "1961": {
        "file_id": 194,
        "content": "Creating a Peewee database, defining a BilibiliUser model, and updating a specific user's username.",
        "type": "comment"
    },
    "1962": {
        "file_id": 195,
        "content": "/tests/unittest_paddlehub_animal_resnet.py",
        "type": "filepath"
    },
    "1963": {
        "file_id": 195,
        "content": "This code uses PaddleHub's Animal ResNet model to classify dogs and cats from video frames, post-processing the results to test accuracy with various images including non-animal ones.",
        "type": "summary"
    },
    "1964": {
        "file_id": 195,
        "content": "# source = \"/root/Desktop/works/pyjom/samples/video/kitty_flash_15fps.gif\"  # check that kitty video!\nsource = \"/root/Desktop/works/pyjom/samples/video/cute_cat_gif.gif\"  # another kitty!\nfrom test_commons import *\nfrom pyjom.videotoolbox import getVideoFrameIteratorWithFPS\nfrom pyjom.imagetoolbox import resizeImageWithPadding\nimport paddlehub as hub\nimport cv2\ndef labelFileReader(filename):\n    with open(filename, \"r\") as f:\n        content = f.read()\n        content = content.split(\"\\n\")\n        content = [elem.replace(\"\\n\", \"\").strip() for elem in content]\n        content = [elem for elem in content if len(elem) > 0]\n    return content\ndog_suffixs = [\"狗\", \"犬\", \"梗\"]\ncat_suffixs = [\"猫\"]  # ends with this, and not containing forbidden words.\ndog_labels = labelFileReader(\n    \"/root/Desktop/works/pyjom/tests/animals_paddlehub_classification_resnet/dogs.txt\"\n)\ncat_labels = labelFileReader(\n    \"/root/Desktop/works/pyjom/tests/animals_paddlehub_classification_resnet/cats.txt\"\n)\nforbidden_words = [\n    \"灵猫\",\n    \"熊猫\",",
        "type": "code",
        "location": "/tests/unittest_paddlehub_animal_resnet.py:1-32"
    },
    "1965": {
        "file_id": 195,
        "content": "This code imports necessary libraries and defines constants for labels and forbidden words. It reads the dog and cat labels from separate text files, and later on, it will use these labels to classify animals in a video. The forbidden words are likely used to exclude certain categories of animals that may appear similar to cats or dogs but should not be confused with them.",
        "type": "comment"
    },
    "1966": {
        "file_id": 195,
        "content": "    \"猫狮\",\n    \"猫头鹰\",\n    \"丁丁猫儿\",\n    \"绿猫鸟\",\n    \"猫鼬\",\n    \"猫鱼\",\n    \"玻璃猫\",\n    \"猫眼\",\n    \"猫蛱蝶\",\n]\ndef dog_cat_name_recognizer(name):\n    if name in dog_labels:\n        return \"dog\"\n    elif name in cat_labels:\n        return \"cat\"\n    elif name not in forbidden_words:\n        for dog_suffix in dog_suffixs:\n            if name.endswith(dog_suffix):\n                return \"dog\"\n        for cat_suffix in cat_suffixs:\n            if name.endswith(cat_suffix):\n                return \"cat\"\n    return None\nfrom lazero.utils.logger import sprint\nclassifier = hub.Module(name=\"resnet50_vd_animals\")\n# 'ResNet50vdAnimals' object has no attribute 'gpu_predictor'\n# no gpu? really?\n# test_flag = \"video\"\ntest_flag = \"image\"\ndef paddleAnimalDetectionResultToList(result):\n    resultDict = result[0]\n    resultList = [(key, value) for key, value in resultDict.items()]\n    resultList.sort(key=lambda item: -item[1])\n    return resultList\ndef translateResultListToDogCatList(resultList):\n    final_result_list = []\n    for name, confidence in resultList:",
        "type": "code",
        "location": "/tests/unittest_paddlehub_animal_resnet.py:33-78"
    },
    "1967": {
        "file_id": 195,
        "content": "Function dog_cat_name_recognizer identifies if a given name is of a dog or cat by checking it against pre-defined labels. If the name does not fit into these categories, it further checks for common suffixes to classify as either a dog or cat. The code imports necessary modules and sets up variables for testing purposes before defining functions for post-processing the result and translating it into a dog/cat list format.",
        "type": "comment"
    },
    "1968": {
        "file_id": 195,
        "content": "        new_name = dog_cat_name_recognizer(name)\n        final_result_list.append((new_name, confidence))\n    return final_result_list\nif test_flag == \"video\":\n    for frame in getVideoFrameIteratorWithFPS(source, -1, -1, fps=1):\n        padded_resized_frame = resizeImageWithPadding(\n            frame, 224, 224, border_type=\"replicate\"\n        )  # pass the test only if three of these containing 'cats'\n        result = classifier.classification(\n            images=[padded_resized_frame], top_k=3, use_gpu=False\n        )  # check it?\n        resultList = paddleAnimalDetectionResultToList(result)\n        final_result_list = translateResultListToDogCatList(resultList)\n        sprint(\"RESULT LIST:\", final_result_list)\n        # RESULT: [{'美国银色短毛猫': 0.23492032289505005, '虎斑猫': 0.14728288352489471, '美国银虎斑猫': 0.13097935914993286}]\n        # so what is the major categories?\n        # thanks to chinese, we are never confused.\n        # check the labels, shall we?\n        # what about samoyed?\n        # sprint(\"RESULT:\", result)",
        "type": "code",
        "location": "/tests/unittest_paddlehub_animal_resnet.py:79-100"
    },
    "1969": {
        "file_id": 195,
        "content": "This code is using PaddleHub's Animal ResNet model to classify frames from a video source, identifying either dogs or cats. The final results are translated to a list of dog and cat names along with their respective confidences. This code checks the major categories in the final result and verifies if \"samoyed\" is included.",
        "type": "comment"
    },
    "1970": {
        "file_id": 195,
        "content": "        breakpoint()\nelif test_flag == \"image\":\n    # source = \"/root/Desktop/works/pyjom/samples/image/samoyed.jpeg\"\n    # [('dog', 0.8835851550102234), ('dog', 0.08754527568817139), ('dog', 0.008648859336972237)]\n    # source = \"/root/Desktop/works/pyjom/samples/image/dog_saturday_night.jpg\"\n    #  [(None, 0.33663231134414673), ('dog', 0.32254937291145325), ('dog', 0.0494903139770031)]\n    # not animal? wtf?\n    # source = \"/root/Desktop/works/pyjom/samples/image/porn_shemale.jpeg\" # definitely not animal\n    # [(None, 0.9894463419914246), ('dog', 1.564090962347109e-05), ('dog', 1.3550661606132053e-05)]\n    # source = \"/root/Desktop/works/pyjom/samples/image/is_this_duck.bmp\"\n    # [(None, 0.9864748120307922), ('dog', 1.2670795513258781e-05), (None, 9.569253961672075e-06)]\n    # source = \"/root/Desktop/works/pyjom/samples/image/pig_really.bmp\" # it's really a dog\n    # [(None, 0.35919442772865295), ('dog', 0.16199783980846405), ('dog', 0.07987158000469208)]\n    # source = \"/root/Desktop/works/pyjom/samples/image/miku_on_green.png\"",
        "type": "code",
        "location": "/tests/unittest_paddlehub_animal_resnet.py:101-114"
    },
    "1971": {
        "file_id": 195,
        "content": "The code includes various image source paths and the corresponding classification outputs. The script seems to be testing the accuracy of an animal recognition model by inputting different images, including some non-animal images for reference. Some images are misclassified or not classified at all, highlighting potential areas for improvement in the model.",
        "type": "comment"
    },
    "1972": {
        "file_id": 195,
        "content": "    # besides calculating \"DOG\" or \"CAT\" we are also concerned about \"NONE\"\n    # [(None, 0.9998186230659485), (None, 1.7534730432089418e-06), (None, 7.280816021193459e-07)]\n    # source = \"/root/Desktop/works/pyjom/samples/image/dog_with_text.jpg\" # no dog\n    #  [(None, 0.9998675584793091), ('dog', 2.565316492564307e-07), (None, 1.562129767762599e-07)]\n    source = \"/root/Desktop/works/pyjom/samples/image/dog_with_text2.png\"  # has dog\n    #  [(None, 0.8876796960830688), ('dog', 0.0498274527490139), ('dog', 0.02175540290772915)]\n    # a little, but not focused.\n    frame = cv2.imread(source)\n    padded_resized_frame = resizeImageWithPadding(\n        frame, 224, 224, border_type=\"replicate\"\n    )\n    result = classifier.classification(\n        images=[padded_resized_frame], top_k=3, use_gpu=False\n    )\n    resultList = paddleAnimalDetectionResultToList(result)\n    final_result_list = translateResultListToDogCatList(resultList)\n    sprint(\"FINAL RESULT LIST:\", final_result_list)\n    breakpoint()\nelse:\n    raise Exception(\"unknown test flag: %s\" % test_flag)",
        "type": "code",
        "location": "/tests/unittest_paddlehub_animal_resnet.py:115-134"
    },
    "1973": {
        "file_id": 195,
        "content": "This code is testing the accuracy of an image classifier for detecting \"dog\" or \"cat\", while also considering the possibility of \"none\". The code reads an image, resizes and pads it, then uses a classifier to predict its categories. It translates the results into a list of \"dog\" or \"cat\" and displays the final result.",
        "type": "comment"
    },
    "1974": {
        "file_id": 196,
        "content": "/tests/unittest_photo_histogram_match_0.2.py",
        "type": "filepath"
    },
    "1975": {
        "file_id": 196,
        "content": "This code performs image processing tasks, including text removal using inpainting and color distribution transfer between images. It displays all processed images before waiting for a key press.",
        "type": "summary"
    },
    "1976": {
        "file_id": 196,
        "content": "# USAGE\n# python example.py --source images/ocean_sunset.jpg --target images/ocean_day.jpg\nimage_0 = \"/root/Desktop/works/pyjom/samples/image/dog_with_text2.png\"\nimage_1 = \"/root/Desktop/works/pyjom/samples/image/cute_cat.bmp\"\n# from lazero.utils.importers import cv2_custom_build_init\nfrom test_commons import *\n# cv2_custom_build_init()\n# import the necessary packages\nfrom color_transfer import color_transfer\nimport cv2\ndef show_image(title, image, width=300):\n    # resize the image to have a constant width, just to\n    # make displaying the images take up less screen real\n    # estate\n    r = width / float(image.shape[1])\n    dim = (width, int(image.shape[0] * r))\n    resized = cv2.resize(image, dim, interpolation=cv2.INTER_AREA)\n    # show the resized image\n    cv2.imshow(title, resized)\n# load the images\nsource = cv2.imread(image_0)\ntarget = cv2.imread(image_1)\n# we inpaint this one from the beginning.\nfrom pyjom.imagetoolbox import (\n    getImageTextAreaRatio,\n    imageFourCornersInpainting,\n)  # also for image text removal.",
        "type": "code",
        "location": "/tests/unittest_photo_histogram_match_0.2.py:1-38"
    },
    "1977": {
        "file_id": 196,
        "content": "The code imports necessary packages, loads two images (source and target), and uses imageFourCornersInpainting for text removal from the source image. It also defines a function show_image to display resized images with constant width for efficient screen usage.",
        "type": "comment"
    },
    "1978": {
        "file_id": 196,
        "content": "target = getImageTextAreaRatio(target, inpaint=True)\ntarget = imageFourCornersInpainting(target)\n# also remove the selected area.\n# transfer the color distribution from the source image\n# to the target image\ntransfer = color_transfer(source, target)\nimport numpy as np\ntransfer_02 = (target * 0.8 + transfer * 0.2).astype(np.uint8)\ntransfer_02_flip = cv2.flip(transfer_02, 1)\n# show the images and wait for a key press\nshow_image(\"Source\", source)\nshow_image(\"Target\", target)\nshow_image(\"Transfer\", transfer)\nshow_image(\"Transfer_02\", transfer_02)\nshow_image(\"Transfer_02_flip\", transfer_02_flip)\ncv2.waitKey(0)",
        "type": "code",
        "location": "/tests/unittest_photo_histogram_match_0.2.py:40-60"
    },
    "1979": {
        "file_id": 196,
        "content": "This code performs image processing tasks. It applies inpainting to the target image, transfers color distribution from source to target, creates a new transfer image with blending, flips one of the images, and displays all images before waiting for a key press.",
        "type": "comment"
    },
    "1980": {
        "file_id": 197,
        "content": "/tests/unittest_music_recognition.py",
        "type": "filepath"
    },
    "1981": {
        "file_id": 197,
        "content": "This code tests the music recognition functionality of different backends (songrec, shazamio, and midomi) by calling the recognizeMusicFromFile function and logging the results. The filepath variable stores the path to the test audio file. The methods list determines which backends will be tested. After each test, there is a 3-second pause before moving on to the next backend.",
        "type": "summary"
    },
    "1982": {
        "file_id": 197,
        "content": "from test_commons import *\nfrom pyjom.musictoolbox import recognizeMusicFromFile\nfrom lazero.utils.logger import sprint\nfilepath = (\n    # \"/root/Desktop/works/pyjom/tests/music_recognization/exciting_bgm_cut_10seconds.mp3\"\n    \"/root/Desktop/works/pyjom/tests/music_analysis/exciting_bgm.mp3\"\n)\n# methods = [\"midomi\"]\nmethods = [\"songrec\", \"shazamio\", \"midomi\"]\nimport time\nfor method in methods:\n    result = recognizeMusicFromFile(filepath, backend=method, debug=True)\n    sprint(\"RESULT:\", result)\n    time.sleep(3)",
        "type": "code",
        "location": "/tests/unittest_music_recognition.py:1-16"
    },
    "1983": {
        "file_id": 197,
        "content": "This code tests the music recognition functionality of different backends (songrec, shazamio, and midomi) by calling the recognizeMusicFromFile function and logging the results. The filepath variable stores the path to the test audio file. The methods list determines which backends will be tested. After each test, there is a 3-second pause before moving on to the next backend.",
        "type": "comment"
    },
    "1984": {
        "file_id": 198,
        "content": "/tests/unittest_sqlalchemy_1.3_alternative_wrapper_peewee.py",
        "type": "filepath"
    },
    "1985": {
        "file_id": 198,
        "content": "The code initializes a Peewee SQLite database, defines models, performs CRUD operations, checks Bilibili videos, and handles non-existent usernames. It skips error handling for exceptions.",
        "type": "summary"
    },
    "1986": {
        "file_id": 198,
        "content": "# now we try to create and persist a database.\n# do not delete it. we will check again.\n# the data we put into are some timestamps.\n# some peewee by the same guy who developed some database.\n# https://github.com/coleifer/peewee\n# 1.3.24 original sqlalchemy version, for our dearly chatterbot.\n# currently: 1.4.42\n# warning! might be incompatible.\nfrom peewee import *\n# some patch on /usr/local/lib/python3.9/dist-packages/peewee.py:3142\n# is it just a single file? no other files?\n# @property\n# def Model(self): # this is interesting. does it work as expected?\n#     class BaseModel(Model):\n#         class Meta:\n#             database = self\n#     return BaseModel\ndb = SqliteDatabase(\"my_database.db\")  # this database exists in local filesystem.\nclass User(db.Model):\n    username = CharField(unique=True)\n    # what about let's modify this shit?\nclass Account(db.Model):\n    # charlie_account.user_id to get username?\n    user = ForeignKeyField(User)  # what is this??\n    # if you don't set field, the user_id will be the default User.id",
        "type": "code",
        "location": "/tests/unittest_sqlalchemy_1.3_alternative_wrapper_peewee.py:1-39"
    },
    "1987": {
        "file_id": 198,
        "content": "This code sets up a Peewee SQLite database named \"my_database.db\" and defines two models: User and Account. The User model has a unique username field, while the Account model references the User model through a ForeignKeyField.",
        "type": "comment"
    },
    "1988": {
        "file_id": 198,
        "content": "    # user = ForeignKeyField(User, field=User.username) # what is this??\n    password = (\n        CharField()\n    )  # you need to create a new table. do not modify this in place.\n    # maybe you want tinydb or something else.\n# User.bind(db) # this can dynamically change the database. maybe.\nclass User2(Model):  # what is this model for? empty?\n    username = CharField(unique=True)\nimport datetime\nclass BilibiliVideo(db.Model):\n    bvid = CharField(unique=True)\n    visible = BooleanField()\n    last_check = DateTimeField(\n        default=datetime.datetime.now\n    )  # this is default callable. will be managed as expected\n    # poster = ForeignKeyField(User) # is it my account anyway?\n# db.connect()\n# if using context manager, it will auto connect. no need to do shit.\n# are you sure you want to comment out the db.connect?\n# actually no need to connect this. it will auto connect.\ndb.create_tables(\n    [User, Account, BilibiliVideo]\n)  # it is the same damn database. but shit has happened already.\n# it is the foreign key reference.",
        "type": "code",
        "location": "/tests/unittest_sqlalchemy_1.3_alternative_wrapper_peewee.py:40-71"
    },
    "1989": {
        "file_id": 198,
        "content": "Code snippet is for creating tables and defining fields in a Peewee database. User2 seems empty, ForeignKeyField references are used, and db.connect() can be omitted with context manager.",
        "type": "comment"
    },
    "1990": {
        "file_id": 198,
        "content": "# charlie = User.create(username='charlie') # fail the unique check. will raise exception.\ncharlie, flag = User.get_or_create(username=\"charlie\")  # will work without exception.\n# print(charlie)\n# breakpoint()\n# why we can pass a function instead of the object?\n# last_check = datetime.datetime.now()\nvideo_record, flag = BilibiliVideo.get_or_create(bvid=\"BV123\", visible=False)\n# print(video_record) # it will be good.\n# breakpoint()\nnext_check_time = datetime.datetime.now() - datetime.timedelta(\n    minutes=20\n)  # every 20 minutes check these things.\n# but for those which are already recognized as visible, we may not want to check these video till we select/search them. this is to reserve bandwidth.\nprint(\"NEXT CHECK TIME:\", next_check_time)\nresults_0 = BilibiliVideo.select().where(\n    BilibiliVideo.last_check < datetime.datetime.now()\n)  # needs to check\nresults_1 = BilibiliVideo.select().where(\n    BilibiliVideo.last_check > datetime.datetime.now()\n)  # no need to check\nprint(results_0)\nprint(results_1)  # these are just raw sql statements. have't executed yet.",
        "type": "code",
        "location": "/tests/unittest_sqlalchemy_1.3_alternative_wrapper_peewee.py:73-102"
    },
    "1991": {
        "file_id": 198,
        "content": "This code uses the `get_or_create` method to create or retrieve a User object with a specific username. It also retrieves a BilibiliVideo object based on a BVID, checks the last check time for videos, and selects videos that need to be checked or those that don't need to be checked. The results are printed for reference.",
        "type": "comment"
    },
    "1992": {
        "file_id": 198,
        "content": "breakpoint()\n# warning: our table name is lowercased. may cause trouble.\n# but many sql statements are lower cased. case insensitive. at least my data are not case insensitive.\ncharlie_account, flag = Account.get_or_create(\n    user=charlie, password=\"abcd\"\n)  # this is not unique. warning!\nprint(charlie_account)\n# breakpoint()\n# charlie = User.update(username='michael') # no insertion?\n# use get_or_create here.\nmichael = User.get_or_create(username=\"michael\")\n# (data, flag)\ndata = User.get()  # this can only get one such instance?\n# get one single instance, aka: first.\n# print(data)\n# breakpoint()\nselection = User.select()  # still iterable?\n# breakpoint()\n# let's bind some database.\n# User2.bind(db)\n# if i don't bind the database what would happen?\n# error!\n# you need create such table first.\n# User2.create_table()\ndb.create_tables([User2])\nUser2.get_or_create(username=\"abcdef\")\nprint([x for x in User2.select()])\nusername = \"nonexistant\"\n# try:\nanswer = User2.get_or_none(User2.username == username)  # still raise exception huh?",
        "type": "code",
        "location": "/tests/unittest_sqlalchemy_1.3_alternative_wrapper_peewee.py:103-139"
    },
    "1993": {
        "file_id": 198,
        "content": "This code defines a Peewee model for a User class and performs CRUD operations like getting, creating, updating, and deleting users. It also demonstrates binding the database and creating tables. The code uses try-except to handle nonexistent usernames and raises an exception if no record is found.",
        "type": "comment"
    },
    "1994": {
        "file_id": 198,
        "content": "print(\"ANSWER:\", answer)  # great this is simpler.\nif answer is None:\n    print(\"username does not exist:\", username)\n# except Exception as e:\n#     # print('exception type:', type(e))\n#     print('username does not exist:', username)\n#     # exception type: <class '__main__.User2DoesNotExist'>",
        "type": "code",
        "location": "/tests/unittest_sqlalchemy_1.3_alternative_wrapper_peewee.py:140-146"
    },
    "1995": {
        "file_id": 198,
        "content": "This code is printing an answer and checking if it's None. If the answer is None, it prints that the username does not exist. It skips error handling for exceptions.",
        "type": "comment"
    },
    "1996": {
        "file_id": 199,
        "content": "/tests/unittest_ocr_filter_large_area_of_text.py",
        "type": "filepath"
    },
    "1997": {
        "file_id": 199,
        "content": "This code sets up libraries and variables for processing image or video files, detects text within frames using EasyOCRReader, calculates text area percentage, draws rectangles, and displays the result.",
        "type": "summary"
    },
    "1998": {
        "file_id": 199,
        "content": "from test_commons import *\n# import pytesseract\n# from pytesseract import Output\nfrom lazero.utils.importers import cv2_custom_build_init\ncv2_custom_build_init()\nimport cv2\n# img = cv2.imread('image.jpg')\nfrom pyjom.videotoolbox import getVideoFrameIteratorWithFPS\ndetectionList = []\nfrom pyjom.imagetoolbox import getEasyOCRReader, LRTBToDiagonal\nreader = getEasyOCRReader((\"en\",))\nimport numpy as np\ntest_subject = \"image\"\nif test_subject == \"video\":\n    videoPath = \"/root/Desktop/works/pyjom/samples/video/dog_with_large_text.gif\"\n    iterator = getVideoFrameIteratorWithFPS(videoPath, start=-1, end=-1, fps=10)\nelif test_subject == \"image\":\n    imagePath = \"/root/Desktop/works/pyjom/samples/image/dog_saturday_night.bmp\"\n    iterator = [cv2.imread(imagePath)]\nelse:\n    raise Exception(\"unknown test_subject:\", test_subject)\n# threshold: {'max':0.3}\nfor frame in iterator:\n    height, width = frame.shape[:2]\n    img = np.zeros((height, width, 3))\n    detection, recognition = reader.detect(frame)  # not very sure.\n    if detection == [[]]:",
        "type": "code",
        "location": "/tests/unittest_ocr_filter_large_area_of_text.py:1-37"
    },
    "1999": {
        "file_id": 199,
        "content": "Code imports necessary libraries and sets up variables for working with an image or video file. It initializes OpenCV, EasyOCRReader, and numpy, then determines the test subject (image or video) to be used. The code creates an iterator based on the test subject and sets a threshold for detection. It loops through each frame in the iterator, creating a blank image, and detects text within the frame using EasyOCRReader.",
        "type": "comment"
    }
}