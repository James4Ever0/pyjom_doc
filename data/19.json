{
    "1900": {
        "file_id": 185,
        "content": "/tests/unittest_extract_cat_cover_from_video.py",
        "type": "filepath"
    },
    "1901": {
        "file_id": 185,
        "content": "This code downloads Bilibili videos, extracts covers for pet videos, and checks frames to display the cover. It uses yt_dlp, image processing libraries, and OpenCV's imshow function. If a clear frame is found, it breaks the loop and waits for a key press before proceeding.",
        "type": "summary"
    },
    "1902": {
        "file_id": 185,
        "content": "videoLink = \"https://www.bilibili.com/video/BV1Cb4y1s7em\"  # this is a dog.\n# videoLink = \"https://www.bilibili.com/video/BV1Lx411B7X6\"  # multipart download\n# from lazero.filesystem.temp import tmpfile\nimport yt_dlp\n# import pyidm\npath = \"/dev/shm/testVideo.mp4\"\nfrom test_commons import *\nfrom lazero.utils.importers import cv2_custom_build_init\ncv2_custom_build_init()\nimport cv2\nfrom pyjom.videotoolbox import getVideoFrameSampler\nfrom pyjom.imagetoolbox import imageDogCatCoverCropAdvanced\n# from pyjom.imagetoolbox import (\n#     bezierPaddleHubResnet50ImageDogCatDetector,\n#     # we deprecate this thing to make it somehow better.\n#     getImageTextAreaRatio,\n#     imageFourCornersInpainting,\n#     imageCropoutBlackArea,\n#     imageCropoutBlurArea,\n#     imageDogCatDetectionForCoverExtraction,\n#     imageLoader,\n# )\nfrom pyjom.commons import checkMinMaxDict\nimport os\n# with tmpfile(path=path, replace=True) as TF:\nif os.path.exists(path):\n    os.remove(path)\nx = yt_dlp.YoutubeDL(\n    {\n        \"outtmpl\": path,  # seems only video p1 is downloaded.",
        "type": "code",
        "location": "/tests/unittest_extract_cat_cover_from_video.py:1-42"
    },
    "1903": {
        "file_id": 185,
        "content": "This code is downloading a video from Bilibili using yt_dlp library and saving it to the path \"/dev/shm/testVideo.mp4\". It also imports various libraries for image processing and video analysis. The commented out section suggests an alternative download method, possibly for multipart downloads.",
        "type": "comment"
    },
    "1904": {
        "file_id": 185,
        "content": "    }\n)\ny = x.download([videoLink])\n# shall you use frame sampler instead of iterator? cause this is dumb.\n# breakpoint()\nfrom pyjom.videotoolbox import corruptVideoFilter\nvideo_fine = corruptVideoFilter(path)\nif not video_fine:\n    print(\"VIDEO FILE CORRUPTED\")\n    exit()\nfrom caer.video.frames_and_fps import get_duration\nduration = get_duration(path)\nmSampleSize = int(duration / 2)  # fps = 0.5 or something?\nprocessed_frame = None\ndog_or_cat = \"dog\"\nfor frame in getVideoFrameSampler(path, -1, -1, sample_size=mSampleSize, iterate=True):\n    # animalCropDiagonalRect = imageDogCatDetectionForCoverExtraction(\n    #     frame,\n    #     dog_or_cat=dog_or_cat,\n    #     confidence_threshold=confidence_threshold,\n    #     crop=False,\n    # )  # you must use gpu this time.\n    # if animalCropDiagonalRect is not None:  # of course this is not None.\n    # we need to identify this shit.\n    # if checkMinMaxDict(text_area_ratio, text_area_threshold):\n    processed_frame = imageDogCatCoverCropAdvanced(frame, dog_or_cat=dog_or_cat)",
        "type": "code",
        "location": "/tests/unittest_extract_cat_cover_from_video.py:43-74"
    },
    "1905": {
        "file_id": 185,
        "content": "This code downloads a video file, checks for corruption, calculates the duration, sets the sample size based on duration, iterates through video frames, and applies an image processing algorithm to extract a cover for either dog or cat videos. The code might benefit from using a frame sampler instead of an iterator as the current implementation is considered inefficient.",
        "type": "comment"
    },
    "1906": {
        "file_id": 185,
        "content": "    if processed_frame is not None:\n        # blurValue = imageCropoutBlurArea(processed_frame, value=True)\n        # print(\"BLUR VALUE:\", blurValue)\n        # if not checkMinMaxDict(blurValue, blurValue_threshold):\n        #     # will skip this one since it is not so clear.\n        #     continue\n        break\nif processed_frame is not None:\n    print(\"COVER IMAGE FOUND!\")\n    processed_frame_show = cv2.resize(processed_frame, (int(1920 / 2), int(1080 / 2)))\n    cv2.imshow(\"image\", processed_frame_show)\n    cv2.waitKey(0)\nelse:\n    print(\"COVER NOT FOUND FOR %s\" % videoLink)",
        "type": "code",
        "location": "/tests/unittest_extract_cat_cover_from_video.py:75-88"
    },
    "1907": {
        "file_id": 185,
        "content": "This code checks for a clear frame in a video and if found, displays it; otherwise, it indicates that the cover was not found. If a clear frame is detected (processed_frame), it will break out of the loop. The processed frame is resized and displayed using OpenCV's imshow function, then the program waits for any key press before proceeding.",
        "type": "comment"
    },
    "1908": {
        "file_id": 186,
        "content": "/tests/unittest_caer_get_gif_width_height.py",
        "type": "filepath"
    },
    "1909": {
        "file_id": 186,
        "content": "This code imports the get_res function from caer.video, sets a video path, and calls the get_res function with the video path to retrieve the width and height of the video, then prints them out.",
        "type": "summary"
    },
    "1910": {
        "file_id": 186,
        "content": "from caer.video.frames_and_fps import get_res\nvideoPath = \"/root/Desktop/works/pyjom/samples/video/cat_invalid_eye_rolling.gif\"\nwidth, height = get_res(videoPath)\nprint(width, height)",
        "type": "code",
        "location": "/tests/unittest_caer_get_gif_width_height.py:1-6"
    },
    "1911": {
        "file_id": 186,
        "content": "This code imports the get_res function from caer.video, sets a video path, and calls the get_res function with the video path to retrieve the width and height of the video, then prints them out.",
        "type": "comment"
    },
    "1912": {
        "file_id": 187,
        "content": "/tests/unittest_cirular_import.py",
        "type": "filepath"
    },
    "1913": {
        "file_id": 187,
        "content": "The code imports a module \"unittest_circular_import\" as \"rea\". It assigns the value 1 to variable x. If the script is executed directly, it prints the value of rea's x. This could be used for testing purposes or handling circular imports.",
        "type": "summary"
    },
    "1914": {
        "file_id": 187,
        "content": "import unittest_circular_import as rea\nx = 1\nif __name__ == \"__main__\":\n    print(rea.x)",
        "type": "code",
        "location": "/tests/unittest_cirular_import.py:1-6"
    },
    "1915": {
        "file_id": 187,
        "content": "The code imports a module \"unittest_circular_import\" as \"rea\". It assigns the value 1 to variable x. If the script is executed directly, it prints the value of rea's x. This could be used for testing purposes or handling circular imports.",
        "type": "comment"
    },
    "1916": {
        "file_id": 188,
        "content": "/tests/unittest_clean_lrc.py",
        "type": "filepath"
    },
    "1917": {
        "file_id": 188,
        "content": "The code checks lyrics' adherence to line requirements, processes a list of lyrics, extracts flags, and formats the lyrics into an LRC string.",
        "type": "summary"
    },
    "1918": {
        "file_id": 188,
        "content": "lyric_string = \"\"\"[00:00.000] 作词 : 苏喜多/挡风玻璃\\n[00:01.000] 作曲 : 苏喜多/陈恒冠\\n[00:02.000] 编曲 : 陈恒冠/陈恒家\\n[00:31.154]你戴上帽子遮住眼睛 轻轻地绕着我 总洋溢着暖\\n[00:44.404]我…我只能唱\\n[00:54.902]你像气泡水直接淘气 爱和星星眨眼睛 轻易抓住我\\n[01:07.903]我…我只能唱\\n[01:16.651]有一个岛屿 在北极冰川\\n[01:23.403]那儿没有花朵 也没有失落\\n[01:30.159]在那个岛屿 洒满了繁星\\n[01:36.904]拥有我和你 再没有失落\\n[02:15.903]你邀请流浪期待欢喜 惹我专心好奇 我看见了光\\n[02:29.153]我…我只能唱\\n[02:39.403]难免坏天气闪电暴雨 练就肩膀和勇气 只为你拥抱我\\n[02:54.156]我…我只能唱\\n[03:01.659]有一个岛屿 在北极冰川\\n[03:08.154]那儿没有花朵 也没有失落\\n[03:14.906]在那个岛屿 洒满了繁星\\n[03:21.651]拥有我和你 再没有失落\\n[03:28.656]有一个岛屿 在北极冰川\\n[03:35.152]\n那儿没有花朵 也没有失落\\n[03:41.904]在那个岛屿 洒满了繁星\\n[03:48.661]拥有我和你 再没有失落\\n[03:59.159]有一个岛屿 在北极冰川\\n[04:05.654]那儿没有花朵 也没有失落\\n[04:12.659]在那个岛屿 洒满了繁星\\n[04:19.152]拥有我和你 再没有失落\\n[04:26.405]有一个岛屿\n在北极冰川\\n[04:33.658]那儿没有花朵 也没有失落…\\n[04:40.401]吉他：陈恒家\\n[04:42.654]钢琴：陈恒冠\\n[04:47.407]混音：陈恒家\\n[04:49.907]母带：陈恒家\\n[04:53.907]监制：1991与她\\n\"\"\"\n# assume song duration here!\nsong_duration = 5 * 60\nimport pylrc\n# you'd better inspect the thing. what is really special about the lyric, which can never appear?",
        "type": "code",
        "location": "/tests/unittest_clean_lrc.py:1-10"
    },
    "1919": {
        "file_id": 188,
        "content": "Lyric string contains time-stamped song lyrics and metadata, assumed song duration is set to 5 minutes, and pylrc module is imported.",
        "type": "comment"
    },
    "1920": {
        "file_id": 188,
        "content": "min_lines_of_lyrics = 5\nmin_total_lines_of_lyrics = 10\npotential_forbidden_chars = [\"[\", \"]\", \"【\", \"】\", \"「\", \"」\", \"《\", \"》\", \"/\", \"(\", \")\"]\ncore_forbidden_chars = [\":\", \"：\", \"@\"]\ndef checkLyricText(text, core_only=False):\n    if core_only:\n        forbidden_chars = core_forbidden_chars\n    else:\n        forbidden_chars = core_forbidden_chars + potential_forbidden_chars\n    return not any([char in text for char in forbidden_chars])\n# also get the total time covered by lyric.\n# the time must be long enough, compared to the total time of the song.\nlrc_parsed = pylrc.parse(lyric_string)\nlrc_parsed_list = [line for line in lrc_parsed]\nlrc_parsed_list.sort(key=lambda line: line.time)\nbegin = False\n# end = False\nline_counter = 0\nnew_lines = []\n# lrc_parsed: pylrc.classes.Lyrics\nflags = []\nfor line in lrc_parsed_list:\n    # print(line)\n    text = line.text.strip()\n    startTime = line.time\n    if not begin:\n        flag = checkLyricText(text, core_only=False)\n        if not flag:\n            begin = True\n    else:\n        flag = checkLyricText(text, core_only=True)",
        "type": "code",
        "location": "/tests/unittest_clean_lrc.py:12-46"
    },
    "1921": {
        "file_id": 188,
        "content": "This code checks if a Lyrics object from pylrc meets certain criteria. It defines minimum line requirements for lyrics and forbidden characters. The function checkLyricText() determines whether a line contains any forbidden characters. The code then processes the lrc_parsed list to get the total time covered by lyrics, ensuring it's long enough compared to the song's total time. It creates new_lines with valid lines and flags for each line based on the presence of forbidden characters.",
        "type": "comment"
    },
    "1922": {
        "file_id": 188,
        "content": "        if flag:\n            begin = False\n    flags.append(flag)\n    # breakpoint()\n# select consecutive spans.\nfrom test_commons import *\nfrom pyjom.mathlib import extract_span\nint_flags = [int(flag) for flag in flags]\nmySpans = extract_span(int_flags, target=1)\nprint(mySpans)  # this will work.\n# this span is for the range function. no need to add one to the end.\ntotal_length = 0\nnew_lyric_list = []\nfor mstart, mend in mySpans:\n    length = mend - mstart\n    total_length += length\n    if length >= min_lines_of_lyrics:\n        # process these lines.\n        for index in range(mstart, mend):\n            line_start_time = lrc_parsed_list[index].time\n            line_text = lrc_parsed_list[index].text\n            if line_start_time <= song_duration:\n                line_end_time = song_duration\n                if index + 1 < len(lrc_parsed_list):\n                    line_end_time = lrc_parsed_list[index + 1].time\n                    if line_end_time > song_duration:\n                        line_end_time = song_duration",
        "type": "code",
        "location": "/tests/unittest_clean_lrc.py:47-78"
    },
    "1923": {
        "file_id": 188,
        "content": "Checks if a flag is set, appends it to the flags list. Filters and extracts consecutive spans from the flags list. Calculates total length of spans. Iterates over the spans, retrieves line start time and text from lrc_parsed_list, checks if line end time is within song duration.",
        "type": "comment"
    },
    "1924": {
        "file_id": 188,
        "content": "                new_lyric_list.append((line_text, line_start_time))\n                if index == mend - 1:\n                    # append one more thing.\n                    new_lyric_list.append((\"\", line_end_time))\n            else:\n                continue\n# for elem in new_lyric_list:\n#     print(elem)\n# exit()\nif total_length >= min_total_lines_of_lyrics:\n    print(\"LYRIC ACCEPTED.\")\n    new_lrc = pylrc.classes.Lyrics()\n    for text, myTime in new_lyric_list:\n        timecode_min, timecode_sec = divmod(myTime, 60)\n        timecode = \"[{:d}:{:.3f}]\".format(int(timecode_min), timecode_sec)\n        myLine = pylrc.classes.LyricLine(timecode, text)\n        new_lrc.append(myLine)\n    new_lrc_string = new_lrc.toLRC()\n    print(new_lrc_string)",
        "type": "code",
        "location": "/tests/unittest_clean_lrc.py:79-98"
    },
    "1925": {
        "file_id": 188,
        "content": "The code processes a list of lyrics and checks if it meets the minimum requirements for length. If so, it formats the lyrics into an LRC string and prints it.",
        "type": "comment"
    },
    "1926": {
        "file_id": 189,
        "content": "/tests/unittest_check_video_corrput.py",
        "type": "filepath"
    },
    "1927": {
        "file_id": 189,
        "content": "The code tests a video file for corruption by using ffmpeg to input the video, output it to null format, and then checks if any error or failure messages appear in the stderr. If such messages are found, the video is considered corrupted.",
        "type": "summary"
    },
    "1928": {
        "file_id": 189,
        "content": "import ffmpeg\nnot_nice = [\"invalid\", \"failed\", \"error\"]\nvideoPath = \"/root/Desktop/works/pyjom/samples/video/dog_with_large_text.gif\"\n# videoPath = \"/root/Desktop/works/pyjom/samples/video/cute_cat_gif.gif\"\n# videoPath = \"/root/Desktop/works/pyjom/samples/video/corrupt_video.gif\"\ncorrupted = False\ntry:\n    stdout, stderr = (\n        ffmpeg.input(videoPath)\n        .output(\"null\", f=\"null\")\n        .run(capture_stdout=True, capture_stderr=True)\n    )\n    stderr_lower = stderr.decode(\"utf-8\").lower()\n    for word in not_nice:\n        if word in stderr_lower:\n            print(\"video is corrupted\")\n            corrupted = True\n            break\nexcept:\n    import traceback\n    traceback.print_exc()\n    corrupted = True\n    print(\"corrupt video\")\nif not corrupted:\n    print(\"video is fine\")",
        "type": "code",
        "location": "/tests/unittest_check_video_corrput.py:1-28"
    },
    "1929": {
        "file_id": 189,
        "content": "The code tests a video file for corruption by using ffmpeg to input the video, output it to null format, and then checks if any error or failure messages appear in the stderr. If such messages are found, the video is considered corrupted.",
        "type": "comment"
    },
    "1930": {
        "file_id": 190,
        "content": "/tests/unittest_property_decorator.py",
        "type": "filepath"
    },
    "1931": {
        "file_id": 190,
        "content": "This code defines a class with a property that increments its value each time accessed. It creates an object of the class, stores the property in a list twice, and prints the property's value three times, showing its dynamic nature.",
        "type": "summary"
    },
    "1932": {
        "file_id": 190,
        "content": "# a dynamic property in set\nclass Obj:\n    def __init__(self):\n        self.val = 0\n    @property\n    def prop(self):\n        self.val += 1\n        return self.val\nobj = Obj()\n# mproperty = obj.prop\nmyData = [{\"a\": lambda: obj.prop}] * 2\nfor d in myData:\n    val = d[\"a\"]()\n    print(val)\n# for _ in range(3):\n#     print(obj.prop) # strange.",
        "type": "code",
        "location": "/tests/unittest_property_decorator.py:1-22"
    },
    "1933": {
        "file_id": 190,
        "content": "This code defines a class with a property that increments its value each time accessed. It creates an object of the class, stores the property in a list twice, and prints the property's value three times, showing its dynamic nature.",
        "type": "comment"
    },
    "1934": {
        "file_id": 191,
        "content": "/tests/unittest_video_cover_extraction_dog_cat_detections.py",
        "type": "filepath"
    },
    "1935": {
        "file_id": 191,
        "content": "This script initializes a YOLOv5 model for object detection, focusing on dogs and cats, using image processing techniques to enhance accuracy. It may have difficulties categorizing certain objects and displays \"NO COVER FOUND.\" if no suitable cover is detected.",
        "type": "summary"
    },
    "1936": {
        "file_id": 191,
        "content": "import torch\nimport os\nfrom lazero.utils.importers import cv2_custom_build_init\n# order:\n# detect if dog/cat is there, satisfying the qualification\n# remove watermark, remove text, remove potential watermark around corners using inpainting\n# use ffmpeg cropdetect, if has significant area change then no further processing\n# if no significant area change, use this blur detection to get the main area\n# remove watermark again?? around corners?\n# then reuse the dog detection and get the crop from processed/cropped image.\ncv2_custom_build_init()\nimport cv2\nos.environ[\"http_proxy\"] = \"\"\nos.environ[\"https_proxy\"] = \"\"\n# Model\n# localModelDir = (\n#     \"/root/Desktop/works/pyjom/pyjom/models/yolov5/ultralytics_yolov5_master/\"\n# )\n# # import os\n# os.environ[\n#     \"YOLOV5_MODEL_DIR\"\n# ] = \"/root/Desktop/works/pyjom/pyjom/models/yolov5/\"  # this is strange. must be a hack in the localModelDir\n# model = torch.hub.load(\n#     localModelDir, \"yolov5s\", source=\"local\"\n# )  # or yolov5m, yolov5l, yolov5x, custom\nfrom test_commons import *",
        "type": "code",
        "location": "/tests/unittest_video_cover_extraction_dog_cat_detections.py:1-31"
    },
    "1937": {
        "file_id": 191,
        "content": "This code is initializing a YOLOv5 model for object detection, specifically detecting dogs and cats. It also sets environment variables to disable proxies, imports necessary libraries, and defines the model path. The code aims to remove watermarks, text, and potentially blurred corners from images, crop detected animals, and possibly re-detect them to get the crop from the processed image.",
        "type": "comment"
    },
    "1938": {
        "file_id": 191,
        "content": "from pyjom.commons import configYolov5\nmodel = configYolov5()\ndog_or_cat = \"dog\"\n# Images\n# img = '/media/root/help/pyjom/samples/image/miku_on_green.png'  # or file, Path, PIL, OpenCV, numpy, list\n# img = \"/root/Desktop/works/pyjom/samples/image/dog_with_text.jpg\"\nimgPath = \"/root/Desktop/works/pyjom/samples/image/dog_blue_sky.png\"\nimg = cv2.imread(imgPath)\ndefaultHeight, defaultWidth = img.shape[:2]\ntotal_area = defaultHeight * defaultWidth\n# Inference\nresults = model(img)\n# print(results)\n# # Results\n# breakpoint()\nanimal_detection_dataframe = results.pandas().xyxy[0]\n# results.show()\n# # results.print() # or .show(),\narea = (animal_detection_dataframe[\"xmax\"] - animal_detection_dataframe[\"xmin\"]) * (\n    animal_detection_dataframe[\"ymax\"] - animal_detection_dataframe[\"ymin\"]\n)\nanimal_detection_dataframe[\"area_ratio\"] = area / total_area\narea_threshold = 0.08  # min area?\nconfidence_threshold = 0.7  # this is image quality maybe.\ny_expansion_rate = 0.03  # to make the starting point on y axis less \"headless\"",
        "type": "code",
        "location": "/tests/unittest_video_cover_extraction_dog_cat_detections.py:32-67"
    },
    "1939": {
        "file_id": 191,
        "content": "The code imports a YOLOv5 configuration, sets the target animal to \"dog\", and reads an image file. It then performs inference using the model on the image and stores the results in the `results` variable. The code extracts the object detection data from `results`, calculates the area ratio for each detected object, and applies thresholds to filter out objects with low confidence or small areas.",
        "type": "comment"
    },
    "1940": {
        "file_id": 191,
        "content": "df = animal_detection_dataframe\nnew_df = df.loc[\n    (df[\"area_ratio\"] >= area_threshold)\n    & (df[\"confidence\"] >= confidence_threshold)\n    & (df[\"name\"] == dog_or_cat)\n].sort_values(\n    by=[\"confidence\"]\n)  # this one is for 0.13\n# count = new_df.count(axis=0)\ncount = len(new_df)\n# print(\"COUNT: %d\" % count)\ndefaultCropWidth, defaultCropHeight = 1920, 1080\n# this is just to maintain the ratio.\n# you shall find the code elsewhere?\nallowedHeight = min(int(defaultWidth / defaultCropWidth * defaultHeight), defaultHeight)\nif count >= 1:\n    selected_col = new_df.iloc[0]  # it is a dict-like object.\n    # print(new_df)\n    # breakpoint()\n    selected_col_dict = dict(selected_col)\n    # these are floating point shits.\n    # {'xmin': 1149.520263671875, 'ymin': 331.6445007324219, 'xmax': 1752.586181640625, 'ymax': 1082.3826904296875, 'confidence': 0.9185908436775208, 'class': 16, 'name': 'dog', 'area_ratio': 0.13691652620239364}\n    x0, y0, x1, y1 = [\n        int(selected_col[key]) for key in [\"xmin\", \"ymin\", \"xmax\", \"ymax\"]",
        "type": "code",
        "location": "/tests/unittest_video_cover_extraction_dog_cat_detections.py:69-98"
    },
    "1941": {
        "file_id": 191,
        "content": "The code filters the animal detection dataframe based on area ratio, confidence threshold, and dog or cat name. It then sorts the filtered dataframe by confidence. If there is at least one row in the filtered dataframe, it selects the first row as 'selected_col'. The selected column contains values for xmin, ymin, xmax, ymax, confidence, class (dog or cat), and area_ratio. These values are used to extract a region of interest from an image by converting them into coordinates and then cropping the image accordingly.",
        "type": "comment"
    },
    "1942": {
        "file_id": 191,
        "content": "    ]\n    y0_altered = max(int(y0 - (y1 - y0) * y_expansion_rate), 0)\n    height_current = min((y1 - y0_altered), allowedHeight)  # reasonable?\n    width_current = min(\n        int((height_current / defaultCropHeight) * defaultCropWidth), defaultWidth\n    )  # just for safety. not for mathematical accuracy.\n    # height_current = min(allowedHeight, int((width_current/defaultCropWidth)*defaultCropHeight))\n    # (x1+x0)/2-width_current/2\n    import random\n    x0_framework = random.randint(\n        max((x1 - width_current), 0), min((x0 + width_current), defaultWidth)\n    )\n    framework_XYWH = (x0_framework, y0_altered, width_current, height_current)\n    x_f, y_f, w_f, h_f = framework_XYWH\n    croppedImageCover = img[y_f : y_f + h_f, x_f : x_f + w_f, :]\n    # breakpoint()\n    # resize image\n    croppedImageCoverResized = cv2.resize(\n        croppedImageCover, (defaultCropWidth, defaultCropHeight)\n    )\n    cv2.imshow(\"CROPPED IMAGE COVER\", croppedImageCover)\n    cv2.imshow(\"CROPPED IMAGE COVER RESIZED\", croppedImageCoverResized)",
        "type": "code",
        "location": "/tests/unittest_video_cover_extraction_dog_cat_detections.py:99-122"
    },
    "1943": {
        "file_id": 191,
        "content": "This code snippet is responsible for cropping an image and resizing it. It adjusts the cropping parameters to ensure a reasonable aspect ratio while maintaining mathematical safety. The random x0_framework value ensures that the cropped image falls within the allowed width, avoiding any potential out-of-bounds errors. The resulting images are then displayed using OpenCV's imshow function.",
        "type": "comment"
    },
    "1944": {
        "file_id": 191,
        "content": "    # print(selected_col_dict)\n    # print(count)\n    # breakpoint()\n    cv2.waitKey(0)\nelse:\n    print(\"NO COVER FOUND.\")\n# # results.save()\n# # # print(type(results),dir(results))\n# breakpoint()\n# import cv2\n# image = cv2.imread(\"runs/detect/exp3/miku_on_green.jpg\")\n# cv2.imshow(\"NONE\",image)\n# # results.print()  # or .show(),\n# # hold it.\n# # image 1/1: 720x1280 1 bird # what the fuck is a bird?\n# # os.system(\"pause\")\n# # input()\n# this shit has been detected but not in the right category.",
        "type": "code",
        "location": "/tests/unittest_video_cover_extraction_dog_cat_detections.py:123-141"
    },
    "1945": {
        "file_id": 191,
        "content": "The code seems to be a part of an image processing script. It checks for the detection of objects (dog and cat) in images, potentially for cover extraction purposes. The code might have issues with the categorization of certain detected objects. If no suitable cover is found, it displays a \"NO COVER FOUND.\" message.",
        "type": "comment"
    },
    "1946": {
        "file_id": 192,
        "content": "/tests/unittest_musictoolbox_netease_music_lyric.py",
        "type": "filepath"
    },
    "1947": {
        "file_id": 192,
        "content": "This code tests the functionality of getting music and lyrics using NeteaseMusic API from the pyjom library. It checks for similarity in keywords and prints the result, but encounters an issue when no lyrics are found. It then plans to test a specific music ID for lyrics retrieval and shows preparedness in case it fails.",
        "type": "summary"
    },
    "1948": {
        "file_id": 192,
        "content": "from test_commons import *\nfrom pyjom.musictoolbox import neteaseMusic\nNMClient = neteaseMusic()\n# import random\nquery = \"linkin park numb\"\nfor sim in [False, True]:\n    result = NMClient.getMusicAndLyricWithKeywords(query, similar=sim, debug=True)\n    print(\"similar?\", sim)\n    # no lyrics! wtf??\n    breakpoint()\n# now let's test something surely will get lyrics.\n# music_id = 497572729\n# lyric_string = NMClient.getMusicLyricFromNetease(music_id)\n# print(\"LYRIC STRING:\",lyric_string)\n# in case we don't get the lyric, you should be prepared.\n# it works.",
        "type": "code",
        "location": "/tests/unittest_musictoolbox_netease_music_lyric.py:1-17"
    },
    "1949": {
        "file_id": 192,
        "content": "This code tests the functionality of getting music and lyrics using NeteaseMusic API from the pyjom library. It checks for similarity in keywords and prints the result, but encounters an issue when no lyrics are found. It then plans to test a specific music ID for lyrics retrieval and shows preparedness in case it fails.",
        "type": "comment"
    },
    "1950": {
        "file_id": 193,
        "content": "/tests/unittest_tempfile_generator.py",
        "type": "filepath"
    },
    "1951": {
        "file_id": 193,
        "content": "This code generates temporary files named with a \".data\" suffix and yields their names. The generated file paths are then printed, data is written to the files, and the content of these files is read and printed. Finally, it closes all temporary files.",
        "type": "summary"
    },
    "1952": {
        "file_id": 193,
        "content": "import tempfile\ndef generateFile():\n    data = b\"abc\"\n    while True:\n        with tempfile.NamedTemporaryFile(\"wb\", suffix=\".data\") as f:\n            name = f.name\n            print(\"tempfile name:\", name)\n            f.write(data)\n            f.seek(0)  # strange.\n            # what the fuck?\n            # f.close()\n            yield name\nif __name__ == \"__main__\":\n    grt = generateFile()\n    filepath = grt.__next__()\n    for _ in range(2):\n        # good?\n        with open(filepath, \"rb\") as f:\n            content = f.read()\n            print(\"content in {}:\".format(filepath), content)",
        "type": "code",
        "location": "/tests/unittest_tempfile_generator.py:1-24"
    },
    "1953": {
        "file_id": 193,
        "content": "This code generates temporary files named with a \".data\" suffix and yields their names. The generated file paths are then printed, data is written to the files, and the content of these files is read and printed. Finally, it closes all temporary files.",
        "type": "comment"
    },
    "1954": {
        "file_id": 194,
        "content": "/tests/unittest_nsfw_video_score.py",
        "type": "filepath"
    },
    "1955": {
        "file_id": 194,
        "content": "The code utilizes a trained model to detect NSFW content in videos and images, ensuring compliance by posting non-sexual content through an API. It stores classification probabilities and handles exceptions for unknown test_flags. However, only GIFs can be posted currently with caution about picture stretching.",
        "type": "summary"
    },
    "1956": {
        "file_id": 194,
        "content": "# we take max for the concerned ones, and take mean for the unconcerned ones.\nfrom test_commons import *\nimport requests\nfrom lazero.network.checker import waitForServerUp\nfrom pyjom.videotoolbox import getVideoFrameIteratorWithFPS\nfrom typing import Literal\ngateway = \"http://localhost:8511/\"\nfrom pyjom.mathlib import superMean, superMax\nfrom lazero.utils.importers import cv2_custom_build_init\ncv2_custom_build_init()\nimport cv2\n# suggest you not to use this shit.\n# import math\nfrom pyjom.imagetoolbox import resizeImageWithPadding, scanImageWithWindowSizeAutoResize\nfrom lazero.filesystem import tmpdir, tmpfile\ntmpdirPath = \"/dev/shm/medialang/nsfw\"\nimport uuid\nwaitForServerUp(8511, \"nsfw nodejs server\")\nimport os\ntest_flag = \"nsfw_video\"\n# test_flag = \"nsfw_image\"\n# test_flag = \"scanning\"\n# test_flag = \"paddinging\"\nsource = \"/root/Desktop/works/pyjom/samples/video/cute_cat_gif.gif\"\nimport numpy as np\ndef processNSFWServerImageReply(reply):\n    mDict = {}\n    for elem in reply:\n        className, probability = elem[\"className\"], elem[\"probability\"]",
        "type": "code",
        "location": "/tests/unittest_nsfw_video_score.py:1-44"
    },
    "1957": {
        "file_id": 194,
        "content": "The code imports necessary libraries, initializes certain functions and variables, and defines the processNSFWServerImageReply function which processes image classification reply from the server. It is for testing NSFW detection in videos or images, with options to test different aspects such as scanning, padding, etc. Note that it may not be recommended to use some parts of the code.",
        "type": "comment"
    },
    "1958": {
        "file_id": 194,
        "content": "        mDict.update({className: probability})\n    return mDict\ndef processNSFWReportArray(\n    NSFWReportArray,\n    average_classes=[\"Neutral\"],\n    get_max_classes=[\"Drawing\", \"Porn\", \"Sexy\", \"Hentai\"],\n):\n    assert set(average_classes).intersection(set(get_max_classes)) == set()\n    NSFWReport = {}\n    for element in NSFWReportArray:\n        for key in element.keys():\n            NSFWReport[key] = NSFWReport.get(key, []) + [element[key]]\n    for average_class in average_classes:\n        NSFWReport[average_class] = superMean(NSFWReport.get(average_class, [0]))\n    for get_max_class in get_max_classes:\n        NSFWReport[get_max_class] = superMax(NSFWReport.get(get_max_class, [0]))\n    return NSFWReport\nfrom pyjom.commons import checkMinMaxDict\n# you can reuse this, really.\ndef NSFWFilter(\n    NSFWReport,\n    filter_dict={\n        \"Neutral\": {\"min\": 0.5},\n        \"Sexy\": {\"max\": 0.5},\n        \"Porn\": {\"max\": 0.5},\n        \"Hentai\": {\"max\": 0.5},\n        \"Drawing\": {\"max\": 0.5},\n    },\n    debug=False,\n):\n    for key in filter_dict:",
        "type": "code",
        "location": "/tests/unittest_nsfw_video_score.py:45-80"
    },
    "1959": {
        "file_id": 194,
        "content": "This code processes an NSFW report array and returns a filtered dictionary. It updates the dictionary with class names as keys and their corresponding probabilities. Then, it calculates the average and maximum scores for certain classes. Lastly, it applies filters to the resulting dictionary based on specified minimum or maximum threshold values for each class.",
        "type": "comment"
    },
    "1960": {
        "file_id": 194,
        "content": "        value = NSFWReport.get(key, 0)\n        key_filter = filter_dict[key]\n        result = checkMinMaxDict(value, key_filter)\n        if not result:\n            if debug:\n                print(\"not passing NSFW filter: %s\" % key)\n                print(\"value: %s\" % value)\n                print(\"filter: %s\" % str(key_filter))\n            return False\n    return True\nif test_flag == \"padding\":\n    for frame in getVideoFrameIteratorWithFPS(source, -1, -1, fps=1):\n        image = resizeImageWithPadding(frame, 1280, 720, border_type=\"replicate\")\n        # i'd like to view this.\n        cv2.imshow(\"PADDED\", image)\n        cv2.waitKey(0)\nelif test_flag == \"scanning\":\n    for frame in getVideoFrameIteratorWithFPS(source, -1, -1, fps=1):\n        scanned_array = scanImageWithWindowSizeAutoResize(\n            frame, 1280, 720, threshold=0.3\n        )\n        for index, image in enumerate(scanned_array):\n            cv2.imshow(\"SCANNED %d\" % index, image)\n            cv2.waitKey(0)\nelif test_flag == \"nsfw_video\":\n    # use another source?",
        "type": "code",
        "location": "/tests/unittest_nsfw_video_score.py:81-108"
    },
    "1961": {
        "file_id": 194,
        "content": "The code snippet checks if a video passes the NSFW filter based on certain key values, and then displays the video frames in different scenarios: when testing for padding, it shows each frame with padding; when testing for scanning, it displays each frame after scanning with a specified threshold; and if test_flag is set to \"nsfw_video\", it processes another source.",
        "type": "comment"
    },
    "1962": {
        "file_id": 194,
        "content": "    with tmpdir(path=tmpdirPath) as T:\n        responses = []\n        for frame in getVideoFrameIteratorWithFPS(source, -1, -1, fps=1):\n            padded_resized_frame = resizeImageWithPadding(\n                frame, 224, 224, border_type=\"replicate\"\n            )\n            # i'd like to view this.\n            basename = \"{}.jpg\".format(uuid.uuid4())\n            jpg_path = os.path.join(tmpdirPath, basename)\n            with tmpfile(path=jpg_path) as TF:\n                cv2.imwrite(jpg_path, padded_resized_frame)\n                files = {\"image\": (basename, open(jpg_path, \"rb\"), \"image/jpeg\")}\n                r = requests.post(\n                    gateway + \"nsfw\", files=files\n                )  # post gif? or just jpg?\n                try:\n                    response_json = r.json()\n                    response_json = processNSFWServerImageReply(response_json)\n                    # breakpoint()\n                    # print(\"RESPONSE:\", response_json)\n                    responses.append(\n                        response_json  # it contain 'messages'",
        "type": "code",
        "location": "/tests/unittest_nsfw_video_score.py:109-130"
    },
    "1963": {
        "file_id": 194,
        "content": "This code is looping through video frames, resizing and saving them as JPEGs in a temporary directory. It then posts each image to an API endpoint for NSFW content classification and appends the response JSON to a list of responses. The breakpoint and print statement are optional for debugging purposes.",
        "type": "comment"
    },
    "1964": {
        "file_id": 194,
        "content": "                    )  # there must be at least one response, i suppose?\n                except:\n                    import traceback\n                    traceback.print_exc()\n                    print(\"error when processing NSFW server response\")\n        NSFWReport = processNSFWReportArray(responses)\n        # print(NSFWReport)\n        # breakpoint()\n        result = NSFWFilter(NSFWReport)\n        if result:\n            print(\"NSFW test passed.\")\n            print(\"source %s\" % source)\n# we don't want drawing dogs.\n# [{'className': 'Neutral', 'probability': 0.9995943903923035}, {'className': 'Drawing', 'probability': 0.00019544694805517793}, {'className': 'Porn', 'probability': 0.00013213469355832785}, {'className': 'Sexy', 'probability': 6.839347042841837e-05}, {'className': 'Hentai', 'probability': 9.632151886762585e-06}]\nelif test_flag == \"nsfw_image\":\n    source = \"/root/Desktop/works/pyjom/samples/image/kitty_flash.bmp\"\n    # RESPONSE: [{'className': 'Neutral', 'probability': 0.9997681975364685}",
        "type": "code",
        "location": "/tests/unittest_nsfw_video_score.py:131-149"
    },
    "1965": {
        "file_id": 194,
        "content": "The code processes a server response for NSFW content classification and checks if the test passed. It uses the processNSFWReportArray function to analyze the responses and stores the result in the variable NSFWReport. If there's at least one response, it proceeds with the NSFWFilter function to evaluate the report. If the result is true, it prints \"NSFW test passed\" and source information. The code includes a case for the NSFW_IMAGE test flag and specifies a source file path.",
        "type": "comment"
    },
    "1966": {
        "file_id": 194,
        "content": ", {'className': 'Drawing', 'probability': 0.0002115015813615173}, {'className': 'Porn', 'probability': 1.3146535820851568e-05}, {'className': 'Hentai', 'probability': 4.075543984072283e-06}, {'className': 'Sexy', 'probability': 3.15313491228153e-06}]\n    # source = '/root/Desktop/works/pyjom/samples/image/pig_really.bmp'\n    # RESPONSE: [{'className': 'Neutral', 'probability': 0.9634107351303101}, {'className': 'Porn', 'probability': 0.0244674663990736}, {'className': 'Drawing', 'probability': 0.006115634460002184}, {'className': 'Hentai', 'probability': 0.003590137232095003}, {'className': 'Sexy', 'probability': 0.002416097791865468}]\n    # source = \"/root/Desktop/works/pyjom/samples/image/dog_with_text.bmp\"\n    # source = '/root/Desktop/works/pyjom/samples/image/dick2.jpeg'\n    # [{'className': 'Porn', 'probability': 0.7400921583175659}, {'className': 'Hentai', 'probability': 0.2109236866235733}, {'className': 'Sexy', 'probability': 0.04403943940997124}, {'className': 'Neutral', 'probability': 0.0034419416915625334}, {'className': 'Drawing', 'probability': 0.0015027812914922833}]",
        "type": "code",
        "location": "/tests/unittest_nsfw_video_score.py:149-154"
    },
    "1967": {
        "file_id": 194,
        "content": "This code demonstrates the results of a classification model for detecting different content categories in images. The provided examples show how the model predicts various probabilities for classes like 'Porn', 'Drawing', 'Hentai', and others, given specific image sources.",
        "type": "comment"
    },
    "1968": {
        "file_id": 194,
        "content": "    # source = '/root/Desktop/works/pyjom/samples/image/dick4.jpeg'\n    # RESPONSE: [{'className': 'Porn', 'probability': 0.8319052457809448}, {'className': 'Hentai', 'probability': 0.16578854620456696}, {'className': 'Sexy', 'probability': 0.002254955470561981}, {'className': 'Neutral', 'probability': 3.2827374525368214e-05}, {'className': 'Drawing', 'probability': 1.8473130694474094e-05}]\n    # source = '/root/Desktop/works/pyjom/samples/image/porn_shemale.jpeg'\n    # no good for this one. this is definitely some unacceptable shit, with just cloth wearing.\n    # RESPONSE: [{'className': 'Neutral', 'probability': 0.6256022453308105}, {'className': 'Hentai', 'probability': 0.1276213526725769}, {'className': 'Porn', 'probability': 0.09777139872312546}, {'className': 'Sexy', 'probability': 0.09318379312753677}, {'className': 'Drawing', 'probability': 0.05582122132182121}]\n    # source ='/root/Desktop/works/pyjom/samples/image/dick3.jpeg'\n    # [{'className': 'Porn', 'probability': 0.9784200787",
        "type": "code",
        "location": "/tests/unittest_nsfw_video_score.py:155-161"
    },
    "1969": {
        "file_id": 194,
        "content": "This code is testing the classification accuracy of an image classification model for NSFW content. The comments describe three test cases with different images and the corresponding classifications provided by the model, highlighting the need for improving the model's ability to accurately identify NSFW content.",
        "type": "comment"
    },
    "1970": {
        "file_id": 194,
        "content": "54425}, {'className': 'Hentai', 'probability': 0.01346961222589016}, {'className': 'Sexy', 'probability': 0.006554164923727512}, {'className': 'Neutral', 'probability': 0.0015426197787746787}, {'className': 'Drawing', 'probability': 1.354961841570912e-05}]\n    # a known source causing unwanted shits.\n    image = cv2.imread(source)\n    basename = \"{}.jpg\".format(uuid.uuid4())\n    jpg_path = os.path.join(tmpdirPath, basename)\n    with tmpfile(path=jpg_path) as TF:\n        # black padding will lower the probability of being porn.\n        padded_resized_frame = resizeImageWithPadding(image, 224, 224)\n        # RESPONSE: [{'className': 'Neutral', 'probability': 0.6441782116889954}, {'className': 'Porn', 'probability': 0.3301379978656769}, {'className': 'Sexy', 'probability': 0.010329035110771656}, {'className': 'Hentai', 'probability': 0.010134727694094181}, {'className': 'Drawing', 'probability': 0.005219993181526661}]\n        # padded_resized_frame = resizeImageWithPadding(image, 224, 224,border_type='replicate')",
        "type": "code",
        "location": "/tests/unittest_nsfw_video_score.py:161-170"
    },
    "1971": {
        "file_id": 194,
        "content": "This code reads an image from a known source, generates a unique filename, saves it temporarily, pads and resizes the image for classification, and then passes the processed image to the model for probability prediction. The goal is to lower the probability of being classified as porn by adding black padding around the image before processing.",
        "type": "comment"
    },
    "1972": {
        "file_id": 194,
        "content": "        # RESPONSE: [{'className': 'Neutral', 'probability': 0.6340386867523193}, {'className': 'Porn', 'probability': 0.3443007171154022}, {'className': 'Sexy', 'probability': 0.011606302112340927}, {'className': 'Hentai', 'probability': 0.006618513725697994}, {'className': 'Drawing', 'probability': 0.0034359097480773926}]\n        # neutral again? try porn!\n        cv2.imwrite(jpg_path, padded_resized_frame)\n        files = {\"image\": (basename, open(jpg_path, \"rb\"), \"image/jpeg\")}\n        r = requests.post(gateway + \"nsfw\", files=files)  # post gif? or just jpg?\n        print(\"RESPONSE:\", r.json())\nelse:\n    raise Exception(\"unknown test_flag: %s\" % test_flag)\n# you can only post gif now, or you want to post some other formats?\n# if you post shit, you know it will strentch your picture and produce unwanted shits.",
        "type": "code",
        "location": "/tests/unittest_nsfw_video_score.py:171-180"
    },
    "1973": {
        "file_id": 194,
        "content": "Code snippet is performing the following actions: \n1. Storing response from API containing classification probabilities for video.\n2. Writing frame to JPG format and posting it to gateway as non-sexual content using requests.\n3. If unknown test_flag, raising exception.\n4. Note mentions that only GIF can be posted now and caution about stretching pictures.",
        "type": "comment"
    },
    "1974": {
        "file_id": 195,
        "content": "/tests/unittest_get_subtid_name_and_majortid_name.py",
        "type": "filepath"
    },
    "1975": {
        "file_id": 195,
        "content": "The function `getMajorMinorTopicMappings` retrieves major and minor topic IDs and names, storing them in the `majorMinorMappings` dictionary. The code uses this function to get the associated topics for a given tid, formats them into tags, and prints the result along with the tid for topic ID 1.",
        "type": "summary"
    },
    "1976": {
        "file_id": 195,
        "content": "from bilibili_api import search\nBSP = search.bilibiliSearchParams\ndef getMajorMinorTopicMappings(debug: bool = False):\n    majorMinorMappings = {}\n    for key, value in BSP.all.tids.__dict__.items():\n        try:\n            major_tid = value.tid\n            if debug:\n                print(\"MAJOR\", key, major_tid)\n            content = {\"major\": {\"tid\": major_tid, \"name\": key}}\n            majorMinorMappings.update(\n                {major_tid: content, key: content, str(major_tid): content}\n            )\n            for subkey, subvalue in value.__dict__.items():\n                if subkey != \"tid\" and type(subvalue) == int:\n                    if debug:\n                        print(\"MINOR\", subkey, subvalue)\n                    content = {\n                        \"major\": {\"tid\": major_tid, \"name\": key},\n                        \"minor\": {\"tid\": subvalue, \"name\": subkey},\n                    }\n                    majorMinorMappings.update(\n                        {subvalue: content, subkey: content, str(subvalue): content}",
        "type": "code",
        "location": "/tests/unittest_get_subtid_name_and_majortid_name.py:1-26"
    },
    "1977": {
        "file_id": 195,
        "content": "This function `getMajorMinorTopicMappings` retrieves major and minor topic IDs and names from `BSP.all.tids` dictionary, storing them in `majorMinorMappings` dictionary for further use. It also prints the major and minor topics if debug mode is enabled.",
        "type": "comment"
    },
    "1978": {
        "file_id": 195,
        "content": "                    )\n        except:\n            pass\n    return majorMinorMappings\ndef getTagStringFromTid(tid):\n    majorMinorTopicMappings = getMajorMinorTopicMappings()\n    topic = majorMinorTopicMappings.get(tid, None)\n    tags = []\n    if topic:\n        majorTopic = topic.get(\"major\", {}).get(\"name\", None)\n        minorTopic = topic.get(\"minor\", {}).get(\"name\", None)\n        if majorTopic:\n            tags.append(majorTopic)\n            if minorTopic:\n                tags.append(minorTopic)\n    return \",\".join(tags)\ntid = 1\ntagString = getTagStringFromTid(tid)\nprint(tid, tagString)",
        "type": "code",
        "location": "/tests/unittest_get_subtid_name_and_majortid_name.py:27-49"
    },
    "1979": {
        "file_id": 195,
        "content": "This code retrieves the major and minor topics associated with a given topic ID (tid) using the getMajorMinorTopicMappings() function. It then formats these topics into a comma-separated string of tags. If there are both a major and minor topic, they are concatenated in that order, else if only one exists, it is printed alone. Finally, the tid and associated tagString are printed to the console for the given topic ID 1.",
        "type": "comment"
    },
    "1980": {
        "file_id": 196,
        "content": "/tests/unittest_update_peewee_while_get.py",
        "type": "filepath"
    },
    "1981": {
        "file_id": 196,
        "content": "Creating a Peewee database, defining a BilibiliUser model, and updating a specific user's username.",
        "type": "summary"
    },
    "1982": {
        "file_id": 196,
        "content": "dbpath = \"test.db\"\nfrom peewee import *\nclass BilibiliUser(Model):\n    username = CharField()\n    user_id = IntegerField(unique=True)\n    is_mine = BooleanField(default=False)\n    followers = IntegerField(\n        null=True\n    )  # how to get that? every time you get some video you do this shit? will get you blocked.\n    # well you can check it later.\n    avatar = CharField(null=True)  # warning! charfield max length is 255\ndb = SqliteDatabase(dbpath)\ndb.create_tables([BilibiliUser])\nimport uuid\nusername = str(uuid.uuid4())\n# u, _ = BilibiliUser.get_and_update_or_create(username=username, user_id=1)\nBilibiliUser.update(username=username).where(BilibiliUser.user_id == 1).execute()\n# why don't you update? need i delete it manually?\nu = BilibiliUser.get(user_id=1)\nprint(\"current username:\", username)\nprint(\"fetched username:\", u.username)",
        "type": "code",
        "location": "/tests/unittest_update_peewee_while_get.py:1-30"
    },
    "1983": {
        "file_id": 196,
        "content": "Creating a Peewee database, defining a BilibiliUser model, and updating a specific user's username.",
        "type": "comment"
    },
    "1984": {
        "file_id": 197,
        "content": "/tests/unittest_houghline_dog_blur_detection.py",
        "type": "filepath"
    },
    "1985": {
        "file_id": 197,
        "content": "The code imports libraries, reads an image, applies blur detection and edge detection, displays edges with lines based on Hough line detection using OpenCV, waits for a key press to close the window.",
        "type": "summary"
    },
    "1986": {
        "file_id": 197,
        "content": "from lazero.utils.importers import cv2_custom_build_init\ncv2_custom_build_init()\nimport cv2\nimport numpy as np\n# command used for reading an image from the disk, cv2.imread function is used\nimagePath = \"/root/Desktop/works/pyjom/samples/image/dog_blue_sky_split_line.png\"\n# cannot find image without dark/black boundaries.\n# use blur detection, both for blur area removal and motion blur detection for key frame sampling/filtering\n# tool for finding non-blur based black borders:\n# ffmpeg -loop 1 -i /root/Desktop/works/pyjom/samples/image/dog_blue_sky_split_line.png -t 15 -vf cropdetect -f null -\n# maybe you can change the seconds to something shorter.\nimg1 = cv2.imread(imagePath)\n# gray1 = cv2.cvtColor(img1,cv2.COLOR_BGR2GRAY)\n# edges1 = cv2.Canny(gray1,50,150,apertureSize=3)\n# blurred = cv2.GaussianBlur(img1, (5, 5), 0)\nblurred = cv2.bilateralFilter(img1, 15, 75, 75)\nedges1 = cv2.Canny(blurred, 20, 210, apertureSize=3)\ncv2.imshow(\"EDGE\", edges1)\ncv2.waitKey(0)\nlines1 = cv2.HoughLines(edges1, 1, np.pi / 180, 200)  # wtf?",
        "type": "code",
        "location": "/tests/unittest_houghline_dog_blur_detection.py:1-28"
    },
    "1987": {
        "file_id": 197,
        "content": "The code imports necessary libraries, reads an image from disk, applies blur detection using bilateral filtering to remove blur and detect motion blur, converts the image to grayscale, detects edges using Canny edge detection, displays the edges, applies HoughLines to find lines in the image, and then waits for a key press to close the window.",
        "type": "comment"
    },
    "1988": {
        "file_id": 197,
        "content": "for rho, theta in lines1[0]:\n    a = np.cos(theta)\n    b = np.sin(theta)\n    x = a * rho\n    y = b * rho\n    x_1 = int(x + 1000 * (-b))\n    y_1 = int(y + 1000 * (a))\n    x_2 = int(x - 1000 * (-b))\n    y_2 = int(y - 1000 * (a))\n    cv2.line(img1, (x_1, y_1), (x_2, y_2), (0, 0, 255), 2)\n# Creation of a GUI window in order to display the image on the screen\ncv2.imshow(\"line detection\", img1)\n# cv2.waitKey method used for holding the window on screen\ncv2.waitKey(0)\ncv2.destroyAllWindows()",
        "type": "code",
        "location": "/tests/unittest_houghline_dog_blur_detection.py:29-43"
    },
    "1989": {
        "file_id": 197,
        "content": "This code generates lines on an image based on Hough line detection. It iterates over the lines, calculates the coordinates of endpoints, and draws lines using OpenCV. The GUI window displays the image with the drawn lines, holds it open for any key press (cv2.waitKey), then destroys all windows upon closing.",
        "type": "comment"
    },
    "1990": {
        "file_id": 198,
        "content": "/tests/unittest_paddlehub_animal_resnet.py",
        "type": "filepath"
    },
    "1991": {
        "file_id": 198,
        "content": "This code uses PaddleHub's Animal ResNet model to classify dogs and cats from video frames, post-processing the results to test accuracy with various images including non-animal ones.",
        "type": "summary"
    },
    "1992": {
        "file_id": 198,
        "content": "# source = \"/root/Desktop/works/pyjom/samples/video/kitty_flash_15fps.gif\"  # check that kitty video!\nsource = \"/root/Desktop/works/pyjom/samples/video/cute_cat_gif.gif\"  # another kitty!\nfrom test_commons import *\nfrom pyjom.videotoolbox import getVideoFrameIteratorWithFPS\nfrom pyjom.imagetoolbox import resizeImageWithPadding\nimport paddlehub as hub\nimport cv2\ndef labelFileReader(filename):\n    with open(filename, \"r\") as f:\n        content = f.read()\n        content = content.split(\"\\n\")\n        content = [elem.replace(\"\\n\", \"\").strip() for elem in content]\n        content = [elem for elem in content if len(elem) > 0]\n    return content\ndog_suffixs = [\"狗\", \"犬\", \"梗\"]\ncat_suffixs = [\"猫\"]  # ends with this, and not containing forbidden words.\ndog_labels = labelFileReader(\n    \"/root/Desktop/works/pyjom/tests/animals_paddlehub_classification_resnet/dogs.txt\"\n)\ncat_labels = labelFileReader(\n    \"/root/Desktop/works/pyjom/tests/animals_paddlehub_classification_resnet/cats.txt\"\n)\nforbidden_words = [\n    \"灵猫\",\n    \"熊猫\",",
        "type": "code",
        "location": "/tests/unittest_paddlehub_animal_resnet.py:1-32"
    },
    "1993": {
        "file_id": 198,
        "content": "This code imports necessary libraries and defines constants for labels and forbidden words. It reads the dog and cat labels from separate text files, and later on, it will use these labels to classify animals in a video. The forbidden words are likely used to exclude certain categories of animals that may appear similar to cats or dogs but should not be confused with them.",
        "type": "comment"
    },
    "1994": {
        "file_id": 198,
        "content": "    \"猫狮\",\n    \"猫头鹰\",\n    \"丁丁猫儿\",\n    \"绿猫鸟\",\n    \"猫鼬\",\n    \"猫鱼\",\n    \"玻璃猫\",\n    \"猫眼\",\n    \"猫蛱蝶\",\n]\ndef dog_cat_name_recognizer(name):\n    if name in dog_labels:\n        return \"dog\"\n    elif name in cat_labels:\n        return \"cat\"\n    elif name not in forbidden_words:\n        for dog_suffix in dog_suffixs:\n            if name.endswith(dog_suffix):\n                return \"dog\"\n        for cat_suffix in cat_suffixs:\n            if name.endswith(cat_suffix):\n                return \"cat\"\n    return None\nfrom lazero.utils.logger import sprint\nclassifier = hub.Module(name=\"resnet50_vd_animals\")\n# 'ResNet50vdAnimals' object has no attribute 'gpu_predictor'\n# no gpu? really?\n# test_flag = \"video\"\ntest_flag = \"image\"\ndef paddleAnimalDetectionResultToList(result):\n    resultDict = result[0]\n    resultList = [(key, value) for key, value in resultDict.items()]\n    resultList.sort(key=lambda item: -item[1])\n    return resultList\ndef translateResultListToDogCatList(resultList):\n    final_result_list = []\n    for name, confidence in resultList:",
        "type": "code",
        "location": "/tests/unittest_paddlehub_animal_resnet.py:33-78"
    },
    "1995": {
        "file_id": 198,
        "content": "Function dog_cat_name_recognizer identifies if a given name is of a dog or cat by checking it against pre-defined labels. If the name does not fit into these categories, it further checks for common suffixes to classify as either a dog or cat. The code imports necessary modules and sets up variables for testing purposes before defining functions for post-processing the result and translating it into a dog/cat list format.",
        "type": "comment"
    },
    "1996": {
        "file_id": 198,
        "content": "        new_name = dog_cat_name_recognizer(name)\n        final_result_list.append((new_name, confidence))\n    return final_result_list\nif test_flag == \"video\":\n    for frame in getVideoFrameIteratorWithFPS(source, -1, -1, fps=1):\n        padded_resized_frame = resizeImageWithPadding(\n            frame, 224, 224, border_type=\"replicate\"\n        )  # pass the test only if three of these containing 'cats'\n        result = classifier.classification(\n            images=[padded_resized_frame], top_k=3, use_gpu=False\n        )  # check it?\n        resultList = paddleAnimalDetectionResultToList(result)\n        final_result_list = translateResultListToDogCatList(resultList)\n        sprint(\"RESULT LIST:\", final_result_list)\n        # RESULT: [{'美国银色短毛猫': 0.23492032289505005, '虎斑猫': 0.14728288352489471, '美国银虎斑猫': 0.13097935914993286}]\n        # so what is the major categories?\n        # thanks to chinese, we are never confused.\n        # check the labels, shall we?\n        # what about samoyed?\n        # sprint(\"RESULT:\", result)",
        "type": "code",
        "location": "/tests/unittest_paddlehub_animal_resnet.py:79-100"
    },
    "1997": {
        "file_id": 198,
        "content": "This code is using PaddleHub's Animal ResNet model to classify frames from a video source, identifying either dogs or cats. The final results are translated to a list of dog and cat names along with their respective confidences. This code checks the major categories in the final result and verifies if \"samoyed\" is included.",
        "type": "comment"
    },
    "1998": {
        "file_id": 198,
        "content": "        breakpoint()\nelif test_flag == \"image\":\n    # source = \"/root/Desktop/works/pyjom/samples/image/samoyed.jpeg\"\n    # [('dog', 0.8835851550102234), ('dog', 0.08754527568817139), ('dog', 0.008648859336972237)]\n    # source = \"/root/Desktop/works/pyjom/samples/image/dog_saturday_night.jpg\"\n    #  [(None, 0.33663231134414673), ('dog', 0.32254937291145325), ('dog', 0.0494903139770031)]\n    # not animal? wtf?\n    # source = \"/root/Desktop/works/pyjom/samples/image/porn_shemale.jpeg\" # definitely not animal\n    # [(None, 0.9894463419914246), ('dog', 1.564090962347109e-05), ('dog', 1.3550661606132053e-05)]\n    # source = \"/root/Desktop/works/pyjom/samples/image/is_this_duck.bmp\"\n    # [(None, 0.9864748120307922), ('dog', 1.2670795513258781e-05), (None, 9.569253961672075e-06)]\n    # source = \"/root/Desktop/works/pyjom/samples/image/pig_really.bmp\" # it's really a dog\n    # [(None, 0.35919442772865295), ('dog', 0.16199783980846405), ('dog', 0.07987158000469208)]\n    # source = \"/root/Desktop/works/pyjom/samples/image/miku_on_green.png\"",
        "type": "code",
        "location": "/tests/unittest_paddlehub_animal_resnet.py:101-114"
    },
    "1999": {
        "file_id": 198,
        "content": "The code includes various image source paths and the corresponding classification outputs. The script seems to be testing the accuracy of an animal recognition model by inputting different images, including some non-animal images for reference. Some images are misclassified or not classified at all, highlighting potential areas for improvement in the model.",
        "type": "comment"
    }
}