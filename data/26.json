{
    "2600": {
        "file_id": 282,
        "content": "# target_video = \"intro_video.mp4\"\n# os.system(\"rm {}\".format(target_video))\n# kill_script()\n# # v = mdict[k]\n# v = intro_text\n# gen_typography_part1(v)\n# # target_video = \"/\".join([typ_0,\"{}.mp4\".format(k)])\n# gen_typography_part2(v,bgm_path,target_video)\n# kill_script()\nbgms = [\"you_got_me_acc.wav\", \"tarot_desc_acc.wav\"]\n# outro_text = \"\"\"今天的你运气不错哦～\n# 喜欢的话请分享点赞，一键三联哦～\"\"\"\n# bgm_path = bgms[0]\n# target_video = \"outro_video.mp4\"\n# os.system(\"rm {}\".format(target_video))\n# kill_script()\n# # v = mdict[k]\n# v = outro_text\n# gen_typography_part1(v)\n# # target_video = \"/\".join([typ_0,\"{}.mp4\".format(k)])\n# gen_typography_part2(v,bgm_path,target_video)\n# kill_script()\nimport random\n# for k in mdict.keys():\n#     if k !=16:\n#         continue\n#     kill_script()\n#     v = mdict[k]\n#     gen_typography_part1(v)\n#     target_video = \"/\".join([typ_0,\"{}.mp4\".format(k)])\n#     gen_typography_part2(v,random.choice(bgms),target_video)\n#     kill_script()\n# for k in smdict.keys():\n#     v = smdict[k]\n#     # kill_script()\n#     # v = mdict[k]",
        "type": "code",
        "location": "/tests/bilibili_practices/bilibili_tarot/all_typography.py:48-93"
    },
    "2601": {
        "file_id": 282,
        "content": "The code removes the target video, generates typography for intro and outro text using different background music, and randomly selects a background music from the given list for each card in mdict and smdict.",
        "type": "comment"
    },
    "2602": {
        "file_id": 282,
        "content": "#     gen_typography_part1(v)\n#     target_video = \"/\".join([typ_1,\"{}.mp4\".format(k)])\n#     gen_typography_part2(v,random.choice(bgms),target_video)\n#     kill_script()",
        "type": "code",
        "location": "/tests/bilibili_practices/bilibili_tarot/all_typography.py:94-97"
    },
    "2603": {
        "file_id": 282,
        "content": "This code section generates typography for a video and stores it in a specific format. It first calls a function `gen_typography_part1` passing some parameter v, then combines the typography name with the video number as the file name. The next step is to call another function `gen_typography_part2`, which takes two parameters: v and a randomly chosen bgm (background music) from some list of choices. It also passes the target video file as an argument. Lastly, it calls the `kill_script()` function to terminate the script execution.",
        "type": "comment"
    },
    "2604": {
        "file_id": 283,
        "content": "/tests/bilibili_practices/bilibili_science_subtitle_with_cn_voice/web_translator.py",
        "type": "filepath"
    },
    "2605": {
        "file_id": 283,
        "content": "The code defines a translator function that randomly selects from multiple translation services to convert English text to either \"zh\" or \"zh-CHS\". It uses the \"translators\" module and imports random for selecting the translation service and language. The code also allows for different combinations of translation services to be tested by uncommenting specific lines in the mtranslators list. If an error occurs during translation, it prints the exception stack trace using traceback.",
        "type": "summary"
    },
    "2606": {
        "file_id": 283,
        "content": "import translators as ts\n# translator = \n# mtranslators = [ts.sogou] #this is pure shit.\n# mtranslators = [ts.baidu,ts.sogou]\n# mtranslators = [ts.baidu,ts.sogou,ts.iciba]\nmtranslators = [ts.youdao,ts.baidu,ts.alibaba] # no yandex, tencent, sogou.\n# mtranslators = [ts.baidu,ts.iciba]\nimport random\ndef translator(text):\n    randomLang = [\"zh\",\"zh-CHS\"]\n    from_language = \"en\"\n    # lang = random.choice(randomLang)\n    while True:\n        t = random.choice(mtranslators)\n        # print(type(translator))\n        for rl in randomLang:\n            try:\n                result = t(text,from_language=from_language,to_language=rl)\n                # if len(result) < 3:\n                #     print(t)\n                #     breakpoint()\n                return result\n            except:\n                import traceback\n                traceback.print_exc()",
        "type": "code",
        "location": "/tests/bilibili_practices/bilibili_science_subtitle_with_cn_voice/web_translator.py:1-27"
    },
    "2607": {
        "file_id": 283,
        "content": "The code defines a translator function that randomly selects from multiple translation services to convert English text to either \"zh\" or \"zh-CHS\". It uses the \"translators\" module and imports random for selecting the translation service and language. The code also allows for different combinations of translation services to be tested by uncommenting specific lines in the mtranslators list. If an error occurs during translation, it prints the exception stack trace using traceback.",
        "type": "comment"
    },
    "2608": {
        "file_id": 284,
        "content": "/tests/bilibili_practices/bilibili_science_subtitle_with_cn_voice/translate_srt.py",
        "type": "filepath"
    },
    "2609": {
        "file_id": 284,
        "content": "Code reads an SRT file, translates each line of the content using a web translator, wraps the translated lines to meet a certain character limit, and then saves the results in a new SRT file. The process involves parsing the input SRT file using the \"srt\" library, translating text with \"web_translator\", and modifying the line wrapping for better readability.",
        "type": "summary"
    },
    "2610": {
        "file_id": 284,
        "content": "src = \"en_.srt\"\nfinal_srt = \"zh_translated.srt\"\nimport srt\nwrap_limit = 20\nsource_srt = open(src, \"r\",encoding=\"utf-8\").read()\nssrt = srt.parse(source_srt)\nfrom web_translator import translator\nimport math\ndef wrapLine(line):\n    lines = [line[x*wrap_limit:(x+1)*wrap_limit] for x in range(math.ceil(len(line)/wrap_limit))]\n    return \"\\n\".join(lines)\ndef fixline(line):\n    notEndings = [\"。\",\"，\"]\n    for x in notEndings:\n        if line.endswith(x): return line[:-1]\n    return line\nnew_ssrt = []\nfor line in ssrt:\n    # print(line)\n    start = line.start\n    end = line.end # timedelta.\n    content = line.content\n    index = line.index\n    unwrapped_content = content.replace(\"\\n\",\" \")\n    result = translator(unwrapped_content)\n    result = fixline(result)\n    print(result)\n    line.content = result\n    new_ssrt.append(line)\n    # wrapped = wrapLine(result)\n    # print(wrapped)\n    # print(start, end, content, index)\nfinal_content = srt.compose(new_ssrt)\nwith open(final_srt,\"w+\",encoding=\"utf-8\") as f:\n    f.write(final_content)",
        "type": "code",
        "location": "/tests/bilibili_practices/bilibili_video_translate/translate_srt.py:1-45"
    },
    "2611": {
        "file_id": 284,
        "content": "Code reads an SRT file, translates each line of the content using a web translator, wraps the translated lines to meet a certain character limit, and then saves the results in a new SRT file. The process involves parsing the input SRT file using the \"srt\" library, translating text with \"web_translator\", and modifying the line wrapping for better readability.",
        "type": "comment"
    },
    "2612": {
        "file_id": 285,
        "content": "/tests/bilibili_practices/bilibili_science_subtitle_with_cn_voice/test2.py",
        "type": "filepath"
    },
    "2613": {
        "file_id": 285,
        "content": "The code imports the os module and defines two commands - one to install yt-dlp using pip3, another to download subtitles from a YouTube video with yt-dlp. It then iterates over each command in the list and runs them using os.system(). This will result in yt-dlp being installed and the subtitles being downloaded for the specified YouTube video.",
        "type": "summary"
    },
    "2614": {
        "file_id": 285,
        "content": "import os\ncommands = [\"pip3 install yt-dlp\",'yt-dlp --write-subs --convert-subtitles srt \"https://m.youtube.com/watch?v=At7ORzmAaT4\"'] # get recommendation this time.\n# we will still get many videoId from curl.\nfor c in commands:\n    os.system(c)",
        "type": "code",
        "location": "/tests/bilibili_practices/bilibili_science_subtitle_with_cn_voice/test2.py:1-8"
    },
    "2615": {
        "file_id": 285,
        "content": "The code imports the os module and defines two commands - one to install yt-dlp using pip3, another to download subtitles from a YouTube video with yt-dlp. It then iterates over each command in the list and runs them using os.system(). This will result in yt-dlp being installed and the subtitles being downloaded for the specified YouTube video.",
        "type": "comment"
    },
    "2616": {
        "file_id": 286,
        "content": "/tests/bilibili_practices/bilibili_science_subtitle_with_cn_voice/test.py",
        "type": "filepath"
    },
    "2617": {
        "file_id": 286,
        "content": "Code installs yt-dlp and downloads subtitles from a YouTube video, then converts them to SRT format. Optionally, it also enables sponsorblock-mark for highlighting ad breaks in the output.",
        "type": "summary"
    },
    "2618": {
        "file_id": 286,
        "content": "import os\ncommands = [\"pip3 install yt-dlp\",'yt-dlp --write-subs --convert-subtitles srt  \"https://m.youtube.com/watch?v=At7ORzmAaT4\"']\n# commands = [\"pip3 install yt-dlp\",'yt-dlp --write-subs --convert-subtitles srt --sponsorblock-mark poi_highlight \"https://m.youtube.com/watch?v=At7ORzmAaT4\"']\n# this will mark the highlights.\nfor c in commands:\n    os.system(c)",
        "type": "code",
        "location": "/tests/bilibili_practices/bilibili_science_subtitle_with_cn_voice/test.py:1-8"
    },
    "2619": {
        "file_id": 286,
        "content": "Code installs yt-dlp and downloads subtitles from a YouTube video, then converts them to SRT format. Optionally, it also enables sponsorblock-mark for highlighting ad breaks in the output.",
        "type": "comment"
    },
    "2620": {
        "file_id": 287,
        "content": "/tests/bilibili_practices/bilibili_science_subtitle_with_cn_voice/init.sh",
        "type": "filepath"
    },
    "2621": {
        "file_id": 287,
        "content": "This script initializes a Kaggle kernel, checks its status, and sets proxy environment variables to download at maximum speed.",
        "type": "summary"
    },
    "2622": {
        "file_id": 287,
        "content": "# kaggle kernels init # we have it do not fuck up again\n# code/jessysisca/some-yt-stuff \n# kaggle kernels push\nkaggle kernels status jessysisca/test-of-yt-dlp2\n# jessysisca/some-yt-stuff has status \"complete\"\n# root@alpharetta ~/android_connect_scrcpy_patch# \n# kaggle kernels status jessysisca/test-of-yt-dlp\n# jessysisca/test-of-yt-dlp has status \"running\"\n# after it is done, we pull back all shit.\n# skip all proxies.\n# export http_proxy=\"\"\n# export https_proxy=\"\"\n# kaggle kernels output jessysisca/test-of-yt-dlp2 # what is the freaking speed?\n# not too slow.",
        "type": "code",
        "location": "/tests/bilibili_practices/bilibili_science_subtitle_with_cn_voice/init.sh:1-14"
    },
    "2623": {
        "file_id": 287,
        "content": "This script initializes a Kaggle kernel, checks its status, and sets proxy environment variables to download at maximum speed.",
        "type": "comment"
    },
    "2624": {
        "file_id": 288,
        "content": "/tests/bilibili_practices/bilibili_science_subtitle_with_cn_voice/get_ytInitialData.py",
        "type": "filepath"
    },
    "2625": {
        "file_id": 288,
        "content": "This code uses BeautifulSoup and JavaScript libraries to extract HTML data, processes it into a JSON object with view count and video length updates.",
        "type": "summary"
    },
    "2626": {
        "file_id": 288,
        "content": "target = \"curl_dump_youtube.html\"\nfrom bs4 import BeautifulSoup\n# this is m.youtube.com/watch?v={videoId}\n# import esprima\nimport js2py\nsoup = open(target,\"r\",encoding=\"utf-8\").read()\nsoup = BeautifulSoup(soup,features=\"lxml\")\nscripts = soup.find_all(\"script\")\njsfunc = lambda x: \"function f9x() { \"+x+ \"  \\n return ytInitialData;}\"\njsfunc2 = lambda x: \"function f9x() { \"+x+ \"  \\n return ytInitialPlayerResponse;}\"\n# breakpoint()\nfrom commons import *\ndata = None\ndata2 = None\nfor script in scripts:\n    content = script.string\n    if content is not None:\n        if \"var ytInitialPlayerResponse = {\" in content:\n            print(\"HAS DATA\") # only one.\n            # script_obj = esprima.parse(content)\n            script_obj = jsfunc2(content)\n            # print(script_obj)\n            obj = js2py.eval_js(script_obj)\n            # print(obj)\n            data2 = obj() # need a json walker, from pyjom.\n            # breakpoint()\n    # print(content)\nfor script in scripts:\n    content = script.string\n    if content is not None:",
        "type": "code",
        "location": "/tests/bilibili_practices/bilibili_science_subtitle_with_cn_voice/get_ytInitialData.py:1-40"
    },
    "2627": {
        "file_id": 288,
        "content": "This code retrieves HTML from a specific target file, parses it using BeautifulSoup, and searches for scripts containing \"ytInitialData\" or \"ytInitialPlayerResponse\". It then uses JavaScript conversion libraries to extract the data from these scripts as Python objects. The data is stored in variables 'data' and 'data2', respectively.",
        "type": "comment"
    },
    "2628": {
        "file_id": 288,
        "content": "        if \"var ytInitialData = {\" in content:\n            print(\"HAS DATA\") # only one.\n            # script_obj = esprima.parse(content)\n            script_obj = jsfunc(content)\n            # print(script_obj)\n            obj = js2py.eval_js(script_obj)\n            # print(obj)\n            data = obj() # need a json walker, from pyjom.\n            # breakpoint()\n    # print(content)\n    # print(\"================================\")\n#     # breakpoint()\ndata_dict =  data.to_dict()\ndata2_dict =  data2.to_dict()\n# print(type(data))\n# breakpoint()\ntarget1 = [\"viewCountText\",\"lengthText\",\"publishedTimeText\"]\ntargets = [\"videoId\", \"simpleText\"]\ninits = ['contents', 'twoColumnWatchNextResults', 'secondaryResults', 'secondaryResults', 'results']\n# inits2 = ['contents', 'twoColumnWatchNextResults', 'secondaryResults', 'secondaryResults', 'results']\nends2 = {\"title\":['compactVideoRenderer', 'title', 'simpleText'],\"viewCountText\": ['compactVideoRenderer', 'viewCountText', 'simpleText'],\"publishTime\":['compactVideoRe",
        "type": "code",
        "location": "/tests/bilibili_practices/bilibili_science_subtitle_with_cn_voice/get_ytInitialData.py:41-67"
    },
    "2629": {
        "file_id": 288,
        "content": "This code checks if the content contains \"var ytInitialData = {\" and then parses it using jsfunc, converts to Python object with js2py, extracts data, converts it to dictionaries, and defines some target variables.",
        "type": "comment"
    },
    "2630": {
        "file_id": 288,
        "content": "nderer', 'publishedTimeText', 'simpleText'],\"lengthText\":['compactVideoRenderer', 'lengthText', 'simpleText'],\"videoId\":['compactVideoRenderer', 'videoId']}\nvideoDetails = data2_dict[\"videoDetails\"]\nvideoDetails = {k:videoDetails[k] for k in [\"viewCount\",\"author\",\"keywords\",\"channelId\",\"shortDescription\",\"lengthSeconds\",\"videoId\",\"title\"]}\n# \"https://i.ytimg.com/vi_webp/{videoId}/maxresdefault.webp # default cover.\nvideoDicts = {}\nfor key, content in json.walk(data_dict):\n    # print(key)\n    final_key = key[-1]\n    if final_key in targets:\n        if list_startswith(key,inits):\n            for k in ends2.keys():\n                v = ends2[k]\n                if list_endswith(key,v):\n                    valueType = k\n                    value = content\n                    valueIndex = key[len(inits)]\n                    if valueIndex not in videoDicts.keys():\n                        videoDicts[valueIndex] = {}\n                    # print(valueIndex,valueType,value)\n                    videoDicts[valueIndex].update({valueType:value})",
        "type": "code",
        "location": "/tests/bilibili_practices/bilibili_science_subtitle_with_cn_voice/get_ytInitialData.py:67-88"
    },
    "2631": {
        "file_id": 288,
        "content": "This code appears to extract specific data from a JSON object, specifically looking for keys that match certain endings and initials. The extracted data is then stored in a dictionary called \"videoDicts\" with the index as the key and the type and value of the data as the values. The purpose seems to be extracting specific information from the given JSON data, potentially for further use or processing.",
        "type": "comment"
    },
    "2632": {
        "file_id": 288,
        "content": "                    break\n        # print(key)  # i want to know the views of these.\n    # breakpoint()\ndef getViewCount(vc): return vc.replace(\",\",\"\").split(\" \")[0]\ndef getLengthSeconds(lt):\n    lt0 = lt.split(\":\")\n    assert len(lt0) <=5 # no more than week please?\n    dicIndex = {0:1,1:60,2:60*60,3:60*60*24,4:60*60*24*7}\n    seconds = 0\n    for i,v in enumerate(reversed(lt0)):\n        vn = int(v)\n        vs = vn*dicIndex[i]\n        seconds += vs\n    return str(seconds)\nfor k in videoDicts.keys():\n    v = videoDicts[k]\n    viewCount = getViewCount(v[\"viewCountText\"])\n    v.update({\"viewCount\":viewCount})\n    lengthSeconds = getLengthSeconds(v[\"lengthText\"])\n    v.update({\"lengthSeconds\":lengthSeconds})\n    print(v)\n    # for k0 in ends2.keys():\n    #     assert k0 in v.keys()\nprint(videoDetails)",
        "type": "code",
        "location": "/tests/bilibili_practices/bilibili_science_subtitle_with_cn_voice/get_ytInitialData.py:89-116"
    },
    "2633": {
        "file_id": 288,
        "content": "This code iterates over the 'videoDicts' dictionary, extracting and updating the view count and video length (in seconds) for each video. The extracted information is stored as values in the dictionary with keys \"viewCount\" and \"lengthSeconds\". Finally, it prints the updated 'videoDicts' dictionary and the 'videoDetails'.",
        "type": "comment"
    },
    "2634": {
        "file_id": 289,
        "content": "/tests/bilibili_practices/bilibili_science_subtitle_with_cn_voice/download_when_complete.py",
        "type": "filepath"
    },
    "2635": {
        "file_id": 289,
        "content": "The code is using the subprocess module to periodically check the status of a Kaggle kernel. If the status is \"running\" or \"complete\", it executes a final command and sets a lock file. It continues checking until the status is one of the valid ones or an unknown status occurs, in which case it breaks the loop.",
        "type": "summary"
    },
    "2636": {
        "file_id": 289,
        "content": "import parse\nimport subprocess\nimport time\nimport os\n# import pathlib\ndownload_lock = \".kaggle_downloaded\"\nif os.path.exists(download_lock):\n    print(\"already fetched content.\")\nwait_duration = 60\nformatx = '{a} has status \"{b}\"'\nvalid_status = [\"running\",\"complete\"]\nfinal_command = \"kaggle kernels output jessysisca/test-of-yt-dlp2\"\ncmd = \"kaggle kernels status jessysisca/test-of-yt-dlp2\"\nwhile True:\n    output = subprocess.check_output(cmd.split(\" \"))\n    output = output.decode('utf-8')\n    output = output.replace('\\n',\"\").strip()\n    result = parse.parse(formatx,output)\n    rb = result['b']\n    print(\"STATUS:\",rb)\n    if rb in valid_status:\n        if rb == \"complete\":\n            print(\"DOWNLOADING OUTPUT\")\n            os.system(final_command)\n            os.system(\"touch {}\".format(download_lock))\n            break\n        else:\n            time.sleep(wait_duration)\n    else:\n        print(\"UNKNOWN STATUS. ERROR.\")\n        break",
        "type": "code",
        "location": "/tests/bilibili_practices/bilibili_science_subtitle_with_cn_voice/download_when_complete.py:1-40"
    },
    "2637": {
        "file_id": 289,
        "content": "The code is using the subprocess module to periodically check the status of a Kaggle kernel. If the status is \"running\" or \"complete\", it executes a final command and sets a lock file. It continues checking until the status is one of the valid ones or an unknown status occurs, in which case it breaks the loop.",
        "type": "comment"
    },
    "2638": {
        "file_id": 290,
        "content": "/tests/bilibili_practices/bilibili_science_subtitle_with_cn_voice/create_output.sh",
        "type": "filepath"
    },
    "2639": {
        "file_id": 290,
        "content": "This code utilizes FFmpeg to merge webm video with subtitle files, trimming and styling as needed. It provides links for similar tasks, and seeks PlayResX, PlayResY, and ssa subtitle coordinates.",
        "type": "summary"
    },
    "2640": {
        "file_id": 290,
        "content": "ffmpeg -y -vsync 0 -hwaccel_output_format cuda -i \"Scientists Discovered a Bubble Around Our Solar System! [At7ORzmAaT4].webm\"  -vf \"subtitles=zh_translated.srt:force_style='MarginV=60',subtitles=en_.srt:force_style='Fontsize=10,PrimaryColour=&H00FFFF00,Alignment=6,MarginV=228'\" scientists_bubbles.mp4\n# ffmpeg -y -vsync 0 -hwaccel_output_format cuda -i \"Scientists Discovered a Bubble Around Our Solar System! [At7ORzmAaT4].webm\" -ss 00:00:07 -to 00:01:00  -vf \"subtitles=zh_translated.srt:force_style='MarginV=60',subtitles=en_.srt:force_style='Fontsize=10,PrimaryColour=&H00FFFF00,Alignment=6,MarginV=228'\" scientists_bubbles.mp4\n# https://www.zhihu.com/question/20779091\n# https://www.jianshu.com/p/cfdbfdc6d3a7\n# https://fileformats.fandom.com/wiki/SubStation_Alpha#Style_overrides\n# PlayResX: 384\n# PlayResY: 288\n# 384×288是标准的4：3画面分辨率之一。ssa字幕里的坐标（字幕的位置）即根据这2个数值的范围来定义。\n# ffmpeg -y -vsync 0 -hwaccel_output_format cuda -i \"Scientists Discovered a Bubble Around Our Solar System! [At7ORzmAaT4].webm\" -ss",
        "type": "code",
        "location": "/tests/bilibili_practices/bilibili_science_subtitle_with_cn_voice/create_output.sh:1-9"
    },
    "2641": {
        "file_id": 290,
        "content": "The code uses FFmpeg to combine a webm video with two subtitle files, creating an mp4 output. It also trims the video for a specific duration and applies style overrides for subtitles. The provided links are for reference material on similar tasks. The final part of the code seeks information about PlayResX and PlayResY, along with ssa subtitle coordinates.",
        "type": "comment"
    },
    "2642": {
        "file_id": 290,
        "content": " 00:00:07 -to 00:01:00  -vf \"subtitles=zh_translated.srt:force_style='MarginV=0',subtitles=en_.srt\" scientists_bubbles.mp4",
        "type": "code",
        "location": "/tests/bilibili_practices/bilibili_science_subtitle_with_cn_voice/create_output.sh:9-9"
    },
    "2643": {
        "file_id": 290,
        "content": "Applying subtitles to a video.",
        "type": "comment"
    },
    "2644": {
        "file_id": 291,
        "content": "/tests/bilibili_practices/bilibili_science_subtitle_with_cn_voice/commons.py",
        "type": "filepath"
    },
    "2645": {
        "file_id": 291,
        "content": "This code defines `jsonWalk` and `jsonLocate` functions that recursively traverse JSON objects, handling dictionaries, lists, tuples, and raising exceptions for non-JSON types. It also updates json's dictionary with new \"walk\" and \"locate\" functions.",
        "type": "summary"
    },
    "2646": {
        "file_id": 291,
        "content": "import json\ndef jsonWalk(jsonObj,location=[]):\n    # this is not tuple. better convert it first?\n    # mlocation = copy.deepcopy(location)\n    if type(jsonObj) == dict:\n        for key in jsonObj:\n            content = jsonObj[key]\n            if type(content) not in [dict,list,tuple]: \n                yield location+[key], content\n            else:\n                # you really ok with this?\n                for mkey, mcontent in jsonWalk(content,location+[key]):\n                    yield mkey, mcontent\n    elif type(jsonObj) in [list,tuple]:\n        for key,content in enumerate(jsonObj):\n        # content = jsonObj[key]\n            if type(content) not in [dict,list,tuple]:\n                yield location+[key], content\n            else:\n                for mkey, mcontent in jsonWalk(content,location+[key]):\n                    yield mkey, mcontent\n    else:\n        raise Exception(\"Not a JSON compatible object: {}\".format(type(jsonObj)))\ndef jsonLocate(jsonObj,location=[]):\n    # print(\"object:\",jsonObj)\n    # print(\"location:\",location)",
        "type": "code",
        "location": "/tests/bilibili_practices/bilibili_science_subtitle_with_cn_voice/commons.py:1-29"
    },
    "2647": {
        "file_id": 291,
        "content": "This code defines two functions, `jsonWalk` and `jsonLocate`, which recursively traverse a JSON object and yield the location and value of each item. It handles dictionaries, lists, and tuples while raising an exception for non-JSON compatible types.",
        "type": "comment"
    },
    "2648": {
        "file_id": 291,
        "content": "    if location!=[]:\n        return jsonLocate(jsonObj[location[0]],location[1:])\n    return jsonObj\njson.__dict__.update({\"walk\":jsonWalk,\"locate\":jsonLocate})\ndef list_startswith(a,b):\n    value = 0\n    if len(a) < len(b): return False\n    for i,v in enumerate(b):\n        v0 = a[i]\n        if v == v0:\n            value +=1\n    return value == len(b)\ndef list_endswith(a,b):\n    value = 0\n    if len(a) < len(b): return False\n    c = a[-len(b):]\n    for i,v in enumerate(b):\n        v0 = c[i]\n        if v == v0:\n            value +=1\n    return value == len(b)\n# list.__dict__.update({\"startswith\": list_startswith,\"endswith\": list_endswith})",
        "type": "code",
        "location": "/tests/bilibili_practices/bilibili_science_subtitle_with_cn_voice/commons.py:30-57"
    },
    "2649": {
        "file_id": 291,
        "content": "The code contains functions for checking if a list starts or ends with another list, but they are not added to the list class. It also updates json's dictionary with \"walk\" and \"locate\" functions.",
        "type": "comment"
    },
    "2650": {
        "file_id": 292,
        "content": "/tests/bilibili_practices/bilibili_dollar/fetch_related_content.py",
        "type": "filepath"
    },
    "2651": {
        "file_id": 292,
        "content": "The code imports the \"VideosSearch\" class from the \"youtube-search-python\" package and uses it to search for videos related to drawing realistic US Dollars. It fetches the first 10 results, then prints each video's title, ID, author name, channel ID, and view count.",
        "type": "summary"
    },
    "2652": {
        "file_id": 292,
        "content": "#!pip3 install youtube-search-python\nfrom youtubesearchpython import VideosSearch\n# videosSearch = VideosSearch('画人民币', limit = 10)\nvideosSearch = VideosSearch('Draw realistic US Dollar', limit = 10)\n# videosSearch = VideosSearch('NoCopyrightSounds', limit = 2)\n# print(videosSearch.result())\ndata = videosSearch.result()\nfor elem in data[\"result\"]:\n    title = elem[\"title\"]\n    videoId = elem[\"id\"]\n    contentType = elem[\"type\"]\n    authorName = elem[\"channel\"][\"name\"]\n    channelId = elem[\"channel\"][\"id\"]\n    viewCount = elem[\"viewCount\"][\"text\"]\n    print(\"title\",title)\n    print(\"videoId\",videoId)\n    print(\"author\",authorName)\n    print(\"channel ID\",channelId)\n    print(\"viewCount\",viewCount)\n    print(\"_______________________________________\")",
        "type": "code",
        "location": "/tests/bilibili_practices/bilibili_dollar/fetch_related_content.py:1-23"
    },
    "2653": {
        "file_id": 292,
        "content": "The code imports the \"VideosSearch\" class from the \"youtube-search-python\" package and uses it to search for videos related to drawing realistic US Dollars. It fetches the first 10 results, then prints each video's title, ID, author name, channel ID, and view count.",
        "type": "comment"
    },
    "2654": {
        "file_id": 293,
        "content": "/tests/optical_flow/sparse_cpu.py",
        "type": "filepath"
    },
    "2655": {
        "file_id": 293,
        "content": "The code initializes an App object, tracks key points using PyrLK algorithm, calculates optical flow between frames, maintains maximum length of tracks and displays results. It uses OpenCV, numpy and Flownet2-pytorch model for processing and detecting key points.",
        "type": "summary"
    },
    "2656": {
        "file_id": 293,
        "content": "#coding=utf-8\nimport numpy as np\nimport cv2\n# from common import anorm2, draw_str\n# from time import clock\nimport cmath\nlk_params = dict(winSize=(15, 15),\n                 maxLevel=2,\n                 criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))\n# maxCorners : 设置最多返回的关键点数量。\n# qualityLevel : 反应一个像素点强度有多强才能成为关键点。\n# minDistance : 关键点之间的最少像素点。\n# blockSize : 计算一个像素点是否为关键点时所取的区域大小。\n# useHarrisDetector :使用原声的 Harris 角侦测器或最小特征值标准。\n# k : 一个用在Harris侦测器中的自由变量。\nfeature_params = dict(maxCorners=5000000,\n                      qualityLevel=0.1,\n                      minDistance=7,\n                      blockSize=7)\nclass App:\n    def __init__(self, video_src):  # 构造方法，初始化一些参数和视频路径\n        self.track_len = 10\n        self.detect_interval = 1\n        self.tracks = []\n        self.cam = cv2.VideoCapture(video_src)\n        self.frame_idx = 0\n        self.num = 0\n        self.i = 0\n        self.all_distance = 0\n        self.count = 0\n    def run(self):  # 光流运行方法\n        while True:\n            ret, frame = self.cam.read()  # 读取视频帧",
        "type": "code",
        "location": "/tests/optical_flow/sparse_cpu.py:1-37"
    },
    "2657": {
        "file_id": 293,
        "content": "App class initialization and video reading\n\nCode for creating and initializing the App object, capturing video frames from a specified source.",
        "type": "comment"
    },
    "2658": {
        "file_id": 293,
        "content": "            if ret == True:\n                frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)  # 转化为灰度虚图像\n                # vis = frame.copy()\n                h, w = frame.shape[:2]\n                vis = np.ones((h, w), )\n                f = open('./shuibo_8_LK(x1,y1,x2,y2).txt','w+')\n                if len(self.tracks) > 0:  # 检测到角点后进行光流跟踪\n                    img0, img1 = self.prev_gray, frame_gray\n                    p0 = np.float32([tr[-1] for tr in self.tracks]).reshape(-1, 1, 2)\n                    \"\"\"\n                    nextPts, status, err = calcOpticalFlowPyrLK(prevImg, nextImg, prevPts[, nextPts[, status[, \n                    err[, winSize[, maxLevel[, criteria[, flags[, minEigThreshold]]]]]]]])\n                    参数说明：\n                      prevImage 前一帧8-bit图像\n                      nextImage 当前帧8-bit图像\n                      prevPts 待跟踪的特征点向量\n                      nextPts 输出跟踪特征点向量\n                      status 特征点是否找到，找到的状态为1，未找到的状态为0\n                      err 输出错误向量，（不太理解用途...）\n                      winSize 搜索窗口的大小",
        "type": "code",
        "location": "/tests/optical_flow/sparse_cpu.py:38-58"
    },
    "2659": {
        "file_id": 293,
        "content": "This code is performing optical flow tracking using the Pyramid Lucas-Kanade algorithm (PyrLK) on a video frame. It reads the previous and current frames, detects key points in the previous frame, calculates the new positions of these key points in the current frame, and updates the tracks list if any key point is found. The status array indicates whether each tracked point was found or not, and err presumably contains error information related to tracking. The code writes the x and y coordinates of each tracked point to a text file.",
        "type": "comment"
    },
    "2660": {
        "file_id": 293,
        "content": "                      maxLevel 最大的金字塔层数\n                      flags 可选标识：OPTFLOW_USE_INITIAL_FLOW   OPTFLOW_LK_GET_MIN_EIGENVALS\n                    \"\"\"\n                    p1, st, err = cv2.calcOpticalFlowPyrLK(img0, img1, p0, None,\n                                                           **lk_params)  # 前一帧的角点和当前帧的图像作为输入来得到角点在当前帧的位置\n                    p0r, st, err = cv2.calcOpticalFlowPyrLK(img1, img0, p1, None,\n                                                            **lk_params)  # 当前帧跟踪到的角点及图像和前一帧的图像作为输入来找到前一帧的角点位置\n                    d = abs(p0 - p0r).reshape(-1, 2).max(-1)  # 得到角点回溯与前一帧实际角点的位置变化关系\n                    # good = d < 1  # 判断d内的值是否小于1，大于1跟踪被认为是错误的跟踪点\n                    good=d\n                    new_tracks = []\n                    for tr, (x, y), good_flag in zip(self.tracks, p1.reshape(-1, 2), good):  # 将跟踪正确的点列入成功跟踪点\n                        if not good_flag:\n                            continue\n                        tr.append((x, y))#tr是前一帧的角点，与当前帧的角点(x,y)合并。标志为good_flag",
        "type": "code",
        "location": "/tests/optical_flow/sparse_cpu.py:59-74"
    },
    "2661": {
        "file_id": 293,
        "content": "This code calculates optical flow between two images using cv2.calcOpticalFlowPyrLK, tracking points from one image to another. It then compares the tracked points with the actual points and measures the displacement. Points with displacement greater than 1 are considered as incorrect and removed. The remaining points form new_tracks, which is a list of successful tracks.",
        "type": "comment"
    },
    "2662": {
        "file_id": 293,
        "content": "                        if len(tr) > self.track_len:\n                            del tr[0]\n                        new_tracks.append(tr)\n                        # print(x,y)\n                        # breakpoint()\n                        cv2.circle(vis, (int(x), int(y)), 2, (0, 255, 0), -1)#当前帧角点画圆\n                    self.tracks = new_tracks #self.tracks中的值的格式是：(前一帧角点)(当前帧角点)\n                    # print(self.tracks[0])\n                    # print(self.tracks[1])\n                    distance = 0\n                    for tr in self.tracks:\n                        # tr[0]=list(tr[0])\n                        # tr[1]=list(tr[1])\n                        x1=tr[0][0]\n                        y1=tr[0][1]\n                        x2 = tr[1][0]\n                        y2 = tr[1][1]\n                        f.writelines([ str(x1), ' ', str(y1), ' ', str(x2), ' ', str(y2),'\\n'])\n                        dis=cmath.sqrt((x2-x1)*(x2-x1)+(y2-y1)*(y2-y1))\n                        #正确追踪的点的个数\n                        print(len(self.tracks))",
        "type": "code",
        "location": "/tests/optical_flow/sparse_cpu.py:75-98"
    },
    "2663": {
        "file_id": 293,
        "content": "This code tracks optical flow points across multiple frames, storing the tracked points in 'tracks'. It appends new tracks and deletes old ones to maintain a maximum length. The x and y coordinates of current points are plotted on a visualization ('vis'). Finally, it calculates the Euclidean distance between consecutive points and writes them into file 'f', while printing the total number of correctly tracked points.",
        "type": "comment"
    },
    "2664": {
        "file_id": 293,
        "content": "                        #每一个正确追踪的点的像素点的位移\n                        print(dis.real)\n                        distance=distance+dis\n                    len_tracks = len(self.tracks)\n                    if len_tracks == 0:continue\n                    distance=distance/len_tracks\n                    self.all_distance=self.all_distance+distance\n                    self.count=self.count+1\n                    print(\"每一帧像素点平均位移：\",distance,\"第几帧：\",self.count)\n                    print(\"所有帧平均位移：\",(self.all_distance/self.count).real)\n                f.close()\n                if self.frame_idx % self.detect_interval == 0:  #每1帧检测一次特征点\n                    mask = np.zeros_like(frame_gray)  # 初始化和视频大小相同的图像\n                    mask[:] = 255  # 将mask赋值255也就是算全部图像的角点\n                    for x, y in [np.int32(tr[-1]) for tr in self.tracks]:  #跟踪的角点画圆\n                        cv2.circle(mask, (x, y), 5, 0, -1)\n                    p = cv2.goodFeaturesToTrack(frame_gray, mask=mask, **feature_params)  # 像素级别角点检测\n                    if p is not None:",
        "type": "code",
        "location": "/tests/optical_flow/sparse_cpu.py:99-117"
    },
    "2665": {
        "file_id": 293,
        "content": "Code calculates average pixel point displacement between frames and prints the results. It keeps track of all pixel point movements in a frame and counts the number of frames. The code checks for features every 1 frame, initializes a mask image, detects corners using goodFeaturesToTrack function, and stores the result if it is not None.",
        "type": "comment"
    },
    "2666": {
        "file_id": 293,
        "content": "                        for x, y in np.float32(p).reshape(-1, 2):\n                            self.tracks.append([(x, y)])  # 将检测到的角点放在待跟踪序列中\n                self.frame_idx += 1\n                self.prev_gray = frame_gray\n                cv2.imshow('lk_track', vis)\n            # ch = 0xFF & \n            if cv2.waitKey(20) == \"q\":\n                # cv2.imwrite(\"./mashiti-result4.png\", vis)\n                break\n# # get flownet2-pytorch source\n# git clone https://github.com/NVIDIA/flownet2-pytorch.git\n# cd flownet2-pytorch\n# # install custom layers\n# bash install.sh\ndef main():\n    import sys\n    try:\n        video_src = sys.argv[1]\n    except:\n        # video_src = \"./F/8/shuibo_8.avi\"\n        video_src = \"/media/root/help/pyjom/samples/video/dog_with_text.mp4\"\n    # print\n    # __doc__\n    App(video_src).run()\n    cv2.destroyAllWindows()\nif __name__ == '__main__':\n    main()",
        "type": "code",
        "location": "/tests/optical_flow/sparse_cpu.py:118-151"
    },
    "2667": {
        "file_id": 293,
        "content": "This code is a part of a video processing program. It reads frames from a video source, detects key points in each frame using the LK tracker, tracks these key points across successive frames to estimate optical flow, and displays the results. The code uses OpenCV library for image processing, numpy for numerical computations, and cv2.waitKey() function for window handling. It also imports a Flownet2-pytorch model from a git repository and installs custom layers.",
        "type": "comment"
    },
    "2668": {
        "file_id": 294,
        "content": "/tests/optical_flow/nvidia_of_test.py",
        "type": "filepath"
    },
    "2669": {
        "file_id": 294,
        "content": "The code converts video frames to grayscale, creates an optical flow object, and uploads the first two frames to GPU for calculation. It downloads a GPU flow, visualizes it using flow_vis library, displays in a window, and quits on 'q'. No garbage collection is performed.",
        "type": "summary"
    },
    "2670": {
        "file_id": 294,
        "content": "from nvidia_common import *\nimport numpy as np \nimport cv2\nimport flow_vis\nvideo_file = \"/media/root/help/pyjom/samples/video/dog_with_text.mp4\"\n# this is the fastest.\nvideo = cv2.VideoCapture(video_file)\nret, img = video.read()\nprevImg = img.copy()\nwhile True:\n    ret, img = video.read()\n    if img is None: break\n    else:\n        frame1 = prevImg\n        frame1 = cv2.cvtColor(frame1,cv2.COLOR_BGR2GRAY)\n        frame2 = img # why freaking grayscale?\n        frame2 =  cv2.cvtColor(frame2,cv2.COLOR_BGR2GRAY)\n        prevImg = img.copy()\n        perfPreset = 5\n        gpuId=0\n        # nvof = cv2.cuda_NvidiaOpticalFlow_2_0.create((frame1.shape[1], frame1.shape[0]),5, False, False, False, 0)\n        gpu_flow =cv2.cuda_FarnebackOpticalFlow.create(5, 0.5, False,\n                                                        15, 3, 5, 1.2, 0)\n        gpu_frame_a = cv2.cuda_GpuMat()\n        gpu_frame_b = cv2.cuda_GpuMat()\n        gpu_frame_a.upload(frame1)\n        gpu_frame_b.upload(frame2)\n        # -- exec flow --\n        gpu_flow = cv2.cuda_FarnebackOpticalFlow.calc(gpu_flow, gpu_frame_a,",
        "type": "code",
        "location": "/tests/optical_flow/nvidia_of_test.py:1-36"
    },
    "2671": {
        "file_id": 294,
        "content": "Reading video file, converting frames to grayscale, creating optical flow object with specified parameters, and uploading the first two frames to GPU for calculation.",
        "type": "comment"
    },
    "2672": {
        "file_id": 294,
        "content": "                                                      gpu_frame_b, None)\n        gpu_flow = gpu_flow.download()\n        # gpu_flow = gpu_flow.transpose(2,0,1)\n        # print(gpu_flow.shape())\n        # breakpoint()\n        # gpu_flow = th.from_numpy(gpu_flow).half()\n        # cv2.writeOpticalFlow('OpticalFlow.flo', flowUpSampled)\n        visualize = flow_vis.flow_to_color(gpu_flow, convert_to_bgr=False)\n        cv2.imshow(\"OPTFLOW\",visualize)\n        if cv2.waitKey(20) == chr(\"q\"):\n            print(\"QUIT THIS SHIT\")\n            break\n        # nvof.collectGarbage()",
        "type": "code",
        "location": "/tests/optical_flow/nvidia_of_test.py:37-53"
    },
    "2673": {
        "file_id": 294,
        "content": "This code downloads a GPU flow, potentially transposes it and prints its shape, then visualizes the flow using flow_vis library. It displays the visualization in a window and quits when 'q' is pressed. No garbage collection is performed.",
        "type": "comment"
    },
    "2674": {
        "file_id": 295,
        "content": "/tests/optical_flow/nvidia_common.py",
        "type": "filepath"
    },
    "2675": {
        "file_id": 295,
        "content": "This code is for setting the path to the OpenCV library for Turing architecture. It first determines the site-packages directory and then checks if there's a specific folder for the current Python version containing .so files, which are loaded into sys.path. If only one such folder exists, it gets added to sys.path before importing cv2. The code prints dir(cv2) for information or potentially debugging purposes.",
        "type": "summary"
    },
    "2676": {
        "file_id": 295,
        "content": "import pathlib\nimport site\nimport sys\n# optical flow sdk is exclusively for Turing architecture.\n# this is root. this is not site-packages.\n# site_path = pathlib.Path([x for x in site.getsitepackages() if \"site-packages\" in x][0])\nsite_path = pathlib.Path(\"/usr/local/lib/python3.9/site-packages\")\ncv2_libs_dir = site_path / 'cv2' / f'python-{sys.version_info.major}.{sys.version_info.minor}'\nprint(cv2_libs_dir)\ncv2_libs = sorted(cv2_libs_dir.glob(\"*.so\"))\nif len(cv2_libs) == 1:\n    print(\"INSERTING:\",cv2_libs[0].parent)\n    sys.path.insert(1, str(cv2_libs[0].parent))\nimport cv2\nprint(dir(cv2)) # shit?",
        "type": "code",
        "location": "/tests/optical_flow/nvidia_common.py:1-19"
    },
    "2677": {
        "file_id": 295,
        "content": "This code is for setting the path to the OpenCV library for Turing architecture. It first determines the site-packages directory and then checks if there's a specific folder for the current Python version containing .so files, which are loaded into sys.path. If only one such folder exists, it gets added to sys.path before importing cv2. The code prints dir(cv2) for information or potentially debugging purposes.",
        "type": "comment"
    },
    "2678": {
        "file_id": 296,
        "content": "/tests/optical_flow/mmof_test/get_frame_flow.py",
        "type": "filepath"
    },
    "2679": {
        "file_id": 296,
        "content": "This code reads frames from a video file, converts them to grayscale (optional), and saves the 40th frame as \"frame0.png\" and the next frame as \"frame1.png\". The loop continues until it encounters an empty frame (none) indicating the end of the video.",
        "type": "summary"
    },
    "2680": {
        "file_id": 296,
        "content": "import cv2\nvideo_file = \"/media/root/help/pyjom/samples/video/dog_with_text.mp4\"\nvideo = cv2.VideoCapture(video_file)\nret, img = video.read()\nprevImg = img.copy()\ncounter = 0\nwhile True:\n    ret, img = video.read()\n    if img is None: break\n    else:\n        frame1 = prevImg\n        # frame1 = cv2.cvtColor(frame1,cv2.COLOR_BGR2GRAY)\n        frame2 = img # why freaking grayscale?\n        # frame2 =  cv2.cvtColor(frame2,cv2.COLOR_BGR2GRAY)\n        if counter == 40:\n            cv2.imwrite(\"frame0.png\",frame1)\n            cv2.imwrite(\"frame1.png\",frame2)\n        prevImg = img.copy()\n        counter +=1",
        "type": "code",
        "location": "/tests/optical_flow/mmof_test/get_frame_flow.py:1-23"
    },
    "2681": {
        "file_id": 296,
        "content": "This code reads frames from a video file, converts them to grayscale (optional), and saves the 40th frame as \"frame0.png\" and the next frame as \"frame1.png\". The loop continues until it encounters an empty frame (none) indicating the end of the video.",
        "type": "comment"
    },
    "2682": {
        "file_id": 297,
        "content": "/tests/optical_flow/mmof_test/execute_me.py",
        "type": "filepath"
    },
    "2683": {
        "file_id": 297,
        "content": "This code initializes an MMFlow model and performs optical flow calculation on video frames, visualizing results and breaking the loop when \"q\" is pressed. It uses BGR to grayscale conversion and can perform Canny edge detection.",
        "type": "summary"
    },
    "2684": {
        "file_id": 297,
        "content": "from mmflow.apis import init_model, inference_model\nfrom mmflow.datasets import visualize_flow, write_flow\nimport mmcv\n# Specify the path to model config and checkpoint file\nconfig_id = 0\nif config_id == 0:\n    config_file = 'flownet2cs_8x1_slong_flyingchairs_384x448.py'\n    checkpoint_file = 'flownet2cs_8x1_slong_flyingchairs_384x448.pth'\nelif config_id == 1:\n    config_file = 'gma_8x2_120k_mixed_368x768.py' # damn slow.\n    checkpoint_file = 'gma_8x2_120k_mixed_368x768.pth'\n# build the model from a config file and a checkpoint file\nmodel = init_model(config_file, checkpoint_file, device='cuda:0')\n# test image pair, and save the results\nimport cv2\nvideo_file = \"/media/root/help/pyjom/samples/video/dog_with_text.mp4\"\nvideo = cv2.VideoCapture(video_file)\nret, img = video.read()\nprevImg = img.copy()\ncounter = 0\nwhile True:\n    ret, img = video.read()\n    if img is None: break\n    else:\n        frame1 = prevImg\n        # frame1 = cv2.cvtColor(frame1,cv2.COLOR_BGR2GRAY)\n        frame2 = img # why freaking grayscale?",
        "type": "code",
        "location": "/tests/optical_flow/mmof_test/execute_me.py:1-35"
    },
    "2685": {
        "file_id": 297,
        "content": "This code initializes a model using MMFlow library and performs optical flow calculation on video frames. It reads a video file, captures frames, applies optical flow algorithm using the initialized model, and saves the results. The model configuration is determined by config_id, with two options specified in the code. Frame1 and frame2 are used to calculate optical flow between these consecutive frames. The code includes color conversion (BGR to grayscale), but this is not clearly explained or justified in the code.",
        "type": "comment"
    },
    "2686": {
        "file_id": 297,
        "content": "        result = inference_model(model, frame1,frame2)\n        prevImg = img.copy()\n        flow_map = visualize_flow(result,None)\n        cv2.imshow(\"flowmap\",flow_map)\n    if cv2.waitKey(20) == ord(\"q\"):\n        break\n        # can also do canny edge detection.",
        "type": "code",
        "location": "/tests/optical_flow/mmof_test/execute_me.py:36-42"
    },
    "2687": {
        "file_id": 297,
        "content": "The code executes inference using the provided model on two frames, visualizes the optical flow map, and displays it in a window. It breaks the loop when \"q\" key is pressed, and can perform Canny edge detection.",
        "type": "comment"
    },
    "2688": {
        "file_id": 298,
        "content": "/tests/cpm_chinese_chitchat_model_gpt2/test.sh",
        "type": "filepath"
    },
    "2689": {
        "file_id": 298,
        "content": "This code changes the directory to GPT2-chitchat, checks RAM consumption and urges to buy new RAM for CPU model testing. It runs 'interact.py' with no CUDA and using a specific model path.",
        "type": "summary"
    },
    "2690": {
        "file_id": 298,
        "content": "# no fucking gpu. just test how much RAM it consumes.\ncd GPT2-chitchat # 1.8GB mem consumption. freaking hell.\n# BUY NEW RAM AND RUN MODELS ON CPU!\npython3 interact.py --no_cuda --model_path ../model",
        "type": "code",
        "location": "/tests/cpm_chinese_chitchat_model_gpt2/test.sh:1-4"
    },
    "2691": {
        "file_id": 298,
        "content": "This code changes the directory to GPT2-chitchat, checks RAM consumption and urges to buy new RAM for CPU model testing. It runs 'interact.py' with no CUDA and using a specific model path.",
        "type": "comment"
    },
    "2692": {
        "file_id": 299,
        "content": "/tests/cpm_chinese_chitchat_model_gpt2/init.sh",
        "type": "filepath"
    },
    "2693": {
        "file_id": 299,
        "content": "Code is cloning the GPT2-chitchat repository with a single commit from GitHub.",
        "type": "summary"
    },
    "2694": {
        "file_id": 299,
        "content": "git clone --depth 1 https://github.com/yangjianxin1/GPT2-chitchat",
        "type": "code",
        "location": "/tests/cpm_chinese_chitchat_model_gpt2/init.sh:1-1"
    },
    "2695": {
        "file_id": 299,
        "content": "Code is cloning the GPT2-chitchat repository with a single commit from GitHub.",
        "type": "comment"
    },
    "2696": {
        "file_id": 300,
        "content": "/tests/conversation_talk_apis/api_tests.py",
        "type": "filepath"
    },
    "2697": {
        "file_id": 300,
        "content": "This code imports modules, disables proxies, and uses requests library to send POST requests to Weibo API's direct messaging endpoint. It creates and sends messages, retrieves responses in JSON format, interacts with Weibo and OwnThink APIs, checks user messages against responses, performs API tests using checkApi function for different chatbot instances.",
        "type": "summary"
    },
    "2698": {
        "file_id": 300,
        "content": "import urllib.parse\nimport requests\n# disable all proxies.\nimport os\nimport time\nos.environ[\"http_proxy\"]=\"\"\nos.environ[\"https_proxy\"]=\"\"\n# do not use freaking proxy, otherwise QingYunKe will not respond.\ndef checkApi(func,message,name):\n    response_message = func(message)\n    if response_message!=None:\n        print(\"{} RESPONSE:\".format(name), response_message)\ndef chatAtri(msg: str, BASE='http://api.nekomimi.icu/v1/'):\n    url = BASE + 'chat?msg=%s' % msg\n    response = requests.get(url)\n    if response.status_code == 200:\n        data = response.json()\n        if data['status'] == 'success':\n            return data['message']\n    # return None\n    # nothing is returned if have error.\n    print(\"ATRI ERROR:\", response.status_code, response.json())\n# import subprocess\n# import json\ndef chatQingKeYun(msg: str, url=\"http://api.qingyunke.com/api.php?key=free&appid=0&msg=\"):\n    msg = urllib.parse.quote(msg)\n    myUrl = url+msg\n    # print(myUrl)\n    # output = subprocess.check_output([\"curl\", myUrl])\n    # data = json.loads(output.decode(\"utf-8\"))",
        "type": "code",
        "location": "/tests/conversation_talk_apis/api_tests.py:1-37"
    },
    "2699": {
        "file_id": 300,
        "content": "Code imports necessary modules, disables proxies, defines a function to check API responses, and includes two chat functions: 'chatAtri' for chatting with Atri using a Chinese language processing API, and 'chatQingKeYun' for chatting with QingYunKe using a free API key. The code also has a commented section that appears to be testing the use of subprocess and json modules but is not implemented yet.",
        "type": "comment"
    }
}