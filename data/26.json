{
    "2600": {
        "file_id": 285,
        "content": "                (x0, y0, x1, y1),\n                fill=\"white\",\n                radius=min(int(x1 - x0) / 2, int(y1 - y0) / 2, getRadius()),\n            )\n        # print(\"___\")\n        # print(imageShape)\n        # print(imageCanvas.shape)\n        # print(image.shape)\n        # print(x0,x1,x1-x0)\n        # print(y0,y1,y1-y0)\n        # print(\"___\")\n        # cv2.imshow(\"mask\", np.array(imageMask))\n        # cv2.waitKey(0)\n        imageCanvas[y0 : image.shape[0] + y0, x0 : image.shape[1] + x0, :] = image\n        imageCoordinates.append(\n            (\n                x0 + image.shape[1] / 2,\n                y0 + image.shape[0] / 2,\n                image.shape[1],\n                image.shape[0],\n            )\n        )  # x_center, y_center, width, height\n    else:\n        basePoints = [\n            (x * half_width, y * half_width)\n            for x, y in [(0, 0), (1, 0), (1, 1), (0, 1)]\n        ]  # width, height\n        for index, image in enumerate(selectedImages):\n            imageShape = image.shape\n            margin = getMarginRatio()",
        "type": "code",
        "location": "/tests/anime_highlight_cuts/theme_collector/create_coco_pip_dataset_standalone.py:264-296"
    },
    "2601": {
        "file_id": 285,
        "content": "This code is creating a mask for an image, adjusting the radius for the ellipse shape, and then composites it with other images if needed. It also calculates the base points for a rectangle, and gets the image shapes of each selected image in the list. The \"else\" part suggests that there is a condition being checked before this code block is executed. The function getMarginRatio() and getRadius() are used to calculate the margins and radius of the ellipse respectively.",
        "type": "comment"
    },
    "2602": {
        "file_id": 285,
        "content": "            base = half_width * (1 - margin * 2)\n            imageHeight, imageWidth = imageShape[:2]\n            if imageHeight > imageWidth:\n                imageShape = (int(base * (imageWidth / imageHeight)), int(base))\n            else:\n                imageShape = (int(base), int(base * (imageHeight / imageWidth)))\n            image = cv2.resize(image, imageShape)\n            x0 = int((half_width - imageShape[0]) / 2) + basePoints[index][0]\n            x1 = x0 + imageShape[0]\n            y0 = int((half_width - imageShape[1]) / 2) + basePoints[index][1]\n            y1 = y0 + imageShape[1]\n            if random.random() > 0.5:\n                draw.rectangle((x0, y0, x1, y1), fill=\"white\")\n            else:\n                draw.rounded_rectangle(\n                    (x0, y0, x1, y1),\n                    fill=\"white\",\n                    radius=min(int(x1 - x0) / 2, int(y1 - y0) / 2, getRadius()),\n                )\n            imageCanvas[y0 : image.shape[0] + y0, x0 : image.shape[1] + x0, :] = image\n            imageCoordinates.append(",
        "type": "code",
        "location": "/tests/anime_highlight_cuts/theme_collector/create_coco_pip_dataset_standalone.py:297-321"
    },
    "2603": {
        "file_id": 285,
        "content": "Resizing image to fit within specified bounds and applying rectangle or rounded rectangle based on random chance, then combining with canvas image.",
        "type": "comment"
    },
    "2604": {
        "file_id": 285,
        "content": "                (\n                    x0 + image.shape[1] / 2,\n                    y0 + image.shape[0] / 2,\n                    image.shape[1],\n                    image.shape[0],\n                )\n            )  # x_center, y_center, width, height\n    ## mix images with mask\n    imageMaskNumpyArray = np.array(imageMask) / 255  # float64\n    imageMaskNumpyArrayInverted = 1 - imageMaskNumpyArray\n    x0 = 0\n    y0 = textTotalHeight if textFormat == \"up\" else 0\n    backgroundImage[y0 : y0 + imageCanvasShape[0], x0 : x0 + imageCanvasShape[1], :] = (\n        backgroundImage[y0 : y0 + imageCanvasShape[0], x0 : x0 + imageCanvasShape[1], :]\n        * imageMaskNumpyArrayInverted\n    ).astype(np.uint8) + (imageCanvas * imageMaskNumpyArray).astype(np.uint8)\n    # print()\n    ## get labels which will be exported to txt\n    contents = []\n    for coord in imageCoordinates:\n        x_center_relative, y_center_relative, imWidth, imHeight = coord\n        x_center, y_center = x_center_relative + x0, y_center_relative + y0\n        dataPoints = [",
        "type": "code",
        "location": "/tests/anime_highlight_cuts/theme_collector/create_coco_pip_dataset_standalone.py:322-349"
    },
    "2605": {
        "file_id": 285,
        "content": "Code snippet combines images with a mask, creates a new image by multiplying original image with mask and inverting the result. This modified image is then added to the background image, creating an overlay effect. The code also collects data points for labels that will be exported to txt files.",
        "type": "comment"
    },
    "2606": {
        "file_id": 285,
        "content": "            x_center / backgroundShape[1],\n            y_center / backgroundShape[0],\n            imWidth / backgroundShape[1],\n            imHeight / backgroundShape[0],\n        ]\n        labelString = \" \".join(([\"0\"] + [f\"{number:.3f}\" for number in dataPoints]))\n        contents.append(labelString)\n        # print(\"LABELSTRING?\", labelString)\n    ## preview\n    # previewImageName = f\"{imageFormat}_{textFormat}_{backgroundFormat}.png\"\n    realIndex = imageIndex + _i\n    cv2.imwrite(\n        os.path.join(train_path_relative, f\"{str(realIndex).zfill(12)}.png\"),\n        backgroundImage,\n    )\n    with open(\n        os.path.join(train_label_path_relative, f\"{str(realIndex).zfill(12)}.txt\"), \"w+\"\n    ) as f:\n        f.write(\"\\n\".join(contents))\n    # cv2.imshow(previewImageName, backgroundImage)\n    # cv2.waitKey(0)\nprint(\"coco pip dataset created!\")",
        "type": "code",
        "location": "/tests/anime_highlight_cuts/theme_collector/create_coco_pip_dataset_standalone.py:350-374"
    },
    "2607": {
        "file_id": 285,
        "content": "Creates COCO-style pip dataset standalone by iterating over images and labels, writing them to train_path_relative and train_label_path_relative, with real indexing. It also prints a confirmation message upon completion.",
        "type": "comment"
    },
    "2608": {
        "file_id": 286,
        "content": "/tests/anime_highlight_cuts/theme_collector/create_coco_pip_dataset.py",
        "type": "filepath"
    },
    "2609": {
        "file_id": 286,
        "content": "The code creates a COCO-PIP dataset by generating four identical images, adjusting parameters randomly. It utilizes numpy arrays and defines functions for gradient images and background colors. It generates bounding boxes, resizes images, and prepares image canvas for data export and preview.",
        "type": "summary"
    },
    "2610": {
        "file_id": 286,
        "content": "# use what? better use some standard library.\n# you must know where you have put all these images.\n# TODO: remember to upload dataset creation things to kaggle as separate python scripts and execute it in separate process to prevent memory leaks (hopefully)\nimport cv2\nimport numpy as np\nimport os\nfrom string import punctuation\nimport random\nimport itertools\nfrom PIL import Image, ImageDraw\nimageBasePath = \"/Users/jamesbrown/Desktop/\"\nimagePaths = [\n    \"Screen Shot 2023-01-17 at 15.35.29.png\"\n] * 4  # let's all be the same, for testing.\nwidth = 800\nhalf_width = int(width / 2)  # either use 1,2,4 images.\ntextTotalHeight = 300  # either add to top or bottom.\ngetMarginRatio = lambda: random.choice(\n    [0, random.random() * 0.15, random.random() * 0.1, random.random() * 0.05]\n)  # this margin is used randomly. we can make it 0 or as is.\ntextOrigin = (-30, 30)\nfontScale = 1\nfont = cv2.FONT_HERSHEY_SIMPLEX\nfontThickness = 2\ngetRadius = lambda: random.randint(1, 30)\nimageIndex = 0  # shall be increased on demand.\nMAX_COCO_PIP_IMAGE_COUNT = 10000  # well, super huge. is it?",
        "type": "code",
        "location": "/tests/anime_highlight_cuts/theme_collector/create_coco_pip_dataset.py:1-34"
    },
    "2611": {
        "file_id": 286,
        "content": "Code imports necessary libraries and defines variables for creating a COCO-PIP dataset. It uses four identical images, adjusts image size and positioning parameters randomly, and sets maximum image count.",
        "type": "comment"
    },
    "2612": {
        "file_id": 286,
        "content": "alphabets = \"abcdefghijklmnopqrstuvwxyz\"\nALPHABETS = alphabets.upper()\nnumbers = \"0123456789\"\ncharacterList = list(alphabets + ALPHABETS + numbers + punctuation + \" \")\ngetRandomCharacter = lambda: random.choice(characterList)\ngetRandomCharacters = lambda charCount: \"\".join(\n    [getRandomCharacter() for _ in range(charCount)]\n)\ngetRandomLinesOfCharacters = lambda lineCount, charCount: \"\\r\".join(\n    [getRandomCharacters(charCount) for _ in range(lineCount)]\n)\nimageFormats = [1, 2, 4]\ntextFormats = [\"up\", \"down\", \"none\"]\nbackgroundFormats = [\"solidColor\", \"horizontalStripes\", \"verticalStripes\", \"gradients\"]\ncolors = [\n    (0, 0, 0),\n    (255, 255, 255),\n    (0, 0, 192),\n    (255, 255, 64),\n    (0, 255, 0),\n    (0, 0, 255),\n    (255, 0, 0),\n]\ncolorsNumpyArray = [np.array(color) for color in colors]\ncolorsWithIndex = [(index, color) for index, color in enumerate(colors)]\n# we are not doing this while testing.\n# imageFormat = random.choice(imageFormats)\n# textFormat = random.choice(textFormats)\n# backgroundFormat = random.choice(backgroundFormats)",
        "type": "code",
        "location": "/tests/anime_highlight_cuts/theme_collector/create_coco_pip_dataset.py:36-68"
    },
    "2613": {
        "file_id": 286,
        "content": "This code generates random characters, lines of characters, and sets up variables for image, text, and background formats along with colors. It uses a lambda function to generate random characters, joining them into lines. The code also includes a list of color options and converts them into numpy arrays. However, while testing, the code is not choosing random values from imageFormats, textFormats, and backgroundFormats.",
        "type": "comment"
    },
    "2614": {
        "file_id": 286,
        "content": "def get_gradient_2d(start, stop, width, height, is_horizontal):\n    if is_horizontal:\n        return np.tile(np.linspace(start, stop, width), (height, 1))\n    else:\n        return np.tile(np.linspace(start, stop, height), (width, 1)).T\ndef get_gradient_3d(width, height, start_list, stop_list, is_horizontal_list):\n    result = np.zeros((height, width, len(start_list)), dtype=np.float64)\n    for i, (start, stop, is_horizontal) in enumerate(\n        zip(start_list, stop_list, is_horizontal_list)\n    ):\n        result[:, :, i] = get_gradient_2d(start, stop, width, height, is_horizontal)\n    return result.astype(np.uint8)\nfor imageFormat, textFormat, backgroundFormat in itertools.product(\n    imageFormats, textFormats, backgroundFormats\n):  # you can use these things to get test output picture names.\n    colorDistances = {}\n    selectedImages = [\n        cv2.imread(os.path.join(imageBasePath, imagePath), cv2.IMREAD_COLOR)\n        for imagePath in random.sample(imagePaths, k=imageFormat)\n    ]\n    for image in selectedImages:",
        "type": "code",
        "location": "/tests/anime_highlight_cuts/theme_collector/create_coco_pip_dataset.py:71-95"
    },
    "2615": {
        "file_id": 286,
        "content": "This code defines two functions: `get_gradient_2d` and `get_gradient_3d`. The former creates a 2D gradient image based on a start, stop value, width, height, and whether the gradient should be horizontal or vertical. The latter function generates a 3D gradient image by applying the former function to multiple pairs of start/stop values in different layers. The code then uses `itertools.product` to generate combinations of image, text, and background formats for creating test output picture names. It reads images from `imageBasePath` based on the selected imagePaths and applies colorDistance calculations.",
        "type": "comment"
    },
    "2616": {
        "file_id": 286,
        "content": "        averageColor = np.average(image.reshape((-1, 3)), axis=0)\n        for index, colorNumpyArray in enumerate(colorsNumpyArray):\n            colorDistances[index] = colorDistances.get(index, []) + [\n                np.sum(np.abs(averageColor - colorNumpyArray))\n            ]\n    sortedColorsWithIndex = sorted(\n        colorsWithIndex, key=lambda element: -np.sum(colorDistances[element[0]])\n    )  # the further the better.\n    # sortedColors = [color for _, color in sortedColorsWithIndex]\n    ## create background first.\n    imageCanvasHeight = half_width if imageFormat == 2 else width\n    textCanvasHeight = 0 if textFormat == \"none\" else textTotalHeight\n    backgroundShape = (imageCanvasHeight + textCanvasHeight, width, 3)  # height, width\n    _, color_main = sortedColorsWithIndex[0]\n    if backgroundFormat in [\"horizontalStripes\", \"verticalStripes\", \"gradients\"]:\n        # fill background with color_main first.\n        _, color_sub = sortedColorsWithIndex[1]\n        if backgroundFormat in [\"horizontalStripes\", \"verticalStripes\"]:",
        "type": "code",
        "location": "/tests/anime_highlight_cuts/theme_collector/create_coco_pip_dataset.py:96-118"
    },
    "2617": {
        "file_id": 286,
        "content": "This code calculates the average color of an image, then compares it to a set of colors to determine their distances. It sorts these colors by distance and uses the furthest as the main background color. If the format requires more than one color (stripes or gradients), it also takes the second-closest color as the sub color.",
        "type": "comment"
    },
    "2618": {
        "file_id": 286,
        "content": "            backgroundImage = np.zeros(backgroundShape, dtype=np.uint8)\n            backgroundImage[:, :, 0] = color_main[0]\n            backgroundImage[:, :, 1] = color_main[1]\n            backgroundImage[:, :, 2] = color_main[2]\n            stripeCount = random.randint(2, 5)\n            if backgroundFormat == \"verticalStripes\":  # slice width\n                arr = np.linspace(0, backgroundShape[1], stripeCount + 1)\n                for width_start, width_end in [\n                    (int(arr[i]), int(arr[i + 1]))\n                    for i in range(stripeCount)\n                    if i % 2 == 1\n                ]:\n                    backgroundImage[:, width_start:width_end, 0] = color_sub[0]\n                    backgroundImage[:, width_start:width_end, 1] = color_sub[1]\n                    backgroundImage[:, width_start:width_end, 2] = color_sub[2]\n            else:  # horizontal. slice height.\n                arr = np.linspace(0, backgroundShape[0], stripeCount + 1)\n                for height_start, height_end in [",
        "type": "code",
        "location": "/tests/anime_highlight_cuts/theme_collector/create_coco_pip_dataset.py:120-139"
    },
    "2619": {
        "file_id": 286,
        "content": "This code creates a background image with vertical or horizontal stripes based on the 'backgroundFormat' parameter. It first initializes a background image, then randomly determines the number of stripes (2-5) using 'stripeCount'. Depending on the format, it slices the image width or height into equal parts and assigns colors to each stripe section with 'arr', 'width_start', 'width_end' or 'height_start', 'height_end' variables.",
        "type": "comment"
    },
    "2620": {
        "file_id": 286,
        "content": "                    (int(arr[i]), int(arr[i + 1]))\n                    for i in range(stripeCount)\n                    if i % 2 == 1\n                ]:\n                    backgroundImage[height_start:height_end, :, 0] = color_sub[0]\n                    backgroundImage[height_start:height_end, :, 1] = color_sub[1]\n                    backgroundImage[height_start:height_end, :, 2] = color_sub[2]\n        else:  # gradient!\n            is_horizontal = [False, False, False]\n            is_horizontal[random.randint(0, 2)] = True\n            backgroundImage = get_gradient_3d(\n                backgroundShape[1],\n                backgroundShape[0],\n                color_main,\n                color_sub,\n                is_horizontal,\n            )\n    else:  # pure color.\n        backgroundImage = np.zeros(backgroundShape, dtype=np.uint8)\n        backgroundImage[:, :, 0] = color_main[0]\n        backgroundImage[:, :, 1] = color_main[1]\n        backgroundImage[:, :, 2] = color_main[2]\n    ## next, paint text!\n    if textFormat != \"none\":",
        "type": "code",
        "location": "/tests/anime_highlight_cuts/theme_collector/create_coco_pip_dataset.py:140-164"
    },
    "2621": {
        "file_id": 286,
        "content": "This code determines the background image of the image by either assigning a solid color, gradient or pure color depending on the input. It also paints text if the format is not \"none\".",
        "type": "comment"
    },
    "2622": {
        "file_id": 286,
        "content": "        ## only calculate text color when needed.\n        backgroundAverageColor = np.average(backgroundImage.reshape((-1, 3)), axis=0)\n        textColorNumpyArray = sorted(\n            colorsNumpyArray,\n            key=lambda colorNumpyArray: -np.sum(\n                np.abs(backgroundAverageColor - np.array(colorNumpyArray))\n            ),\n        )[0]\n        textColor = textColorNumpyArray.tolist()\n        # let's paint it all over the place!\n        textShift = 40\n        # TODO: check if string is **just enough** to fill the background.\n        for textLineIndex in range(\n            int((backgroundShape[0] / (textTotalHeight + width)) * 27)\n        ):\n            baseNumber = 50\n            baseNumber2 = random.randint(1, baseNumber)\n            textContent = random.choice(\n                [\n                    \"\",\n                    (\" \" * baseNumber2)\n                    + getRandomCharacters(random.randint(0, baseNumber - baseNumber2)),\n                ]\n            )\n            backgroundImage = cv2.putText(",
        "type": "code",
        "location": "/tests/anime_highlight_cuts/theme_collector/create_coco_pip_dataset.py:165-189"
    },
    "2623": {
        "file_id": 286,
        "content": "This code calculates a text color that is complementary to the background image. It then generates random texts and places them on the image with varying positions and sizes, using OpenCV's putText function.",
        "type": "comment"
    },
    "2624": {
        "file_id": 286,
        "content": "                backgroundImage,\n                textContent,\n                (textOrigin[0], textOrigin[1] + textShift * textLineIndex),\n                font,\n                fontScale,\n                textColor,\n                fontThickness,\n                cv2.LINE_AA,\n            )\n    ## put pictures!\n    imageCanvasShape = (imageCanvasHeight, width, 3)\n    imageMask = Image.new(\n        \"RGB\", (imageCanvasShape[1], imageCanvasShape[0]), \"black\"\n    )  # width, height?\n    draw = ImageDraw.Draw(imageMask)\n    imageCanvas = np.zeros(imageCanvasShape, dtype=np.uint8)\n    imageCoordinates = []\n    if imageFormat == 1:\n        image = selectedImages[0]\n        imageShape = image.shape\n        margin = getMarginRatio()\n        base = width * (1 - margin * 2)\n        imageHeight, imageWidth = imageShape[:2]\n        if imageHeight > imageWidth:\n            imageShape = (int(base * (imageWidth / imageHeight)), int(base))\n        else:\n            imageShape = (int(base), int(base * (imageHeight / imageWidth)))\n        # print(image.shape)",
        "type": "code",
        "location": "/tests/anime_highlight_cuts/theme_collector/create_coco_pip_dataset.py:190-221"
    },
    "2625": {
        "file_id": 286,
        "content": "This code prepares an image canvas for a specific image format. It creates an image mask with the same dimensions as the canvas and fills it with black color. It then initializes the main image canvas with zeros and starts preparing an array of coordinates for drawing images onto the canvas based on the selected image format.",
        "type": "comment"
    },
    "2626": {
        "file_id": 286,
        "content": "        image = cv2.resize(image, imageShape)\n        x0 = int((width - imageShape[0]) / 2)\n        x1 = x0 + imageShape[0]\n        y0 = int((width - imageShape[1]) / 2)\n        y1 = y0 + imageShape[1]\n        if random.random() > 0.5:\n            draw.rectangle((x0, y0, x1, y1), fill=\"white\")\n        else:\n            draw.rounded_rectangle(\n                (x0, y0, x1, y1),\n                fill=\"white\",\n                radius=min(int(x1 - x0) / 2, int(y1 - y0) / 2, getRadius()),\n            )\n        # print(\"___\")\n        # print(imageShape)\n        # print(imageCanvas.shape)\n        # print(image.shape)\n        # print(x0,x1,x1-x0)\n        # print(y0,y1,y1-y0)\n        # print(\"___\")\n        # cv2.imshow(\"mask\", np.array(imageMask))\n        # cv2.waitKey(0)\n        imageCanvas[y0 : image.shape[0] + y0, x0 : image.shape[1] + x0, :] = image\n        imageCoordinates.append(\n            (\n                x0 + image.shape[1] / 2,\n                y0 + image.shape[0] / 2,\n                image.shape[1],\n                image.shape[0],",
        "type": "code",
        "location": "/tests/anime_highlight_cuts/theme_collector/create_coco_pip_dataset.py:222-254"
    },
    "2627": {
        "file_id": 286,
        "content": "The code resizes an image, generates a bounding box around the image, and randomly chooses between drawing a rectangle or rounded rectangle with white fill color. It then combines the image and the canvas by assigning the image to specific coordinates on the canvas and appends the bounding box coordinates to the imageCoordinates list.",
        "type": "comment"
    },
    "2628": {
        "file_id": 286,
        "content": "            )\n        )  # x_center, y_center, width, height\n    else:\n        basePoints = [\n            (x * half_width, y * half_width)\n            for x, y in [(0, 0), (1, 0), (1, 1), (0, 1)]\n        ]  # width, height\n        for index, image in enumerate(selectedImages):\n            imageShape = image.shape\n            margin = getMarginRatio()\n            base = half_width * (1 - margin * 2)\n            imageHeight, imageWidth = imageShape[:2]\n            if imageHeight > imageWidth:\n                imageShape = (int(base * (imageWidth / imageHeight)), int(base))\n            else:\n                imageShape = (int(base), int(base * (imageHeight / imageWidth)))\n            image = cv2.resize(image, imageShape)\n            x0 = int((half_width - imageShape[0]) / 2) + basePoints[index][0]\n            x1 = x0 + imageShape[0]\n            y0 = int((half_width - imageShape[1]) / 2) + basePoints[index][1]\n            y1 = y0 + imageShape[1]\n            if random.random() > 0.5:\n                draw.rectangle((x0, y0, x1, y1), fill=\"white\")",
        "type": "code",
        "location": "/tests/anime_highlight_cuts/theme_collector/create_coco_pip_dataset.py:255-281"
    },
    "2629": {
        "file_id": 286,
        "content": "This code resizes images to fit a specified rectangle shape and randomly colors the rectangles white or leaves them transparent.",
        "type": "comment"
    },
    "2630": {
        "file_id": 286,
        "content": "            else:\n                draw.rounded_rectangle(\n                    (x0, y0, x1, y1),\n                    fill=\"white\",\n                    radius=min(int(x1 - x0) / 2, int(y1 - y0) / 2, getRadius()),\n                )\n            imageCanvas[y0 : image.shape[0] + y0, x0 : image.shape[1] + x0, :] = image\n            imageCoordinates.append(\n                (\n                    x0 + image.shape[1] / 2,\n                    y0 + image.shape[0] / 2,\n                    image.shape[1],\n                    image.shape[0],\n                )\n            )  # x_center, y_center, width, height\n    ## mix images with mask\n    imageMaskNumpyArray = np.array(imageMask) / 255  # float64\n    imageMaskNumpyArrayInverted = 1 - imageMaskNumpyArray\n    x0 = 0\n    y0 = textTotalHeight if textFormat == \"up\" else 0\n    backgroundImage[y0 : y0 + imageCanvasShape[0], x0 : x0 + imageCanvasShape[1], :] = (\n        backgroundImage[y0 : y0 + imageCanvasShape[0], x0 : x0 + imageCanvasShape[1], :]\n        * imageMaskNumpyArrayInverted",
        "type": "code",
        "location": "/tests/anime_highlight_cuts/theme_collector/create_coco_pip_dataset.py:282-308"
    },
    "2631": {
        "file_id": 286,
        "content": "This code creates a rounded rectangle with white fill, then combines it with an image and stores the coordinates. It also adjusts mask images and blends them into a background image using numpy arrays.",
        "type": "comment"
    },
    "2632": {
        "file_id": 286,
        "content": "    ).astype(np.uint8) + (imageCanvas * imageMaskNumpyArray).astype(np.uint8)\n    print()\n    ## get labels which will be exported to txt\n    for coord in imageCoordinates:\n        x_center_relative, y_center_relative, imWidth, imHeight = coord\n        x_center, y_center = x_center_relative + x0, y_center_relative + y0\n        dataPoints = [\n            x_center / backgroundShape[1],\n            y_center / backgroundShape[0],\n            imWidth / backgroundShape[1],\n            imHeight / backgroundShape[0],\n        ]\n        labelString = \" \".join(([\"0\"] + [f\"{number:.3f}\" for number in dataPoints]))\n        print(\"LABELSTRING?\", labelString)\n    ## preview\n    previewImageName = f\"{imageFormat}_{textFormat}_{backgroundFormat}.png\"\n    cv2.imshow(previewImageName, backgroundImage)\n    cv2.waitKey(0)",
        "type": "code",
        "location": "/tests/anime_highlight_cuts/theme_collector/create_coco_pip_dataset.py:309-329"
    },
    "2633": {
        "file_id": 286,
        "content": "This code segment performs image manipulation, extracts coordinates, and generates labels for data export. It also creates a preview image before saving.",
        "type": "comment"
    },
    "2634": {
        "file_id": 287,
        "content": "/tests/anime_highlight_cuts/theme_collector/bv2av.py",
        "type": "filepath"
    },
    "2635": {
        "file_id": 287,
        "content": "This code defines functions for converting BV and AV IDs. It uses a table to perform the conversion, where 'dec' function is used for decoding BV to AV and 'enc' function is used for encoding AV to BV. The 'bv_av_conversion' function takes a string input as source and returns the corresponding converted ID based on whether it starts with 'BV' or 'AV'. If the input is invalid, it prompts the user to reenter.",
        "type": "summary"
    },
    "2636": {
        "file_id": 287,
        "content": "table = 'fZodR9XQDSUm21yCkr6zBqiveYah8bt4xsWpHnJE7jL5VG3guMTKNPAwcF'\ntr = {}\nfor i in range(58):\n    tr[table[i]] = i\ns = [11, 10, 3, 8, 4, 6]\nxor = 177451812\nadd = 8728348608\ndef dec(x):\n    r = 0\n    for i in range(6):\n        r += tr[x[s[i]]] * 58 ** i\n    return (r - add) ^ xor\ndef enc(x):\n    x = (x ^ xor) + add\n    r = list('BV1  4 1 7  ')\n    for i in range(6):\n        r[s[i]] = table[x // 58 ** i % 58]\n    return ''.join(r)\ndef bv_av_conversion(source:str):\n    print(\"请输入BV或AV号,需要带上BV或AV前缀:\") \n    av_bv =source+\"  \"\n    head = str(av_bv[0]) + str(av_bv[1])\n    av = [\"av\", \"AV\", \"Av\", \"aV\"]\n    bv = [\"bv\", \"BV\", \"Bv\", \"bV\"]\n    if head in av:\n        val = (enc(int(av_bv[2:-2])))\n    elif head in bv:\n        val = f\"av{dec('BV' + av_bv[2:-2])}\"\n    else:\n        print(\"你的输入有误请重新输入\")\n        val = None\n    return val",
        "type": "code",
        "location": "/tests/anime_highlight_cuts/theme_collector/bv2av.py:3-40"
    },
    "2637": {
        "file_id": 287,
        "content": "This code defines functions for converting BV and AV IDs. It uses a table to perform the conversion, where 'dec' function is used for decoding BV to AV and 'enc' function is used for encoding AV to BV. The 'bv_av_conversion' function takes a string input as source and returns the corresponding converted ID based on whether it starts with 'BV' or 'AV'. If the input is invalid, it prompts the user to reenter.",
        "type": "comment"
    },
    "2638": {
        "file_id": 288,
        "content": "/tests/anime_highlight_cuts/theme_collector/create_dataset.py",
        "type": "filepath"
    },
    "2639": {
        "file_id": 288,
        "content": "This code creates a new directory, reads video frames, extracts data points, labels them, and saves images as JPEGs in YAML format. The process ends with \"dataset created.\" message after the loop.",
        "type": "summary"
    },
    "2640": {
        "file_id": 288,
        "content": "import yaml\n# why you are taking so much RAM?\n## suggest that you label some (many) still image and mark out the picture-in-picture parts from it? about 2000 images?\n## man just make sure these pictures are not \"pip\" so we can put borders and arrange them randomly to create our super dataset. use MSCOCO/coco128?\ntrain_path = \"images/train\"\ntest_path = \"images/test\"\ntrain_label_path = \"labels/train\"\ntest_label_path = \"labels/test\"\nbasepath = \"pip_dataset\"\ndata = {\n    \"path\": f\"../{basepath}\",  # dataset root dir\n    \"train\": train_path,  # train images (relative to 'path')\n    \"val\": train_path,  # val images (relative to 'path')\n    \"test\": test_path,\n    \"names\": {0: \"active_frame\"},\n}\nimport os\nos.system(f\"rm -rf {basepath}\")\nindex = 1\nos.makedirs(os.path.join(basepath, train_path), exist_ok=True)\nos.makedirs(os.path.join(basepath, test_path), exist_ok=True)\nos.makedirs(os.path.join(basepath, train_label_path), exist_ok=True)\nos.makedirs(os.path.join(basepath, test_label_path), exist_ok=True)\nwith open(\"pip_dataset/pip_dataset.yaml\", \"w+\") as f:",
        "type": "code",
        "location": "/tests/anime_highlight_cuts/theme_collector/create_dataset.py:1-39"
    },
    "2641": {
        "file_id": 288,
        "content": "This code creates a new dataset by copying and renaming files from existing \"train\" and \"test\" directories into a new directory named \"pip_dataset\". It also creates label files for the copied images in the same manner. The resulting dataset is stored in a YAML file named \"pip_dataset/pip_dataset.yaml\".",
        "type": "comment"
    },
    "2642": {
        "file_id": 288,
        "content": "    f.write(yaml.dump(data, default_flow_style=False))\nimport cv2\nimport pandas\ncsvNames = [fpath for fpath in os.listdir(\".\") if fpath.endswith(\".csv\")]\nimport progressbar\nremainder = 7 # changed? heck?\nfor csvName in csvNames:\n    dataframe = pandas.read_csv(csvName)\n    videoFileName = f'{csvName.split(\".\")[0]}.mp4'\n    #\n    frameIndex = 0\n    cap = cv2.VideoCapture(videoFileName)\n    myIterator = progressbar.progressbar(dataframe.iterrows())\n    frame_height, frame_width = cap.get(cv2.CAP_PROP_FRAME_HEIGHT), cap.get(\n        cv2.CAP_PROP_FRAME_WIDTH\n    )\n    while True:\n        succ, image = cap.read()\n        nextRow = next(myIterator, None)\n        if nextRow is None:\n            break\n        if succ:\n            frameIndex += 1\n            if frameIndex % remainder != 0:\n                continue\n            _, _, min_x, min_y, w, h = nextRow[1].tolist()\n            if (min_x, min_y, w, h) == (0, 0, 0, 0) or w == 0 or h == 0:\n                continue\n            index += 1\n            imageName = f'{f\"{index}\".zfill(12)}.png'",
        "type": "code",
        "location": "/tests/anime_highlight_cuts/theme_collector/create_dataset.py:40-73"
    },
    "2643": {
        "file_id": 288,
        "content": "Reading CSV files, extracting relevant data and frame index, creating image names based on the index, writing dataset to YAML format.",
        "type": "comment"
    },
    "2644": {
        "file_id": 288,
        "content": "            labelName = f'{f\"{index}\".zfill(12)}.txt'\n            dataPoints = [\n                (min_x + w / 2) / frame_width,\n                (min_y + h / 2) / frame_height,\n                w / frame_width,\n                h / frame_height,\n            ]\n            with open(os.path.join(basepath, train_label_path, labelName), \"w+\") as f:\n                content = \" \".join(([\"0\"] + [f\"{number:.3f}\" for number in dataPoints]))\n                f.write(content)\n            cv2.imwrite(os.path.join(basepath, train_path, imageName), image)\n            del image\n        else:\n            break\n    cap.release()\n    del cap\n    del dataframe\ntestVideo = \"output.mp4\"\nw, h = 1152, 648\nmin_x, min_y = 384, 216\nprint(\"creating 4min pip dataset\")\ncap = cv2.VideoCapture(testVideo)\nframe_height, frame_width = cap.get(cv2.CAP_PROP_FRAME_HEIGHT), cap.get(\n    cv2.CAP_PROP_FRAME_WIDTH\n)\ndataPoints = [\n    (min_x + w / 2) / frame_width,\n    (min_y + h / 2) / frame_height,\n    w / frame_width,\n    h / frame_height,\n]\nframeCounter = 0",
        "type": "code",
        "location": "/tests/anime_highlight_cuts/theme_collector/create_dataset.py:74-110"
    },
    "2645": {
        "file_id": 288,
        "content": "The code reads an input video file, extracts and saves data points for each frame, and stores the labeled data in a text file. It also writes each frame to an image file.",
        "type": "comment"
    },
    "2646": {
        "file_id": 288,
        "content": "while True:\n    succ, image = cap.read()\n    if succ:\n        frameCounter += 1\n        if frameCounter % remainder != 0:\n            continue\n        index += 1\n        imageName = f'{f\"{index}\".zfill(12)}.png'\n        labelName = f'{f\"{index}\".zfill(12)}.txt'\n        with open(os.path.join(basepath, train_label_path, labelName), \"w+\") as f:\n            content = \" \".join(([\"0\"] + [f\"{number:.3f}\" for number in dataPoints]))\n            f.write(content)\n        cv2.imwrite(os.path.join(basepath, train_path, imageName), image)\n        del image\n    else:\n        break\ncap.release()\ndel cap\nprint(\"creating reference dataset\")\ntestVideo = \"output_1.mp4\"\ncap = cv2.VideoCapture(testVideo)\nframe_height, frame_width = cap.get(cv2.CAP_PROP_FRAME_HEIGHT), cap.get(\n    cv2.CAP_PROP_FRAME_WIDTH\n)\ndataPoints = [0.5, 0.5, 1, 1]\nframeCounter = 0\nwhile True:\n    succ, image = cap.read()\n    if succ:\n        frameCounter += 1\n        if frameCounter % remainder != 0:\n            continue\n        index += 1\n        imageName = f'{f\"{index}\".zfill(12)}.png'",
        "type": "code",
        "location": "/tests/anime_highlight_cuts/theme_collector/create_dataset.py:111-152"
    },
    "2647": {
        "file_id": 288,
        "content": "Code snippet reads frames from a video, creates label files for each frame containing data points, and saves corresponding images. The loop iterates until the end of the video is reached, skipping non-remainder frames. It uses OpenCV to read and write image data and handles file writing.",
        "type": "comment"
    },
    "2648": {
        "file_id": 288,
        "content": "        labelName = f'{f\"{index}\".zfill(12)}.txt'\n        with open(os.path.join(basepath, train_label_path, labelName), \"w+\") as f:\n            content = \" \".join(([\"0\"] + [f\"{number:.3f}\" for number in dataPoints]))\n            f.write(content)\n        cv2.imwrite(os.path.join(basepath, train_path, imageName), image)\n        del image\n    else:\n        break\ncap.release()\ndel cap\nprint(\"dataset created.\")",
        "type": "code",
        "location": "/tests/anime_highlight_cuts/theme_collector/create_dataset.py:153-166"
    },
    "2649": {
        "file_id": 288,
        "content": "This code creates a dataset by iterating over data points and saving them as labels in text files. It also saves corresponding images as JPEGs. Finally, it prints \"dataset created.\" after the loop ends.",
        "type": "comment"
    },
    "2650": {
        "file_id": 289,
        "content": "/tests/anime_highlight_cuts/theme_collector/strip_optimizer_from_trained_best_model.py",
        "type": "filepath"
    },
    "2651": {
        "file_id": 289,
        "content": "This code imports the \"strip_optimizer\" function from the \"torch_utils\" module, then it specifies the original model path (\"general_ver1_with_optimizer.pt\") and the exported model path (\"general_ver1.pt\"). The \"strip_optimizer\" function is called with these paths to remove any optimizers associated with the original model while saving a new model without them at the specified export path.",
        "type": "summary"
    },
    "2652": {
        "file_id": 289,
        "content": "from ultralytics.yolo.utils.torch_utils import strip_optimizer\nmodel_path = \"general_ver1_with_optimizer.pt\"\nexport_path = \"general_ver1.pt\"\nstrip_optimizer(f=model_path, s=export_path)",
        "type": "code",
        "location": "/tests/anime_highlight_cuts/theme_collector/strip_optimizer_from_trained_best_model.py:1-6"
    },
    "2653": {
        "file_id": 289,
        "content": "This code imports the \"strip_optimizer\" function from the \"torch_utils\" module, then it specifies the original model path (\"general_ver1_with_optimizer.pt\") and the exported model path (\"general_ver1.pt\"). The \"strip_optimizer\" function is called with these paths to remove any optimizers associated with the original model while saving a new model without them at the specified export path.",
        "type": "comment"
    },
    "2654": {
        "file_id": 290,
        "content": "/tests/anime_highlight_cuts/theme_collector/screenshot_tracemoe.py",
        "type": "filepath"
    },
    "2655": {
        "file_id": 290,
        "content": "This code sends a JPEG image to the trace.moe API for anime character recognition and stores the results in the 'data' variable. It then prints the data using rich library, handles potential errors, and retrieves necessary information from the results, including anilist ID, filename, episode (if available), start and end timestamps, and similarity rating.",
        "type": "summary"
    },
    "2656": {
        "file_id": 290,
        "content": "# anilist has typo on \"Yahari Ore no Seishun Lovecome wa Machigatte Iru.\" which might be harmful.\n# imagePath = \"/Users/jamesbrown/Downloads/anime_download/dress_test_pictures/女装0.jpeg\"\nimagePath = \"/Users/jamesbrown/Downloads/gay_anime_shot.jpeg\"\nimport requests\ndata =requests.post(\"https://api.trace.moe/search\",\n  data=open(imagePath, \"rb\"), # since this is smallest\n  headers={\"Content-Type\": \"image/jpeg\"}\n).json() # remember you must change your ip later.\nimport rich\nrich.print(data) # the anime character recognition website is not running so well.\nerror = data['error']\nassert error == \"\"\nresults = data.get('result',[])\nfor result in results: # already sorted.\n    anilist_id = result['anilist'] # well. we only got one.\n    filename = result['filename'] # need parsing right?\n    episode = result.get('episode', None) # really we don't have episode here.\n    start, end = result['from'], result['to']\n    similarity = result['similarity']\nbreakpoint()",
        "type": "code",
        "location": "/tests/anime_highlight_cuts/theme_collector/screenshot_tracemoe.py:1-25"
    },
    "2657": {
        "file_id": 290,
        "content": "This code sends a JPEG image to the trace.moe API for anime character recognition and stores the results in the 'data' variable. It then prints the data using rich library, handles potential errors, and retrieves necessary information from the results, including anilist ID, filename, episode (if available), start and end timestamps, and similarity rating.",
        "type": "comment"
    },
    "2658": {
        "file_id": 291,
        "content": "/tests/anime_highlight_cuts/theme_collector/screenshot_saucenao.py",
        "type": "filepath"
    },
    "2659": {
        "file_id": 291,
        "content": "This code uses the SauceNao API to identify an anime source from a given image file, displaying results such as similarity and URLs, and extracting relevant data for anime cuts including part, title, estimated time, and IDs from various platforms.",
        "type": "summary"
    },
    "2660": {
        "file_id": 291,
        "content": "# saucenao (if fail, use trace.moe)\n# use proxies, since we are using free tiers.\nimport os\nSAUCENAO_API_KEY=os.environ.get('SAUCENAO_API_KEY') # how to run this without api key?\nprint(\"API KEY?\", SAUCENAO_API_KEY)\n# sauce = SauceNao(api_key=SAUCENAO_API_KEY) # shit. not working!\nfilepath = \"/Users/jamesbrown/Downloads/anime_download/dress_test_pictures/女装0.jpeg\"\n# import asyncio\n# loop = asyncio.get_event_loop()\n# results = loop.run_until_complete(sauce.from_file(filepath))\n# results = await sauce.from_url('https://i.imgur.com/QaKpV3s.png')\n# no api key. fuck.\nfrom saucenao_api import SauceNao\nsauce = SauceNao(SAUCENAO_API_KEY)\nwith open(filepath,'rb') as f:\n    results = sauce.from_file(f)\n    long_remaining = results.long_remaining # wait till next day? wtf?\n    short_remaining = results.short_remaining\n    result_results = len(results)\n    print(results)\n    best = results[0]\n    similarity = best.similarity\n    # just trust anilist.\n    urls = best.urls # https://anilist.co/anime/ https://anidb.net/anime/ https://myanimelist.net/anime/",
        "type": "code",
        "location": "/tests/anime_highlight_cuts/theme_collector/screenshot_saucenao.py:1-25"
    },
    "2661": {
        "file_id": 291,
        "content": "The code aims to use the SauceNao API to identify an anime source from a given image file. It first checks if the SAUCENAO_API_KEY is set in the environment variables and prints it out. Then, it creates a SauceNao object with the API key and tries to find similar images using the image file path. The code then displays the remaining time before the results become available (long_remaining and short_remaining), the number of results found (result_results), the best match's similarity, and the URLs associated with the best match. If no API key is provided, the code indicates that it will not be able to use SauceNao API.",
        "type": "comment"
    },
    "2662": {
        "file_id": 291,
        "content": "    best_data =  best.raw.get('data',{})\n    part = best_data.get('part', None) # not always.\n    title = best.title\n    est_time =best_data.get('est_time',None) # be like: '00:16:21 / 00:25:12'\n    if est_time:\n        start_end = [timestamp.strip() for timestamp in est_time.split(\"/\")]\n        start_time, end_time = start_end\n    # these ids must be the same across different images.\n    anidb_aid = best_data.get('anidb_aid',None)\n    mal_id = best_data.get('mal_id',None)\n    anilist_id = best_data.get('anilist_id',None)\n    breakpoint()",
        "type": "code",
        "location": "/tests/anime_highlight_cuts/theme_collector/screenshot_saucenao.py:26-37"
    },
    "2663": {
        "file_id": 291,
        "content": "Extracts relevant data for an anime cut: part (may not always be available), title, estimated time (in the format \"start / end\"), and IDs from different platforms - AniDB (anidb_aid), MyAnimeList (mal_id), and AniList (anilist_id).",
        "type": "comment"
    },
    "2664": {
        "file_id": 292,
        "content": "/tests/anime_highlight_cuts/theme_collector/pack_source_dataset.sh",
        "type": "filepath"
    },
    "2665": {
        "file_id": 292,
        "content": "This command uses 7-Zip (7z) to create a compressed archive named \"pip_source_dataset.7z\" that includes all .mp4 and .csv files in the current directory, likely for efficient storage or transfer.",
        "type": "summary"
    },
    "2666": {
        "file_id": 292,
        "content": "7z a pip_source_dataset.7z *.mp4 *.csv",
        "type": "code",
        "location": "/tests/anime_highlight_cuts/theme_collector/pack_source_dataset.sh:1-1"
    },
    "2667": {
        "file_id": 292,
        "content": "This command uses 7-Zip (7z) to create a compressed archive named \"pip_source_dataset.7z\" that includes all .mp4 and .csv files in the current directory, likely for efficient storage or transfer.",
        "type": "comment"
    },
    "2668": {
        "file_id": 293,
        "content": "/tests/anime_highlight_cuts/theme_collector/yolov8_train_save_test.py",
        "type": "filepath"
    },
    "2669": {
        "file_id": 293,
        "content": "The code is training a YOLO object detection model, validating its performance, and then exporting it to be used later. It uses the \"yolov8n.pt\" pre-trained model, trains it for 3 epochs with the provided dataset, evaluates its validation accuracy, displays the results, and finally exports the trained model as \"pip_detector.pth\".",
        "type": "summary"
    },
    "2670": {
        "file_id": 293,
        "content": "from ultralytics import YOLO\n# pip install opencv-python==4.5.5.64\n# shit?\n# https://github.com/asweigart/pyautogui/issues/706\nmodel = YOLO(\"yolov8n.pt\")\n# print(model)\n# model.to('mps')\n# The operator 'aten::_slow_conv2d_forward' is not current implemented for the MPS device.\n# fuck.\n# breakpoint()\nimport rich\ntrain_result = model.train(epochs=3, data=\"./pip_dataset/pip_dataset.yaml\")\nprint(\"TRAIN RESULT?\")\nrich.print(train_result)\nval_result = model.val()\nprint(\"VALIDATION RESULT?\")\nrich.print(val_result)\ntest_result = model(\"./pip_dataset/images/test/000000003099.png\")\ntest_boxes = test_result[0].boxes\ntest_classes, test_xywh, test_confidence = (\n    test_boxes.cls.numpy(),\n    test_boxes.xywh.numpy(), # the xy in this xywh means the center of the bounding box.\n    test_boxes.conf.numpy(),\n)\nprint(\"XYWH?\", test_xywh)\nprint(\"CLASSES?\", test_classes)\nprint(\"CONFIDENCE?\", test_confidence)\n# model.export(format=\"pytorch\", path=\"./pip_detector.pth\")",
        "type": "code",
        "location": "/tests/anime_highlight_cuts/theme_collector/yolov8_train_save_test.py:1-39"
    },
    "2671": {
        "file_id": 293,
        "content": "The code is training a YOLO object detection model, validating its performance, and then exporting it to be used later. It uses the \"yolov8n.pt\" pre-trained model, trains it for 3 epochs with the provided dataset, evaluates its validation accuracy, displays the results, and finally exports the trained model as \"pip_detector.pth\".",
        "type": "comment"
    },
    "2672": {
        "file_id": 294,
        "content": "/tests/anime_highlight_cuts/theme_collector/yolov8_test.py",
        "type": "filepath"
    },
    "2673": {
        "file_id": 294,
        "content": "This code imports and processes images using the YOLO model, filters frames based on criteria, selects candidates for main frame detection, draws a rectangle around the detected frame, and displays an image with the PIP frame.",
        "type": "summary"
    },
    "2674": {
        "file_id": 294,
        "content": "from ultralytics import YOLO\n## yolov8 tracking needs special ultralytics version. it is been updated too damn often. you need to downgrade.\n## https://github.com/mikel-brostrom/yolov8_tracking\n## this might add unwanted overheads. warning!\n# no one will miss `genesis.pt`, right?\nmodel = YOLO(\"general_ver1.pt\")\n## TODO: create dataset to prevent detection of pure color/gradient borders\n# model = YOLO(\"ver3.pt\")\n# find trained weights on huggingface:\n# https://huggingface.co/James4Ever0/yolov8_pip_ultralytics\n# imagePaths = [\n#     \"000000003099.png\",\n#     \"simple_pip.png\",\n#     \"no_border_0.jpg\",\n#     \"has_border_0.jpg\",\n#     \"has_border_1.jpg\",\n#     \"has_border_2.jpg\",\n# ]\nimport os\nimagePaths = [\n    fpath\n    for fpath in os.listdir(\".\")\n    if fpath.split(\".\")[-1].lower() in (\"jpg\", \"jpeg\", \"png\")\n]\nimport cv2\nframeRatioFilters = [(16 / 9, 0.2, \"landscape\")]\nframeAreaThreshold = 0.15\nfor imagePath in imagePaths:\n    image = cv2.imread(imagePath)\n    output = model(image)\n    height, width, _ = image.shape\n    center = (width / 2, height / 2)",
        "type": "code",
        "location": "/tests/anime_highlight_cuts/theme_collector/yolov8_test.py:1-43"
    },
    "2675": {
        "file_id": 294,
        "content": "This code imports the YOLO model from ultralytics, loads a specific model file, and defines image paths. It also retrieves all image files in the current directory, filters frames based on aspect ratio and area threshold, and processes each image using the loaded YOLO model. This might involve downgrading the ultralytics version due to frequent updates and creating a dataset to prevent detection of pure color/gradient borders as TODO tasks.",
        "type": "comment"
    },
    "2676": {
        "file_id": 294,
        "content": "    # print(\"CENTER:\",center)\n    candidates = []\n    for xyxy in output[0].boxes.xyxy.numpy().astype(int).tolist():\n        x0, y0, x1, y1 = xyxy\n        currentFrameWidth = x1 - x0\n        currentFrameHeight = y1 - y0\n        currentFrameArea = currentFrameWidth * currentFrameHeight\n        # area filter? a must.\n        if currentFrameArea / (height * width) < frameAreaThreshold:\n            continue\n        else:\n            # filter out malformed frames? just for anime?\n            currentFrameRatio = currentFrameWidth / currentFrameHeight\n            if all(\n                [\n                    (\n                        currentFrameRatio < frameRatioStandard - frameRatioMargin\n                        or currentFrameRatio > frameRatioStandard + frameRatioMargin\n                    )\n                    for frameRatioStandard, frameRatioMargin, _ in frameRatioFilters\n                ]\n            ):\n                continue\n            candidates.append((x0, y0, x1, y1))\n    # sort it by area, then by centrality?",
        "type": "code",
        "location": "/tests/anime_highlight_cuts/theme_collector/yolov8_test.py:44-69"
    },
    "2677": {
        "file_id": 294,
        "content": "This code filters out detected frames based on area, frame aspect ratio, and possibly malformed frames. It appends valid frames to the 'candidates' list, which may be sorted by area and centrality later in the script.",
        "type": "comment"
    },
    "2678": {
        "file_id": 294,
        "content": "    candidates.sort(\n        key=lambda points: -(points[2] - points[0]) * (points[3] - points[1])\n    )\n    # print(\"SORT_AREA:\", [(points[2] - points[0]) * (points[3] - points[1]) for points in candidates])\n    candidates = candidates[:2]\n    candidates.sort(\n        key=lambda points: (((points[2] + points[0]) / 2) - center[0]) ** 2\n        + (((points[3] + points[1]) / 2) - center[1]) ** 2\n    )\n    # print(\"SORT_CENTRALITY:\", [(((points[2] + points[0]) / 2) - center[0]) ** 2\n    # + (((points[3] + points[1]) / 2) - center[1]) ** 2 for points in candidates])\n    if len(candidates) > 0:\n        print(\"main frame found.\")\n        x0, y0, x1, y1 = candidates[0]\n        cv2.rectangle(image, (x0, y0), (x1, y1), (0, 0, 255), thickness=10)\n    else:\n        print(\"no main frame found.\")\n    cv2.imshow(\"PIP\", image)\n    cv2.waitKey(0)",
        "type": "code",
        "location": "/tests/anime_highlight_cuts/theme_collector/yolov8_test.py:71-89"
    },
    "2679": {
        "file_id": 294,
        "content": "The code sorts the candidates by area and centrality, selects two candidates, and if a main frame is found, it draws a rectangle around it. If no main frame is found, it displays a message. Finally, it shows the image with the PIP frame.",
        "type": "comment"
    },
    "2680": {
        "file_id": 295,
        "content": "/tests/youtube_shorts_heuristic_search/README.md",
        "type": "filepath"
    },
    "2681": {
        "file_id": 295,
        "content": "Code contains information about search and downloadability of YouTube Shorts, as well as mentioning the limitations of current video feed acquisition from various platforms. It also provides a link to a Python library for YouTube search and details on a simple bilibili search API.",
        "type": "summary"
    },
    "2682": {
        "file_id": 295,
        "content": "turned out youtube shorts are searchable, downloadable.\nbut the video feed is not yet acquired, just like all other video feeds from youtube, twitch, reddit, qq小世界, bilibili, 抖音, tiktok and the trending ones.\nthe youtube advanced filter is embedded in the search results. you can only jump to one embedded link at a time\nto get the next page on youtube:\nthe key seems to be the unified unlimited api key for youtube.\nPOST https://www.youtube.com/youtubei/v1/search?key=AIzaSyAO_FJ2SlqU8Q4STEHLGCilw_Y9_11qcW8&prettyPrint=false with a lot of headaching parameters.\nyou can bypass them. here are some basic parameters without lots of combinations. if you want to combine them, better figure it out yourself.\nsome of them might already fail to work.\nhttps://github.com/alexmercerind/youtube-search-python/blob/fc12c05747f1f7bd89d71699403762b86b523da5/youtubesearchpython/core/constants.py#L45\nbilibili search api is currently simple:\nhttps://search.bilibili.com/video?keyword=%E6%B1%AA%E6%B1%AA&from_source=webtop_search&spm_id_from=333.1007&search_source=3&tids=219&order=dm&duration=2",
        "type": "code",
        "location": "/tests/youtube_shorts_heuristic_search/README.md:1-21"
    },
    "2683": {
        "file_id": 295,
        "content": "Code contains information about search and downloadability of YouTube Shorts, as well as mentioning the limitations of current video feed acquisition from various platforms. It also provides a link to a Python library for YouTube search and details on a simple bilibili search API.",
        "type": "comment"
    },
    "2684": {
        "file_id": 296,
        "content": "/tests/youtube_shorts_heuristic_search/heuristic_model.py",
        "type": "filepath"
    },
    "2685": {
        "file_id": 296,
        "content": "The code defines a function called \"getSearchQueryFromHeuristicSpace\" which takes a seed as input and returns a search query. This model aims to reduce repetition and increase the likelihood of finding relevant videos by utilizing heuristics. The application is intended to be server-based.",
        "type": "summary"
    },
    "2686": {
        "file_id": 296,
        "content": "seed = 'dog cute'\ndef getSearchQueryFromHeuristicSpace(seed):\n    # less likely to repeat, and more possibility to get needed videos.\n    return searchQuery\n# this heuristic search model shall be a server based application.",
        "type": "code",
        "location": "/tests/youtube_shorts_heuristic_search/heuristic_model.py:2-8"
    },
    "2687": {
        "file_id": 296,
        "content": "The code defines a function called \"getSearchQueryFromHeuristicSpace\" which takes a seed as input and returns a search query. This model aims to reduce repetition and increase the likelihood of finding relevant videos by utilizing heuristics. The application is intended to be server-based.",
        "type": "comment"
    },
    "2688": {
        "file_id": 297,
        "content": "/tests/bilibili_practices/bilibili_science_subtitle_with_cn_voice/web_translator.py",
        "type": "filepath"
    },
    "2689": {
        "file_id": 297,
        "content": "The code defines a translator function that randomly selects from multiple translation services to convert English text to either \"zh\" or \"zh-CHS\". It uses the \"translators\" module and imports random for selecting the translation service and language. The code also allows for different combinations of translation services to be tested by uncommenting specific lines in the mtranslators list. If an error occurs during translation, it prints the exception stack trace using traceback.",
        "type": "summary"
    },
    "2690": {
        "file_id": 297,
        "content": "import translators as ts\n# translator = \n# mtranslators = [ts.sogou] #this is pure shit.\n# mtranslators = [ts.baidu,ts.sogou]\n# mtranslators = [ts.baidu,ts.sogou,ts.iciba]\nmtranslators = [ts.youdao,ts.baidu,ts.alibaba] # no yandex, tencent, sogou.\n# mtranslators = [ts.baidu,ts.iciba]\nimport random\ndef translator(text):\n    randomLang = [\"zh\",\"zh-CHS\"]\n    from_language = \"en\"\n    # lang = random.choice(randomLang)\n    while True:\n        t = random.choice(mtranslators)\n        # print(type(translator))\n        for rl in randomLang:\n            try:\n                result = t(text,from_language=from_language,to_language=rl)\n                # if len(result) < 3:\n                #     print(t)\n                #     breakpoint()\n                return result\n            except:\n                import traceback\n                traceback.print_exc()",
        "type": "code",
        "location": "/tests/bilibili_practices/bilibili_science_subtitle_with_cn_voice/web_translator.py:1-27"
    },
    "2691": {
        "file_id": 297,
        "content": "The code defines a translator function that randomly selects from multiple translation services to convert English text to either \"zh\" or \"zh-CHS\". It uses the \"translators\" module and imports random for selecting the translation service and language. The code also allows for different combinations of translation services to be tested by uncommenting specific lines in the mtranslators list. If an error occurs during translation, it prints the exception stack trace using traceback.",
        "type": "comment"
    },
    "2692": {
        "file_id": 298,
        "content": "/tests/bilibili_practices/bilibili_science_subtitle_with_cn_voice/translate_srt.py",
        "type": "filepath"
    },
    "2693": {
        "file_id": 298,
        "content": "Code reads an SRT file, translates each line of the content using a web translator, wraps the translated lines to meet a certain character limit, and then saves the results in a new SRT file. The process involves parsing the input SRT file using the \"srt\" library, translating text with \"web_translator\", and modifying the line wrapping for better readability.",
        "type": "summary"
    },
    "2694": {
        "file_id": 298,
        "content": "src = \"en_.srt\"\nfinal_srt = \"zh_translated.srt\"\nimport srt\nwrap_limit = 20\nsource_srt = open(src, \"r\",encoding=\"utf-8\").read()\nssrt = srt.parse(source_srt)\nfrom web_translator import translator\nimport math\ndef wrapLine(line):\n    lines = [line[x*wrap_limit:(x+1)*wrap_limit] for x in range(math.ceil(len(line)/wrap_limit))]\n    return \"\\n\".join(lines)\ndef fixline(line):\n    notEndings = [\"。\",\"，\"]\n    for x in notEndings:\n        if line.endswith(x): return line[:-1]\n    return line\nnew_ssrt = []\nfor line in ssrt:\n    # print(line)\n    start = line.start\n    end = line.end # timedelta.\n    content = line.content\n    index = line.index\n    unwrapped_content = content.replace(\"\\n\",\" \")\n    result = translator(unwrapped_content)\n    result = fixline(result)\n    print(result)\n    line.content = result\n    new_ssrt.append(line)\n    # wrapped = wrapLine(result)\n    # print(wrapped)\n    # print(start, end, content, index)\nfinal_content = srt.compose(new_ssrt)\nwith open(final_srt,\"w+\",encoding=\"utf-8\") as f:\n    f.write(final_content)",
        "type": "code",
        "location": "/tests/bilibili_practices/bilibili_video_translate/translate_srt.py:1-45"
    },
    "2695": {
        "file_id": 298,
        "content": "Code reads an SRT file, translates each line of the content using a web translator, wraps the translated lines to meet a certain character limit, and then saves the results in a new SRT file. The process involves parsing the input SRT file using the \"srt\" library, translating text with \"web_translator\", and modifying the line wrapping for better readability.",
        "type": "comment"
    },
    "2696": {
        "file_id": 299,
        "content": "/tests/bilibili_practices/bilibili_science_subtitle_with_cn_voice/test2.py",
        "type": "filepath"
    },
    "2697": {
        "file_id": 299,
        "content": "The code imports the os module and defines two commands - one to install yt-dlp using pip3, another to download subtitles from a YouTube video with yt-dlp. It then iterates over each command in the list and runs them using os.system(). This will result in yt-dlp being installed and the subtitles being downloaded for the specified YouTube video.",
        "type": "summary"
    },
    "2698": {
        "file_id": 299,
        "content": "import os\ncommands = [\"pip3 install yt-dlp\",'yt-dlp --write-subs --convert-subtitles srt \"https://m.youtube.com/watch?v=At7ORzmAaT4\"'] # get recommendation this time.\n# we will still get many videoId from curl.\nfor c in commands:\n    os.system(c)",
        "type": "code",
        "location": "/tests/bilibili_practices/bilibili_science_subtitle_with_cn_voice/test2.py:1-8"
    },
    "2699": {
        "file_id": 299,
        "content": "The code imports the os module and defines two commands - one to install yt-dlp using pip3, another to download subtitles from a YouTube video with yt-dlp. It then iterates over each command in the list and runs them using os.system(). This will result in yt-dlp being installed and the subtitles being downloaded for the specified YouTube video.",
        "type": "comment"
    }
}