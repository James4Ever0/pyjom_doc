{
    "5000": {
        "file_id": 650,
        "content": "class {{ channelName }}:\n    tid = {{ channelTid }}{% for subChannelName, subChannelTid in subChannels %}\n    {{ subChannelName }} = {{ subChannelTid }}{% endfor %}",
        "type": "code",
        "location": "/tests/bilibili_search_api_modification_section_params_get_related_videos/template.j2:2-4"
    },
    "5001": {
        "file_id": 650,
        "content": "This code defines a class with properties named after channel names, where the values are their respective channel TIDs. It also includes subchannels as additional properties, each with its own TID.",
        "type": "comment"
    },
    "5002": {
        "file_id": 651,
        "content": "/tests/bilibili_search_api_modification_section_params_get_related_videos/searchDataParser.py",
        "type": "filepath"
    },
    "5003": {
        "file_id": 651,
        "content": "The code processes generators, repairs links, detects errors, and extracts links using regular expressions. It also parses video descriptions for BGM detection and author keyword extraction with Jieba segmentation, and updates video information by processing video-related data.",
        "type": "summary"
    },
    "5004": {
        "file_id": 651,
        "content": "import json\nfrom bs4 import BeautifulSoup\nfrom lazero.utils.logger import sprint\ndef generatorToList(generator):\n    return [x for x in generator]\ndef linkFixer(link, prefix=\"http:\"):\n    if link.startswith(\"//\"):\n        return prefix + link\n    return link\ndef traceError(errorMsg: str = \"error!\", _breakpoint: bool = False):\n    import traceback\n    traceback.print_exc()\n    sprint(errorMsg)\n    if _breakpoint:\n        return breakpoint()\ndef extractLinks(description, extract_bgm=True):\n    \"\"\"Extract and remove links in description\"\"\"\n    import re\n    # notice, we don't need to go wild here. we just want the title and the cover, and the tags.\n    expression = r\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\"\n    # expr = re.compile(expression)\n    links = re.findall(expression, description)\n    # if links == None:\n    #     links = []\n    desc_without_link = re.sub(expression, \"\", description)\n    desc_without_link_per_line = [\n        x.replace(\"\\n\", \"\").strip() for x in desc_without_link.split(\"\\n\")",
        "type": "code",
        "location": "/tests/bilibili_search_api_modification_section_params_get_related_videos/searchDataParser.py:1-37"
    },
    "5005": {
        "file_id": 651,
        "content": "The code contains functions for handling generators, fixing links, error tracing, and extracting links from descriptions. It uses regular expressions to find links in the description and removes them while preserving other relevant information like titles and tags.",
        "type": "comment"
    },
    "5006": {
        "file_id": 651,
        "content": "    ]\n    desc_without_link_per_line = [x for x in desc_without_link_per_line if len(x) > 0]\n    bgms = []\n    final_desc_list = []\n    if not extract_bgm:\n        final_desc_list = desc_without_link_per_line\n    else:\n        for line in desc_without_link_per_line:\n            bgmCandidateTemplates = [\"{}：\", \"{}:\", \"{} \"]\n            fixers = [x.format(\"\") for x in bgmCandidateTemplates]\n            bgmCandidates = [x.format(\"bgm\") + \"(.+)\" for x in bgmCandidateTemplates]\n            has_bgm = False\n            for candidate in bgmCandidates:\n                bgm_parse_result = re.findall(candidate, line.lower())\n                if len(bgm_parse_result) > 0:\n                    has_bgm = True\n                    # bgm = line[len(bgmCandidates) :]\n                    bgm = bgm_parse_result[0]\n                    bgm = bgm.strip()\n                    for fixer in fixers:\n                        bgm = bgm.strip(fixer)\n                    if len(bgm) > 0:\n                        bgms.append(bgm)\n                    break",
        "type": "code",
        "location": "/tests/bilibili_search_api_modification_section_params_get_related_videos/searchDataParser.py:38-61"
    },
    "5007": {
        "file_id": 651,
        "content": "This code extracts background music (BGMs) from a list of descriptions. It checks each description line for specific patterns using regular expressions and adds them to the bgms list if found. If no BGMs are found, it stores all the description lines without links in final_desc_list.",
        "type": "comment"
    },
    "5008": {
        "file_id": 651,
        "content": "            if not has_bgm:\n                final_desc_list.append(line)\n    desc_without_link = \"\\n\".join(final_desc_list)\n    return links, bgms, desc_without_link\ndef videoDurationStringToSeconds(durationString):\n    if type(durationString) == int:\n        return durationString  # not string at all.\n    if type(durationString) != str:\n        print(\"unknown durationString type: %s\" % type(durationString))\n        return None\n    durationString = durationString.strip()\n    mList = durationString.split(\":\")[::-1]\n    if len(mList) > 3:\n        print(\"DURATION STRING TOO LONG\")\n        return None\n    seconds = 0\n    for index, elem in enumerate(mList):\n        elem = int(elem)\n        seconds += (60**index) * elem\n    return seconds\ndef clearHtmlTags(htmlObject):\n    a = BeautifulSoup(htmlObject, features=\"lxml\")\n    return a.text\ndef detectAuthorRelatedKeywords(title_tag, author_keywords):\n    abandon = False\n    for keyword in author_keywords:\n        if len(keyword) > 1:\n            if keyword in title_tag:\n                abandon = True  # detected this thing.",
        "type": "code",
        "location": "/tests/bilibili_search_api_modification_section_params_get_related_videos/searchDataParser.py:62-96"
    },
    "5009": {
        "file_id": 651,
        "content": "The code contains functions for parsing video descriptions, converting duration strings to seconds, and detecting related keywords. It also handles cases where a background music (BGM) is or isn't present. The final description without links is returned along with the links and BGMs.",
        "type": "comment"
    },
    "5010": {
        "file_id": 651,
        "content": "                break\n    return abandon\ndef getAuthorKeywords(author):\n    author = author.strip()\n    import jieba\n    author_keywords = jieba.lcut(author)\n    author_keywords = [x.strip() for x in author_keywords]\n    author_keywords = [x for x in author_keywords if len(x) > 0]\n    return author_keywords\ndef removeAuthorRelatedTags(description_or_title, author):\n    templates = [\"【{}】\", \"@{}\", \"{}\"]\n    tags = [template.format(author) for template in templates]\n    for tag in tags:\n        description_or_title = description_or_title.replace(tag, \"\")\n    return description_or_title\ndef splitTitleTags(title, author_keywords):\n    import re\n    pattern = r\"【.+】\"\n    title_tags = re.findall(pattern, title)\n    title = re.sub(pattern, \"\", title)\n    title_tags = [x.lstrip(\"【\").rstrip(\"】\").strip() for x in title_tags]\n    title_tags = [x for x in title_tags if len(x) > 0]\n    final_title_tags = []\n    for title_tag in title_tags:\n        detected = detectAuthorRelatedKeywords(title_tag, author_keywords)\n        if not detected:",
        "type": "code",
        "location": "/tests/bilibili_search_api_modification_section_params_get_related_videos/searchDataParser.py:97-131"
    },
    "5011": {
        "file_id": 651,
        "content": "This code performs the following tasks:\n1. Extracts author keywords using Jieba segmentation and removes leading/trailing whitespace, while discarding empty strings.\n2. Removes author-related tags from the description or title by replacing them with an empty string.\n3. Splits the title into tags, removing any leading/trailing brackets, and eliminating empty strings.\n4. Detects if each tag contains any of the author's keywords and adds it to a list called \"final_title_tags\" only if it does.",
        "type": "comment"
    },
    "5012": {
        "file_id": 651,
        "content": "            final_title_tags.append(title_tag)\n    return title, title_tags\ndef parseVideoSearchItem(video, disableList: list = [], debug=False):\n    bvid = video[\"bvid\"]\n    pubdate = video['pubdate']\n    if \"author\" not in disableList:\n        author = video[\"author\"]\n        author_id = video[\"mid\"] # this is important. may let us able to find out the fans count.\n    else:\n        author = \"\"\n        author_id = -1\n    author_keywords = getAuthorKeywords(author)\n    if \"tag\" not in disableList:\n        tag = video[\"tag\"]\n        tags = tag.split(\",\")\n        tags = [\n            tag for tag in tags if not detectAuthorRelatedKeywords(tag, author_keywords)\n        ]\n    else:\n        tags = []\n    if \"typeid\" not in disableList and \"typename\" not in disableList:\n        categoryId = int(video.get(\"typeid\", video.get(\"type_id\")))\n        categoryName = video.get(\"typename\", video.get(\"type_name\"))\n    else:\n        categoryId = 0\n        categoryName = \"\"\n    title = video[\"title\"]  # remove those markers, please?",
        "type": "code",
        "location": "/tests/bilibili_search_api_modification_section_params_get_related_videos/searchDataParser.py:132-160"
    },
    "5013": {
        "file_id": 651,
        "content": "The function takes a video object, optional disabled list for author and tag keywords, and debug flag as input. It extracts the bvid and pubdate from the video object. If author is not in disableList, it retrieves the author name and id. The author's keywords are obtained using getAuthorKeywords function. If tag is not in disableList, it splits the tags and removes any related to author keywords. The typeid and typename are also extracted if not in disableList, otherwise set to default values. Finally, title removal markers are applied.",
        "type": "comment"
    },
    "5014": {
        "file_id": 651,
        "content": "    title = clearHtmlTags(title)\n    title = removeAuthorRelatedTags(title, author)\n    title, title_tags = splitTitleTags(\n        title, author_keywords\n    )  # use author for filtering unwanted title tags.\n    duration = video[\"duration\"]  # this is not recommended. we need seconds.\n    play = video.get(\"play\", video.get(\"view\"))  # select some hot videos.\n    cover = video[\"pic\"]\n    cover = linkFixer(cover)\n    if \"description\" not in disableList:\n        description = video.get(\"description\", video.get(\"desc\"))\n        description = clearHtmlTags(description)\n        description = removeAuthorRelatedTags(description, author)\n    else:\n        description = \"\"\n    links_in_description, bgms, description = extractLinks(description)\n    duration_seconds = videoDurationStringToSeconds(duration)\n    resultTuple = (\n        author,\n        author_id,\n        bvid,\n        tags,\n        categoryId,\n        categoryName,\n        title,\n        duration_seconds,\n        play,\n        cover,\n        description,\n        links_in_description,",
        "type": "code",
        "location": "/tests/bilibili_search_api_modification_section_params_get_related_videos/searchDataParser.py:161-190"
    },
    "5015": {
        "file_id": 651,
        "content": "This code parses video data from a bilibili search API response and extracts relevant information such as author, title, duration, play count, cover image, and description. It applies filters to remove unwanted HTML tags and uses author keywords for filtering. It converts duration strings to seconds and extracts links from the description.",
        "type": "comment"
    },
    "5016": {
        "file_id": 651,
        "content": "        bgms,\n        title_tags,\n        pubdate\n    )\n    if debug:\n        for metadata in resultTuple:\n            print(metadata)\n    from lazero.utils.logger import sprint\n    if debug:\n        sprint()\n    return resultTuple\n# you might want the creater's name, to filter out unwanted parts.\ndef iterateResultList(resultList, debug=False):\n    for video in resultList:\n        # be warned cause all these things might fail.\n        try:\n            if video[\"type\"] == \"video\":\n                yield parseVideoSearchItem(video, debug=debug)\n        except:\n            traceError(\"error iterating video metadata\")\n            continue\ndef parseSearchAllResult(data, debug=False):\n    # if not generator:\n    #     return generatorToList(parseSearchAllResult(data, debug=debug,generator=True))\n    results = data[\"result\"]\n    for elem in results:\n        try:\n            if elem[\"result_type\"] == \"video\":\n                resultList = elem[\"data\"]\n                for videoMetadata in iterateResultList(resultList, debug=debug):",
        "type": "code",
        "location": "/tests/bilibili_search_api_modification_section_params_get_related_videos/searchDataParser.py:191-227"
    },
    "5017": {
        "file_id": 651,
        "content": "This code defines a function `parseSearchAllResult` that takes in data and a boolean debug parameter. It extracts the \"result\" list from the data, then iterates through each element checking if its type is 'video'. For each video, it yields a parsed video metadata using the `iterateResultList` function, while handling any exceptions that may occur. The `iterateResultList` function iterates over a result list of video items, yielding the parsed data for videos and handling exceptions related to parsing video metadata.",
        "type": "comment"
    },
    "5018": {
        "file_id": 651,
        "content": "                    yield videoMetadata\n        except:\n            traceError(\"error iterating data results\")\ndef parseSearchVideoResult(data, debug=False):\n    # if not generator:\n    #     return generatorToList(parseSearchVideoResult(data, debug=debug,generator=True))\n    try:\n        resultList = data[\"result\"]\n        try:\n            for videoMetadata in iterateResultList(resultList, debug=debug):\n                try:\n                    yield videoMetadata\n                except:\n                    traceError(\"error iterating video metadata\")\n        except:\n            traceError(\"error iterating result list\")\n    except:\n        traceError(\"error parsing search video result\")\ndef parseVideoInfo(videoInfo, debug=False):\n    data = videoInfo\n    # no tag out here.\n    secondaryVideoInfoList = []\n    data_copy = data.copy()\n    data_copy.update({\"author\": data[\"owner\"][\"name\"], \"mid\": data[\"owner\"][\"mid\"]})\n    data_copy.update(data[\"stat\"])\n    primaryVideoInfo = parseVideoSearchItem(\n        data_copy, disableList=[\"tag\", \"typeid\", \"typename\"], debug=debug",
        "type": "code",
        "location": "/tests/bilibili_search_api_modification_section_params_get_related_videos/searchDataParser.py:228-258"
    },
    "5019": {
        "file_id": 651,
        "content": "The code defines two functions, `parseSearchVideoResult` and `parseVideoInfo`, which are responsible for parsing video search results and video information respectively. The code utilizes exception handling to handle errors while iterating over data and result lists. It also includes a function `iterateResultList` to iterate over the result list.",
        "type": "comment"
    },
    "5020": {
        "file_id": 651,
        "content": "    )\n    # videoInfoList.append(primaryVideoInfo)\n    season = data.get(\"ugc_season\", {})  # we only care about this thing.\n    season_cover = season.get(\"cover\", None)  # it could be noting.\n    sections = season.get(\"sections\", [])\n    for section in sections:\n        for episode in section[\"episodes\"]:\n            # print(episode.keys())\n            # breakpoint()\n            arc = episode[\"arc\"]\n            stat = arc[\"stat\"]\n            videoInfo = episode.copy()\n            videoInfo.update(stat)\n            videoInfo.update(arc)\n            authorRelatedVideoInfo = parseVideoSearchItem(\n                videoInfo,\n                disableList=[\"tag\", \"typeid\", \"typename\", \"description\", \"author\"],\n                debug=debug,\n            )  # author is the same as the original video.\n            secondaryVideoInfoList.append(authorRelatedVideoInfo)\n            # BV1Cb4y1s7em\n            # []\n            # 0\n            # 这次真的燃起来了！！！\n            # 217\n            # 27911\n            # http://i2.hdslb.com/bfs/archive/c5a0d18ee077fb6a4ac0970ccb0a3788e137d14f.jpg",
        "type": "code",
        "location": "/tests/bilibili_search_api_modification_section_params_get_related_videos/searchDataParser.py:259-286"
    },
    "5021": {
        "file_id": 651,
        "content": "Code iterates through season episodes, extracts arc and stat information from each episode, creates a videoInfo dictionary with episode and arc data, updates the author-related video information by parsing the original video, and appends it to secondaryVideoInfoList.",
        "type": "comment"
    },
    "5022": {
        "file_id": 651,
        "content": "    return primaryVideoInfo, secondaryVideoInfoList\ndef parseVideoRelated(videoRelatedData, debug=False):\n    data = videoRelatedData\n    # if not generator:\n    #     return generatorToList(parseVideoRelated(data, debug=debug,generator=True))\n    try:\n        for videoInfo in data:\n            try:\n                videoInfo2 = videoInfo.copy()\n                videoInfo2.update({\"author\": videoInfo[\"owner\"][\"name\"]})\n                videoInfo2.update({\"mid\": videoInfo[\"owner\"][\"mid\"]})\n                # also update the stat.\n                videoInfo2.update(videoInfo[\"stat\"])\n                try:\n                    yield parseVideoSearchItem(\n                        videoInfo2,\n                        disableList=[\"tag\", \"typeid\", \"typename\"],\n                        debug=debug,\n                    )\n                    # print(videoMetadata)\n                except:\n                    traceError()\n            except:\n                traceError()\n    except:\n        traceError()\nif __name__ == \"__main__\":\n    # test_subject = \"search_video\"",
        "type": "code",
        "location": "/tests/bilibili_search_api_modification_section_params_get_related_videos/searchDataParser.py:287-318"
    },
    "5023": {
        "file_id": 651,
        "content": "This code defines a function `parseVideoRelated` that parses video-related data and yields parsed video information, and also includes an if block for generator handling. It updates the video info with author name and mid, and applies the `parseVideoSearchItem` to each item in the data list. If any error occurs during processing, it traces the error.",
        "type": "comment"
    },
    "5024": {
        "file_id": 651,
        "content": "    # test_subject = \"search_all\"\n    # test_subject = 'video_related'\n    test_subject = \"video_info\"\n    # test_subject = 'extract_links'\n    if test_subject == \"search_all\":\n        with open(\"search_result_all.json\", \"r\") as f:\n            data = f.read()\n            data = json.loads(data)\n        for mresult in parseSearchAllResult(data):\n            print(\"RESULT:\")\n            sprint(mresult)\n    elif test_subject == \"search_video\":\n        with open(\"search_by_type_result_video.json\", \"r\") as f:\n            data = f.read()\n            data = json.loads(data)\n        for mresult in parseSearchVideoResult(data):\n            print(\"VIDEO SEARCH RESULT:\")\n            sprint(mresult)\n    elif test_subject == \"video_info\":\n        with open(\"video_info.json\", \"r\") as f:\n            data = f.read()\n            data = json.loads(data)\n        primaryVideoInfo, secondaryVideoInfoList = parseVideoInfo(data)\n        videoInfoList = [primaryVideoInfo] + secondaryVideoInfoList\n        for mVideoInfo in videoInfoList:",
        "type": "code",
        "location": "/tests/bilibili_search_api_modification_section_params_get_related_videos/searchDataParser.py:319-343"
    },
    "5025": {
        "file_id": 651,
        "content": "This code is testing different APIs by reading JSON files and parsing the data. It tests \"search_all\", \"search_video\", and \"video_info\" sections. For each section, it reads a corresponding JSON file, loads the data, and then prints the results after parsing. This appears to be part of API testing for a video search application.",
        "type": "comment"
    },
    "5026": {
        "file_id": 651,
        "content": "            print(mVideoInfo)\n            sprint()\n    elif test_subject == \"video_related\":\n        with open(\"video_related.json\", \"r\") as f:\n            data = f.read()\n            data = json.loads(data)\n        for videoMetadata in parseVideoRelated(data):\n            print(videoMetadata)\n            sprint()\n    elif test_subject == \"extract_links\":\n        description = (\n            \"http://www.toutiao.com/a6347649852365897986/ 男子送走从小养大的狗，狗狗用泪汪汪的眼神看着他\\n\"\n            + \"https://www.youtube.com/watch?v=r724w57oXyU\"\n            + \" https://www.youtube.com/shorts/UYCy8HD1C7o\"\n        )\n        links, desc = extractLinks(description)\n        print(links)\n        print(desc)\n    else:\n        raise Exception(\"unknown test_subject:\", test_subject)",
        "type": "code",
        "location": "/tests/bilibili_search_api_modification_section_params_get_related_videos/searchDataParser.py:344-363"
    },
    "5027": {
        "file_id": 651,
        "content": "The code snippet appears to handle different test subjects, each with a specific task. For \"video_related\", it reads data from a JSON file and processes it using the parseVideoRelated function, then prints videoMetadata for each videoMetadata in the parsed data. The \"extract_links\" subject extracts links from a given description and prints them. Unknown test subjects will raise an Exception.",
        "type": "comment"
    },
    "5028": {
        "file_id": 652,
        "content": "/tests/bilibili_search_api_modification_section_params_get_related_videos/recursive_search_bilibili_test.py",
        "type": "filepath"
    },
    "5029": {
        "file_id": 652,
        "content": "The code changes directory, initializes OpenCV, and fetches video metadata for production. It imports necessary modules and displays an image using imshow, pausing until a keyboard event occurs for visualization purposes.",
        "type": "summary"
    },
    "5030": {
        "file_id": 652,
        "content": "import sys\nimport os\nos.chdir(\"../../\")\nsys.path.append(\".\")\n# ignore the global proxy now, we are not going to use that.\nos.environ[\"http_proxy\"] = \"\"\nos.environ[\"https_proxy\"] = \"\"\nfrom lazero.utils.importers import cv2_custom_build_init\ncv2_custom_build_init()\nimport cv2\nfrom pyjom.platforms.bilibili.postMetadata import getBilibiliPostMetadataForDogCat\n# metatopic = {\n#     \"optional\": [\n#         [\n#             \"狗狗\",\n#             \"狗\",\n#             \"汪汪\",\n#             \"修勾\",\n#             \"汪\",\n#             \"狗子\",\n#         ],\n#         [\"喵喵\", \"猫\", \"猫咪\", \"喵\"],\n#     ],\n#     \"dynamic\": [[\"可爱\", \"萌\", \"萌宠\", \"行为\", \"燃\"]],\n# }\n# maybe this is not task specific. just maybe.\nif __name__ == \"__main__\":\n    for (\n        mCover,\n        mTagSeries,\n        mTitle,\n        mBgm,\n        mDescription,\n        dog_or_cat,\n    ) in getBilibiliPostMetadataForDogCat():\n        print(\"FETCHED VIDEO METADATA FOR PRODUCTION:\")\n        videoMetadata = mCover, mTagSeries, mTitle, mBgm, mDescription, dog_or_cat\n        print(videoMetadata)\n        mCover2 = cv2.resize(mCover, (int(1920 / 2), int(1080 / 2)))",
        "type": "code",
        "location": "/tests/bilibili_search_api_modification_section_params_get_related_videos/recursive_search_bilibili_test.py:1-45"
    },
    "5031": {
        "file_id": 652,
        "content": "The code changes the directory, appends the current path to Python's sys.path, and removes the global proxy environment variables. It then initializes OpenCV with a custom build and imports necessary modules. Finally, it loops through fetched video metadata for production, resizing the cover image, and prints the metadata.",
        "type": "comment"
    },
    "5032": {
        "file_id": 652,
        "content": "        cv2.imshow(\"COVER\", mCover2)\n        cv2.waitKey(0)\n        breakpoint()",
        "type": "code",
        "location": "/tests/bilibili_search_api_modification_section_params_get_related_videos/recursive_search_bilibili_test.py:46-48"
    },
    "5033": {
        "file_id": 652,
        "content": "The code snippet displays an image using OpenCV's imshow function, pauses the execution until a keyboard event occurs with waitKey, and then terminates the loop with breakpoint. It is used for visualizing an image, potentially during debugging or analysis.",
        "type": "comment"
    },
    "5034": {
        "file_id": 653,
        "content": "/tests/bilibili_search_api_modification_section_params_get_related_videos/load_json5_render_template.py",
        "type": "filepath"
    },
    "5035": {
        "file_id": 653,
        "content": "The code reads a JSON5 file named 'channelConfig.json5' and a Jinja2 template file named 'template.j2'. It then parses the JSON5 data to obtain channel information such as name and tid. For each channel, it tries to extract subChannel information (name and tid). If successful, it appends them to a list called subChannels. The code renders the Jinja2 template with the extracted channel and subchannel information, and then prints the rendered data.",
        "type": "summary"
    },
    "5036": {
        "file_id": 653,
        "content": "import json5\nimport jinja2\ntemplate = open('template.j2','r').read()\ntemplate = jinja2.Template(template)\ndata = open(\"channelConfig.json5\",'r').read()\ndata = json5.loads(data)\nchannelList = data['channelList']\nfor channel in channelList:\n    try:\n        channelName = channel['name']\n        channelTid = channel['tid']\n        subChannels = []\n        for subChannel in channel['sub']:\n            try:\n                subChannelName = subChannel['name']\n                subChannelTid = subChannel['tid']\n                subChannels.append((subChannelName, subChannelTid))\n            except:\n                continue\n        rendered_data = template.render(channelName=channelName, channelTid=channelTid, subChannels=subChannels)\n        print(rendered_data)\n    except:\n        continue",
        "type": "code",
        "location": "/tests/bilibili_search_api_modification_section_params_get_related_videos/load_json5_render_template.py:1-26"
    },
    "5037": {
        "file_id": 653,
        "content": "The code reads a JSON5 file named 'channelConfig.json5' and a Jinja2 template file named 'template.j2'. It then parses the JSON5 data to obtain channel information such as name and tid. For each channel, it tries to extract subChannel information (name and tid). If successful, it appends them to a list called subChannels. The code renders the Jinja2 template with the extracted channel and subchannel information, and then prints the rendered data.",
        "type": "comment"
    },
    "5038": {
        "file_id": 654,
        "content": "/tests/download_sections_video_portion_partial_download_youtube_yt_dlp_bilibili/test_bilibili.sh",
        "type": "filepath"
    },
    "5039": {
        "file_id": 654,
        "content": "This code utilizes yt-dlp to download Bilibili video sections with authentication, handles subtitles and danmaku, supports multiple portions, updates cookies, allows title-only downloads, and retrieves metadata.",
        "type": "summary"
    },
    "5040": {
        "file_id": 654,
        "content": "# 关于视频合集 分p视频的分析逻辑：\n# https://github.com/Satoing/python_bilibili_downloader/blob/master/bilibili_video.py\n# 解析这个接口可以得到分p或者合集的信息 以及字幕信息 AI生成的字幕\n# https://api.bilibili.com/x/web-interface/view?bvid=BV1Fs411k7e9\n# https://api.bilibili.com/x/web-interface/view?bvid=BV1Cg411E7NF\nURL=\"https://www.bilibili.com/video/BV1Fs411k7e9\" #老戴 马克思佩恩 分p视频\n# 也可以直接网页parse\n# executing this you will get \"subtitle\" in \"danmaku\" as language, in xml format.\n# 对于海量弹幕的某些视频 （超电磁炮 12w asoul的某些二创 3w）不建议进行弹幕分析 可以通过API获取弹幕总数 不下载弹幕 \n# yt-dlp --skip-download --list-subs -I 1 \"https://www.bilibili.com/video/BV1Fs411k7e9\"\n# URL=\"https://www.bilibili.com/video/BV1Cg411E7NF\" #苏打baka 魔改机箱 合集\n# 合集视频 用bilibili_api 或者直接网页parse即可\n# it has multiple videos. what to do?\n# --force-keyframes-at-cuts\n# man i just need the first chapter.\n# yt-dlp --download-sections \"*0:05:00-0:06:30\" --playlist-items \"1\" \"$URL\" # only first video.\n# premium?\n# this feature is awesome! how to extract cookies programmatically from browser?\n# Use --cookies-from-browser o",
        "type": "code",
        "location": "/tests/download_sections_video_portion_partial_download_youtube_yt_dlp_bilibili/test_bilibili.sh:1-26"
    },
    "5041": {
        "file_id": 654,
        "content": "This code snippet is for downloading specific sections of Bilibili videos using yt-dlp. It provides URLs for both single video parts and video collections, explains how to handle subtitles and danmaku (comments), and suggests using the --cookies-from-browser option for premium access.",
        "type": "comment"
    },
    "5042": {
        "file_id": 654,
        "content": "r --cookies for the authentication. See  https://github.com/yt-dlp/yt-dlp/wiki/FAQ#how-do-i-pass-cookies-to-yt-dlp \n# not working for chromium on kali? (no bilibili cookie found) maybe it is relocated.\n# cookies = yt_dlp.cookies.extract_cookies_from_browser(BROWSER_NAME) -> YourubeDLCookieJar\n# save as Netscape HTTP Cookie File.\n# cookies.save(OUTPUT_FILE_PATH) \n# since we have issue playing content at tail of video, we do this.\n# yt-dlp --download-sections \"*0:05:00-0:06:30\" --playlist-items \"1\" --cookies-from-browser firefox --force-keyframes-at-cuts \"$URL\" # pass cookies.\n# forcing keyframe is much slower. but it produces better results.\n# yt-dlp --download-sections \"*0:05:00-0:06:30\" --playlist-items \"1\" --cookies-from-browser firefox --force-keyframes-at-cuts \"$URL\" # pass cookies.\n# you may want to add some margin at tail (or head) if not using \"--force-keyframes-at-cuts\", be it 10 seconds. usually jigs happens at 5 secs. but we are careful.\n# yt-dlp --download-sections \"*0:04:50-0:06:40\" --playlist-items \"1\" --cookies-from-browser firefox \"$URL\" # pass cookies.",
        "type": "code",
        "location": "/tests/download_sections_video_portion_partial_download_youtube_yt_dlp_bilibili/test_bilibili.sh:26-42"
    },
    "5043": {
        "file_id": 654,
        "content": "The code is trying to download a specific portion of a Bilibili video, ensuring authentication by passing cookies from the browser (Firefox in this case) to yt-dlp. It forces keyframes at cuts for better results but notes that it's slower. The code provides different options to account for potential issues and suggests adding margin at tail or head if not using --force-keyframes-at-cuts, with a recommended 10 seconds or even 5 seconds depending on the need for caution.",
        "type": "comment"
    },
    "5044": {
        "file_id": 654,
        "content": "# what if we download multiple sections?\n# no combination? shit.\n# if not at the very tail, other tails can be better than the last tail. but it is just my guess. better to keep all these margins!\n# yt-dlp --download-sections \"*0:04:50-0:05:40\" --download-sections \"*0:05:50-0:06:40\" --playlist-items \"1\" --cookies-from-browser firefox -o \"%(uploader_id)s-%(id)s-%(title)s-%(autonumber)s.%(ext)s\" \"$URL\" # pass cookies.\n# since we have cron job now, no need to do the old-school thing.\nyt-dlp --download-sections \"*0:04:50-0:05:40\" --download-sections \"*0:05:50-0:06:40\" --playlist-items \"1\" --cookies /root/.browser_cookies_exported/firefox.cookies -o \"%(uploader_id)s-%(id)s-%(title)s-%(autonumber)s.%(ext)s\" \"$URL\" # pass cookies in different way\n# like this: '2142762-BV1Fs411k7e9_p1-老戴《马克思佩恩 3》全收集流程攻略【共14期完结】 p01 EP-01-00002.mp4'\n# https://github.com/yt-dlp/yt-dlp#readme -> \"OUTPUT TEMPLATE\"\n# https://github.com/yt-dlp/yt-dlp/issues/4579\n# you better use stored cookies instead of retrieving cookies every time.",
        "type": "code",
        "location": "/tests/download_sections_video_portion_partial_download_youtube_yt_dlp_bilibili/test_bilibili.sh:44-59"
    },
    "5045": {
        "file_id": 654,
        "content": "The code tests downloading multiple video portions from Bilibili using yt-dlp with cookies stored, instead of retrieving them every time. It mentions that keeping all margins is better and suggests using a different format for the output file name.",
        "type": "comment"
    },
    "5046": {
        "file_id": 654,
        "content": "# or you can update cookies regularly with cronjob.\n# just want metadata?\n# if you want title for each video in playlist, you just get it from elsewhere or parse the damn output filename (slow, man!)\n# this seems to only have video description. nothing else! not even video length.\n# yt-dlp --write-description --write-playlist-metafiles --skip-download \"$URL\"\n# hey i don't want many download links. i just want title.\n# yt-dlp --write-info-json  --write-playlist-metafiles --skip-download \"$URL\" # this will get metadata main playlist and every video in the playlist in separate json files.\n# this is one of the video in that playlist. \"https://www.bilibili.com/video/BV1Fs411k7e9?p=1\n# you can get comments with this tool.\n## no comments?\n# yt-dlp --write-info-json --skip-download \"$URL\"\n# download-sections can be used multiple times?",
        "type": "code",
        "location": "/tests/download_sections_video_portion_partial_download_youtube_yt_dlp_bilibili/test_bilibili.sh:60-75"
    },
    "5047": {
        "file_id": 654,
        "content": "This code snippet provides various options for downloading or obtaining metadata from a Bilibili playlist using yt-dlp. The user can choose to update cookies regularly, download only the video title, or retrieve metadata for the entire playlist and each individual video in separate JSON files. The user can also use specific URLs to obtain comments without actually downloading the videos. The code suggests multiple usage scenarios for the 'download-sections' functionality.",
        "type": "comment"
    },
    "5048": {
        "file_id": 655,
        "content": "/tests/download_sections_video_portion_partial_download_youtube_yt_dlp_bilibili/cron_update_cookies_stored_under_root_home.py",
        "type": "filepath"
    },
    "5049": {
        "file_id": 655,
        "content": "This script creates a directory for cookie storage if it doesn't already exist, and then extracts and saves cookies from Firefox and Chromium browsers.",
        "type": "summary"
    },
    "5050": {
        "file_id": 655,
        "content": "# 0 * * * * /usr/bin/python3 /root/Desktop/works/pyjom/tests/download_sections_video_portion_partial_download_youtube_yt_dlp_bilibili/cron_update_cookies_stored_under_root_home.py\nimport os\nimport shutil\ncookies_path = \"/root/.browser_cookies_exported\"\nif not (os.path.exists(cookies_path) or os.path.isdir(cookies_path)):\n    if os.path.isfile(cookies_path):\n        os.remove(cookies_path)\n    elif os.path.isdir(cookies_path):\n        shutil.rmtree(cookies_path)\n    elif os.path.islink(cookies_path):\n        os.unlink(cookies_path)\n    os.mkdir(cookies_path)\nimport yt_dlp\nbrowser_names = [\"firefox\",\"chromium\"]\nfor browser_name in browser_names:\n    cookies = yt_dlp.cookies.extract_cookies_from_browser(browser_name)\n    filepath = os.path.join(cookies_path,f\"{browser_name}.cookies\")\n    cookies.save(filepath)",
        "type": "code",
        "location": "/tests/download_sections_video_portion_partial_download_youtube_yt_dlp_bilibili/cron_update_cookies_stored_under_root_home.py:1-24"
    },
    "5051": {
        "file_id": 655,
        "content": "This script creates a directory for cookie storage if it doesn't already exist, and then extracts and saves cookies from Firefox and Chromium browsers.",
        "type": "comment"
    },
    "5052": {
        "file_id": 656,
        "content": "/tests/video_phash_deduplication/test_video_hash.py",
        "type": "filepath"
    },
    "5053": {
        "file_id": 656,
        "content": "The code defines `getVideoPHash` to calculate a video's phash using the `videohashes` tool, testing it by comparing pairwise differences between hash values for different videos and considering duplicates based on a threshold.",
        "type": "summary"
    },
    "5054": {
        "file_id": 656,
        "content": "# use some delogo stuff.\nfrom lazero.program.subprocess import runCommandGetJson\n# these two are similar. can be used as threshold.\n# aaaa3d8a2eaa1f8a delogo\n# aaaa398a2faa5d8a not delogoed.\n# aaaa3c8a2faa5e8a mp4 (very similar to delogoed version)\ndef getVideoPHash(filepath,debug=False, timeout=100):\n    import os\n    import imagehash\n    assert os.path.exists(filepath)\n    assert os.path.isfile(filepath)\n    if not os.path.isabs(filepath):\n        filepath = os.path.abspath(filepath)\n    commandLine = [\n        \"videohashes\", # installed in path.\n        # \"/root/Desktop/works/pyjom/tests/video_phash_deduplication/videohashes/videohashes-linux\",\n        \"-json\",\n        filepath,\n    ]\n    success, myJson = runCommandGetJson(commandLine, debug=debug, timeout=timeout)\n    if debug:\n        print(\"SUCCESS?\", success)\n        print(myJson, type(myJson))\n    if not success:\n        return\n    # breakpoint()\n    phashString = myJson[\"phash\"]\n    phash = imagehash.hex_to_hash(phashString)\n    if debug:\n        print(\"FILEPATH: %s\" % filepath)",
        "type": "code",
        "location": "/tests/video_phash_deduplication/test_video_hash.py:1-32"
    },
    "5055": {
        "file_id": 656,
        "content": "The code defines a function `getVideoPHash` that calculates a video's phash (a unique identifier for an image or video) using the `videohashes` command-line tool. It takes a filepath as input, checks if it exists and is a file, then runs the command to generate the JSON output. The function also converts the returned phash string to a binary hash and optionally prints debug information.",
        "type": "comment"
    },
    "5056": {
        "file_id": 656,
        "content": "        print(myJson)\n        print(\"PHASH:\", phash)\n    # if withDuration:\n    #     duration = myJson[\"duration\"]\n    #     return duration, phash\n    # duration is inaccurate\n    return phash\nif __name__ == \"__main__\":\n    videoPaths = [\n        \"cute_cat_gif.mp4\",\n        \"cute_cat_gif.gif\",\n        \"cat_delogo.gif\",\n        \"/root/Desktop/works/pyjom/samples/video/dog_with_large_text.gif\",\n    ]\n    hashs = [getVideoPHash(filepath,debug=True) for filepath in videoPaths]\n    dis0 = hashs[0] - hashs[1]  # small\n    dis1 = hashs[1] - hashs[2]  # big\n    dis2 = hashs[0] - hashs[2]  # big\n    dis3 = hashs[0] - hashs[3]  # big\n    print(dis0, dis1, dis2, dis3)\n    # 4 4 4\n    # strange. why?\n    # 4 4 4 42\n    # huge difference.\n    # what value do you decide to be duplicate?\n    # phash < 7 (really?)\n    # so how do we run this test?",
        "type": "code",
        "location": "/tests/video_phash_deduplication/test_video_hash.py:33-65"
    },
    "5057": {
        "file_id": 656,
        "content": "This code tests the video hashing function by calculating pairwise differences between hash values for different video files. It then compares the differences to determine potential duplicates and prints the results. The hash difference threshold for considering duplicates is set to 7, but this seems low and may need adjustment based on further testing.",
        "type": "comment"
    },
    "5058": {
        "file_id": 657,
        "content": "/tests/video_phash_deduplication/test_milvus_library.py",
        "type": "filepath"
    },
    "5059": {
        "file_id": 657,
        "content": "This code defines a Milvus function for connecting, managing collections, and caching. It creates Collections with specified data types, searches duplicated videos, retrieves video duration/hash, indexes videos, and reloads collection if necessary.",
        "type": "summary"
    },
    "5060": {
        "file_id": 657,
        "content": "# # duplicate -> remove, do not insert\n# # not duplicate -> get the data, insert\n# # you want to clear the collection after this run?\n# from functools import lru_cache\n# from pymilvus import connections\n# @lru_cache(maxsize=1)\n# def connectMilvusDatabase(alias=\"default\", host=\"localhost\", port=\"19530\"):\n#     connection = connections.connect(\n#         alias=alias, host=host, port=port\n#     )  # can we reconnect?\n#     print(\"milvus connected\")\n# # connectMilvusDatabase()\n# # connectMilvusDatabase() # will not connect again.\n# from pymilvus import Collection\n# from pymilvus import utility\n# from pymilvus import CollectionSchema, FieldSchema, DataType\n# import traceback\n# def getMilvusVideoDeduplicationCollection(\n#     get_existing: bool = False,\n# ):  # most of the time we just use the same\n#     collection_name = \"video_deduplication\"\n#     try:\n#         if utility.has_collection(collection_name):  # be prudent.\n#             if get_existing:\n#                 return Collection(collection_name)\n#             utility.drop_collection(collection_name)",
        "type": "code",
        "location": "/tests/video_phash_deduplication/test_milvus_library.py:1-35"
    },
    "5061": {
        "file_id": 657,
        "content": "The code defines a function to connect to a Milvus database, get or remove an existing collection named \"video_deduplication\", and returns the collection if it already exists. The function uses caching and checks if the collection already exists before performing any actions.",
        "type": "comment"
    },
    "5062": {
        "file_id": 657,
        "content": "#     except:\n#         traceback.print_exc()\n#         print(\"maybe the collection does not exist\")\n#     video_semantic_id = FieldSchema(  # how to insert this shit without prior knowledge?\n#         name=\"video_semantic_id\",\n#         dtype=DataType.INT64,\n#         is_primary=True,  # if is primary, will do check for 'not duplicate' or something.\n#         auto_id=True,  # no need for id generation.\n#     )\n#     video_length = FieldSchema(\n#         name=\"video_length\",\n#         dtype=DataType.FLOAT,\n#     )\n#     video_phash = FieldSchema(\n#         name=\"video_phash\", dtype=DataType.BINARY_VECTOR, dim=64\n#     )  # 64\n#     # single dimension? no multi dimension support?\n#     schema = CollectionSchema(\n#         fields=[video_semantic_id, video_length, video_phash],\n#         description=\"Test video deduplication\",\n#     )\n#     collection = Collection(\n#         name=collection_name,\n#         schema=schema,\n#         using=\"default\",\n#         shards_num=2,\n#     )\n#     # is this demo collection?\n#     return collection",
        "type": "code",
        "location": "/tests/video_phash_deduplication/test_milvus_library.py:36-65"
    },
    "5063": {
        "file_id": 657,
        "content": "This code defines a CollectionSchema and Collection for Milvus library. The schema contains fields for video_semantic_id, video_length, and video_phash, with their respective data types and properties. The Collection is created with a name, schema, database usage, and number of shards.",
        "type": "comment"
    },
    "5064": {
        "file_id": 657,
        "content": "# # seems hard to setup.\n# # not started!\n# # https://milvus.io/docs/v2.0.0/metric.md#binary\n# # the metric is important to us.\n# import numpy as np\n# import bitarray\n# @lru_cache(maxsize=1)\n# def transformVideoPhash(videoPhash):\n#     # we need the raw phash.\n#     queryData = videoPhash.hash  # videoPhashTruthTable8x8 or something\n#     queryData = queryData.reshape(-1).tolist()\n#     queryData = [\"1\" if x else \"0\" for x in queryData]\n#     queryData = bitarray.bitarray(\"\".join(queryData), endian=\"little\")\n#     queryData = queryData.tobytes()\n#     return queryData\n# # dimension: 8*8=64\n# def indexVideoWithVideoDurationAndPhash(collection, videoDuration, videoPhash):\n#     queryData = transformVideoPhash(videoPhash)\n#     collection.insert([[np.float32(videoDuration)], [queryData]])\n# # can release even if not loaded.\n# from test_video_hash import getVideoPHash\n# import caer\n# @lru_cache(maxsize=1)\n# def getVideoDurationAndPhashFromFile(videoFilePath):\n#     videoDuration = caer.video.frames_and_fps.get_duration(videoFilePath)",
        "type": "code",
        "location": "/tests/video_phash_deduplication/test_milvus_library.py:68-103"
    },
    "5065": {
        "file_id": 657,
        "content": "Function `transformVideoPhash` takes a video phash and converts it into a binary format for Milvus library indexing. Function `indexVideoWithVideoDurationAndPhash` inserts the video duration and transformed phash into the specified collection. The `getVideoDurationAndPhashFromFile` function retrieves the video duration and corresponding phash of a given video file using caer's video module. All functions are cached to avoid redundant computations.",
        "type": "comment"
    },
    "5066": {
        "file_id": 657,
        "content": "#     videoPhash = getVideoPHash(videoFilePath)\n#     return videoDuration, videoPhash\n# def indexVideoWithVideoDurationAndPhashFromFile(collection, videoFilePath):\n#     videoDuration, videoPhash = getVideoDurationAndPhashFromFile(videoFilePath)\n#     indexVideoWithVideoDurationAndPhash(collection, videoDuration, videoPhash)\n# def reloadMilvusCollection(collection):\n#     collection.release()  # unload.\n#     collection.load()\n# # make it into some library!\n# # insert after load?\n# # # 1,64\n# # what is wrong? wtf?\n# # queryData = queryData.tolist()\n# def getDistancesBySearchingDuplicatedVideoInMilvusByFile(\n#     collection,\n#     videoFilePath,\n#     search_params={\"metric_type\": \"Jaccard\", \"params\": {\"nprobe\": 10}},\n#     autoreload: bool = True,\n#     span: float = 2,\n#     debug: bool = False,\n#     limit: int = 10,\n# ):\n#     if autoreload:\n#         reloadMilvusCollection(collection)\n#     videoDuration, videoPhash = getVideoDurationAndPhashFromFile(videoFilePath)\n#     queryData = transformVideoPhash(videoPhash)",
        "type": "code",
        "location": "/tests/video_phash_deduplication/test_milvus_library.py:104-136"
    },
    "5067": {
        "file_id": 657,
        "content": "This code snippet defines a function for searching duplicated videos in Milvus by their file path, using Jaccard metric. It also includes functions to get video duration and hash from the file, index videos with duration and hash, and reload Milvus collection if necessary. The search parameters include metric type, probe count, span, limit, and whether to enable debug mode.",
        "type": "comment"
    },
    "5068": {
        "file_id": 657,
        "content": "#     minVideoLength = max(0, videoDuration - span)\n#     maxVideoLength = videoDuration + span\n#     results = collection.search(\n#         data=[queryData],  # this is the float dimension.\n#         anns_field=\"video_phash\",\n#         param=search_params,\n#         output_fields=[\"video_length\"],\n#         limit=limit,\n#         expr=\"video_length > {minVideoLength} and video_length < {maxVideoLength}\".format(\n#             minVideoLength=minVideoLength, maxVideoLength=maxVideoLength\n#         ),\n#     )\n#     theHit = results[0]\n#     # print(theHit)\n#     # so we can perform search without filtering afterwards.\n#     # results[0][0].entity.get('video_length')\n#     # print(results[0].ids)\n#     # now, we want to have the 'distance' parameter.\n#     # print(results[0])\n#     # print(theHit)\n#     distances = list(theHit.distances)\n#     if debug:\n#         print(\"distances: %s\" % distances)\n#     return distances\n#     # what is the distance? we need to try.\n#     # returh the closest distance?\n#     # results = [x for x in theHit]",
        "type": "code",
        "location": "/tests/video_phash_deduplication/test_milvus_library.py:137-164"
    },
    "5069": {
        "file_id": 657,
        "content": "This code searches for videos within a specified range of video length in the Milvus library. It sets minimum and maximum lengths based on the query's duration and span, and uses these values to filter results from the search. The closest distance between the query and each result is then returned.",
        "type": "comment"
    },
    "5070": {
        "file_id": 657,
        "content": "#     # hits = len(theHit)\n#     # breakpoint()\n#     # how to get document by id? wtf\n# def checkDuplicatedVideoAndInsertVector(\n#     collection,\n#     videoPath,\n#     threshold: float = 0.15,  # are you sure?\n#     insertDuplicatedVector: bool = True,\n#     debug: bool = True,\n# ):\n#     reloadMilvusCollection(collection)\n#     distances = getDistancesBySearchingDuplicatedVideoInMilvusByFile(\n#         collection, videoPath, debug=debug\n#     )\n#     minDistance = min(distances + [1])  # empty!\n#     duplicated = minDistance < threshold\n#     if insertDuplicatedVector or (not duplicated):\n#         indexVideoWithVideoDurationAndPhashFromFile(\n#             collection, videoPath\n#         )  # anyway let's do this.\n#     return duplicated\n# shall we insert that vector or not, even if we have detected the duplicated media?\n# you choose.\nimport sys\nimport os\n# os.chdir(\"../../\")\nsys.path.append(\"../../\")\n# ignore the global proxy now, we are not going to use that.\nos.environ[\"http_proxy\"] = \"\"\nos.environ[\"https_proxy\"] = \"\"",
        "type": "code",
        "location": "/tests/video_phash_deduplication/test_milvus_library.py:165-200"
    },
    "5071": {
        "file_id": 657,
        "content": "Function checkDuplicatedVideoAndInsertVector checks if a video file exists in Milvus collection and returns whether the video is duplicated or not. If insertDuplicatedVector is True, it indexes the video regardless of duplication status. The function uses getDistancesBySearchingDuplicatedVideoInMilvusByFile to find distances between the new video and existing videos in Milvus.",
        "type": "comment"
    },
    "5072": {
        "file_id": 657,
        "content": "from pyjom.videotoolbox import getMilvusVideoDeduplicationCollection,checkDuplicatedVideoAndInsertVector\nif __name__ == \"__main__\":\n    # connectMilvusDatabase()\n    collection = (\n        getMilvusVideoDeduplicationCollection()\n    )  # will not get existing collections\n    videoPaths = [\n        \"cute_cat_gif.mp4\",\n        \"cute_cat_gif.gif\",\n        \"cat_delogo.gif\",\n        \"/root/Desktop/works/pyjom/samples/video/dog_with_large_text.gif\",\n    ]\n    # for videoPath in videoPaths:\n    from lazero.utils.logger import sprint\n    for videoPath in videoPaths:\n        print(\"filepath: %s\" % videoPath)\n        duplicated = checkDuplicatedVideoAndInsertVector(collection, videoPath)\n        sprint(\"duplicated?\", duplicated)\n\"\"\"\nfilepath: cute_cat_gif.mp4\ndistances: [0.0, 0.11764705926179886, 0.11764705926179886, 0.7200000286102295, 0.7200000286102295, 0.7346938848495483, 0.7659574747085571, 0.7924528121948242]\n______________________________\nfilepath: cute_cat_gif.gif\ndistances: [0.0, 0.11764705926179886, 0.117647",
        "type": "code",
        "location": "/tests/video_phash_deduplication/test_milvus_library.py:201-226"
    },
    "5073": {
        "file_id": 657,
        "content": "The code connects to a Milvus database, retrieves the video deduplication collection, and checks if each given video path is already in the collection. It prints the file paths of the videos and whether they are duplicated or not using `checkDuplicatedVideoAndInsertVector` function from `lazero.utils.logger` module. The distances between the new video and existing ones in the database are also printed.",
        "type": "comment"
    },
    "5074": {
        "file_id": 657,
        "content": "05926179886, 0.7200000286102295, 0.7200000286102295, 0.7346938848495483, 0.7659574747085571, 0.7692307829856873]\n______________________________\nfilepath: cat_delogo.gif\ndistances: [0.0, 0.11764705926179886, 0.11764705926179886, 0.7200000286102295, 0.7200000286102295, 0.7346938848495483, 0.7659574747085571, 0.7924528121948242]\n______________________________\nfilepath: /root/Desktop/works/pyjom/samples/video/dog_with_large_text.gif\ndistances: [0.0, 0.6808510422706604, 0.6938775777816772, 0.6938775777816772, 0.739130437374115, 0.7692307829856873, 0.7924528121948242, 0.7924528121948242]\n______________________________\n\"\"\"",
        "type": "code",
        "location": "/tests/video_phash_deduplication/test_milvus_library.py:226-234"
    },
    "5075": {
        "file_id": 657,
        "content": "The code appears to be storing and comparing distances between video phashes for different files. Each line contains a file path followed by an array of distances, indicating the similarity of that video phash to other video phashes in the system. The lower the distance value, the more similar the videos are.",
        "type": "comment"
    },
    "5076": {
        "file_id": 658,
        "content": "/tests/video_phash_deduplication/test_milvus.py",
        "type": "filepath"
    },
    "5077": {
        "file_id": 658,
        "content": "The code demonstrates Milvus database operations, including creating a \"video\" collection, inserting data and performing searches. It is part of debugging process to retrieve documents by ID. The programmer is stuck and requires further investigation.",
        "type": "summary"
    },
    "5078": {
        "file_id": 658,
        "content": "# duplicate -> remove, do not insert\n# not duplicate -> get the data, insert\n# you want to clear the collection after this run?\n# import pymilvus\nfrom pymilvus import connections\nfrom functools import lru_cache\n@lru_cache(maxsize=1)\ndef connectMilvusDatabase(alias=\"default\", host=\"localhost\", port=\"19530\"):\n\tconnection = connections.connect(alias=alias, host=host, port=port)# can we reconnect?\n\tprint('milvus connected')\nconnectMilvusDatabase()\nconnectMilvusDatabase() # will not connect again.\ncollection_name = \"video_deduplication\"\nfrom pymilvus import Collection\n# Collection(collection_name)\n# remote this thing.\nfrom pymilvus import utility\ntry:\n    if utility.has_collection(collection_name):  # be prudent.\n        utility.drop_collection(collection_name)\nexcept:\n    import traceback\n    traceback.print_exc()\n    print(\"maybe the collection does not exist\")\nfrom pymilvus import CollectionSchema, FieldSchema, DataType\nvideo_semantic_id = FieldSchema(  # how to insert this shit without prior knowledge?\n    name=\"video_semantic_id\",",
        "type": "code",
        "location": "/tests/video_phash_deduplication/test_milvus.py:1-39"
    },
    "5079": {
        "file_id": 658,
        "content": "This code establishes a connection to a Milvus database, checks if the \"video_deduplication\" collection exists, and if so, removes it before creating a new one. The `connectMilvusDatabase` function sets up a connection with specified alias, host, and port (default values used in this code). The `utility.has_collection` and `utility.drop_collection` functions from the `pymilvus` utility module are used to check for and remove an existing collection named \"video_deduplication\". A `CollectionSchema` is defined for the new collection, specifying a field schema named \"video_semantic_id\".",
        "type": "comment"
    },
    "5080": {
        "file_id": 658,
        "content": "    dtype=DataType.INT64,\n    is_primary=True,  # if is primary, will do check for 'not duplicate' or something.\n    auto_id=True,  # no need for id generation.\n)\nvideo_length = FieldSchema(\n    name=\"video_length\",\n    dtype=DataType.FLOAT,\n)\nvideo_phash = FieldSchema(\n    name=\"video_phash\", dtype=DataType.BINARY_VECTOR, dim=64\n)  # 64\n# single dimension? no multi dimension support?\nschema = CollectionSchema(\n    fields=[video_semantic_id, video_length, video_phash],\n    description=\"Test video deduplication\",\n)\n# collection = Collection(\"video\")      # Get an existing collection.\ncollection = Collection(\n    name=collection_name,\n    schema=schema,\n    using=\"default\",\n    shards_num=2,\n)\n# is this demo collection?\n# seems hard to setup.\n# not started!\n# https://milvus.io/docs/v2.0.0/metric.md#binary\n# the metric is important to us.\nsearch_params = {\"metric_type\": \"Jaccard\", \"params\": {\"nprobe\": 10}}\nimport numpy as np\nqueryData = np.array(\n    [\n        [True, True, True, False, False, True, False, True],\n        [True, False, False, True, False, True, True, False],",
        "type": "code",
        "location": "/tests/video_phash_deduplication/test_milvus.py:40-76"
    },
    "5081": {
        "file_id": 658,
        "content": "This code is defining a schema for a Milvus collection, specifying the data types and field names. The schema includes video_semantic_id, video_length, and video_phash fields, which are used in video deduplication. The code creates a collection named \"video\" with 2 shards using the specified schema and sets the metric type for searching as Jaccard with nprobe parameter set to 10. It also imports numpy and creates queryData, which seems to be a binary vector.",
        "type": "comment"
    },
    "5082": {
        "file_id": 658,
        "content": "        [True, False, False, True, True, False, False, True],\n        [True, True, True, True, True, False, False, True],\n        [True, False, False, True, False, True, True, False],\n        [False, True, True, False, False, False, False, True],\n        [True, True, False, False, False, True, True, False],\n        [False, False, True, False, False, True, False, False],\n    ]\n)\nqueryData = queryData.reshape(-1).tolist()\nqueryData = [\"1\" if x else \"0\" for x in queryData]\nimport bitarray\nqueryData = bitarray.bitarray(\"\".join(queryData), endian=\"little\")\nqueryData2 = queryData.copy()\nqueryData2[1:4] = 0\nqueryData3 = queryData2.copy()\nqueryData2 = queryData2.tobytes()\nqueryData3[8:15] = 0\nqueryData3 = queryData3.tobytes()\nqueryData = queryData.tobytes()\n# dimension: 8*8=64\n# collection.insert([[1], [np.float32(3.5)], [queryData]])\n# collection.insert([[np.float32(3.5)], [queryData]])\n# for _ in range(8):\ncollection.insert([[np.float32(3.5)], [queryData]])\ncollection.insert([[np.float32(3.5)], [queryData2]])  # slight difference.",
        "type": "code",
        "location": "/tests/video_phash_deduplication/test_milvus.py:77-102"
    },
    "5083": {
        "file_id": 658,
        "content": "The code is preparing and inserting data into a Milvus collection. It creates binary representations of queryData, queryData2, and queryData3 (slightly different from queryData2), then inserts them along with a float value (np.float32(3.5)) into the collection, representing a 64-dimensional vector.",
        "type": "comment"
    },
    "5084": {
        "file_id": 658,
        "content": "collection.insert([[np.float32(3.5)], [queryData3]])  # more difference.\n# print(len(queryData), len(queryData)*8)\n# # print(queryData.shape)\n# breakpoint()\n# collection.load()\ncollection.insert([[np.float32(3.5)], [queryData]]) # still three.\n# can release even if not loaded.\ncollection.release() # unload.\ncollection.load()\n# make it into some library!\n# insert after load?\n# # 1,64\n# what is wrong? wtf?\n# queryData = queryData.tolist()\nresults = collection.search(\n    data=[queryData],  # this is the float dimension.\n    anns_field=\"video_phash\",\n    param=search_params,\n    output_fields=[\"video_length\"],\n    limit=10,\n    expr=\"video_length > 1.2 and video_length < 4\",\n    # expr='video_length < 1.2',\n)\ntheHit = results[0]\nprint(theHit)\n# so we can perform search without filtering afterwards.\n# results[0][0].entity.get('video_length')\n# print(results[0].ids)\n# now, we want to have the 'distance' parameter.\n# print(results[0])\n# print(theHit)\n# distances = theHit.distances\n# results = [x for x in theHit]\n# hits = len(theHit)",
        "type": "code",
        "location": "/tests/video_phash_deduplication/test_milvus.py:103-139"
    },
    "5085": {
        "file_id": 658,
        "content": "The code is inserting data into a collection, releasing and reloading it, performing a search based on specific parameters, and accessing the results. The purpose seems to be searching for video data within a database based on certain criteria, such as length, and extracting relevant information from the resulting hits.",
        "type": "comment"
    },
    "5086": {
        "file_id": 658,
        "content": "# breakpoint()\n# how to get document by id? wtf",
        "type": "code",
        "location": "/tests/video_phash_deduplication/test_milvus.py:140-141"
    },
    "5087": {
        "file_id": 658,
        "content": "This code appears to be part of a debugging process, where the programmer is trying to understand how to retrieve a document by its ID using the Milvus database. The \"breakpoint()\" comment suggests they are currently stuck or needing to pause execution for further investigation.",
        "type": "comment"
    },
    "5088": {
        "file_id": 659,
        "content": "/tests/video_phash_deduplication/test_image_hash_milvus_database_search.py",
        "type": "filepath"
    },
    "5089": {
        "file_id": 659,
        "content": "This code is initializing image paths and loading necessary libraries. It calculates image hashes for three different images using the phash function from imagehash library. It then creates a connection to a Milvus database and loads an existing collection.",
        "type": "summary"
    },
    "5090": {
        "file_id": 659,
        "content": "pic_0 = \"cat.png\"\npic_0_similar = \"cat3.png\"\npic_1 = \"/root/Desktop/works/pyjom/samples/image/dick.png\"\nfrom PIL import Image\n# >>> import imagehash\n# >>> hash = imagehash.average_hash(Image.open\nimport imagehash\npics = [pic_0, pic_0_similar, pic_1]\nhashs = [imagehash.phash(Image.open(pic)) for pic in pics]\n# dis0 = hashs[0]-hashs[1]\n# dis1 = hashs[1]-hashs[2]\n# print(dis0, dis1)\n# 0 24\n# 6 24\n# well, let's check?\n# print(hashs)\n# three truth tables.\n# 2^4 = 16, total 2*8 digits=16?\n# towhee(multimodal search like jina), haystack, milvus\n# import pymilvus\nfrom pymilvus import connections\nconnection = connections.connect(alias=\"default\", host=\"localhost\", port=\"19530\")\nfrom pymilvus import Collection\ncollection = Collection(\"book\")  # Get an existing collection.\ncollection.load()\n# seems hard to setup.\n# not started!",
        "type": "code",
        "location": "/tests/video_phash_deduplication/test_image_hash_milvus_database_search.py:1-37"
    },
    "5091": {
        "file_id": 659,
        "content": "This code is initializing image paths and loading necessary libraries. It calculates image hashes for three different images using the phash function from imagehash library. It then creates a connection to a Milvus database and loads an existing collection.",
        "type": "comment"
    },
    "5092": {
        "file_id": 660,
        "content": "/tests/video_phash_deduplication/test_image_hash.py",
        "type": "filepath"
    },
    "5093": {
        "file_id": 660,
        "content": "This code uses the Python Imaging Library (PIL) and imagehash module to calculate average color hash values for images. It opens three images, 'pic_0', 'pic_0_similar', and 'pic_1', calculates their color hash values using the phash function from imagehash, and then calculates the differences between the hashes of each pair of images. The code checks the types of the calculated hashes and prints the distances (dis0 and dis1) between the pairs of images. It also mentions a possible discrepancy in the expected number of digits for the distances.",
        "type": "summary"
    },
    "5094": {
        "file_id": 660,
        "content": "pic_0= 'cat.png'\npic_0_similar = \"cat3.png\"\npic_1 = \"/root/Desktop/works/pyjom/samples/image/dick.png\"\nfrom PIL import Image\n# >>> import imagehash\n# >>> hash = imagehash.average_hash(Image.open\nimport imagehash\npics = [pic_0, pic_0_similar, pic_1]\nhashs = [imagehash.phash(Image.open(pic)) for pic in pics]\ndis0 = hashs[0]-hashs[1]\ndis1 = hashs[1]-hashs[2]\n# 0 24\n# 6 24\n# well, let's check?\nprint([type(h) for h in hashs])\nbreakpoint()\nprint(dis0, dis1)\n# three truth tables.\n# 2^4 = 16, total 2*8 digits=16?",
        "type": "code",
        "location": "/tests/video_phash_deduplication/test_image_hash.py:1-23"
    },
    "5095": {
        "file_id": 660,
        "content": "This code uses the Python Imaging Library (PIL) and imagehash module to calculate average color hash values for images. It opens three images, 'pic_0', 'pic_0_similar', and 'pic_1', calculates their color hash values using the phash function from imagehash, and then calculates the differences between the hashes of each pair of images. The code checks the types of the calculated hashes and prints the distances (dis0 and dis1) between the pairs of images. It also mentions a possible discrepancy in the expected number of digits for the distances.",
        "type": "comment"
    },
    "5096": {
        "file_id": 661,
        "content": "/tests/video_phash_deduplication/README.md",
        "type": "filepath"
    },
    "5097": {
        "file_id": 661,
        "content": "Code detects identical video files and finds repeated intervals within each video file.",
        "type": "summary"
    },
    "5098": {
        "file_id": 661,
        "content": "two main problems, one is to detect identical video files, one is to find 'repeated interval' inside each other.",
        "type": "code",
        "location": "/tests/video_phash_deduplication/README.md:1-1"
    },
    "5099": {
        "file_id": 661,
        "content": "Code detects identical video files and finds repeated intervals within each video file.",
        "type": "comment"
    }
}