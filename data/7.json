{
    "700": {
        "file_id": 50,
        "content": "                x1 = int(x0 + 1000*(-b))\n                # y1 stores the rounded off value of (rsin(theta)+1000cos(theta))\n                y1 = int(y0 + 1000*(a))\n                # x2 stores the rounded off value of (rcos(theta)+1000sin(theta))\n                x2 = int(x0 - 1000*(-b))\n                # y2 stores the rounded off value of (rsin(theta)-1000cos(theta))\n                y2 = int(y0 - 1000*(a))\n                # cv2.line draws a line in img from the point(x1,y1) to (x2,y2).\n                # (0,0,255) denotes the colour of the line to be\n                #drawn. In this case, it is red.\n                df_x = abs(x1-x2)\n                df_y = abs(y1-y2)\n                lineType = \"vertical\"\n                if df_x > df_y:\n                    lineType = \"horizontal\"\n                # we just need one single point and lineType.\n                linePoint = (x1,y1)\n                # for p_rect in rect:\n                mlines[lineType].append(linePoint)\n                # mlines2[lineType].append([(x1,y1), (x2,y2)])",
        "type": "code",
        "location": "/pyjom/medialang/functions/detectors/frameborder_Detector.py:392-417"
    },
    "701": {
        "file_id": 50,
        "content": "This code calculates the coordinates of a line using trigonometry and draws it on an image in PyJOM's media language detector. It also determines the type of line (vertical or horizontal) based on the difference between its endpoints' x and y values. The line's starting point is stored in 'linePoint', and the lines are appended to the 'mlines' list according to their type.",
        "type": "comment"
    },
    "702": {
        "file_id": 50,
        "content": "                # lineTrans.update({str((x1,y1)+lineType):(x2,y2)})\n                # cv2.line(img,(x1,y1), (x2,y2), (0,0,255),2)\n                # would not draw lines this time. draw found rects instead.\n        # get rectangle points. or just all possible rectangles?\n        yd,xd = diff_img_output.shape\n        # print(\"IMAGE SHAPE\",xd,yd)\n        rangeLimitRatio = 0.1\n        # rangeLimit = \n        visual=False\n        for lineType in mlines.keys():\n            dropIndexs = []\n            data0 =mlines[lineType]\n            if lineType == \"vertical\":\n                selectedPoints = [x[0] for x in data0]\n                rangeLimit = xd*rangeLimitRatio\n                selectedRanges = list_to_range(selectedPoints,rangeLimit)\n                # print(selectedRanges)\n                selectedRangesFlattened = [x for y in selectedRanges for x in y]\n                # print(selectedRangesFlattened)\n                data0 = [x for x in data0 if x[0] in selectedRangesFlattened]\n            else:\n                selectedPoints = [x[1] for x in data0]",
        "type": "code",
        "location": "/pyjom/medialang/functions/detectors/frameborder_Detector.py:418-439"
    },
    "703": {
        "file_id": 50,
        "content": "This code is iterating over line types in the mlines dictionary, specifically targeting vertical and horizontal lines. For vertical lines, it selects points and determines a range limit for filtering out unnecessary data. It then applies this range to flatten the data and refine the list of valid points. For horizontal lines, it performs a similar process but selects different coordinates.",
        "type": "comment"
    },
    "704": {
        "file_id": 50,
        "content": "                rangeLimit = yd*rangeLimitRatio\n                selectedRanges = list_to_range(selectedPoints,rangeLimit)\n                selectedRangesFlattened = [x for y in selectedRanges for x in y]\n                data0 = [x for x in data0 if x[1] in selectedRangesFlattened]\n            # for index, linePoint in enumerate(data0):\n            #     # linePoint = mlines[lineType]\n            #     if checkLineIntersectOcrRect(ocr_result,linePoint,lineType): dropIndexs.append(index)\n            # newdata0 = [data0[i] for i in range(len(data0)) if i not in dropIndexs]\n            newdata0 = data0\n            mlines[lineType] = newdata0\n            if visual:\n                for linePoint in newdata0:\n                    (x1,y1) = linePoint\n                    # (x2,y2) = lineTrans[str((x1,y1))+lineType]\n                    if lineType == \"vertical\":\n                        x2 = x1\n                        y2 = yd\n                    else:\n                        x2 = xd\n                        y2 = y1\n                    cv2.line(img,(x1,y1), (x2,y2), (0,0,255),2)",
        "type": "code",
        "location": "/pyjom/medialang/functions/detectors/frameborder_Detector.py:440-460"
    },
    "705": {
        "file_id": 50,
        "content": "This code section is responsible for updating the line data based on a specified range and removing any lines that intersect with the OCR result. It then generates a new list of line points and visualizes them on the image, if visualization is enabled.",
        "type": "comment"
    },
    "706": {
        "file_id": 50,
        "content": "        # enumerate all possible lines.\n        if len(mlines[\"horizontal\"]) < 2 or len(mlines[\"vertical\"]) < 2:\n            # print(\"unable to form rectangles.\")\n            continue\n        else:\n            rects =[] # list of rectangles\n            eliminateRatio = 0.3\n            for line_h1, line_h2 in itertools.combinations(mlines[\"horizontal\"],2): # this is a problem.\n                ymin, ymax = list(sorted([line_h1[1],line_h2[1]]))\n                yspan = ymax-ymin\n                if yspan < yd*eliminateRatio:\n                    continue\n                for line_v1, line_v2 in itertools.combinations(mlines[\"vertical\"], 2): # this is a problem.\n                    xmin, xmax = list(sorted([line_v1[0],line_v2[0]]))\n                    rect = ((xmin,ymin),(xmax,ymax))\n                    # pr1,pr2 = rect\n                    # for p_rect in rect:\n                    #     if checkPointInOcrRect(ocr_result,p_rect): continue # skip those traitors.\n                    xspan = xmax-xmin\n                    if xspan < xd*eliminateRatio:",
        "type": "code",
        "location": "/pyjom/medialang/functions/detectors/frameborder_Detector.py:461-481"
    },
    "707": {
        "file_id": 50,
        "content": "This code snippet combines horizontal and vertical lines to form rectangles. If there are less than two horizontal or vertical lines, the process is skipped. For each pair of horizontal lines and pairs of vertical lines, it checks if the aspect ratio is above a certain threshold before proceeding.",
        "type": "comment"
    },
    "708": {
        "file_id": 50,
        "content": "                        continue\n                    rects.append(rect)\n            # for index,elem in enumerate(rect_dict_main_list):\n            rect_dict_main_list = rectSurge(rect_dict_main_list,rects,diff_img_output)\n            # print(\"RECT DICT MAIN LIST:\")\n            # print(rect_dict_main_list) # maybe i want this shit?\n            total_rect_dict = updateTotalRects(total_rect_dict,rect_dict_main_list,frameIndex,diff_img_output,minRectArea=config_minRectArea)\n            mdisplayed_rect_count = 0\n            for rect_dict in rect_dict_main_list:\n                life = rect_dict[\"life\"]\n                if life < min_rect_life_display_thresh:\n                    continue # this is needed.\n                # draw shit now.\n                mdisplayed_rect_count +=1\n                (xmin,ymin),(xmax,ymax) = rect_dict[\"rect\"]\n                cv2.rectangle(img,(xmin,ymin),(xmax,ymax) , (255,0,0), 2)\n            #     (xmin,ymin),(xmax,ymax) = rect\n            #     rect_area = (xmax-xmin) * (ymax-ymin)",
        "type": "code",
        "location": "/pyjom/medialang/functions/detectors/frameborder_Detector.py:482-500"
    },
    "709": {
        "file_id": 50,
        "content": "The code iterates through a list of rectangles, appends them to the rects list if their life is above a certain threshold. It then updates the total_rect_dict and displays rectangles with a life above a minimum threshold on the image. The rectangles are drawn as bounding boxes using cv2.rectangle function.",
        "type": "comment"
    },
    "710": {
        "file_id": 50,
        "content": "            #     print(\"rect found:\",rect,rect_area)\n            prevFrame = img.copy()\n            # print(\"total rects:\",mdisplayed_rect_count)\n        if visual:\n            cv2.imshow('linesDetected.jpg', img)\n            # cv2.imshow(\"edges.jpg\",edges) # not for fun.\n            if cv2.waitKey(20) == ord(\"q\"):\n                print(\"QUIT INTERFACE.\")\n                break\n    # All the changes made in the input image are finally\n    # written on a new image houghlines.jpg\n    # if mode == 1:\n    print(\"FINAL RESULT:\")\n    for key in total_rect_dict.keys():\n        elem = total_rect_dict[key]\n        print(\"RECT UUID\",key)\n        print(\"RECT CONTENT\",elem)\n    popKeys = []\n    for key in total_rect_dict.keys():\n        elem = total_rect_dict[key]\n        if elem[\"endFrame\"] is None:\n            popKeys.append(key)\n    for key in popKeys:\n        total_rect_dict.pop(key) # remove premature rectangles.\n    # break\n    return total_rect_dict\ndef framedifference_talib_FrameIterator(mediapath,**config):\n    algorithm = (",
        "type": "code",
        "location": "/pyjom/medialang/functions/detectors/frameborder_Detector.py:501-529"
    },
    "711": {
        "file_id": 50,
        "content": "The code is detecting frames in an image using computer vision techniques. It creates a rectangle around each frame, counts the total number of rectangles displayed, and displays the image with detected frames. The program checks for user input to quit and then removes any prematurely ended rectangles from the dictionary before returning the final list of rectangles. The function is part of a larger algorithm for frame detection and iteration using talib library in PyJOM.",
        "type": "comment"
    },
    "712": {
        "file_id": 50,
        "content": "        bgs.FrameDifference()\n    )  # this is not stable since we have more boundaries. shall we group things?\n    video_file = (\n        mediapath  # this is doggy video without borders.\n    )\n    # video_file = \"../../samples/video/LiEIfnsvn.mp4\" # this one with cropped boundaries.\n    capture = cv2.VideoCapture(video_file)\n    while not capture.isOpened():\n        capture = cv2.VideoCapture(video_file)\n        # cv2.waitKey(1000)\n        # print(\"Wait for the header\")\n    pos_frame = capture.get(1)\n    past_frames = config[\"past_frames\"]\n    def getAppendArray(mx1, min_x, past_frames=past_frames):\n        return np.append(mx1[-past_frames:], min_x)\n    def getFrameAppend(frameArray, pointArray, past_frames=past_frames):\n        mx1, mx2, my1, my2 = [\n            getAppendArray(a, b, past_frames=past_frames)\n            for a, b in zip(frameArray, pointArray)\n        ]\n        return mx1, mx2, my1, my2\n    timeperiod = config[\"timeperiod\"]\n    def getStreamAvg(a, timeperiod=timeperiod):  # to maintain stability.",
        "type": "code",
        "location": "/pyjom/medialang/functions/detectors/frameborder_Detector.py:530-559"
    },
    "713": {
        "file_id": 50,
        "content": "The code is initializing a video file and capturing frames from it. It uses various functions such as FrameDifference, getAppendArray, and getFrameAppend for processing the frames and points. The purpose is to maintain stability in the data by averaging over a certain time period using the getStreamAvg function.",
        "type": "comment"
    },
    "714": {
        "file_id": 50,
        "content": "        return talib.stream.EMA(a, timeperiod=timeperiod)\n    change_threshold = config[\"change_threshold\"]\n    def checkChange(frame_x1, val_x1, h, change_threshold=change_threshold):\n        return (abs(frame_x1 - val_x1) / h) > change_threshold  # really changed.\n    mx1, mx2, my1, my2 = [np.array([]) for _ in range(4)]\n    # past_frames = 19\n    #already set.\n    perc = config[\"perc\"]\n    frame_num = 0 # not to change.\n    # what is the time to update the frame?\n    frame_x1, frame_y1, frame_x2, frame_y2 = [None for _ in range(4)]\n    reputation = 0\n    max_reputation = config[\"max_reputation\"]\n    minVariance =config[\"minVariance\"]\n    frameDict = {}  # include index, start, end, coords.\n    frameIndex = 0\n    framePeriod = config[\"framePeriod\"]\n    frame_total_count = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n    # print(\"FRAME_TOTAL_COUNT:\",frame_total_count)\n    # breakpoint()\n    for _ in progressbar.progressbar(range(frame_total_count)):\n        flag, frame = capture.read()\n        if not frameIndex%framePeriod == 0:",
        "type": "code",
        "location": "/pyjom/medialang/functions/detectors/frameborder_Detector.py:560-588"
    },
    "715": {
        "file_id": 50,
        "content": "This code appears to be a part of a larger function or program. It initializes variables and loops through frames of a video, likely for analysis or processing purposes. The code uses the EMA (Exponential Moving Average) from the talib library and has conditions to check changes in frame values. It also sets up dictionaries and tracks the frame index and total count of frames in the video. Further understanding would require knowledge about the rest of the codebase and how this specific function contributes to its overall functionality.",
        "type": "comment"
    },
    "716": {
        "file_id": 50,
        "content": "            continue # skip shit.\n        frameIndex += 1\n        if flag:\n            pos_frame = capture.get(1)  # this is getting previous frame without read again.\n            img_output = algorithm.apply(frame)\n            img_bgmodel = algorithm.getBackgroundModel()\n            _, contours = cv2.findContours(\n                img_output, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE\n            )\n            # maybe you should merge all active areas.\n            if contours is not None:\n                # continue\n                counted = False\n                for contour in contours:\n                    [x, y, w, h] = cv2.boundingRect(img_output)\n                    if not counted:\n                        min_x, min_y = x, y\n                        max_x, max_y = x + w, y + h\n                        counted = True\n                    else:\n                        min_x = min(min_x, x)\n                        min_y = min(min_y, y)\n                        max_x = max(max_x, x + w)\n                        max_y = max(max_y, y + h)",
        "type": "code",
        "location": "/pyjom/medialang/functions/detectors/frameborder_Detector.py:589-612"
    },
    "717": {
        "file_id": 50,
        "content": "Continuing loop through frames and skipping invalid ones. Updates frame index and retrieves previous frame without re-reading. Applies an algorithm to the current frame and gets the background model. Finds contours in the processed frame using cv2 library, iterates through each contour, calculates bounding rectangles for detection areas, updates minimum/maximum coordinates if not counted, and continues until all contours are analyzed.",
        "type": "comment"
    },
    "718": {
        "file_id": 50,
        "content": "                        # only create one single bounding box.\n                # print(\"points:\",min_x, min_y, max_x,max_y)\n                this_w = max_x - min_x\n                this_h = max_y - min_y\n                thresh_x = max(minVariance, int(perc * (this_w)))\n                thresh_y = max(minVariance, int(perc * (this_h)))\n                mx1, mx2, my1, my2 = getFrameAppend(\n                    (mx1, mx2, my1, my2), (min_x, max_x, min_y, max_y)\n                )\n                val_x1, val_x2, val_y1, val_y2 = [\n                    getStreamAvg(a) for a in (mx1, mx2, my1, my2)\n                ]\n                # not a number. float\n                # will return False on any comparison, including equality.\n                if (\n                    abs(val_x1 - min_x) < thresh_x\n                    and abs(val_x2 - max_x) < thresh_x\n                    and abs(val_y1 - min_y) < thresh_y\n                    and abs(val_y2 - max_y) < thresh_y\n                ):\n                    needChange = False\n                    # this will create bounding rect.",
        "type": "code",
        "location": "/pyjom/medialang/functions/detectors/frameborder_Detector.py:613-634"
    },
    "719": {
        "file_id": 50,
        "content": "This code creates a single bounding box by calculating the threshold values based on the percentage and bounding box dimensions. It checks if the average stream values of the bounding box corners are within the calculated thresholds, and if so, it sets needChange to False, indicating that no change is needed in the bounding rect creation.",
        "type": "comment"
    },
    "720": {
        "file_id": 50,
        "content": "                    # this cannot handle multiple active rects.\n                    reputation = max_reputation\n                    if frame_x1 == None:\n                        needChange = True\n                    elif (\n                        checkChange(frame_x1, val_x1, this_w)\n                        or checkChange(frame_x2, val_x2, this_w)\n                        or checkChange(frame_y1, val_y1, this_h)\n                        or checkChange(frame_y2, val_y2, this_h)\n                    ):\n                        needChange = True\n                        # the #2 must be of this reason.\n                    if needChange:\n                        frame_x1, frame_y1, frame_x2, frame_y2 = [\n                            int(a) for a in (min_x, min_y, max_x, max_y)\n                        ]\n                        print()\n                        print(\"########FRAME CHANGED########\")\n                        frame_num += 1\n                        frame_area = (frame_x2 - frame_x1) * (frame_y2 - frame_y1)\n                        # update the shit.",
        "type": "code",
        "location": "/pyjom/medialang/functions/detectors/frameborder_Detector.py:635-655"
    },
    "721": {
        "file_id": 50,
        "content": "This code checks if the frame boundaries have changed and updates them accordingly. If a change occurs, it increments the frame number and prints \"########FRAME CHANGED########\". The frame area is then calculated based on the updated frame boundaries.",
        "type": "comment"
    },
    "722": {
        "file_id": 50,
        "content": "                        coords = ((frame_x1, frame_y1), (frame_x2, frame_y2))\n                        frameDict[frame_num] = {\n                            \"coords\": coords,\n                            \"start\": frameIndex,\n                            \"end\": frameIndex,\n                        }\n                        print(\n                            \"FRAME INDEX: {}\".format(frame_num)\n                        )  # this is the indexable frame. not uuid.\n                        print(\"FRAME AREA: {}\".format(frame_area))\n                        print(\"FRAME COORDS: {}\".format(str(coords)))\n                    # allow us to introduce our new frame determinism.\n                else:\n                    if reputation > 0:\n                        reputation -= 1\n                if frame_x1 is not None and reputation > 0:\n                    # you may choose to keep cutting the frame? with delay though.\n                    cv2.rectangle(\n                        frame, (frame_x1, frame_y1), (frame_x2, frame_y2), (255, 0, 0), 2",
        "type": "code",
        "location": "/pyjom/medialang/functions/detectors/frameborder_Detector.py:656-674"
    },
    "723": {
        "file_id": 50,
        "content": "Code is detecting frames in an image, storing their coordinates and index range in a dictionary. If the frame's reputation is positive, it will reduce the reputation and continue cutting the frame if needed. It also prints information about the detected frame to the console.",
        "type": "comment"
    },
    "724": {
        "file_id": 50,
        "content": "                    )\n                    frameDict[frame_num][\"end\"] = frameIndex\n                    # we mark the first and last time to display this frame.\n                # how to stablize this shit?\n            cv2.imshow(\"video\", frame)\n            # just video.\n            # cv2.imshow('img_output', img_output)\n            # cv2.imshow('img_bgmodel', img_bgmodel)\n        else:\n            # cv2.waitKey(1000) # what the heck?\n            break\n        # if 0xFF & cv2.waitKey(10) == 27:\n        #     break\n    # cv2.destroyAllWindows()\n    print(\"FINAL FRAME DETECTIONS:\")\n    print(frameDict)\n    return frameDict\n    # {1: {'coords': ((80, 199), (496, 825)), 'start': 13, 'end': 269}, 2: {'coords': ((80, 381), (483, 644)), 'start': 297, 'end': 601}}\ndef frameborder_default_configs(model=\"framedifference_talib\"):\n    assert model in [\"framedifference_talib\",\"huffline_horizontal_vertical\"]\n    if model == \"framedifference_talib\":\n        df_config = {\"past_frames\":19,\"timeperiod\":10, \"change_threshold\":0.2,\"perc\":0.03, \"max_reputation\": 3,\"framePeriod\":1,\"minVariance\" :10}",
        "type": "code",
        "location": "/pyjom/medialang/functions/detectors/frameborder_Detector.py:675-699"
    },
    "725": {
        "file_id": 50,
        "content": "This code is a part of the frameborder_Detector function, which detects frames with high frame borders. It uses OpenCV's imshow function to display the video and frames. The frameDict variable stores the frame numbers and their respective coordinates and timeframes. If a specific key is pressed, it breaks out of the loop. After detecting final frame detections, it returns the frameDict. This code also includes a default config function for different models.",
        "type": "comment"
    },
    "726": {
        "file_id": 50,
        "content": "    else:\n        df_config = {\"line_thresh\":150, # original 150\n        \"includeBoundaryLines\":True,\"blurKernel\":(5,5),\"angle_error\":0.00003,\"delta_thresh\":0.1,\"min_rect_life\":0,\"max_rect_life\":6,\"framePeriod\":1,\"min_rect_life_display_thresh\":3,\"minRectArea\":1}\n    return df_config\ndef frameborder_Detector(mediapaths, model=\"framedifference_talib\",config={}):\n    print(\"MODEL:\",model)\n    # breakpoint()\n    yconfig = copy.deepcopy(config)\n    # breakpoint()\n    # if config is None:\n    #     breakpoint()\n    #     config = {}\n    # any better detectors? deeplearning?\n    assert model in [\"framedifference_talib\",\"huffline_horizontal_vertical\"]\n    results = []\n    keyword = \"{}_detector\".format(model)\n    data_key = keyword # different than yolo.\n    for mediapath in mediapaths:\n        print(\"mediapath:\", mediapath)\n        # breakpoint()\n        mediatype = getFileType(mediapath)\n        print(\"subtitle of mediatype:\", mediatype)\n        assert mediatype in [\"video\", \"image\"]  # gif? anything like that?\n        if model == \"framedifference_talib\":",
        "type": "code",
        "location": "/pyjom/medialang/functions/detectors/frameborder_Detector.py:700-725"
    },
    "727": {
        "file_id": 50,
        "content": "The code is defining a function called \"frameborder_Detector\" which takes in media paths and a model parameter. The function checks the model and applies different detection algorithms based on the input model. It also initializes a deepcopy of the configuration and checks if the model is either \"framedifference_talib\" or \"huffline_horizontal_vertical\". If not, it raises an assertion error. Finally, it iterates through the media paths and performs some actions specific to the given model.",
        "type": "comment"
    },
    "728": {
        "file_id": 50,
        "content": "            assert mediatype == \"video\"\n            # advice you to check out only those areas with rectangles, not something else, if detected any rectangular area.\n            # but does that happen for normal videos? you might want huffline transforms.\n            # you can eliminate unwanted rectangles by time duration and huffline transforms.\n        result = {\"type\": mediatype, data_key: {}}\n        default_config = frameborder_default_configs(model)\n        xconfig = default_config\n        xconfig.update(yconfig) # override default configs.\n        # do not freaking assign directly after update.\n        config = xconfig\n        # print(\"YCONFIG:\", yconfig)\n        # breakpoint()\n        if mediatype == \"image\":\n            data = cv2.imread(mediapath)\n            data = keywordDecorator(huffline_stillImage_Identifier, **config)(data) # this is just oldfashioned function decorator\n            result[data_key].update({keyword: data})\n            result[data_key].update({\"config\": config})\n        else:",
        "type": "code",
        "location": "/pyjom/medialang/functions/detectors/frameborder_Detector.py:726-744"
    },
    "729": {
        "file_id": 50,
        "content": "This code is performing frame detection on video or image media. It checks if the mediatype is \"video\" and provides advice for checking only rectangular areas, suggesting huffline transforms to eliminate unwanted rectangles by time duration. If the mediatype is \"image\", it reads the image using cv2.imread, applies keywordDecorator with huffline_stillImage_Identifier and config, updates the result dictionary with the data and config.",
        "type": "comment"
    },
    "730": {
        "file_id": 50,
        "content": "            # you may not want videoFrameIterator.\n            if model == \"framedifference_talib\":\n                mdata= framedifference_talib_FrameIterator(\n                    mediapath,**config)\n            elif model == \"huffline_horizontal_vertical\":\n                mdata = huffline_horizontal_vertical_FrameIterator(mediapath,**config)\n            metadata = {\"config\": config}\n            result[data_key][keyword] = mdata\n            result[data_key].update(metadata)\n        results.append(result)\n    return results",
        "type": "code",
        "location": "/pyjom/medialang/functions/detectors/frameborder_Detector.py:745-755"
    },
    "731": {
        "file_id": 50,
        "content": "This code selects a specific detector model based on the input \"model\" parameter. If the model is \"framedifference_talib\", it creates an instance of framedifference_talib_FrameIterator, and if the model is \"huffline_horizontal_vertical\", it creates an instance of huffline_horizontal_vertical_FrameIterator. It then adds the created iterator to the result dictionary along with a metadata dictionary containing the original config parameters. The final results list is appended with this result dictionary.",
        "type": "comment"
    },
    "732": {
        "file_id": 51,
        "content": "/pyjom/medialang/functions/detectors/entityDetector.py",
        "type": "filepath"
    },
    "733": {
        "file_id": 51,
        "content": "The code performs text processing, device movement detection, and entity identification using Levenshtein's algorithm. It calculates string similarities, generates test results, compares content and location changes to find or create entity structures, initializes variables, detects and tracks entities, updates text detection results, merges entries based on similarity and temporal proximity, and returns a formatted result dictionary.",
        "type": "summary"
    },
    "734": {
        "file_id": 51,
        "content": "from .mediaDetector import *\nimport Levenshtein\nimport string\nimport zhon.hanzi\nimport wordninja\ndef resplitedEnglish(string2,skipSpecial=True):\n    if skipSpecial:\n        header = string2[0]\n        if header in string.punctuation or header in zhon.hanzi.punctuation:\n            return string2 # this could be buggy.\n    result = wordninja.split(string2)\n    if len(result)>1:\n        mlist = zip(result[:-1], result[1:])\n        for a,b in mlist:\n            combined = \"{} {}\".format(a,b)\n            error = a+b\n            string2 = string2.replace(error,combined)\n    return string2 # really? this is not good. maybe you should provide some version of continuality, for channel id watermarks.\n    # @MA SECO. -> @MASECO\n# maybe you can read it here?\n# you need double language check. both chinese and english. or really?\ndef ocrEntityDetector(mdata):\n    alteredData = [] # we should do a demo. \n    return alteredData # now we are on the same page, paddleocr is using cuda 11.2 which is compatible to 11.3\ndef getMinLenStr(a,b):",
        "type": "code",
        "location": "/pyjom/medialang/functions/detectors/entityDetector.py:1-29"
    },
    "735": {
        "file_id": 51,
        "content": "The code includes a function \"resplitedEnglish\" that splits the given string into individual words, potentially skipping special characters. It uses wordninja for splitting and attempts to merge adjacent words if they have been mistakenly separated by special characters. Another function, \"ocrEntityDetector\", receives data as input and returns alteredData, but the code lacks implementation details. Lastly, there's a helper function \"getMinLenStr\" that takes two inputs 'a' and 'b', likely for comparison purposes.",
        "type": "comment"
    },
    "736": {
        "file_id": 51,
        "content": "    la,lb = len(a),len(b)\n    if la < lb:return a\n    return b\ndef getBlockType(dlocation,dcontent):\n    if not dlocation:\n        if not dcontent: return \"stationary\"\n        else: return \"typing\"\n    else:\n        if not dcontent: return \"typing_moving\"\n        else: return \"moving\"\ndef getStringDistance(a,b):\n    return Levenshtein.distance(a,b)\ndef getStringSimilarity(a,b):\n    return Levenshtein.ratio(a,b)\ndef getChineseLen(string2):\n    counter  = 0\n    upperLimit, lowerLimit = 0x4e00, 0x9fff\n    for elem in string2:\n        ordNum = ord(elem)\n        if ordNum <= upperLimit and ordNum>=lowerLimit:\n            counter+=1\n    return counter\ndef getPunctualLen(string2):\n    counter = 0\n    chinesePunctuals = zhon.hanzi.punctuation\n    englishPunctuals = string.punctuation\n    standardString = chinesePunctuals+englishPunctuals\n    for elem in string2:\n        if elem in standardString:\n            counter+=2\n    return counter\ndef getEnglishLen(string2):\n    counter = 0\n    standardString = \"abcdefghijklmnopqrstuvwxyz\"",
        "type": "code",
        "location": "/pyjom/medialang/functions/detectors/entityDetector.py:30-70"
    },
    "737": {
        "file_id": 51,
        "content": "This code defines several functions for text processing, including getting the length of Chinese characters, English punctuation marks, and English words. It also calculates string similarity and distance using Levenshtein's algorithm. The \"getBlockType\" function determines if the device is stationary or moving based on its location and content.",
        "type": "comment"
    },
    "738": {
        "file_id": 51,
        "content": "    standardString += standardString.upper()\n    standardString += \"0123456789\"\n    # it won't split. you may need double check the thing.\n    # standardString += \" \"\n    for elem in string2:\n        if elem in standardString:\n            counter+=1\n    return counter\ndef getMinMaxText(a,b):\n    mlist = [a,b]\n    clens = [getChineseLen(x) for x in mlist]\n    elens = [getEnglishLen(x) for x in mlist]\n    slens = [getPunctualLen(x) for x in mlist]\n    mlens = [x[0]+x[1]+x[2] for x in zip(clens,elens,slens)]\n    if len(a) > len(b):\n        if mlens[0] > mlens[1]:\n            return a\n        return b\n    else:\n        if mlens[0] < mlens[1]:\n            return b\n        return a\ndef pointDifference(a,b):\n    return [a[0] - b[0], a[1] - b[1]]\ndef makeOCREntity(ocrData,minMaxThresh = 24 ,# max difference is ten pixel. or it is considered as moving.\nstrDisThreshold = 2 ,# or considered as changing?\ncertThreshold = 0.6,\nchangingMinMaxThresh = 45,\nchangingstrDisThreshold = 3,\ntimeThreshold = 0.3 ,# i intentially set it.\nblockTimeThreshold = 0.3, # at least last this long?",
        "type": "code",
        "location": "/pyjom/medialang/functions/detectors/entityDetector.py:71-104"
    },
    "739": {
        "file_id": 51,
        "content": "The code defines a series of functions for detecting changes in textual data, such as entity detection and calculating point differences. It also involves calculating Chinese, English, and punctuation lengths and uses them to determine if the text is changing or moving based on certain thresholds.",
        "type": "comment"
    },
    "740": {
        "file_id": 51,
        "content": "strSimThreshold = 0.8):\n    testElemIds = {} # just show processed items.\n    for line in ocrData:\n        mtime,mframe,mresult = line[\"time\"],line[\"frame\"],line[\"paddleocr\"]\n        # print(\"______________________\")\n        # print(\"time:\",mtime)\n        # newlyAddedIds = [] # will directly added if not in.\n        # initiate things here.\n        for mid in testElemIds.keys(): # must trt\n            testElemIds[mid][\"hasIdentical\"] =False  #initialization.\n        for presult in mresult:\n            location, mtext = presult\n            p1, p2, p3, p4 = location\n            text, certainty = mtext\n            print(\"RECOGNIZED TEXT:\",text)\n            mtimestamp = {\"frame\":mframe,\"time\":mtime}\n            # print(\"location:\",location)\n            # print(\"text:\",text)\n            # print(\"certainty:\",certainty)\n            foundIdentical = False\n            for mid in testElemIds.keys():\n                myid = testElemIds[mid]\n                myLastLocation = myid[\"locations\"][-1]\n                px1,px2,px3,px4 = myLastLocation",
        "type": "code",
        "location": "/pyjom/medialang/functions/detectors/entityDetector.py:105-128"
    },
    "741": {
        "file_id": 51,
        "content": "This code initializes variables and iterates through OCR data, specifically focusing on recognized text from each result. It checks if any previously identified elements have identical locations to the current element, updating a flag accordingly. The purpose seems to be detecting entities based on overlapping locations in different frames or times.",
        "type": "comment"
    },
    "742": {
        "file_id": 51,
        "content": "                myMinMax = max([max(pointDifference(a,b)) for a,b in zip(location,myLastLocation)])\n                myMinMaxs = [max([max(pointDifference(a,b)) for a,b in zip(location,myLL)]) for myLL in myid[\"locations\"]] # changing it. the max movement.\n                mLastTime = myid[\"timestamps\"][-1][\"time\"]\n                timeDelta = mLastTime - mtime\n                myLastContent = myid[\"contents\"][-1]\n                strDistance = getStringDistance(myLastContent,text)\n                strDistances = [getStringDistance(myLastContent,text) for myLC in myid[\"contents\"]]\n                strSim = getStringSimilarity(myLastContent,text)\n                strSims = [getStringSimilarity(myLC,text) for myLC in myid[\"contents\"]]\n                foundIdentical = False\n                movementMap = {\"location\":False,\"content\":False,\"continual\":False}\n                if timeDelta < timeThreshold:\n                    if myMinMax <= minMaxThresh and ((strDistance <= strDisThreshold) or (strSim >= strSimThreshold )) : # wrong logic.",
        "type": "code",
        "location": "/pyjom/medialang/functions/detectors/entityDetector.py:129-142"
    },
    "743": {
        "file_id": 51,
        "content": "This code calculates the maximum movement and string similarity between consecutive entities, considering time deltas and thresholds. If the movement is within a threshold and the string distance or similarity meets specific criteria, it indicates an identical entity. However, the current logic for identifying identical entities may be incorrect.",
        "type": "comment"
    },
    "744": {
        "file_id": 51,
        "content": "                        foundIdentical = True\n                        print(\"test result:\",myMinMax <= minMaxThresh ,(strDistance <= strDisThreshold) , (strSim >= strSimThreshold ))\n                        print(myMinMax,strDistance,strSim)\n                        print(minMaxThresh,strDisThreshold,strSimThreshold)\n                        print(\"line a\")\n                        pass\n                        # stricter limit, to know if really is movement?\n                    elif myMinMax <= changingMinMaxThresh:\n                        foundIdentical = True\n                        movementMap[\"location\"] = True\n                        if max(strDistances) <= strDisThreshold or max(strSims) >= strSimThreshold:\n                            # make sure it is globally the same.\n                            print(\"line b\")\n                            pass\n                        elif min(strDistances) <= changingstrDisThreshold:\n                            print(\"line c\")\n                            movementMap[\"content\"] = True",
        "type": "code",
        "location": "/pyjom/medialang/functions/detectors/entityDetector.py:143-161"
    },
    "745": {
        "file_id": 51,
        "content": "This code block is checking for identical entities or potential movements in a video. It uses thresholds to determine if the entity is the same, and if it's a movement, it checks if it's the same content or location. The code also prints test results and specific lines for debugging purposes.",
        "type": "comment"
    },
    "746": {
        "file_id": 51,
        "content": "                        else: # consider something else\n                            print(\"line d\")\n                            foundIdentical = False\n                    elif strDistance <= changingstrDisThreshold or strSim >= strSimThreshold:\n                        foundIdentical = True\n                        print(\"line e\")\n                        movementMap[\"content\"] = True\n                        if max(myMinMaxs) <= minMaxThresh:\n                            print(\"line f\")\n                            pass\n                        elif min(myMinMaxs) <= changingMinMaxThresh:\n                            print(\"line g\")\n                            movementMap[\"location\"] = True\n                        else:\n                            print(\"line h\")\n                            foundIdentical = False\n                else:\n                    foundIdentical = False\n                if foundIdentical:\n                    print(\"FOUND IDENTICAL\",text,myLastContent)\n                    print(\"REASON\",movementMap)",
        "type": "code",
        "location": "/pyjom/medialang/functions/detectors/entityDetector.py:162-183"
    },
    "747": {
        "file_id": 51,
        "content": "This code checks if the text has a matching content or location change using string similarity and minimum-maximum thresholds. If a match is found, it prints \"FOUND IDENTICAL\" with the original text and reason for identification from the movementMap dictionary.",
        "type": "comment"
    },
    "748": {
        "file_id": 51,
        "content": "                    print(\"ID\",mid)\n                    print()\n                    # care about continuality here.\n                    if timeDelta < timeThreshold:\n                        movementMap[\"continual\"] = True\n                    if myid[\"hasIdentical\"] or timeDelta == 0: # eliminate duplicates.\n                        continue # do not check this.\n                    # print(\"found Identical:\",mid)\n                    testElemIds[mid][\"hasIdentical\"]=True\n                    testElemIds[mid][\"locations\"].append(location)\n                    testElemIds[mid][\"contents\"].append(text)\n                    testElemIds[mid][\"timestamps\"].append(mtimestamp)\n                    testElemIds[mid][\"movements\"].append(movementMap)\n                    break\n            if not foundIdentical:\n                if certainty > certThreshold:\n                    minitStruct = {str(uuid.uuid4()):{\"locations\":[copy.deepcopy(location)],\"contents\":[copy.deepcopy(text)],\"movements\":[],\"hasIdentical\":False,\"timestamps\":[mtimestamp]}}",
        "type": "code",
        "location": "/pyjom/medialang/functions/detectors/entityDetector.py:184-200"
    },
    "749": {
        "file_id": 51,
        "content": "This code is searching for identical entities based on location, text content, and timestamps. If a duplicate is found or time difference is small, it continues to the next entity. If no duplicates are found and certainty level is high, it creates a new entity structure with unique ID, locations, contents, movements, and timestamps.",
        "type": "comment"
    },
    "750": {
        "file_id": 51,
        "content": "                    testElemIds.update(minitStruct) # do you really expect it? i mean it could have cracks.\n    # print(json.dumps(testElemIds,indent=4))\n    keys= list(testElemIds.keys())\n    print(\"keyNum:\",len(keys))\n    mfinal = {}\n    for key in keys:\n        mblocks = []\n        kElem = testElemIds[key]\n        # we try to compress this thing.\n        initBlock = {\"type\":\"stationary\",\"text\":None,\"location\":None,\"timespan\":{\"start\":None,\"end\":None}} # we will have shortest text.\n        for index, mtimestamp in enumerate(kElem[\"timestamps\"]):\n            thisText = kElem[\"contents\"][index]\n            thisLocation = kElem[\"locations\"][index]\n            if index == 0:\n                initBlock[\"timespan\"][\"start\"] = mtimestamp\n                initBlock[\"timespan\"][\"end\"] = mtimestamp\n                initBlock[\"text\"] = thisText\n                initBlock[\"location\"] = copy.deepcopy(thisLocation)\n            else:\n                movementMap = kElem[\"movements\"][index-1]\n                dlocation, dcontent, dtime = movementMap[\"location\"], movementMap[\"content\"], movementMap[\"continual\"]",
        "type": "code",
        "location": "/pyjom/medialang/functions/detectors/entityDetector.py:201-221"
    },
    "751": {
        "file_id": 51,
        "content": "This code is initializing variables for block detection and compression. It creates an empty dictionary, loops through the timestamps and contents of elements, and assigns the first timestamp and content as the starting point of a stationary block. It also copies the location from the first content to the \"location\" field in the initBlock.",
        "type": "comment"
    },
    "752": {
        "file_id": 51,
        "content": "                lastType = initBlock[\"type\"]\n                thisType = getBlockType(dlocation,dcontent)\n                if (not dtime) or (thisType != lastType):\n                    # will abandon all no matter what. cause it is not continual.\n                    mblocks.append(copy.deepcopy(initBlock))\n                    initBlock = {\"type\":thisType,\"timespan\":{\"start\":mtimestamp,\"end\":mtimestamp}} # get the content.\n                    if thisType in [\"stationary\", \"moving\"]:\n                        # lastText = initBlock[\"text\"]\n                        # mselectedText = getMinMaxText(lastText,thisText)\n                        initBlock.update({\"text\":thisText}) # not right. we select the best one.\n                    if thisType in [\"stationary\", \"typing\"]:\n                        initBlock.update({\"location\":copy.deepcopy(thisLocation)})\n                    if thisType in [\"typing_moving\",\"typing\"]:\n                        initBlock.update({\"texts\":[thisText]})\n                    if thisType in [\"moving\", \"typing_moving\"]:",
        "type": "code",
        "location": "/pyjom/medialang/functions/detectors/entityDetector.py:222-237"
    },
    "753": {
        "file_id": 51,
        "content": "This code snippet initializes a new block for text detection whenever there is a change in type or no time stamp. The block's details are updated based on the current type, location, and texts. It also stores the previous and new types, with the option to select the best text if the type is \"stationary\" or \"moving\".",
        "type": "comment"
    },
    "754": {
        "file_id": 51,
        "content": "                        initBlock.update({\"locations\":[copy.deepcopy(thisLocation)]})\n                else:\n                    initBlock[\"timespan\"][\"end\"] = mtimestamp\n                    if thisType in [\"moving\",\"typing_moving\"]:\n                        initBlock[\"locations\"].append(copy.deepcopy(thisLocation))\n                    if thisType in [\"typing_moving\",\"typing\"]:\n                        initBlock[\"texts\"].append(thisText)\n                    if thisType in [\"moving\",\"stationary\"]: # we don't change stationary/typing's location. or do we?\n                        mLastText = initBlock[\"text\"]\n                        initBlock[\"text\"] = getMinMaxText(mLastText,thisText)\n                        # initBlock[\"text\"] = getMinLenStr(mLastText,thisText)\n        mblocks.append(copy.deepcopy(initBlock))\n        mblocks2 = []\n        for block in mblocks:\n            start,end = block[\"timespan\"][\"start\"], block[\"timespan\"][\"end\"]\n            timedelta = end[\"time\"] - start[\"time\"]\n            if timedelta > blockTimeThreshold:",
        "type": "code",
        "location": "/pyjom/medialang/functions/detectors/entityDetector.py:238-254"
    },
    "755": {
        "file_id": 51,
        "content": "This code appears to be part of a function that detects and tracks entities such as text or movement. It updates the initialization block based on the detected entity type, appends locations or texts, creates a copy of the block for later use, and filters out blocks with duration shorter than the given threshold.",
        "type": "comment"
    },
    "756": {
        "file_id": 51,
        "content": "                mblocks2.append(block)\n        mfinal.update({key:mblocks2})\n    return mfinal\n    # print(json.dumps(mfinal,indent=4))\n    # print(\"___________\")\ndef staticOCRCombinator(myresult,simThreshold= 0.8):\n    # we use wordninja to do the english spliting.\n    # you can also get this working for non-static.\n    myNewResult = {}\n    lastWordResult = None\n    lastId = None\n    lastLocation = None\n    lastTimeStamp = {\"start\":{},\"end\":{}}\n    for key in myresult.keys():\n        melems = myresult[key]\n        for melem in melems:\n            mtype = melem[\"type\"]\n            if mtype in [\"stationary\", \"moving\"]:\n                mtext = melem[\"text\"] # maybe there are some moving things out there?\n            else:\n                mtext = melem[\"texts\"][0]\n            mtext = resplitedEnglish(mtext)\n            if lastWordResult is None:\n                lastWordResult = mtext\n                lastId = key\n                lastTimeStamp[\"start\"] = melem[\"timespan\"][\"start\"]\n                lastTimeStamp[\"end\"] = melem[\"timespan\"][\"end\"]",
        "type": "code",
        "location": "/pyjom/medialang/functions/detectors/entityDetector.py:255-283"
    },
    "757": {
        "file_id": 51,
        "content": "This function takes a dictionary of results and applies static OCR combining by processing each element in the dictionary. It uses WordNinja for English splitting, and stores last word result, id, location, and timestamp.",
        "type": "comment"
    },
    "758": {
        "file_id": 51,
        "content": "                if mtype in [\"stationary\", \"typing\"]:\n                    lastLocation = melem[\"location\"]\n                else:\n                    lastLocation = melem[\"locations\"][0]\n            else:\n                if getStringSimilarity(lastWordResult,mtext) > simThreshold:\n                    # merge the thing.\n                    lastTimeStamp[\"start\"] = list(sorted([melem[\"timespan\"][\"start\"],lastTimeStamp[\"start\"]],key=lambda x:x[\"time\"]))[0]\n                    lastTimeStamp[\"end\"] = list(sorted([melem[\"timespan\"][\"end\"],lastTimeStamp[\"end\"]],key=lambda x:-x[\"time\"]))[0]\n                else:\n                    myNewResult.update({lastId:{\"content\":lastWordResult,\"timespan\":lastTimeStamp,\"location\":lastLocation}})\n                    lastWordResult = mtext\n                    lastId = key\n                    if mtype in [\"stationary\", \"typing\"]:\n                        lastLocation = melem[\"location\"]\n                    else:\n                        lastLocation = melem[\"locations\"][0]\n                    lastTimeStamp[\"start\"] = melem[\"timespan\"][\"start\"]",
        "type": "code",
        "location": "/pyjom/medialang/functions/detectors/entityDetector.py:284-301"
    },
    "759": {
        "file_id": 51,
        "content": "This code is merging and updating text detection results based on similarity and temporal proximity. It keeps track of the last detected word, its location, and timestamp, updating or creating new result entries accordingly.",
        "type": "comment"
    },
    "760": {
        "file_id": 51,
        "content": "                    lastTimeStamp[\"end\"] = melem[\"timespan\"][\"end\"]\n            # else:\n            #     print(\"found different type:\",mtype)\n            #     print(\"text:\",mtext)\n            #     print(\"element:\",melem)\n    myNewResult.update({lastId:{\"content\":lastWordResult,\"timespan\":lastTimeStamp,\"location\":lastLocation}})\n    print(\"process complete\")\n    return myNewResult",
        "type": "code",
        "location": "/pyjom/medialang/functions/detectors/entityDetector.py:302-309"
    },
    "761": {
        "file_id": 51,
        "content": "Checking if the element matches the expected format, if not print relevant details and proceed with updating the result dictionary. Finally, print \"process complete\" and return the result dictionary.",
        "type": "comment"
    },
    "762": {
        "file_id": 52,
        "content": "/pyjom/medialang/functions/detectors/blackoutDetector.py",
        "type": "filepath"
    },
    "763": {
        "file_id": 52,
        "content": "This code detects blackouts in media by calculating scores per frame block and storing results. It works for videos and images, using OpenCV or videoFrameIterator. The code updates metadata dictionaries and appends updated results to a list before returning the final list of results.",
        "type": "summary"
    },
    "764": {
        "file_id": 52,
        "content": "from .mediaDetector import *\ndef blackoutIdentifier(frame_a, cut=3, threshold=30, method=\"average\"):\n    assert cut >= 1\n    methods = {\"average\": np.average, \"max\": np.max, \"min\": np.min}\n    mshape = frame_a.shape\n    width, height = mshape[:2]\n    mcut = int(min(width, height) / cut)\n    if len(mshape) == 3:\n        result = methods[method](result, axis=2)\n        shape0 = int(width / mcut)\n    shape1 = int(height / mcut)\n    diff = np.zeros((shape0, shape1)).tolist()\n    # mapping = {}\n    for x in range(shape0):\n        for y in range(shape1):\n            area = result[x * mcut : (x + 1) * mcut, y * mcut : (y + 1) * mcut]\n            score = (area < threshold).astype(int)\n            diff[x][y] = score.sum() / score.size\n    return {\n        \"blackout\": diff,\n        \"blocksize\": mcut,\n    }  # required for recovering center points.\ndef blackoutDetector(mediapaths, cut=3, threshold=30, method=\"average\", timestep=0.2):\n    # any better detectors? deeplearning?\n    results = []\n    data_key = \"blackout_score\"",
        "type": "code",
        "location": "/pyjom/medialang/functions/detectors/blackoutDetector.py:1-30"
    },
    "765": {
        "file_id": 52,
        "content": "blackoutIdentifier function: Calculates blackout score per block of a frame using average, max, or min method; accepts cut, threshold, and method parameters.\n\nblackoutDetector function: Detects blackouts across multiple media paths by calling blackoutIdentifier; stores results in 'data_key' for further use.",
        "type": "comment"
    },
    "766": {
        "file_id": 52,
        "content": "    for mediapath in mediapaths:\n        print(\"mediapath:\", mediapath)\n        mediatype = getFileType(mediapath)\n        print(\"subtitle of mediatype:\", mediatype)\n        assert mediatype in [\"video\", \"image\"]  # gif? anything like that?\n        result = {\"type\": mediatype, data_key: {}}\n        config = {\"cut\": cut, \"threshold\": threshold, \"method\": method}\n        if mediatype == \"image\":\n            data = cv2.imread(mediapath)\n            data = keywordDecorator(blackoutIdentifier, **config)(data)\n            result[data_key].update({\"blackout_detector\": data})\n            result[data_key].update({\"config\": config})\n        else:\n            keyword = \"blackout_detector\"\n            mdata, metadata = videoFrameIterator(\n                mediapath,\n                data_producer=keywordDecorator(blackoutIdentifier, **config),\n                framebatch=1,\n                timestep=timestep,\n                keyword=keyword,\n            )\n            metadata.update({\"config\": config})\n            result[data_key][keyword] = mdata",
        "type": "code",
        "location": "/pyjom/medialang/functions/detectors/blackoutDetector.py:31-54"
    },
    "767": {
        "file_id": 52,
        "content": "This code iterates over mediapaths, detects media type (video or image), applies blackout detection configuration, and stores the result in a dictionary. For images, it uses OpenCV to read image data and apply keyword decorator for blackout identification. For videos, it uses videoFrameIterator with keyword decorator as data producer for frame-by-frame blackout detection.",
        "type": "comment"
    },
    "768": {
        "file_id": 52,
        "content": "            result[data_key].update(metadata)\n        results.append(result)\n    return results",
        "type": "code",
        "location": "/pyjom/medialang/functions/detectors/blackoutDetector.py:55-57"
    },
    "769": {
        "file_id": 52,
        "content": "This code block takes a metadata dictionary and updates it to an existing data key in the result dictionary. After updating, it appends the updated result dictionary to the results list and returns the final list of results.",
        "type": "comment"
    },
    "770": {
        "file_id": 53,
        "content": "/pyjom/medialang/functions/detectors/__init__.py",
        "type": "filepath"
    },
    "771": {
        "file_id": 53,
        "content": "This code imports various detector functions, defines a medialang input function, and creates a dictionary of detectors for processing media data. It handles potential problematic inputs.",
        "type": "summary"
    },
    "772": {
        "file_id": 53,
        "content": "# from pyjom.medialang.functions.detectors.mediaDetector import *\nfrom .blackoutDetector import *\nfrom .subtitleDetector import *\nfrom .videoDiffDetector import *\nfrom .yolov5_Detector import *\nfrom .frameborder_Detector import *\n# maybe these shits are gonna ruin my life...\ndef getMedialangInputFixed(medialangPathsInput):\n    for fbase0 in medialangPathsInput:\n        if type(fbase0) == str:\n            yield fbase0\n        elif (\n            type(fbase0) == list\n            and len(fbase0) == 1\n            and type(fbase0[0] == dict)\n            and \"cache\" in fbase0[0].keys()\n        ):\n            yield fbase0[0][\"cache\"]\n        else:\n            print(\"weird medialang detector input\")\n            print(fbase0)\n        # then it must be the medialang shit.\ndef processInputWrapperFunction(function, wrapperFunction):\n    def mFunction(data, *args, **kwargs):\n        return function(wrapperFunction(data), *args, **kwargs)\n    return mFunction\nmedialangDetectors = {\n    \"subtitle_detector\": mediaSubtitleDetector,",
        "type": "code",
        "location": "/pyjom/medialang/functions/detectors/__init__.py:1-36"
    },
    "773": {
        "file_id": 53,
        "content": "This code imports various detector functions, defines a function for getting medialang input, and creates a dictionary of detectors. It seems to be part of a larger program used for processing media data. The comment on line 30 suggests that the code is concerned with potentially difficult or problematic inputs.",
        "type": "comment"
    },
    "774": {
        "file_id": 53,
        "content": "    \"framediff_detector\": videoDiffDetector,\n    \"blackout_detector\": blackoutDetector,\n    \"yolov5_detector\": yolov5_Detector,\n    \"frameborder_detector\": frameborder_Detector,\n}\nmedialangDetectors = { # strange. i don't feel it.\n    key: processInputWrapperFunction(medialangDetectors[key], getMedialangInputFixed)\n    for key in medialangDetectors.keys()\n}",
        "type": "code",
        "location": "/pyjom/medialang/functions/detectors/__init__.py:37-46"
    },
    "775": {
        "file_id": 53,
        "content": "This code initializes and processes media language detectors with input wrappers. It maps detector names to corresponding functions and applies a processing function to each detector for handling inputs.",
        "type": "comment"
    },
    "776": {
        "file_id": 54,
        "content": "/pyjom/platforms/bilibili/utils.py",
        "type": "filepath"
    },
    "777": {
        "file_id": 54,
        "content": "The code uses functions for API synchronization, error handling, and list conversions to parse BGM information and durations. It also defines cleaning functions like `clearHtmlTags` and `detectAuthorRelatedKeywords` to remove unwanted characters and author-related keywords from video titles and tags.",
        "type": "summary"
    },
    "778": {
        "file_id": 54,
        "content": "import types\nfrom bilibili_api import sync\n# import json\nfrom bs4 import BeautifulSoup\nfrom lazero.utils.logger import sprint\n# wtf is async generator type?\ndef bilibiliSync(func):\n    def wrapper(*args, **kwargs):\n        coroutineMaybe = func(*args, **kwargs)\n        if type(coroutineMaybe) == types.CoroutineType:\n            return sync(coroutineMaybe)\n        else:\n            return coroutineMaybe\n    return wrapper\n######## import all below functions to searchDataParser.\n# from pyjom.platforms.bilibili.utils import generatorToList, linkFixer,traceError, extractLinks,videoDurationStringToSeconds,getAuthorKeywords,clearHtmlTags,splitTitleTags,removeAuthorRelatedTags\ndef generatorToList(generator):\n    return [x for x in generator]\ndef linkFixer(link, prefix=\"http:\"):\n    if link.startswith(\"//\"):\n        return prefix + link\n    return link\ndef traceError(errorMsg: str = \"error!\", _breakpoint: bool = False):\n    import traceback\n    traceback.print_exc()\n    sprint(errorMsg)\n    if _breakpoint:\n        return breakpoint()",
        "type": "code",
        "location": "/pyjom/platforms/bilibili/utils.py:1-41"
    },
    "779": {
        "file_id": 54,
        "content": "This code is related to the bilibili platform and contains functions for synchronizing API calls, converting generator to list, fixing links, handling errors with traceback, and more. It's likely part of a larger project focused on working with the bilibili API.",
        "type": "comment"
    },
    "780": {
        "file_id": 54,
        "content": "def extractLinks(description, extract_bgm=True):\n    \"\"\"Extract and remove links in description\"\"\"\n    import re\n    # notice, we don't need to go wild here. we just want the title and the cover, and the tags.\n    expression = r\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\"\n    # expr = re.compile(expression)\n    links = re.findall(expression, description)\n    # if links == None:\n    #     links = []\n    desc_without_link = re.sub(expression, \"\", description)\n    desc_without_link_per_line = [\n        x.replace(\"\\n\", \"\").strip() for x in desc_without_link.split(\"\\n\")\n    ]\n    desc_without_link_per_line = [x for x in desc_without_link_per_line if len(x) > 0]\n    bgms = []\n    final_desc_list = []\n    if not extract_bgm:\n        final_desc_list = desc_without_link_per_line\n    else:\n        for line in desc_without_link_per_line:\n            bgmCandidateTemplates = [\"{}\", \"{}:\", \"{} \"]\n            fixers = [x.format(\"\") for x in bgmCandidateTemplates]\n            bgmCandidates = [x.format(\"bgm\") + \"(.+)\" for x in bgmCandidateTemplates]",
        "type": "code",
        "location": "/pyjom/platforms/bilibili/utils.py:44-67"
    },
    "781": {
        "file_id": 54,
        "content": "The code defines a function `extractLinks` that takes in a description and optionally extracts background music (BGM) links. It uses regular expressions to find and remove links from the description, then splits the description into lines. If BGM extraction is enabled, it searches for BGM candidates using templates and formats them correctly. The final result is a list of non-empty lines without links or potential BGM information if BGM extraction is disabled.",
        "type": "comment"
    },
    "782": {
        "file_id": 54,
        "content": "            has_bgm = False\n            for candidate in bgmCandidates:\n                bgm_parse_result = re.findall(candidate, line.lower())\n                if len(bgm_parse_result) > 0:\n                    has_bgm = True\n                    # bgm = line[len(bgmCandidates) :]\n                    bgm = bgm_parse_result[0]\n                    bgm = bgm.strip()\n                    for fixer in fixers:\n                        bgm = bgm.strip(fixer)\n                    if len(bgm) > 0:\n                        bgms.append(bgm)\n                    break\n            if not has_bgm:\n                final_desc_list.append(line)\n    desc_without_link = \"\\n\".join(final_desc_list)\n    return links, bgms, desc_without_link\nfrom typing import Literal\nimport re\nfrom typing import Union\ndef videoDurationStringToSeconds(\n    durationString:Union[str, None], method: Literal[\"vtc\", \"basic\"] = \"vtc\"\n):\n    if durationString in [\"-\", None]:\n        return None\n    if type(durationString) != str:\n        return None\n    if re.findall(r\"\\d\", durationString) == []:",
        "type": "code",
        "location": "/pyjom/platforms/bilibili/utils.py:68-99"
    },
    "783": {
        "file_id": 54,
        "content": "The code is parsing a line for background music (BGM) information using regular expressions. If BGM is found, it appends to bgms list; if not, the line is added to final_desc_list. The function videoDurationStringToSeconds converts video duration string to seconds based on given method (vtc or basic). It checks for invalid input (empty string or None) and returns None in those cases.",
        "type": "comment"
    },
    "784": {
        "file_id": 54,
        "content": "        return None\n    try:\n        if method == \"vtc\":\n            import vtc\n            timecode = \"{}:0\".format(durationString)\n            decimal_seconds = vtc.Timecode(timecode, rate=1).seconds\n            seconds = round(decimal_seconds)\n            return seconds\n        elif method == \"basic\":\n            if type(durationString) == int:\n                return durationString  # not string at all.\n            if type(durationString) != str:\n                print(\"unknown durationString type: %s\" % type(durationString))\n                return None\n            durationString = durationString.strip()\n            mList = durationString.split(\":\")[::-1]\n            if len(mList) > 3:\n                print(\"DURATION STRING TOO LONG\")\n                return None\n            seconds = 0\n            for index, elem in enumerate(mList):\n                elem = int(elem)\n                seconds += (60**index) * elem\n            return seconds\n        else:\n            raise Exception(\"method %s does not exist\" % method)",
        "type": "code",
        "location": "/pyjom/platforms/bilibili/utils.py:100-126"
    },
    "785": {
        "file_id": 54,
        "content": "This function converts a duration string, either in basic format or \"vtc\" method, into seconds. It checks the input type and handles invalid formats, returning None for errors or the converted duration in seconds if successful.",
        "type": "comment"
    },
    "786": {
        "file_id": 54,
        "content": "    except:\n        import traceback\n        traceback.print_exc()\n        print(\"exception durion video duration string conversion\")\ndef clearHtmlTags(htmlObject):\n    a = BeautifulSoup(htmlObject, features=\"lxml\")\n    return a.text\ndef detectAuthorRelatedKeywords(title_tag, author_keywords):\n    abandon = False\n    for keyword in author_keywords:\n        if len(keyword) > 1:\n            if keyword in title_tag:\n                abandon = True  # detected this thing.\n                break\n    return abandon\ndef getAuthorKeywords(author):\n    author = author.strip()\n    import jieba\n    author_keywords = jieba.lcut(author)\n    author_keywords = [x.strip() for x in author_keywords]\n    author_keywords = [x for x in author_keywords if len(x) > 0]\n    return author_keywords\ndef removeAuthorRelatedTags(description_or_title, author):\n    templates = [\"{}\", \"@{}\", \"{}\"]\n    tags = [template.format(author) for template in templates]\n    for tag in tags:\n        description_or_title = description_or_title.replace(tag, \"\")",
        "type": "code",
        "location": "/pyjom/platforms/bilibili/utils.py:127-163"
    },
    "787": {
        "file_id": 54,
        "content": "The code defines several functions:\n1. `clearHtmlTags` removes HTML tags from an object using BeautifulSoup and returns the text content.\n2. `detectAuthorRelatedKeywords` checks if a given title contains any keywords from a list of author-related keywords, returning True if detected.\n3. `getAuthorKeywords` takes an author's name, tokenizes it with Jieba, filters out empty strings and returns a list of non-empty tokens.\n4. `removeAuthorRelatedTags` replaces specific author-related tags in the description or title with an empty string.",
        "type": "comment"
    },
    "788": {
        "file_id": 54,
        "content": "    return description_or_title\ndef splitTitleTags(title, author_keywords):\n    import re\n    pattern = r\".+\"\n    title_tags = re.findall(pattern, title)\n    title = re.sub(pattern, \"\", title)\n    title_tags = [x.lstrip(\"\").rstrip(\"\").strip() for x in title_tags]\n    title_tags = [x for x in title_tags if len(x) > 0]\n    final_title_tags = []\n    for title_tag in title_tags:\n        detected = detectAuthorRelatedKeywords(title_tag, author_keywords)\n        if not detected:\n            final_title_tags.append(title_tag)\n    return title, title_tags",
        "type": "code",
        "location": "/pyjom/platforms/bilibili/utils.py:164-181"
    },
    "789": {
        "file_id": 54,
        "content": "This function splits the title and tags in a video using regular expressions, removes unnecessary characters, filters out empty strings, and detects author-related keywords. It returns the cleaned title and a list of remaining title tags.",
        "type": "comment"
    },
    "790": {
        "file_id": 55,
        "content": "/pyjom/platforms/bilibili/database.py",
        "type": "filepath"
    },
    "791": {
        "file_id": 55,
        "content": "The code is for a video recommendation program that uses text processing, SQLite database, BM25 algorithm, and hybrid search algorithms to retrieve Bilibili user videos, handle pagination, and update databases. It defines functions for video searching and refreshing status, interacts with a database, initializes a scheduler, creates tables, sets up FastAPI application, contains video search endpoints, retrieves video info from generators, schedules tasks, handles forms, registers videos on Bilibili platform, uses Uvicorn server, and runs bilibiliRecommendationServer function.",
        "type": "summary"
    },
    "792": {
        "file_id": 55,
        "content": "from lazero.utils.json import jsonify\n# ellipsis = type(...)\n# serve my video, serve my cat video, dog video, set priority, serve others video\n# by means of query? or just directly ask me for it.\n# you'd better mimic the video that you have never recommend, and these audience have never seen before.\nimport time\n# utils.\ndef default(value, default_, isInstance=lambda v: v in [..., None]):\n    if isInstance(value):\n        return default_\n    return value\nimport datetime\nfrom typing import Union, Literal\nfrom functools import lru_cache\nimport random\n# you might want to add this to bilibili platform api, if there's no use of pyjom.commons\nfrom pyjom.platforms.bilibili.credentials import getCredentialByDedeUserId\nfrom pyjom.platforms.bilibili.utils import (\n    linkFixer,\n    videoDurationStringToSeconds,\n    clearHtmlTags,\n)\nfrom lazero.search.preprocessing import getFourVersionsOfProcessedLine\nimport jieba\nimport opencc\nimport jieba.analyse as ana\nimport progressbar\nimport pydantic\n@lru_cache(maxsize=4)\ndef getOpenCCConverter(converter_type: str = \"t2s\"):",
        "type": "code",
        "location": "/pyjom/platforms/bilibili/database.py:1-43"
    },
    "793": {
        "file_id": 55,
        "content": "This code appears to be a part of a larger program that deals with video recommendations and retrieval on the bilibili platform. It imports various libraries and functions, such as jieba for text processing, opencc for Chinese-to-simplified Chinese conversion, and pydantic for data validation. The getOpenCCConverter function is a memoized function that converts text using OpenCC's \"t2s\" type.",
        "type": "comment"
    },
    "794": {
        "file_id": 55,
        "content": "    converter = opencc.OpenCC(converter_type)\n    return converter\ndef isChineseCharacter(char):\n    assert len(char) == 1\n    return char >= \"\\u4e00\" and char <= \"\\u9fff\"\ndef containChineseCharacters(text):\n    for char in text:\n        if isChineseCharacter(char):\n            return True\n    return False\nfrom lazero.utils.mathlib import extract_span\ndef textPreprocessing(text):\n    converter = getOpenCCConverter()\n    text = converter.convert(text)\n    (\n        final_line,\n        final_cutted_line,\n        final_stemmed_line,\n        final_cutted_stemmed_line,\n    ) = getFourVersionsOfProcessedLine(text)\n    # breakpoint()\n    wordlist = jieba.lcut(final_cutted_line)\n    final_wordlist = []\n    for w in wordlist:\n        word = w.strip()\n        if len(word) > 0:\n            final_wordlist.append(word)\n    flags = [int(containChineseCharacters(word)) for word in final_wordlist]\n    chineseSpans = extract_span(flags, target=1)\n    nonChineseSpans = extract_span(flags, target=0)\n    finalSpans = [(span, True) for span in chineseSpans] + [",
        "type": "code",
        "location": "/pyjom/platforms/bilibili/database.py:44-82"
    },
    "795": {
        "file_id": 55,
        "content": "This code snippet performs text preprocessing on a given text. It uses the OpenCC library to convert the text to simplified Chinese if needed, and then applies Jieba's jieba.lcut method for word segmentation. The resulting words are checked for their presence in the Chinese character range (u4e00-u9fff). Flags are created based on whether each word contains a Chinese character or not. The extract_span function is then used to identify spans of consecutive words with similar flags, which are assumed to be either Chinese or non-Chinese text.",
        "type": "comment"
    },
    "796": {
        "file_id": 55,
        "content": "        (span, False) for span in nonChineseSpans\n    ]\n    finalSpans.sort(key=lambda span: span[0])\n    finalWordList = []\n    for span, isChineseSpan in finalSpans:\n        subWordList = final_wordlist[span[0] : span[1]]\n        subChars = \"\".join(subWordList)\n        subCharList = [c for c in subChars]  #  \n        if isChineseSpan:\n            subWordList = jieba.lcut_for_search(subChars)\n        finalWordList.extend(subWordList)\n        finalWordList.extend(subCharList)\n    return \" \".join(finalWordList)\nfrom nltk.corpus import stopwords\n@lru_cache(maxsize=1)\ndef getStopwords(languages: tuple = (\"chinese\", \"english\")):\n    stopword_list = []\n    for lang in languages:\n        stopword_list.extend(stopwords.words(lang))\n    return stopword_list\ndef keywordExtracting(\n    text,\n    method: Literal[\"tfidf\", \"random\"] = \"tfidf\",\n    languages: tuple = (\"chinese\", \"english\"),\n    topK: int = 5,\n):\n    # remove all stopwords.\n    keyword_list = textPreprocessing(text).split(\" \")\n    stopword_list = getStopwords(languages=languages)",
        "type": "code",
        "location": "/pyjom/platforms/bilibili/database.py:83-117"
    },
    "797": {
        "file_id": 55,
        "content": "This code performs text preprocessing and keyword extraction. It removes stopwords, utilizes a custom function for Chinese word segmentation, and implements TF-IDF or random selection of top keywords from the preprocessed text. It also stores commonly used stopwords in memory using LRU cache.",
        "type": "comment"
    },
    "798": {
        "file_id": 55,
        "content": "    results = []\n    for k in keyword_list:\n        if k.lower() not in stopword_list:\n            results.append(k)\n    if method == \"random\":\n        random.shuffle(results)\n        return results[:topK]\n    elif method == \"tfidf\":\n        myText = \" \".join(results)\n        tags = ana.extract_tags(myText, topK=topK)\n        return tags\n    else:\n        raise Exception(\"Unknown keyword extraction method: %s\" % method)\n################################BILIBILI QUERY DATA MODELS######################\n# @reloading\nclass queryForm(pydantic.BaseModel):\n    query: str  # required?\n    page_size: Union[int, None] = None\n    page_num: int = 1\n    query_for_search_cached: Union[str, None] = None\n    # you are going to inherit this.\n    @property\n    def query_for_search(\n        self,\n    ):  # make sure the preprocessing is only called once. really?\n        if self.query_for_search_cached is None:\n            query = self.query\n            self.query_for_search_cached = textPreprocessing(query)\n        return self.query_for_search_cached",
        "type": "code",
        "location": "/pyjom/platforms/bilibili/database.py:118-151"
    },
    "799": {
        "file_id": 55,
        "content": "This code performs keyword extraction based on the provided method. It filters out stopwords, shuffles results if using the \"random\" method, and uses TF-IDF for the \"tfidf\" method. The `queryForm` class is a Pydantic model for BiliBili query data models with properties for query, page size, page number, and cached preprocessed search query.",
        "type": "comment"
    }
}