{
    "3800": {
        "file_id": 464,
        "content": "        this.parseRGBA();\n    }\n    parseHeader() {\n        this.fileSize = this.readUInt32LE();\n        this.reserved1 = this.buffer.readUInt16LE(this.pos);\n        this.pos += 2;\n        this.reserved2 = this.buffer.readUInt16LE(this.pos);\n        this.pos += 2;\n        this.offset = this.readUInt32LE();\n        // End of BITMAP_FILE_HEADER\n        this.headerSize = this.readUInt32LE();\n        if (!(this.headerSize in HeaderTypes)) {\n            throw new Error(`Unsupported BMP header size ${this.headerSize}`);\n        }\n        this.width = this.readUInt32LE();\n        this.height = this.readUInt32LE();\n        this.planes = this.buffer.readUInt16LE(this.pos);\n        this.pos += 2;\n        this.bitPP = this.buffer.readUInt16LE(this.pos);\n        this.pos += 2;\n        this.compression = this.readUInt32LE();\n        this.rawSize = this.readUInt32LE();\n        this.hr = this.readUInt32LE();\n        this.vr = this.readUInt32LE();\n        this.colors = this.readUInt32LE();\n        this.importantColors = this.readUInt32LE();",
        "type": "code",
        "location": "/tests/nsfw_violence_drug_detection/nsfwjs_test.js:49-74"
    },
    "3801": {
        "file_id": 464,
        "content": "The code reads and parses the BMP file header information. It begins by reading and setting values for file size, reserved bytes, and offset. Then it checks the header size to ensure compatibility before proceeding to read and set values for width, height, planes, bits per pixel, compression type, raw data size, and color depth.",
        "type": "comment"
    },
    "3802": {
        "file_id": 464,
        "content": "        // De facto defaults\n        if (this.bitPP === 32) {\n            this.maskAlpha = 0;\n            this.maskRed = 0x00ff0000;\n            this.maskGreen = 0x0000ff00;\n            this.maskBlue = 0x000000ff;\n        } else if (this.bitPP === 16) {\n            this.maskAlpha = 0;\n            this.maskRed = 0x7c00;\n            this.maskGreen = 0x03e0;\n            this.maskBlue = 0x001f;\n        }\n        // End of BITMAP_INFO_HEADER\n        if (this.headerSize > HeaderTypes.BITMAP_INFO_HEADER ||\n            this.compression === 3 /* BI_BIT_FIELDS */ ||\n            this.compression === 6 /* BI_ALPHA_BIT_FIELDS */ ) {\n            this.maskRed = this.readUInt32LE();\n            this.maskGreen = this.readUInt32LE();\n            this.maskBlue = this.readUInt32LE();\n        }\n        // End of BITMAP_V2_INFO_HEADER\n        if (this.headerSize > HeaderTypes.BITMAP_V2_INFO_HEADER ||\n            this.compression === 6 /* BI_ALPHA_BIT_FIELDS */ ) {\n            this.maskAlpha = this.readUInt32LE();\n        }\n        // End of BITMAP_V3_INFO_HEADER",
        "type": "code",
        "location": "/tests/nsfw_violence_drug_detection/nsfwjs_test.js:75-100"
    },
    "3803": {
        "file_id": 464,
        "content": "The code checks the bitPP value and sets default mask values accordingly. It then verifies if the headerSize exceeds specific limits or if the compression type is BI_BIT_FIELDS or BI_ALPHA_BIT_FIELDS, in which case it reads and assigns mask values. This code handles different header types and compression types to set appropriate mask values for image processing.",
        "type": "comment"
    },
    "3804": {
        "file_id": 464,
        "content": "        if (this.headerSize > HeaderTypes.BITMAP_V3_INFO_HEADER) {\n            this.pos +=\n                HeaderTypes.BITMAP_V4_HEADER - HeaderTypes.BITMAP_V3_INFO_HEADER;\n        }\n        // End of BITMAP_V4_HEADER\n        if (this.headerSize > HeaderTypes.BITMAP_V4_HEADER) {\n            this.pos += HeaderTypes.BITMAP_V5_HEADER - HeaderTypes.BITMAP_V4_HEADER;\n        }\n        // End of BITMAP_V5_HEADER\n        if (this.bitPP <= 8 || this.colors > 0) {\n            const len = this.colors === 0 ? 1 << this.bitPP : this.colors;\n            this.palette = new Array(len);\n            for (let i = 0; i < len; i++) {\n                const blue = this.buffer.readUInt8(this.pos++);\n                const green = this.buffer.readUInt8(this.pos++);\n                const red = this.buffer.readUInt8(this.pos++);\n                const quad = this.buffer.readUInt8(this.pos++);\n                this.palette[i] = {\n                    red,\n                    green,\n                    blue,\n                    quad\n                };",
        "type": "code",
        "location": "/tests/nsfw_violence_drug_detection/nsfwjs_test.js:101-123"
    },
    "3805": {
        "file_id": 464,
        "content": "This code handles different header types in a file format. It checks the header size and adjusts the position accordingly for BITMAP_V4_HEADER and BITMAP_V5_HEADER. If bitPP is less than or equal to 8 or colors are 0, it creates a palette array by reading RGB values and quad value from the buffer.",
        "type": "comment"
    },
    "3806": {
        "file_id": 464,
        "content": "            }\n        }\n        // End of color table\n        // Can the height ever be negative?\n        if (this.height < 0) {\n            this.height *= -1;\n            this.bottomUp = false;\n        }\n        const coloShift = maskColor(this.maskRed, this.maskGreen, this.maskBlue, this.maskAlpha);\n        this.shiftRed = coloShift.shiftRed;\n        this.shiftGreen = coloShift.shiftGreen;\n        this.shiftBlue = coloShift.shiftBlue;\n        this.shiftAlpha = coloShift.shiftAlpha;\n    }\n    parseRGBA() {\n        this.data = Buffer.alloc(this.width * this.height * 4);\n        switch (this.bitPP) {\n            case 1:\n                this.bit1();\n                break;\n            case 4:\n                this.bit4();\n                break;\n            case 8:\n                this.bit8();\n                break;\n            case 16:\n                this.bit16();\n                break;\n            case 24:\n                this.bit24();\n                break;\n            default:\n                this.bit32();\n        }\n    }",
        "type": "code",
        "location": "/tests/nsfw_violence_drug_detection/nsfwjs_test.js:124-159"
    },
    "3807": {
        "file_id": 464,
        "content": "The code initializes variables based on the given color table, checks if the height is negative and adjusts accordingly. It then calculates RGBA shift values for subsequent image parsing based on the bit-per-pixel value provided.",
        "type": "comment"
    },
    "3808": {
        "file_id": 464,
        "content": "    bit1() {\n        const xLen = Math.ceil(this.width / 8);\n        const mode = xLen % 4;\n        const padding = mode !== 0 ? 4 - mode : 0;\n        let lastLine;\n        this.scanImage(padding, xLen, (x, line) => {\n            if (line !== lastLine) {\n                lastLine = line;\n            }\n            const b = this.buffer.readUInt8(this.pos++);\n            const location = line * this.width * 4 + x * 8 * 4;\n            for (let i = 0; i < 8; i++) {\n                if (x * 8 + i < this.width) {\n                    const rgb = this.palette[(b >> (7 - i)) & 0x1];\n                    this.data[location + i * this.locAlpha] = 0;\n                    this.data[location + i * 4 + this.locBlue] = rgb.blue;\n                    this.data[location + i * 4 + this.locGreen] = rgb.green;\n                    this.data[location + i * 4 + this.locRed] = rgb.red;\n                } else {\n                    break;\n                }\n            }\n        });\n    }\n    bit4() {\n        if (this.compression === 2 /* BI_RLE4 */ ) {",
        "type": "code",
        "location": "/tests/nsfw_violence_drug_detection/nsfwjs_test.js:160-185"
    },
    "3809": {
        "file_id": 464,
        "content": "This function reads the image data in 8-bit chunks (bit1) and converts it into RGBA format. The width of the image is divided into segments of 8 bits, and based on the mode (remainder when width is divided by 4), padding is applied. The scanImage method reads the bits in lines and processes each bit using a for loop to extract red, green, and blue values from the palette and assign them to their respective locations in the data array. If the compression type is BI_RLE4 (bit4 function), it indicates that the image uses RLE4 compression.",
        "type": "comment"
    },
    "3810": {
        "file_id": 464,
        "content": "            this.data.fill(0);\n            let lowNibble = false; //for all count of pixel\n            let lines = this.bottomUp ? this.height - 1 : 0;\n            let location = 0;\n            while (location < this.data.length) {\n                const a = this.buffer.readUInt8(this.pos++);\n                const b = this.buffer.readUInt8(this.pos++);\n                //absolute mode\n                if (a === 0) {\n                    if (b === 0) {\n                        //line end\n                        lines += this.bottomUp ? -1 : 1;\n                        location = lines * this.width * 4;\n                        lowNibble = false;\n                        continue;\n                    }\n                    if (b === 1) {\n                        // image end\n                        break;\n                    }\n                    if (b === 2) {\n                        // offset x, y\n                        const x = this.buffer.readUInt8(this.pos++);\n                        const y = this.buffer.readUInt8(this.pos++);",
        "type": "code",
        "location": "/tests/nsfw_violence_drug_detection/nsfwjs_test.js:186-209"
    },
    "3811": {
        "file_id": 464,
        "content": "This code reads an image file's metadata and processes it. It initializes the data array with zeros, handles absolute mode for lines and pixel positions, checks for line end and image end conditions, and reads offset values for x and y coordinates.",
        "type": "comment"
    },
    "3812": {
        "file_id": 464,
        "content": "                        lines += this.bottomUp ? -y : y;\n                        location += y * this.width * 4 + x * 4;\n                    } else {\n                        let c = this.buffer.readUInt8(this.pos++);\n                        for (let i = 0; i < b; i++) {\n                            location = this.setPixelData(location, lowNibble ? c & 0x0f : (c & 0xf0) >> 4);\n                            if (i & 1 && i + 1 < b) {\n                                c = this.buffer.readUInt8(this.pos++);\n                            }\n                            lowNibble = !lowNibble;\n                        }\n                        if ((((b + 1) >> 1) & 1) === 1) {\n                            this.pos++;\n                        }\n                    }\n                } else {\n                    //encoded mode\n                    for (let i = 0; i < a; i++) {\n                        location = this.setPixelData(location, lowNibble ? b & 0x0f : (b & 0xf0) >> 4);\n                        lowNibble = !lowNibble;\n                    }",
        "type": "code",
        "location": "/tests/nsfw_violence_drug_detection/nsfwjs_test.js:210-230"
    },
    "3813": {
        "file_id": 464,
        "content": "This code handles image data encoding and decoding. It checks the mode (encoded or not) to determine how to process the pixel data. For unencoded mode, it calculates coordinates and updates location by reading bytes from the buffer. In encoded mode, it processes blocks of pixels using low nibble bit manipulation. The code also handles odd-sized blocks by incrementing the position in the buffer.",
        "type": "comment"
    },
    "3814": {
        "file_id": 464,
        "content": "                }\n            }\n        } else {\n            const xLen = Math.ceil(this.width / 2);\n            const mode = xLen % 4;\n            const padding = mode !== 0 ? 4 - mode : 0;\n            this.scanImage(padding, xLen, (x, line) => {\n                const b = this.buffer.readUInt8(this.pos++);\n                const location = line * this.width * 4 + x * 2 * 4;\n                const first4 = b >> 4;\n                let rgb = this.palette[first4];\n                this.data[location] = 0;\n                this.data[location + 1] = rgb.blue;\n                this.data[location + 2] = rgb.green;\n                this.data[location + 3] = rgb.red;\n                if (x * 2 + 1 >= this.width) {\n                    // throw new Error('Something');\n                    return false;\n                }\n                const last4 = b & 0x0f;\n                rgb = this.palette[last4];\n                this.data[location + 4] = 0;\n                this.data[location + 4 + 1] = rgb.blue;\n                this.data[location + 4 + 2] = rgb.green;",
        "type": "code",
        "location": "/tests/nsfw_violence_drug_detection/nsfwjs_test.js:231-254"
    },
    "3815": {
        "file_id": 464,
        "content": "This code initializes the image scanning process for a specific region, reading and setting pixel colors based on their respective nibbles. The padding is determined by the mode of the width divided by 2, and the image data is updated accordingly.",
        "type": "comment"
    },
    "3816": {
        "file_id": 464,
        "content": "                this.data[location + 4 + 3] = rgb.red;\n            });\n        }\n    }\n    bit8() {\n        if (this.compression === 1 /* BI_RLE8 */ ) {\n            this.data.fill(0);\n            let lines = this.bottomUp ? this.height - 1 : 0;\n            let location = 0;\n            while (location < this.data.length) {\n                const a = this.buffer.readUInt8(this.pos++);\n                const b = this.buffer.readUInt8(this.pos++);\n                //absolute mode\n                if (a === 0) {\n                    if (b === 0) {\n                        //line end\n                        lines += this.bottomUp ? -1 : 1;\n                        location = lines * this.width * 4;\n                        continue;\n                    }\n                    if (b === 1) {\n                        //image end\n                        break;\n                    }\n                    if (b === 2) {\n                        //offset x,y\n                        const x = this.buffer.readUInt8(this.pos++);\n                        const y = this.buffer.readUInt8(this.pos++);",
        "type": "code",
        "location": "/tests/nsfw_violence_drug_detection/nsfwjs_test.js:255-282"
    },
    "3817": {
        "file_id": 464,
        "content": "This code implements a RLE8 compression for image data. It iterates through the compressed data, decoding absolute mode values to populate an array with pixel values (RGB). When it encounters line end or image end markers, it adjusts the location accordingly.",
        "type": "comment"
    },
    "3818": {
        "file_id": 464,
        "content": "                        lines += this.bottomUp ? -y : y;\n                        location += y * this.width * 4 + x * 4;\n                    } else {\n                        for (let i = 0; i < b; i++) {\n                            const c = this.buffer.readUInt8(this.pos++);\n                            location = this.setPixelData(location, c);\n                        }\n                        // @ts-ignore\n                        const shouldIncrement = b & (1 === 1);\n                        if (shouldIncrement) {\n                            this.pos++;\n                        }\n                    }\n                } else {\n                    //encoded mode\n                    for (let i = 0; i < a; i++) {\n                        location = this.setPixelData(location, b);\n                    }\n                }\n            }\n        } else {\n            const mode = this.width % 4;\n            const padding = mode !== 0 ? 4 - mode : 0;\n            this.scanImage(padding, this.width, (x, line) => {\n                const b = this.buffer.readUInt8(this.pos++);",
        "type": "code",
        "location": "/tests/nsfw_violence_drug_detection/nsfwjs_test.js:283-307"
    },
    "3819": {
        "file_id": 464,
        "content": "This code processes image pixel data based on its mode and other conditions. It updates the location and buffer positions accordingly, applies specific functions to set pixel data depending on mode, and increments pos if necessary.",
        "type": "comment"
    },
    "3820": {
        "file_id": 464,
        "content": "                const location = line * this.width * 4 + x * 4;\n                if (b < this.palette.length) {\n                    const rgb = this.palette[b];\n                    this.data[location] = 0;\n                    this.data[location + 1] = rgb.blue;\n                    this.data[location + 2] = rgb.green;\n                    this.data[location + 3] = rgb.red;\n                } else {\n                    this.data[location] = 0;\n                    this.data[location + 1] = 0xff;\n                    this.data[location + 2] = 0xff;\n                    this.data[location + 3] = 0xff;\n                }\n            });\n        }\n    }\n    bit16() {\n        const padding = (this.width % 2) * 2;\n        this.scanImage(padding, this.width, (x, line) => {\n            const loc = line * this.width * 4 + x * 4;\n            const px = this.buffer.readUInt16LE(this.pos);\n            this.pos += 2;\n            this.data[loc + this.locRed] = this.shiftRed(px);\n            this.data[loc + this.locGreen] = this.shiftGreen(px);",
        "type": "code",
        "location": "/tests/nsfw_violence_drug_detection/nsfwjs_test.js:308-331"
    },
    "3821": {
        "file_id": 464,
        "content": "Calculates the pixel location in the image data based on x and y coordinates. If the pixel index is within the palette range, sets RGB values from the palette; otherwise, sets all values to 255. Utilizes bit16 method for processing pixels in groups of two.",
        "type": "comment"
    },
    "3822": {
        "file_id": 464,
        "content": "            this.data[loc + this.locBlue] = this.shiftBlue(px);\n            this.data[loc + this.locAlpha] = this.shiftAlpha(px);\n        });\n    }\n    bit24() {\n        const padding = this.width % 4;\n        this.scanImage(padding, this.width, (x, line) => {\n            const loc = line * this.width * 4 + x * 4;\n            const blue = this.buffer.readUInt8(this.pos++);\n            const green = this.buffer.readUInt8(this.pos++);\n            const red = this.buffer.readUInt8(this.pos++);\n            this.data[loc + this.locRed] = red;\n            this.data[loc + this.locGreen] = green;\n            this.data[loc + this.locBlue] = blue;\n            this.data[loc + this.locAlpha] = 0;\n        });\n    }\n    bit32() {\n        this.scanImage(0, this.width, (x, line) => {\n            const loc = line * this.width * 4 + x * 4;\n            const px = this.readUInt32LE();\n            this.data[loc + this.locRed] = this.shiftRed(px);\n            this.data[loc + this.locGreen] = this.shiftGreen(px);\n            this.data[loc + this.locBlue] = this.shiftBlue(px);",
        "type": "code",
        "location": "/tests/nsfw_violence_drug_detection/nsfwjs_test.js:332-355"
    },
    "3823": {
        "file_id": 464,
        "content": "This code snippet is part of an image processing library that supports various color formats (bit24, bit32). The bit24 function processes pixels in a 24-bit RGB format and sets the alpha channel to 0. The bit32 function reads pixel values in a 32-bit RGBA format and directly assigns the RGB values while setting the alpha channel to an undefined value. These functions iterate through each pixel of the image and store their respective color data in the \"data\" array.",
        "type": "comment"
    },
    "3824": {
        "file_id": 464,
        "content": "            this.data[loc + this.locAlpha] = this.shiftAlpha(px);\n        });\n    }\n    scanImage(padding = 0, width = this.width, processPixel) {\n        for (let y = this.height - 1; y >= 0; y--) {\n            const line = this.bottomUp ? y : this.height - 1 - y;\n            for (let x = 0; x < width; x++) {\n                const result = processPixel.call(this, x, line);\n                if (result === false) {\n                    return;\n                }\n            }\n            this.pos += padding;\n        }\n    }\n    readUInt32LE() {\n        const value = this.buffer.readUInt32LE(this.pos);\n        this.pos += 4;\n        return value;\n    }\n    setPixelData(location, rgbIndex) {\n        const { blue, green, red } = this.palette[rgbIndex];\n        this.data[location + this.locAlpha] = 0;\n        this.data[location + 1 + this.locBlue] = blue;\n        this.data[location + 2 + this.locGreen] = green;\n        this.data[location + 3 + this.locRed] = red;\n        return location + 4;\n    }\n}\nconst express = require('express')",
        "type": "code",
        "location": "/tests/nsfw_violence_drug_detection/nsfwjs_test.js:356-386"
    },
    "3825": {
        "file_id": 464,
        "content": "This code defines a class with methods for image processing, including scanning an image line by line and setting pixel data. The class uses a buffer to store data, and has properties such as locAlpha, locBlue, locGreen, and locRed for organizing the data in the buffer. It also utilizes the express module from the Node.js framework.",
        "type": "comment"
    },
    "3826": {
        "file_id": 464,
        "content": "const multer = require('multer')\nconst jpeg = require('jpeg-js')\n    // const bmp = require('bmp-js')\n    // const bmp = require('bmp-ts').default;\n    // const bmpBuffer = fs.readFileSync('bit24.bmp');\nconst { PNG } = require('pngjs')\nconst tf = require('@tensorflow/tfjs-node')\nconst nsfw = require('nsfwjs')\nconst app = express()\nconst upload = multer()\nlet _model\n// this even works for gif!\n// it will normalize and resize the image if needed.\n// shall we check for gif?\nconst convert = async(img, type) => {\n    // Decoded image in UInt8 Byte array\n    let image\n    if (type == 'image/jpeg') {\n        image = await jpeg.decode(img, true)\n            // RGBA\n    } //wtf?\n    // order: rgba\n    else if (type == 'image/png') {\n        image = PNG.sync.read(img)\n    } else if (type == 'image/bmp') {\n        // image = await bmp.decode(img, true)\n        image = new BmpDecoder(img, { toRGBA: true });\n    }\n    const numChannels = 3\n    const numPixels = image.width * image.height // will raise an error if image is not acquired.",
        "type": "code",
        "location": "/tests/nsfw_violence_drug_detection/nsfwjs_test.js:387-423"
    },
    "3827": {
        "file_id": 464,
        "content": "The code imports necessary libraries for image processing and model loading. It defines an asynchronous function \"convert\" to handle different image types (JPEG, PNG, BMP) and convert them into a standard RGBA format. The function reads the image using appropriate libraries based on its MIME type, determines the number of color channels and total pixels in the image.",
        "type": "comment"
    },
    "3828": {
        "file_id": 464,
        "content": "    const values = new Int32Array(numPixels * numChannels)\n        // are you sure about the width?\n    // can you make this faster? shit?\n    // this shit is no numpy. fuck.\n    for (let i = 0; i < numPixels; i++)\n        for (let c = 0; c < numChannels; ++c)\n        // if (type == 'bmp') {\n        //     // ABGR?\n        //     // values[i * numChannels + c] = image.data[i * 4+c]\n        //     values[i * numChannels + c] = image.data[i * 4 + 3 - c]\n        // } else {\n            values[i * numChannels + c] = image.data[i * 4 + c]\n            // }\n    return tf.tensor3d(values, [image.height, image.width, numChannels], 'int32')\n}\napp.get('/', async(req, res) => {\n    res.send('nsfw nodejs server')\n})\napp.post('/nsfw', upload.single('image'), async(req, res) => {\n    if (!req.file) res.status(400).send('Missing image multipart/form-data')\n    else {\n        try {\n            console.log('file uploaded:', req.file)\n            if (req.file.fieldname == 'image') {\n                type = req.file.mimetype // deal with it later.",
        "type": "code",
        "location": "/tests/nsfw_violence_drug_detection/nsfwjs_test.js:424-452"
    },
    "3829": {
        "file_id": 464,
        "content": "This code defines a function that converts an image into a TensorFlow tensor3d array. It takes the number of pixels and channels as inputs, iterates over each pixel and channel, and assigns values from the image data to the tensor3d array based on the image's format (BMP or other). The function returns the resulting tensor3d array. The app also has two routes: a GET route that returns \"nsfw nodejs server\" and a POST route for handling image uploads, which logs the file information if it is an image.",
        "type": "comment"
    },
    "3830": {
        "file_id": 464,
        "content": "                extension = req.file.originalname.split(\".\").slice(-1)[0].toLowerCase()\n                if (extension == 'gif' || type == 'image/gif') {\n                    let image = req.file.buffer\n                    let predictions = await _model.classifyGif(image, { topk: 3, fps: 1 })\n                        // image.dispose()\n                    predictions.message = 'success'\n                    res.json(predictions)\n                } else {\n                    if (extension == 'bmp') {\n                        type = 'image/bmp'\n                    }\n                    let image = await convert(req.file.buffer, type) // here we have buffer.\n                    let predictions = await _model.classify(image)\n                    predictions.message = 'success'\n                        // image.dispose()\n                    res.json(predictions)\n                }\n            }\n            // we need some file format hints.\n        } catch (e) {\n            console.log(e)\n            res.json({ message: 'error' })",
        "type": "code",
        "location": "/tests/nsfw_violence_drug_detection/nsfwjs_test.js:453-475"
    },
    "3831": {
        "file_id": 464,
        "content": "This code handles file uploads and classifies images based on their format. It checks if the image is a GIF, in which case it uses a specialized function for classification, otherwise it converts non-GIF images to a specified type and performs classification. The code catches any errors that occur during this process and sends an appropriate response.",
        "type": "comment"
    },
    "3832": {
        "file_id": 464,
        "content": "        }\n    }\n})\nconst load_model = async() => {\n    _model = await nsfw.load()\n}\n// Keep the model in memory, make sure it's loaded only once\nload_model().then(() => {\n    console.log('server ready')\n    app.listen(8511)\n})\n// curl --request POST localhost:8080/nsfw --header 'Content-Type: multipart/form-data' --data-binary 'image=@/full/path/to/picture.jpg'",
        "type": "code",
        "location": "/tests/nsfw_violence_drug_detection/nsfwjs_test.js:476-491"
    },
    "3833": {
        "file_id": 464,
        "content": "The code loads an NSFW content detection model and keeps it in memory. It ensures the model is loaded only once and starts the server on port 8511. When a POST request with an image is received, the model classifies the content as NSFW or not.",
        "type": "comment"
    },
    "3834": {
        "file_id": 465,
        "content": "/tests/nsfw_violence_drug_detection/nsfwjs_test.mjs",
        "type": "filepath"
    },
    "3835": {
        "file_id": 465,
        "content": "The code imports libraries, sets up an Express app and Multer for file uploads, converts images to RGBA format, initializes a model, calculates pixel counts using TensorFlow, handles requests, checks missing files, logs uploaded files, returns server status, loads NSFW image detection model, serves predictions on port 8511.",
        "type": "summary"
    },
    "3836": {
        "file_id": 465,
        "content": "import { createRequire } from \"module\";\nconst require = createRequire(import.meta.url);\nconst express = require('express')\nconst multer = require('multer')\nconst jpeg = require('jpeg-js')\n    // const bmp = require('bmp-js')\nconst bmp = require('bmp-ts');\n// const bmpBuffer = fs.readFileSync('bit24.bmp');\nconst { PNG } = require('pngjs')\nconst tf = require('@tensorflow/tfjs-node')\nconst nsfw = require('nsfwjs')\nconst app = express()\nconst upload = multer()\nlet _model\n// this even works for gif!\n// it will normalize and resize the image if needed.\n// shall we check for gif?\nconst convert = async(img, type) => {\n    // Decoded image in UInt8 Byte array\n    let image\n    if (type == 'image/jpeg') {\n        image = await jpeg.decode(img, true)\n            // RGBA\n    } //wtf?\n    // order: rgba\n    else if (type == 'image/png') {\n        image = PNG.sync.read(img)\n    } else if (type == 'image/bmp') {\n        // image = await bmp.decode(img, true)\n        image = bmp.decode(img, { toRGBA: true });\n    }\n    const numChannels = 3",
        "type": "code",
        "location": "/tests/nsfw_violence_drug_detection/nsfwjs_test.mjs:1-40"
    },
    "3837": {
        "file_id": 465,
        "content": "The code imports necessary libraries, sets up an express application and multer middleware for handling file uploads. It also defines a function to convert images of different types (JPEG, PNG, BMP) into the same RGBA format for further processing using TensorFlow.js. The code also initializes a model and mentions that it works for GIFs as well.",
        "type": "comment"
    },
    "3838": {
        "file_id": 465,
        "content": "    const numPixels = image.width * image.height // will raise an error if image is not acquired.\n    const values = new Int32Array(numPixels * numChannels)\n        // are you sure about the width?\n    // can you make this faster? shit?\n    // this shit is no numpy. fuck.\n    for (let i = 0; i < numPixels; i++)\n        for (let c = 0; c < numChannels; ++c)\n        // if (type == 'bmp') {\n        //     // ABGR?\n        //     // values[i * numChannels + c] = image.data[i * 4+c]\n        //     values[i * numChannels + c] = image.data[i * 4 + 3 - c]\n        // } else {\n            values[i * numChannels + c] = image.data[i * 4 + c]\n            // }\n    return tf.tensor3d(values, [image.height, image.width, numChannels], 'int32')\n}\napp.get('/', async(req, res) => {\n    res.send('nsfw nodejs server')\n})\napp.post('/nsfw', upload.single('image'), async(req, res) => {\n    if (!req.file) res.status(400).send('Missing image multipart/form-data')\n    else {\n        try {\n            console.log('file uploaded:', req.file)",
        "type": "code",
        "location": "/tests/nsfw_violence_drug_detection/nsfwjs_test.mjs:41-68"
    },
    "3839": {
        "file_id": 465,
        "content": "The code snippet is calculating the number of pixels in an image and storing its values into a TensorFlow tensor. It's then using the NodeJS framework to handle requests for images, checking if a file is missing, logging uploaded files, and returning a response with the server status. The code uses different data access methods based on the image format (BMP or others). However, there are concerns about potential width calculation errors, improving performance, and frustration with working outside of the Python ecosystem.",
        "type": "comment"
    },
    "3840": {
        "file_id": 465,
        "content": "            if (req.file.fieldname == 'image') {\n                type = req.file.mimetype // deal with it later.\n                extension = req.file.originalname.split(\".\").slice(-1)[0].toLowerCase()\n                if (extension == 'gif' || type == 'image/gif') {\n                    let image = req.file.buffer\n                    let predictions = await _model.classifyGif(image, { topk: 3, fps: 1 })\n                        // image.dispose()\n                    predictions.message = 'success'\n                    res.json(predictions)\n                } else {\n                    if (extension == 'bmp') {\n                        type = 'image/bmp'\n                    }\n                    let image = await convert(req.file.buffer, type) // here we have buffer.\n                    let predictions = await _model.classify(image)\n                    predictions.message = 'success'\n                        // image.dispose()\n                    res.json(predictions)\n                }\n            }\n            // we need some file format hints.",
        "type": "code",
        "location": "/tests/nsfw_violence_drug_detection/nsfwjs_test.mjs:69-89"
    },
    "3841": {
        "file_id": 465,
        "content": "Checks if the file field is 'image' and deals with GIF files separately by classifying them directly. For other formats, converts the buffer to the appropriate type before classification. Responds with predictions and success message.",
        "type": "comment"
    },
    "3842": {
        "file_id": 465,
        "content": "        } catch (e) {\n            console.log(e)\n            res.json({ message: 'error' })\n        }\n    }\n})\nconst load_model = async() => {\n    _model = await nsfw.load()\n}\n// Keep the model in memory, make sure it's loaded only once\nload_model().then(() => {\n    console.log('server ready')\n    app.listen(8511)\n})\n// curl --request POST localhost:8080/nsfw --header 'Content-Type: multipart/form-data' --data-binary 'image=@/full/path/to/picture.jpg'",
        "type": "code",
        "location": "/tests/nsfw_violence_drug_detection/nsfwjs_test.mjs:91-109"
    },
    "3843": {
        "file_id": 465,
        "content": "The code loads an NSFW image detection model, ensures it's only loaded once, and starts a server on port 8511. It accepts POST requests with image data from the client.",
        "type": "comment"
    },
    "3844": {
        "file_id": 466,
        "content": "/tests/nsfw_violence_drug_detection/porndetect_test.sh",
        "type": "filepath"
    },
    "3845": {
        "file_id": 466,
        "content": "This code is making multiple HTTP POST requests with different image files to a local server on port 8511, specifically targeting the \"/nsfw\" endpoint. The images being tested are JPEG and BMP formats, and the script is checking for errors or issues related to the BMP decoder in the response.",
        "type": "summary"
    },
    "3846": {
        "file_id": 466,
        "content": "curl -X POST -F 'image=@/root/Desktop/works/pyjom/samples/image/porn_shemale.jpeg' http://localhost:8511/nsfw\necho\necho\ncurl -X POST -F 'image=@/root/Desktop/works/pyjom/samples/image/porn_shemale.bmp' http://localhost:8511/nsfw\necho\necho\ncurl -X POST -F 'image=@/root/Desktop/works/pyjom/samples/image/dick.jpeg' http://localhost:8511/nsfw\necho\necho\ncurl -X POST -F 'image=@/root/Desktop/works/pyjom/samples/image/dick.bmp' http://localhost:8511/nsfw # something is wrong with bmp decoder.\necho\necho\necho '__________________________________________'\ncurl -X POST -F 'image=@/root/Desktop/works/pyjom/samples/image/dick2.jpeg' http://localhost:8511/nsfw\necho\necho\ncurl -X POST -F 'image=@/root/Desktop/works/pyjom/samples/image/dick3.jpeg' http://localhost:8511/nsfw\necho\necho\ncurl -X POST -F 'image=@/root/Desktop/works/pyjom/samples/image/dick4.jpeg' http://localhost:8511/nsfw\necho\necho",
        "type": "code",
        "location": "/tests/nsfw_violence_drug_detection/porndetect_test.sh:1-22"
    },
    "3847": {
        "file_id": 466,
        "content": "This code is making multiple HTTP POST requests with different image files to a local server on port 8511, specifically targeting the \"/nsfw\" endpoint. The images being tested are JPEG and BMP formats, and the script is checking for errors or issues related to the BMP decoder in the response.",
        "type": "comment"
    },
    "3848": {
        "file_id": 467,
        "content": "/tests/nsfw_violence_drug_detection/post_jpg_to_nsfwjs.sh",
        "type": "filepath"
    },
    "3849": {
        "file_id": 467,
        "content": "This code is making multiple POST requests to http://localhost:8511/nsfw, using different image formats and file paths. The purpose appears to be testing the API's response for NSFW content detection with various images. The last comment suggests that BMP format might have an issue, but consistency is maintained with the original model. A real adult content image is being tested next.",
        "type": "summary"
    },
    "3850": {
        "file_id": 467,
        "content": "curl http://localhost:8511/\necho\necho\ncurl -X POST -F 'image=@/root/Desktop/works/pyjom/samples/image/dog_saturday_night.jpg' http://localhost:8511/nsfw\necho\necho\ncurl -X POST -F 'image=@/root/Desktop/works/pyjom/samples/image/dog_saturday_night.bmp' http://localhost:8511/nsfw\necho\necho\ncurl -X POST -F 'image=@/root/Desktop/works/pyjom/samples/video/cute_cat_gif.gif' http://localhost:8511/nsfw\necho\necho\ncurl -X POST -F 'image=@/root/Desktop/works/pyjom/samples/image/dog_with_text.png' http://localhost:8511/nsfw\necho\necho\ncurl -X POST -F 'image=@/root/Desktop/works/pyjom/samples/image/dog_with_text.jpg' http://localhost:8511/nsfw\necho\necho\ncurl -X POST -F 'image=@/root/Desktop/works/pyjom/samples/image/dog_with_text.bmp' http://localhost:8511/nsfw\necho\necho\n# but the bmp looks right. is that the format issue?\n# we have consistency with the original model. how about a real porno?",
        "type": "code",
        "location": "/tests/nsfw_violence_drug_detection/post_jpg_to_nsfwjs.sh:1-23"
    },
    "3851": {
        "file_id": 467,
        "content": "This code is making multiple POST requests to http://localhost:8511/nsfw, using different image formats and file paths. The purpose appears to be testing the API's response for NSFW content detection with various images. The last comment suggests that BMP format might have an issue, but consistency is maintained with the original model. A real adult content image is being tested next.",
        "type": "comment"
    },
    "3852": {
        "file_id": 468,
        "content": "/tests/optical_flow/mmof_test/execute_me.py",
        "type": "filepath"
    },
    "3853": {
        "file_id": 468,
        "content": "This code initializes an MMFlow model and performs optical flow calculation on video frames, visualizing results and breaking the loop when \"q\" is pressed. It uses BGR to grayscale conversion and can perform Canny edge detection.",
        "type": "summary"
    },
    "3854": {
        "file_id": 468,
        "content": "from mmflow.apis import init_model, inference_model\nfrom mmflow.datasets import visualize_flow, write_flow\nimport mmcv\n# Specify the path to model config and checkpoint file\nconfig_id = 0\nif config_id == 0:\n    config_file = 'flownet2cs_8x1_slong_flyingchairs_384x448.py'\n    checkpoint_file = 'flownet2cs_8x1_slong_flyingchairs_384x448.pth'\nelif config_id == 1:\n    config_file = 'gma_8x2_120k_mixed_368x768.py' # damn slow.\n    checkpoint_file = 'gma_8x2_120k_mixed_368x768.pth'\n# build the model from a config file and a checkpoint file\nmodel = init_model(config_file, checkpoint_file, device='cuda:0')\n# test image pair, and save the results\nimport cv2\nvideo_file = \"/media/root/help/pyjom/samples/video/dog_with_text.mp4\"\nvideo = cv2.VideoCapture(video_file)\nret, img = video.read()\nprevImg = img.copy()\ncounter = 0\nwhile True:\n    ret, img = video.read()\n    if img is None: break\n    else:\n        frame1 = prevImg\n        # frame1 = cv2.cvtColor(frame1,cv2.COLOR_BGR2GRAY)\n        frame2 = img # why freaking grayscale?",
        "type": "code",
        "location": "/tests/optical_flow/mmof_test/execute_me.py:1-35"
    },
    "3855": {
        "file_id": 468,
        "content": "This code initializes a model using MMFlow library and performs optical flow calculation on video frames. It reads a video file, captures frames, applies optical flow algorithm using the initialized model, and saves the results. The model configuration is determined by config_id, with two options specified in the code. Frame1 and frame2 are used to calculate optical flow between these consecutive frames. The code includes color conversion (BGR to grayscale), but this is not clearly explained or justified in the code.",
        "type": "comment"
    },
    "3856": {
        "file_id": 468,
        "content": "        result = inference_model(model, frame1,frame2)\n        prevImg = img.copy()\n        flow_map = visualize_flow(result,None)\n        cv2.imshow(\"flowmap\",flow_map)\n    if cv2.waitKey(20) == ord(\"q\"):\n        break\n        # can also do canny edge detection.",
        "type": "code",
        "location": "/tests/optical_flow/mmof_test/execute_me.py:36-42"
    },
    "3857": {
        "file_id": 468,
        "content": "The code executes inference using the provided model on two frames, visualizes the optical flow map, and displays it in a window. It breaks the loop when \"q\" key is pressed, and can perform Canny edge detection.",
        "type": "comment"
    },
    "3858": {
        "file_id": 469,
        "content": "/tests/optical_flow/mmof_test/get_frame_flow.py",
        "type": "filepath"
    },
    "3859": {
        "file_id": 469,
        "content": "This code reads frames from a video file, converts them to grayscale (optional), and saves the 40th frame as \"frame0.png\" and the next frame as \"frame1.png\". The loop continues until it encounters an empty frame (none) indicating the end of the video.",
        "type": "summary"
    },
    "3860": {
        "file_id": 469,
        "content": "import cv2\nvideo_file = \"/media/root/help/pyjom/samples/video/dog_with_text.mp4\"\nvideo = cv2.VideoCapture(video_file)\nret, img = video.read()\nprevImg = img.copy()\ncounter = 0\nwhile True:\n    ret, img = video.read()\n    if img is None: break\n    else:\n        frame1 = prevImg\n        # frame1 = cv2.cvtColor(frame1,cv2.COLOR_BGR2GRAY)\n        frame2 = img # why freaking grayscale?\n        # frame2 =  cv2.cvtColor(frame2,cv2.COLOR_BGR2GRAY)\n        if counter == 40:\n            cv2.imwrite(\"frame0.png\",frame1)\n            cv2.imwrite(\"frame1.png\",frame2)\n        prevImg = img.copy()\n        counter +=1",
        "type": "code",
        "location": "/tests/optical_flow/mmof_test/get_frame_flow.py:1-23"
    },
    "3861": {
        "file_id": 469,
        "content": "This code reads frames from a video file, converts them to grayscale (optional), and saves the 40th frame as \"frame0.png\" and the next frame as \"frame1.png\". The loop continues until it encounters an empty frame (none) indicating the end of the video.",
        "type": "comment"
    },
    "3862": {
        "file_id": 470,
        "content": "/tests/optical_flow/nvidia_common.py",
        "type": "filepath"
    },
    "3863": {
        "file_id": 470,
        "content": "This code is for setting the path to the OpenCV library for Turing architecture. It first determines the site-packages directory and then checks if there's a specific folder for the current Python version containing .so files, which are loaded into sys.path. If only one such folder exists, it gets added to sys.path before importing cv2. The code prints dir(cv2) for information or potentially debugging purposes.",
        "type": "summary"
    },
    "3864": {
        "file_id": 470,
        "content": "import pathlib\nimport site\nimport sys\n# optical flow sdk is exclusively for Turing architecture.\n# this is root. this is not site-packages.\n# site_path = pathlib.Path([x for x in site.getsitepackages() if \"site-packages\" in x][0])\nsite_path = pathlib.Path(\"/usr/local/lib/python3.9/site-packages\")\ncv2_libs_dir = site_path / 'cv2' / f'python-{sys.version_info.major}.{sys.version_info.minor}'\nprint(cv2_libs_dir)\ncv2_libs = sorted(cv2_libs_dir.glob(\"*.so\"))\nif len(cv2_libs) == 1:\n    print(\"INSERTING:\",cv2_libs[0].parent)\n    sys.path.insert(1, str(cv2_libs[0].parent))\nimport cv2\nprint(dir(cv2)) # shit?",
        "type": "code",
        "location": "/tests/optical_flow/nvidia_common.py:1-19"
    },
    "3865": {
        "file_id": 470,
        "content": "This code is for setting the path to the OpenCV library for Turing architecture. It first determines the site-packages directory and then checks if there's a specific folder for the current Python version containing .so files, which are loaded into sys.path. If only one such folder exists, it gets added to sys.path before importing cv2. The code prints dir(cv2) for information or potentially debugging purposes.",
        "type": "comment"
    },
    "3866": {
        "file_id": 471,
        "content": "/tests/optical_flow/nvidia_of_test.py",
        "type": "filepath"
    },
    "3867": {
        "file_id": 471,
        "content": "The code converts video frames to grayscale, creates an optical flow object, and uploads the first two frames to GPU for calculation. It downloads a GPU flow, visualizes it using flow_vis library, displays in a window, and quits on 'q'. No garbage collection is performed.",
        "type": "summary"
    },
    "3868": {
        "file_id": 471,
        "content": "from nvidia_common import *\nimport numpy as np \nimport cv2\nimport flow_vis\nvideo_file = \"/media/root/help/pyjom/samples/video/dog_with_text.mp4\"\n# this is the fastest.\nvideo = cv2.VideoCapture(video_file)\nret, img = video.read()\nprevImg = img.copy()\nwhile True:\n    ret, img = video.read()\n    if img is None: break\n    else:\n        frame1 = prevImg\n        frame1 = cv2.cvtColor(frame1,cv2.COLOR_BGR2GRAY)\n        frame2 = img # why freaking grayscale?\n        frame2 =  cv2.cvtColor(frame2,cv2.COLOR_BGR2GRAY)\n        prevImg = img.copy()\n        perfPreset = 5\n        gpuId=0\n        # nvof = cv2.cuda_NvidiaOpticalFlow_2_0.create((frame1.shape[1], frame1.shape[0]),5, False, False, False, 0)\n        gpu_flow =cv2.cuda_FarnebackOpticalFlow.create(5, 0.5, False,\n                                                        15, 3, 5, 1.2, 0)\n        gpu_frame_a = cv2.cuda_GpuMat()\n        gpu_frame_b = cv2.cuda_GpuMat()\n        gpu_frame_a.upload(frame1)\n        gpu_frame_b.upload(frame2)\n        # -- exec flow --\n        gpu_flow = cv2.cuda_FarnebackOpticalFlow.calc(gpu_flow, gpu_frame_a,",
        "type": "code",
        "location": "/tests/optical_flow/nvidia_of_test.py:1-36"
    },
    "3869": {
        "file_id": 471,
        "content": "Reading video file, converting frames to grayscale, creating optical flow object with specified parameters, and uploading the first two frames to GPU for calculation.",
        "type": "comment"
    },
    "3870": {
        "file_id": 471,
        "content": "                                                      gpu_frame_b, None)\n        gpu_flow = gpu_flow.download()\n        # gpu_flow = gpu_flow.transpose(2,0,1)\n        # print(gpu_flow.shape())\n        # breakpoint()\n        # gpu_flow = th.from_numpy(gpu_flow).half()\n        # cv2.writeOpticalFlow('OpticalFlow.flo', flowUpSampled)\n        visualize = flow_vis.flow_to_color(gpu_flow, convert_to_bgr=False)\n        cv2.imshow(\"OPTFLOW\",visualize)\n        if cv2.waitKey(20) == chr(\"q\"):\n            print(\"QUIT THIS SHIT\")\n            break\n        # nvof.collectGarbage()",
        "type": "code",
        "location": "/tests/optical_flow/nvidia_of_test.py:37-53"
    },
    "3871": {
        "file_id": 471,
        "content": "This code downloads a GPU flow, potentially transposes it and prints its shape, then visualizes the flow using flow_vis library. It displays the visualization in a window and quits when 'q' is pressed. No garbage collection is performed.",
        "type": "comment"
    },
    "3872": {
        "file_id": 472,
        "content": "/tests/optical_flow/sparse_cpu.py",
        "type": "filepath"
    },
    "3873": {
        "file_id": 472,
        "content": "The code initializes an App object, tracks key points using PyrLK algorithm, calculates optical flow between frames, maintains maximum length of tracks and displays results. It uses OpenCV, numpy and Flownet2-pytorch model for processing and detecting key points.",
        "type": "summary"
    },
    "3874": {
        "file_id": 472,
        "content": "#coding=utf-8\nimport numpy as np\nimport cv2\n# from common import anorm2, draw_str\n# from time import clock\nimport cmath\nlk_params = dict(winSize=(15, 15),\n                 maxLevel=2,\n                 criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))\n# maxCorners : 设置最多返回的关键点数量。\n# qualityLevel : 反应一个像素点强度有多强才能成为关键点。\n# minDistance : 关键点之间的最少像素点。\n# blockSize : 计算一个像素点是否为关键点时所取的区域大小。\n# useHarrisDetector :使用原声的 Harris 角侦测器或最小特征值标准。\n# k : 一个用在Harris侦测器中的自由变量。\nfeature_params = dict(maxCorners=5000000,\n                      qualityLevel=0.1,\n                      minDistance=7,\n                      blockSize=7)\nclass App:\n    def __init__(self, video_src):  # 构造方法，初始化一些参数和视频路径\n        self.track_len = 10\n        self.detect_interval = 1\n        self.tracks = []\n        self.cam = cv2.VideoCapture(video_src)\n        self.frame_idx = 0\n        self.num = 0\n        self.i = 0\n        self.all_distance = 0\n        self.count = 0\n    def run(self):  # 光流运行方法\n        while True:\n            ret, frame = self.cam.read()  # 读取视频帧",
        "type": "code",
        "location": "/tests/optical_flow/sparse_cpu.py:1-37"
    },
    "3875": {
        "file_id": 472,
        "content": "App class initialization and video reading\n\nCode for creating and initializing the App object, capturing video frames from a specified source.",
        "type": "comment"
    },
    "3876": {
        "file_id": 472,
        "content": "            if ret == True:\n                frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)  # 转化为灰度虚图像\n                # vis = frame.copy()\n                h, w = frame.shape[:2]\n                vis = np.ones((h, w), )\n                f = open('./shuibo_8_LK(x1,y1,x2,y2).txt','w+')\n                if len(self.tracks) > 0:  # 检测到角点后进行光流跟踪\n                    img0, img1 = self.prev_gray, frame_gray\n                    p0 = np.float32([tr[-1] for tr in self.tracks]).reshape(-1, 1, 2)\n                    \"\"\"\n                    nextPts, status, err = calcOpticalFlowPyrLK(prevImg, nextImg, prevPts[, nextPts[, status[, \n                    err[, winSize[, maxLevel[, criteria[, flags[, minEigThreshold]]]]]]]])\n                    参数说明：\n                      prevImage 前一帧8-bit图像\n                      nextImage 当前帧8-bit图像\n                      prevPts 待跟踪的特征点向量\n                      nextPts 输出跟踪特征点向量\n                      status 特征点是否找到，找到的状态为1，未找到的状态为0\n                      err 输出错误向量，（不太理解用途...）\n                      winSize 搜索窗口的大小",
        "type": "code",
        "location": "/tests/optical_flow/sparse_cpu.py:38-58"
    },
    "3877": {
        "file_id": 472,
        "content": "This code is performing optical flow tracking using the Pyramid Lucas-Kanade algorithm (PyrLK) on a video frame. It reads the previous and current frames, detects key points in the previous frame, calculates the new positions of these key points in the current frame, and updates the tracks list if any key point is found. The status array indicates whether each tracked point was found or not, and err presumably contains error information related to tracking. The code writes the x and y coordinates of each tracked point to a text file.",
        "type": "comment"
    },
    "3878": {
        "file_id": 472,
        "content": "                      maxLevel 最大的金字塔层数\n                      flags 可选标识：OPTFLOW_USE_INITIAL_FLOW   OPTFLOW_LK_GET_MIN_EIGENVALS\n                    \"\"\"\n                    p1, st, err = cv2.calcOpticalFlowPyrLK(img0, img1, p0, None,\n                                                           **lk_params)  # 前一帧的角点和当前帧的图像作为输入来得到角点在当前帧的位置\n                    p0r, st, err = cv2.calcOpticalFlowPyrLK(img1, img0, p1, None,\n                                                            **lk_params)  # 当前帧跟踪到的角点及图像和前一帧的图像作为输入来找到前一帧的角点位置\n                    d = abs(p0 - p0r).reshape(-1, 2).max(-1)  # 得到角点回溯与前一帧实际角点的位置变化关系\n                    # good = d < 1  # 判断d内的值是否小于1，大于1跟踪被认为是错误的跟踪点\n                    good=d\n                    new_tracks = []\n                    for tr, (x, y), good_flag in zip(self.tracks, p1.reshape(-1, 2), good):  # 将跟踪正确的点列入成功跟踪点\n                        if not good_flag:\n                            continue\n                        tr.append((x, y))#tr是前一帧的角点，与当前帧的角点(x,y)合并。标志为good_flag",
        "type": "code",
        "location": "/tests/optical_flow/sparse_cpu.py:59-74"
    },
    "3879": {
        "file_id": 472,
        "content": "This code calculates optical flow between two images using cv2.calcOpticalFlowPyrLK, tracking points from one image to another. It then compares the tracked points with the actual points and measures the displacement. Points with displacement greater than 1 are considered as incorrect and removed. The remaining points form new_tracks, which is a list of successful tracks.",
        "type": "comment"
    },
    "3880": {
        "file_id": 472,
        "content": "                        if len(tr) > self.track_len:\n                            del tr[0]\n                        new_tracks.append(tr)\n                        # print(x,y)\n                        # breakpoint()\n                        cv2.circle(vis, (int(x), int(y)), 2, (0, 255, 0), -1)#当前帧角点画圆\n                    self.tracks = new_tracks #self.tracks中的值的格式是：(前一帧角点)(当前帧角点)\n                    # print(self.tracks[0])\n                    # print(self.tracks[1])\n                    distance = 0\n                    for tr in self.tracks:\n                        # tr[0]=list(tr[0])\n                        # tr[1]=list(tr[1])\n                        x1=tr[0][0]\n                        y1=tr[0][1]\n                        x2 = tr[1][0]\n                        y2 = tr[1][1]\n                        f.writelines([ str(x1), ' ', str(y1), ' ', str(x2), ' ', str(y2),'\\n'])\n                        dis=cmath.sqrt((x2-x1)*(x2-x1)+(y2-y1)*(y2-y1))\n                        #正确追踪的点的个数\n                        print(len(self.tracks))",
        "type": "code",
        "location": "/tests/optical_flow/sparse_cpu.py:75-98"
    },
    "3881": {
        "file_id": 472,
        "content": "This code tracks optical flow points across multiple frames, storing the tracked points in 'tracks'. It appends new tracks and deletes old ones to maintain a maximum length. The x and y coordinates of current points are plotted on a visualization ('vis'). Finally, it calculates the Euclidean distance between consecutive points and writes them into file 'f', while printing the total number of correctly tracked points.",
        "type": "comment"
    },
    "3882": {
        "file_id": 472,
        "content": "                        #每一个正确追踪的点的像素点的位移\n                        print(dis.real)\n                        distance=distance+dis\n                    len_tracks = len(self.tracks)\n                    if len_tracks == 0:continue\n                    distance=distance/len_tracks\n                    self.all_distance=self.all_distance+distance\n                    self.count=self.count+1\n                    print(\"每一帧像素点平均位移：\",distance,\"第几帧：\",self.count)\n                    print(\"所有帧平均位移：\",(self.all_distance/self.count).real)\n                f.close()\n                if self.frame_idx % self.detect_interval == 0:  #每1帧检测一次特征点\n                    mask = np.zeros_like(frame_gray)  # 初始化和视频大小相同的图像\n                    mask[:] = 255  # 将mask赋值255也就是算全部图像的角点\n                    for x, y in [np.int32(tr[-1]) for tr in self.tracks]:  #跟踪的角点画圆\n                        cv2.circle(mask, (x, y), 5, 0, -1)\n                    p = cv2.goodFeaturesToTrack(frame_gray, mask=mask, **feature_params)  # 像素级别角点检测\n                    if p is not None:",
        "type": "code",
        "location": "/tests/optical_flow/sparse_cpu.py:99-117"
    },
    "3883": {
        "file_id": 472,
        "content": "Code calculates average pixel point displacement between frames and prints the results. It keeps track of all pixel point movements in a frame and counts the number of frames. The code checks for features every 1 frame, initializes a mask image, detects corners using goodFeaturesToTrack function, and stores the result if it is not None.",
        "type": "comment"
    },
    "3884": {
        "file_id": 472,
        "content": "                        for x, y in np.float32(p).reshape(-1, 2):\n                            self.tracks.append([(x, y)])  # 将检测到的角点放在待跟踪序列中\n                self.frame_idx += 1\n                self.prev_gray = frame_gray\n                cv2.imshow('lk_track', vis)\n            # ch = 0xFF & \n            if cv2.waitKey(20) == \"q\":\n                # cv2.imwrite(\"./mashiti-result4.png\", vis)\n                break\n# # get flownet2-pytorch source\n# git clone https://github.com/NVIDIA/flownet2-pytorch.git\n# cd flownet2-pytorch\n# # install custom layers\n# bash install.sh\ndef main():\n    import sys\n    try:\n        video_src = sys.argv[1]\n    except:\n        # video_src = \"./F/8/shuibo_8.avi\"\n        video_src = \"/media/root/help/pyjom/samples/video/dog_with_text.mp4\"\n    # print\n    # __doc__\n    App(video_src).run()\n    cv2.destroyAllWindows()\nif __name__ == '__main__':\n    main()",
        "type": "code",
        "location": "/tests/optical_flow/sparse_cpu.py:118-151"
    },
    "3885": {
        "file_id": 472,
        "content": "This code is a part of a video processing program. It reads frames from a video source, detects key points in each frame using the LK tracker, tracks these key points across successive frames to estimate optical flow, and displays the results. The code uses OpenCV library for image processing, numpy for numerical computations, and cv2.waitKey() function for window handling. It also imports a Flownet2-pytorch model from a git repository and installs custom layers.",
        "type": "comment"
    },
    "3886": {
        "file_id": 473,
        "content": "/tests/patch_requests_timeout/client.py",
        "type": "filepath"
    },
    "3887": {
        "file_id": 473,
        "content": "This code patches the requests library to set a default timeout for all requests made using it. It uses patchy module to modify the HTTPAdapter's send method, checking if a timeout is provided and setting it to REQUESTS_TIMEOUT (3 seconds) if none is given. The patch is then applied using the patchy module.",
        "type": "summary"
    },
    "3888": {
        "file_id": 473,
        "content": "import patchy\nfrom requests.adapters import HTTPAdapter\nREQUESTS_TIMEOUT=3 # working! great.\ndef patch_requests_default_timeout() -> None:\n    \"\"\"\n    Set a default timeout for all requests made with “requests”.\n    Upstream is waiting on this longstanding issue:\n    https://github.com/psf/requests/issues/3070\n    \"\"\"\n    patchy.patch(\n        HTTPAdapter.send,\n        f\"\"\"\\\n        @@ -14,6 +14,8 @@\n             :param proxies: (optional) The proxies dictionary to apply to the request.\n             :rtype: requests.Response\n             \\\"\"\"\n        +    if timeout is None:\n        +        timeout = {REQUESTS_TIMEOUT}\n             try:\n                 conn = self.get_connection(request.url, proxies)\n        \"\"\",\n    )\npatch_requests_default_timeout()\nimport requests\nfrom server import SERVER_PORT\nr = requests.get(f\"http://localhost:{SERVER_PORT}\")",
        "type": "code",
        "location": "/tests/patch_requests_timeout/client.py:2-36"
    },
    "3889": {
        "file_id": 473,
        "content": "This code patches the requests library to set a default timeout for all requests made using it. It uses patchy module to modify the HTTPAdapter's send method, checking if a timeout is provided and setting it to REQUESTS_TIMEOUT (3 seconds) if none is given. The patch is then applied using the patchy module.",
        "type": "comment"
    },
    "3890": {
        "file_id": 474,
        "content": "/tests/patch_requests_timeout/server.py",
        "type": "filepath"
    },
    "3891": {
        "file_id": 474,
        "content": "This code sets up a FastAPI server using uvicorn, listens on port 9341, and has a single route (\"/\") that returns \"hello world\" after a 10-second delay. The run() function is used to start the configured uvicorn server.",
        "type": "summary"
    },
    "3892": {
        "file_id": 474,
        "content": "SERVER_PORT = 9341\nif __name__ == \"__main__\":\n    from fastapi import FastAPI\n    app = FastAPI()\n    import time\n    @app.get(\"/\")\n    def receiveImage():\n        time.sleep(10)\n        return \"hello world\"\n    import uvicorn\n    # checking: https://9to5answer.com/python-how-to-use-fastapi-and-uvicorn-run-without-blocking-the-thread\n    def run(host='0.0.0.0',port=SERVER_PORT): \n        \"\"\"\n        This function to run configured uvicorn server.\n        \"\"\"\n        uvicorn.run(app=app, host=host, port=port)\n    run()",
        "type": "code",
        "location": "/tests/patch_requests_timeout/server.py:1-21"
    },
    "3893": {
        "file_id": 474,
        "content": "This code sets up a FastAPI server using uvicorn, listens on port 9341, and has a single route (\"/\") that returns \"hello world\" after a 10-second delay. The run() function is used to start the configured uvicorn server.",
        "type": "comment"
    },
    "3894": {
        "file_id": 475,
        "content": "/tests/post_numpy_array/client.py",
        "type": "filepath"
    },
    "3895": {
        "file_id": 475,
        "content": "Importing numpy, requests, and numpy_serializer; using SERVER_PORT from server module; creating a test image array; converting the image to bytes using numpy_serializer; sending the image data as a POST request to localhost; printing the response received. Includes a malformatted docstring function with textwrap usage.",
        "type": "summary"
    },
    "3896": {
        "file_id": 475,
        "content": "import numpy as np\nimport requests\nimport numpy_serializer\n# this is pure magic. shit.\nfrom server import SERVER_PORT\nimage = np.array([1,2,3])\nimage_bytes = numpy_serializer.to_bytes(image)\ndata = {'image':image_bytes}\nprint(\"BYTES?\", image_bytes)\nr = requests.post(\"http://localhost:{}\".format(SERVER_PORT),data=data,params={'isBytes':True,'debug':True})\nprint('RESPONSE?',r.text)\ndef docstring(): # malformat\n    import textwrap\n    a =\"\"\"\n    lmn\n    abcdefg \n    hijk\n    \"\"\"\n    print(a)\n    print()\n    print(textwrap.dedent(a))\n    # inspect.cleandoc\n    # https://9to5answer.com/how-to-remove-extra-indentation-of-python-triple-quoted-multi-line-strings\ndocstring()",
        "type": "code",
        "location": "/tests/post_numpy_array/client.py:1-28"
    },
    "3897": {
        "file_id": 475,
        "content": "Importing numpy, requests, and numpy_serializer; using SERVER_PORT from server module; creating a test image array; converting the image to bytes using numpy_serializer; sending the image data as a POST request to localhost; printing the response received. Includes a malformatted docstring function with textwrap usage.",
        "type": "comment"
    },
    "3898": {
        "file_id": 476,
        "content": "/tests/post_numpy_array/server.py",
        "type": "filepath"
    },
    "3899": {
        "file_id": 476,
        "content": "The code sets up a FastAPI server on port 5463, defines an endpoint that receives an image and returns \"good\", and runs a non-blocking Uvicorn server.",
        "type": "summary"
    }
}