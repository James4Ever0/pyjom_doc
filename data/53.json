{
    "5300": {
        "file_id": 698,
        "content": "        \"COCO-InstanceSegmentation/mask_rcnn_R_50_C4_3x\": \"137849525/model_final_4ce675.pkl\",\n        \"COCO-InstanceSegmentation/mask_rcnn_R_50_DC5_3x\": \"137849551/model_final_84107b.pkl\",\n        \"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x\": \"137849600/model_final_f10217.pkl\",\n        \"COCO-InstanceSegmentation/mask_rcnn_R_101_C4_3x\": \"138363239/model_final_a2914c.pkl\",\n        \"COCO-InstanceSegmentation/mask_rcnn_R_101_DC5_3x\": \"138363294/model_final_0464b7.pkl\",\n        \"COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x\": \"138205316/model_final_a3ec72.pkl\",\n        \"COCO-InstanceSegmentation/mask_rcnn_X_101_32x8d_FPN_3x\": \"139653917/model_final_2d9806.pkl\",  # noqa\n        # New baselines using Large-Scale Jitter and Longer Training Schedule\n        \"new_baselines/mask_rcnn_R_50_FPN_100ep_LSJ\": \"42047764/model_final_bb69de.pkl\",\n        \"new_baselines/mask_rcnn_R_50_FPN_200ep_LSJ\": \"42047638/model_final_89a8d3.pkl\",\n        \"new_baselines/mask_rcnn_R_50_FPN_400ep_LSJ\": \"42019571/model_final_14d201.pkl\",",
        "type": "code",
        "location": "/tests/video_detector_tests/detectron2_model_zoo_url.py:35-45"
    },
    "5301": {
        "file_id": 698,
        "content": "This code maps different model names to their corresponding checkpoint files in the Detectron2 Model Zoo. It includes COCO instance segmentation models and new baselines with Large-Scale Jitter and longer training schedules.",
        "type": "comment"
    },
    "5302": {
        "file_id": 698,
        "content": "        \"new_baselines/mask_rcnn_R_101_FPN_100ep_LSJ\": \"42025812/model_final_4f7b58.pkl\",\n        \"new_baselines/mask_rcnn_R_101_FPN_200ep_LSJ\": \"42131867/model_final_0bb7ae.pkl\",\n        \"new_baselines/mask_rcnn_R_101_FPN_400ep_LSJ\": \"42073830/model_final_f96b26.pkl\",\n        \"new_baselines/mask_rcnn_regnetx_4gf_dds_FPN_100ep_LSJ\": \"42047771/model_final_b7fbab.pkl\",  # noqa\n        \"new_baselines/mask_rcnn_regnetx_4gf_dds_FPN_200ep_LSJ\": \"42132721/model_final_5d87c1.pkl\",  # noqa\n        \"new_baselines/mask_rcnn_regnetx_4gf_dds_FPN_400ep_LSJ\": \"42025447/model_final_f1362d.pkl\",  # noqa\n        \"new_baselines/mask_rcnn_regnety_4gf_dds_FPN_100ep_LSJ\": \"42047784/model_final_6ba57e.pkl\",  # noqa\n        \"new_baselines/mask_rcnn_regnety_4gf_dds_FPN_200ep_LSJ\": \"42047642/model_final_27b9c1.pkl\",  # noqa\n        \"new_baselines/mask_rcnn_regnety_4gf_dds_FPN_400ep_LSJ\": \"42045954/model_final_ef3a80.pkl\",  # noqa\n        # COCO Person Keypoint Detection Baselines with Keypoint R-CNN\n        \"COCO-Keypoints/keypoint_rcnn_R_50_FPN_1x\": \"137261548/model_final_04e291.pkl\",",
        "type": "code",
        "location": "/tests/video_detector_tests/detectron2_model_zoo_url.py:46-56"
    },
    "5303": {
        "file_id": 698,
        "content": "This code defines a mapping of model names to their corresponding final pickle files. The models are Detectron2 COCO Person Keypoint Detection Baselines and include variations of Mask R-CNN, Mask R-CNN with RegNetX/Y, and the COCO-Keypoints Keypoint R-CNN.",
        "type": "comment"
    },
    "5304": {
        "file_id": 698,
        "content": "        \"COCO-Keypoints/keypoint_rcnn_R_50_FPN_3x\": \"137849621/model_final_a6e10b.pkl\",\n        \"COCO-Keypoints/keypoint_rcnn_R_101_FPN_3x\": \"138363331/model_final_997cc7.pkl\",\n        \"COCO-Keypoints/keypoint_rcnn_X_101_32x8d_FPN_3x\": \"139686956/model_final_5ad38f.pkl\",\n        # COCO Panoptic Segmentation Baselines with Panoptic FPN\n        \"COCO-PanopticSegmentation/panoptic_fpn_R_50_1x\": \"139514544/model_final_dbfeb4.pkl\",\n        \"COCO-PanopticSegmentation/panoptic_fpn_R_50_3x\": \"139514569/model_final_c10459.pkl\",\n        \"COCO-PanopticSegmentation/panoptic_fpn_R_101_3x\": \"139514519/model_final_cafdb1.pkl\",\n        # LVIS Instance Segmentation Baselines with Mask R-CNN\n        \"LVISv0.5-InstanceSegmentation/mask_rcnn_R_50_FPN_1x\": \"144219072/model_final_571f7c.pkl\",  # noqa\n        \"LVISv0.5-InstanceSegmentation/mask_rcnn_R_101_FPN_1x\": \"144219035/model_final_824ab5.pkl\",  # noqa\n        \"LVISv0.5-InstanceSegmentation/mask_rcnn_X_101_32x8d_FPN_1x\": \"144219108/model_final_5e3439.pkl\",  # noqa",
        "type": "code",
        "location": "/tests/video_detector_tests/detectron2_model_zoo_url.py:57-67"
    },
    "5305": {
        "file_id": 698,
        "content": "This code is a dictionary mapping model names to their corresponding checkpoint files. These models are for Detectron2's object detection, keypoint estimation, and segmentation tasks on COCO and LVIS datasets. The checkpoint files store the trained model parameters for each specific configuration.",
        "type": "comment"
    },
    "5306": {
        "file_id": 698,
        "content": "        # Cityscapes & Pascal VOC Baselines\n        \"Cityscapes/mask_rcnn_R_50_FPN\": \"142423278/model_final_af9cf5.pkl\",\n        \"PascalVOC-Detection/faster_rcnn_R_50_C4\": \"142202221/model_final_b1acc2.pkl\",\n        # Other Settings\n        \"Misc/mask_rcnn_R_50_FPN_1x_dconv_c3-c5\": \"138602867/model_final_65c703.pkl\",\n        \"Misc/mask_rcnn_R_50_FPN_3x_dconv_c3-c5\": \"144998336/model_final_821d0b.pkl\",\n        \"Misc/cascade_mask_rcnn_R_50_FPN_1x\": \"138602847/model_final_e9d89b.pkl\",\n        \"Misc/cascade_mask_rcnn_R_50_FPN_3x\": \"144998488/model_final_480dd8.pkl\",\n        \"Misc/mask_rcnn_R_50_FPN_3x_syncbn\": \"169527823/model_final_3b3c51.pkl\",\n        \"Misc/mask_rcnn_R_50_FPN_3x_gn\": \"138602888/model_final_dc5d9e.pkl\",\n        \"Misc/scratch_mask_rcnn_R_50_FPN_3x_gn\": \"138602908/model_final_01ca85.pkl\",\n        \"Misc/scratch_mask_rcnn_R_50_FPN_9x_gn\": \"183808979/model_final_da7b4c.pkl\",\n        \"Misc/scratch_mask_rcnn_R_50_FPN_9x_syncbn\": \"184226666/model_final_5ce33e.pkl\",\n        \"Misc/panoptic_fpn_R_101_dconv_cascade_gn_3x\": \"139797668/model_final_be35db.pkl\",",
        "type": "code",
        "location": "/tests/video_detector_tests/detectron2_model_zoo_url.py:68-81"
    },
    "5307": {
        "file_id": 698,
        "content": "This code defines model configurations and their corresponding checkpoint file paths for various tasks like Cityscapes, Pascal VOC detection, and panoptic segmentation. These configurations include different architectures and training strategies such as syncBN and gn. The checkpoint files store the trained model parameters which can be loaded to replicate the results.",
        "type": "comment"
    },
    "5308": {
        "file_id": 698,
        "content": "        \"Misc/cascade_mask_rcnn_X_152_32x8d_FPN_IN5k_gn_dconv\": \"18131413/model_0039999_e76410.pkl\",  # noqa\n        # D1 Comparisons\n        \"Detectron1-Comparisons/faster_rcnn_R_50_FPN_noaug_1x\": \"137781054/model_final_7ab50c.pkl\",  # noqa\n        \"Detectron1-Comparisons/mask_rcnn_R_50_FPN_noaug_1x\": \"137781281/model_final_62ca52.pkl\",  # noqa\n        \"Detectron1-Comparisons/keypoint_rcnn_R_50_FPN_1x\": \"137781195/model_final_cce136.pkl\",\n    }\n    @staticmethod\n    def query(config_path: str) -> Optional[str]:\n        \"\"\"\n        Args:\n            config_path: relative config filename\n        \"\"\"\n        name = config_path.replace(\".yaml\", \"\").replace(\".py\", \"\")\n        if name in _ModelZooUrls.CONFIG_PATH_TO_URL_SUFFIX:\n            suffix = _ModelZooUrls.CONFIG_PATH_TO_URL_SUFFIX[name]\n            return _ModelZooUrls.S3_PREFIX + name + \"/\" + suffix\n        return None\ndef get_checkpoint_url(config_path):\n    \"\"\"\n    Returns the URL to the model trained using the given config\n    Args:\n        config_path (str): config file name relative to detectron2's \"configs/\"",
        "type": "code",
        "location": "/tests/video_detector_tests/detectron2_model_zoo_url.py:82-107"
    },
    "5309": {
        "file_id": 698,
        "content": "This code provides a function to query the model URL and checkpoint from a given configuration path. It maps specific configurations to their respective URL suffixes and uses them to generate the model's URL, including a prefix. The function returns the URL if a valid mapping is found; otherwise, it returns None.",
        "type": "comment"
    },
    "5310": {
        "file_id": 698,
        "content": "            directory, e.g., \"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_1x.yaml\"\n    Returns:\n        str: a URL to the model\n    \"\"\"\n    url = _ModelZooUrls.query(config_path)\n    if url is None:\n        raise RuntimeError(\"Pretrained model for {} is not available!\".format(config_path))\n    return url\nif __name__ == \"__main__\":\n    test_config = \"Detectron1-Comparisons/mask_rcnn_R_50_FPN_noaug_1x\"\n    url = get_checkpoint_url(test_config)\n    print(\"model name:\",test_config)\n    print(\"model url:\",url)",
        "type": "code",
        "location": "/tests/video_detector_tests/detectron2_model_zoo_url.py:108-122"
    },
    "5311": {
        "file_id": 698,
        "content": "This code defines a function `get_checkpoint_url` that returns the URL of a pretrained model based on its configuration path. It also checks if a valid URL exists for the given config, and raises an error if not. The provided example demonstrates how to use this function with a specific config, printing the model name and URL.",
        "type": "comment"
    },
    "5312": {
        "file_id": 699,
        "content": "/tests/video_detector_tests/cocoNames.py",
        "type": "filepath"
    },
    "5313": {
        "file_id": 699,
        "content": "The code defines two dictionaries, \"cocoName\" and \"cocoRealName\", used for image classification tasks based on the MS COCO dataset. It maps labels to object names and indexes respectively, correcting 0-indexing in dataset labels.",
        "type": "summary"
    },
    "5314": {
        "file_id": 699,
        "content": "cocoName = {0: '__background__',\n\t 1: 'person',\n\t 2: 'bicycle',\n\t 3: 'car',\n\t 4: 'motorcycle',\n\t 5: 'airplane',\n\t 6: 'bus',\n\t 7: 'train',\n\t 8: 'truck',\n\t 9: 'boat',\n\t 10: 'traffic light',\n\t 11: 'fire hydrant',\n\t 12: 'stop sign',\n\t 13: 'parking meter',\n\t 14: 'bench',\n\t 15: 'bird',\n\t 16: 'cat',\n\t 17: 'dog',\n\t 18: 'horse',\n\t 19: 'sheep',\n\t 20: 'cow',\n\t 21: 'elephant',\n\t 22: 'bear',\n\t 23: 'zebra',\n\t 24: 'giraffe',\n\t 25: 'backpack',\n\t 26: 'umbrella',\n\t 27: 'handbag',\n\t 28: 'tie',\n\t 29: 'suitcase',\n\t 30: 'frisbee',\n\t 31: 'skis',\n\t 32: 'snowboard',\n\t 33: 'sports ball',\n\t 34: 'kite',\n\t 35: 'baseball bat',\n\t 36: 'baseball glove',\n\t 37: 'skateboard',\n\t 38: 'surfboard',\n\t 39: 'tennis racket',\n\t 40: 'bottle',\n\t 41: 'wine glass',\n\t 42: 'cup',\n\t 43: 'fork',\n\t 44: 'knife',\n\t 45: 'spoon',\n\t 46: 'bowl',\n\t 47: 'banana',\n\t 48: 'apple',\n\t 49: 'sandwich',\n\t 50: 'orange',\n\t 51: 'broccoli',\n\t 52: 'carrot',\n\t 53: 'hot dog',\n\t 54: 'pizza',\n\t 55: 'donut',\n\t 56: 'cake',\n\t 57: 'chair',\n\t 58: 'couch',\n\t 59: 'potted plant',\n\t 60: 'bed',\n\t 61: 'dining table',\n\t 62: 'toilet',\n\t 63: 'tv',",
        "type": "code",
        "location": "/tests/video_detector_tests/cocoNames.py:1-64"
    },
    "5315": {
        "file_id": 699,
        "content": "This code defines a dictionary named \"cocoName\" that maps integer labels to object names, used for image classification tasks based on the MS COCO dataset.",
        "type": "comment"
    },
    "5316": {
        "file_id": 699,
        "content": "\t 64: 'laptop',\n\t 65: 'mouse',\n\t 66: 'remote',\n\t 67: 'keyboard',\n\t 68: 'cell phone',\n\t 69: 'microwave',\n\t 70: 'oven',\n\t 71: 'toaster',\n\t 72: 'sink',\n\t 73: 'refrigerator',\n\t 74: 'book',\n\t 75: 'clock',\n\t 76: 'vase',\n\t 77: 'scissors',\n\t 78: 'teddy bear',\n\t 79: 'hair drier',\n\t 80: 'toothbrush'}\ncocoRealName = {k-1:cocoName[k] for k in cocoName.keys()}",
        "type": "code",
        "location": "/tests/video_detector_tests/cocoNames.py:65-83"
    },
    "5317": {
        "file_id": 699,
        "content": "The code defines a dictionary named \"cocoRealName\" that maps object names to corresponding COCO indexes. It uses a dictionary comprehension to subtract 1 from each key in the original \"cocoName\" dictionary, which assumes an offset of 0-indexing in the dataset labels.",
        "type": "comment"
    },
    "5318": {
        "file_id": 700,
        "content": "/tests/video_detector_tests/rect_active_rect_frame_difference.py",
        "type": "filepath"
    },
    "5319": {
        "file_id": 700,
        "content": "This code aims to merge frame difference with rectangle detection, calculate the most active rect region, and consider 4 boundary lines as a custom switch.",
        "type": "summary"
    },
    "5320": {
        "file_id": 700,
        "content": "# merge framedifference with rectangle detection.\n# calculate the most active rect region.\n# also you might want to include 4 boundary lines as custom switch.",
        "type": "code",
        "location": "/tests/video_detector_tests/rect_active_rect_frame_difference.py:1-5"
    },
    "5321": {
        "file_id": 700,
        "content": "This code aims to merge frame difference with rectangle detection, calculate the most active rect region, and consider 4 boundary lines as a custom switch.",
        "type": "comment"
    },
    "5322": {
        "file_id": 701,
        "content": "/tests/video_detector_tests/pip_meanVariance_stablize.py",
        "type": "filepath"
    },
    "5323": {
        "file_id": 701,
        "content": "The function uses calculations, filters, and analyses to process data and perform tasks such as linear regression and image analysis. The code creates a rectangular plot using matplotlib with randomly chosen colors and displays it using plt.show().",
        "type": "summary"
    },
    "5324": {
        "file_id": 701,
        "content": "from mathlib import *\n# from ...pyjom.mathlib import sequentialToMergedRanges\n# you can use yolo to train network to detect these sharp corners, total four sharp corners.\n# but it might fail to do so.\n# but what about other stuff?\n# whatever let's just use this.\ndef sampledStablePipRegionExporter(data, defaultWidth, defaultHeight):\n    defaultWidth, defaultHeight = int(defaultWidth), int(defaultHeight)\n    import numpy as np\n    data = np.array(data)\n    def getAlikeValueMerged(mArray, threshold=35):\n        for index, elem in enumerate(mArray[:-1]):\n            nextElem = mArray[index + 1]\n            if abs(nextElem - elem) < threshold:\n                mArray[index + 1] = elem\n        return mArray\n    def listToRangedDictWithLabel(mList, label):\n        resultDict = {}\n        for index, elem in enumerate(mList):\n            mKey = \"{}:{}\".format(label, int(elem))\n            resultDict.update({mKey: resultDict.get(mKey, []) + [(index, index + 1)]})\n        return resultDict\n    def get1DArrayEMA(mArray,N=5):",
        "type": "code",
        "location": "/tests/video_detector_tests/pip_meanVariance_stablize.py:1-33"
    },
    "5325": {
        "file_id": 701,
        "content": "The function sampledStablePipRegionExporter takes data, defaultWidth, and defaultHeight as inputs. It converts the data into a numpy array, then provides three helper functions: getAlikeValueMerged, listToRangedDictWithLabel, and get1DArrayEMA. The purpose of these helper functions is to manipulate and process the data into desired ranges for further analysis or processing.",
        "type": "comment"
    },
    "5326": {
        "file_id": 701,
        "content": "        weights=np.exp(np.linspace(0,1,N))\n        weights =weights/np.sum(weights)\n        ema = np.convolve(weights, mArray, mode='valid')\n        return ema\n    def pointsToRangedDictWithLabel(mArray, label, threshold=35):\n        mArray = get1DArrayEMA(mArray)\n        mArray = getAlikeValueMerged(mArray, threshold=threshold)\n        return listToRangedDictWithLabel(mArray, label)\n    threshold = int(max(defaultWidth, defaultHeight)*0.02734375)\n    xLeftPoints = pointsToRangedDictWithLabel(data[:, 0, 0], \"xleft\", threshold = threshold)\n    yLeftPoints = pointsToRangedDictWithLabel(data[:, 0, 1], \"yleft\", threshold = threshold)\n    xRightPoints = pointsToRangedDictWithLabel(data[:, 1, 0], \"xright\", threshold = threshold)\n    yRightPoints = pointsToRangedDictWithLabel(data[:, 1, 1], \"yright\", threshold = threshold)\n    commandDict = {}\n    for mDict in [xLeftPoints, yLeftPoints, xRightPoints, yRightPoints]:\n        commandDict.update(mDict)\n    commandDict = getContinualMappedNonSympyMergeResult(commandDict)",
        "type": "code",
        "location": "/tests/video_detector_tests/pip_meanVariance_stablize.py:34-52"
    },
    "5327": {
        "file_id": 701,
        "content": "The code calculates the exponential moving average (EMA) of a 1D array and converts points into a ranged dictionary with labels. It then updates a command dictionary using the labeled points and applies non-sympy merge to get the final result. The threshold value is determined based on the maximum width or height, and there are four types of points processed: xLeft, yLeft, xRight, and yRight.",
        "type": "comment"
    },
    "5328": {
        "file_id": 701,
        "content": "    commandDictSequential = mergedRangesToSequential(commandDict)\n    def getSpanDuration(span):\n        start, end = span\n        return end - start\n    itemDurationThreshold = 10\n    # framerate?\n    while True:\n        # print(\"LOOP COUNT:\", loopCount)\n        # loopCount+=1\n        # noAlter = True\n        beforeChange = [item[0] for item in commandDictSequential].copy()\n        for i in range(len(commandDictSequential) - 1):\n            currentItem = commandDictSequential[i]\n            nextItem = commandDictSequential[i + 1]\n            currentItemCommand = currentItem[0]\n            currentItemDuration = getSpanDuration(currentItem[1])\n            nextItemCommand = nextItem[0]\n            nextItemDuration = getSpanDuration(nextItem[1])\n            if currentItemDuration < itemDurationThreshold:\n                if nextItemCommand != currentItemCommand and nextItemDuration >= itemDurationThreshold:\n                    # print(\"HERE0\",i, currentItemCommand, nextItemCommand)\n                    commandDictSequential[i][0] = nextItemCommand",
        "type": "code",
        "location": "/tests/video_detector_tests/pip_meanVariance_stablize.py:53-77"
    },
    "5329": {
        "file_id": 701,
        "content": "This code loops through a sequence of command pairs, adjusting commands with durations below the threshold by taking the next command if it has a duration above the threshold. This ensures that there is no gap between consecutive commands and maintains smooth video detection.",
        "type": "comment"
    },
    "5330": {
        "file_id": 701,
        "content": "                    # noAlter=False\n            if nextItemDuration < itemDurationThreshold:\n                if nextItemCommand != currentItemCommand :\n                    # print(\"HERE1\",i, currentItemCommand, nextItemCommand)\n                    commandDictSequential[i + 1][0] = currentItemCommand\n                    # noAlter=False\n        afterChange = [item[0] for item in commandDictSequential].copy()\n        noAlter = beforeChange == afterChange\n        if noAlter:\n            break\n    preFinalCommandDict = sequentialToMergedRanges(commandDictSequential)\n    finalCommandDict = {}\n    for key, elem in preFinalCommandDict.items():\n        # print(key,elem)\n        varNames = [\"xleft\", \"yleft\", \"xright\", \"yright\"]\n        defaultValues = [0, 0, defaultWidth, defaultHeight]\n        for varName, defaultValue in zip(varNames, defaultValues):\n            key = key.replace(\n                \"{}:empty\".format(varName), \"{}:{}\".format(varName, defaultValue)\n            )\n        # print(key,elem)\n        # breakpoint()",
        "type": "code",
        "location": "/tests/video_detector_tests/pip_meanVariance_stablize.py:78-99"
    },
    "5331": {
        "file_id": 701,
        "content": "The code checks if there is a change in item commands and updates the command dictionary accordingly. If no changes occur, it breaks the loop. It then converts the sequential command dictionary to merged ranges and stores default values for variables.",
        "type": "comment"
    },
    "5332": {
        "file_id": 701,
        "content": "        import parse\n        formatString = (\n            \"xleft:{xleft:d}|yleft:{yleft:d}|xright:{xright:d}|yright:{yright:d}\"\n        )\n        commandArguments = parse.parse(formatString, key)\n        x, y, w, h = (\n            commandArguments[\"xleft\"],\n            commandArguments[\"yleft\"],\n            commandArguments[\"xright\"] - commandArguments[\"xleft\"],\n            commandArguments[\"yright\"] - commandArguments[\"yleft\"],\n        )\n        if w <= 0 or h <= 0:\n            continue\n        cropCommand = \"crop_{}_{}_{}_{}\".format(x, y, w, h)\n        # print(cropCommand)\n        finalCommandDict.update({cropCommand: elem})\n        # print(elem)\n        # the parser shall be in x,y,w,h with keywords.\n        # we might want to parse the command string and reengineer this shit.\n    return finalCommandDict\ndef kalmanStablePipRegionExporter(data, defaultWidth, defaultHeight):\n    defaultWidth, defaultHeight = int(defaultWidth), int(defaultHeight)\n    import numpy as np\n    data = np.array(data)\n    from pykalman import KalmanFilter",
        "type": "code",
        "location": "/tests/video_detector_tests/pip_meanVariance_stablize.py:100-129"
    },
    "5333": {
        "file_id": 701,
        "content": "The code imports the \"parse\" library, defines a format string for command arguments, uses parse to extract x, y, w, and h values, checks if these values are valid, creates a crop command based on these values, updates finalCommandDict with this command and corresponding element, and finally imports numpy and pykalman for further processing.",
        "type": "comment"
    },
    "5334": {
        "file_id": 701,
        "content": "    def Kalman1D(observations, damping=0.2):\n        # To return the smoothed time series data\n        observation_covariance = damping\n        initial_value_guess = observations[0]\n        transition_matrix = 1\n        transition_covariance = 0.1\n        initial_value_guess\n        kf = KalmanFilter(\n            initial_state_mean=initial_value_guess,\n            initial_state_covariance=observation_covariance,\n            observation_covariance=observation_covariance,\n            transition_covariance=transition_covariance,\n            transition_matrices=transition_matrix,\n        )\n        pred_state, state_cov = kf.smooth(observations)\n        return pred_state\n    def getSinglePointStableState(xLeftPoints, signalFilterThreshold=10, commandFloatMergeThreshold = 15, \n        stdThreshold = 1,\n        slopeThreshold = 0.2):\n        xLeftPointsFiltered = Kalman1D(xLeftPoints)\n        xLeftPointsFiltered = xLeftPointsFiltered.reshape(-1)\n        from itertools import groupby\n        def extract_span(mlist, target=0):",
        "type": "code",
        "location": "/tests/video_detector_tests/pip_meanVariance_stablize.py:131-155"
    },
    "5335": {
        "file_id": 701,
        "content": "This code defines two functions: \"Kalman1D\" and \"getSinglePointStableState\". \"Kalman1D\" uses a Kalman filter algorithm to smooth time-series observations, while \"getSinglePointStableState\" filters and processes left points data for single-point stable states. The input parameters allow customization of the filtering and processing thresholds.",
        "type": "comment"
    },
    "5336": {
        "file_id": 701,
        "content": "            counter = 0\n            spanList = []\n            target_list = [(a, len(list(b))) for a, b in groupby(mlist)]\n            for a, b in target_list:\n                nextCounter = counter + b\n                if a == target:\n                    spanList.append((counter, nextCounter))\n                counter = nextCounter\n            return spanList\n        # solve diff.\n        xLeftPointsFilteredDiff = np.diff(xLeftPointsFiltered)\n        # xLeftPointsFilteredDiff3 = np.diff(xLeftPointsFilteredDiff)\n        # import matplotlib.pyplot as plt\n        # plt.plot(xLeftPointsFilteredDiff)\n        # plt.plot(xLeftPointsFiltered)\n        # plt.plot(xLeftPoints)\n        # plt.show()\n        # xLeftPointsFilteredDiff3Filtered = Kalman1D(xLeftPointsFilteredDiff3)\n        derivativeThreshold = 3\n        # derivative3Threshold = 3\n        xLeftPointsSignal = (\n            (abs(xLeftPointsFilteredDiff) < derivativeThreshold)\n            .astype(np.uint8)\n            .tolist()\n        )\n        def signalFilter(signal, threshold=10):",
        "type": "code",
        "location": "/tests/video_detector_tests/pip_meanVariance_stablize.py:156-184"
    },
    "5337": {
        "file_id": 701,
        "content": "The code filters and extracts a list of spans from a given input, likely for text processing or analysis purposes. It uses groupby function to split the input into consecutive repetitions of the same element, then iterates over the resulting list of tuples (element, count) to create a new span list based on a target element. The code also performs differential calculations on the filtered xLeftPoints and applies a derivative threshold to generate a binary signal list likely for further processing or visualization purposes.",
        "type": "comment"
    },
    "5338": {
        "file_id": 701,
        "content": "            newSignal = np.zeros(len(signal))\n            signalFiltered = extract_span(xLeftPointsSignal, target=1)\n            newSignalRanges = []\n            for start, end in signalFiltered:\n                length = end - start\n                if length >= threshold:\n                    newSignalRanges.append((start, end))\n                    newSignal[start : end + 1] = 1\n            return newSignal, newSignalRanges\n        xLeftPointsSignalFiltered, newSignalRanges = signalFilter(xLeftPointsSignal, threshold = signalFilterThreshold)\n        xLeftPointsSignalFiltered *= 255\n        mShrink = 2\n        from sklearn.linear_model import LinearRegression\n        target = []\n        for start, end in newSignalRanges:\n            # could we shrink the boundaries?\n            mStart, mEnd = start + mShrink, end - mShrink\n            if mEnd <= mStart:\n                continue\n            sample = xLeftPointsFiltered[mStart:mEnd]\n            std = np.std(sample)\n            if std > stdThreshold:\n                continue",
        "type": "code",
        "location": "/tests/video_detector_tests/pip_meanVariance_stablize.py:185-210"
    },
    "5339": {
        "file_id": 701,
        "content": "This code segment performs signal filtering and preparation for subsequent analysis. It extracts a filtered signal, selects ranges above a threshold, scales the values to 255, applies boundary shrinking, and checks if the standard deviation exceeds a threshold before passing it on for further processing.",
        "type": "comment"
    },
    "5340": {
        "file_id": 701,
        "content": "            model = LinearRegression()\n            X, y = np.array(range(sample.shape[0])).reshape(-1, 1), sample\n            model.fit(X, y)\n            coef = model.coef_[0]  # careful!\n            if abs(coef) > slopeThreshold:\n                continue\n            meanValue = int(np.mean(sample))\n            target.append({\"range\": (start, end), \"mean\": meanValue})\n            # print((start, end), std, coef)\n        newTarget = {}\n        for elem in target:\n            meanStr = str(elem[\"mean\"])\n            mRange = elem[\"range\"]\n            newTarget.update({meanStr: newTarget.get(meanStr, []) + [mRange]})\n        mStart = 0\n        mEnd = len(xLeftPoints)\n        newTarget = getContinualMappedNonSympyMergeResultWithRangedEmpty(\n            newTarget, mStart, mEnd\n        )\n        newTargetSequential = mergedRangesToSequential(newTarget)\n        if (newTargetSequential) == 1:\n            if newTargetSequential[0][0] == \"empty\":\n                # the whole thing is empty now. no need to investigate.\n                print(\"NO STATIC PIP FOUND HERE.\")",
        "type": "code",
        "location": "/tests/video_detector_tests/pip_meanVariance_stablize.py:211-238"
    },
    "5341": {
        "file_id": 701,
        "content": "Performs linear regression on sample data, if slope is within threshold range, adds mean value and range to target list. Combines target elements into newTarget dictionary, and converts newTarget to sequential format. If entire sequential format is empty, prints \"NO STATIC PIP FOUND HERE.\"",
        "type": "comment"
    },
    "5342": {
        "file_id": 701,
        "content": "                return {}\n        else:\n            # newTargetSequential\n            newTargetSequentialUpdated = []\n            for index in range(len(newTargetSequential) - 1):\n                elem = newTargetSequential[index]\n                commandString, commandTimeSpan = elem\n                nextElem = newTargetSequential[index + 1]\n                nextCommandString, nextCommandTimeSpan = nextElem\n                if commandString == \"empty\":\n                    newTargetSequential[index][0] = nextCommandString\n                else:\n                    if nextCommandString == \"empty\":\n                        newTargetSequential[index + 1][0] = commandString\n                    else:  # compare the two!\n                        commandFloat = float(commandString)\n                        nextCommandFloat = float(nextCommandString)\n                        if (\n                            abs(commandFloat - nextCommandFloat)\n                            < commandFloatMergeThreshold\n                        ):",
        "type": "code",
        "location": "/tests/video_detector_tests/pip_meanVariance_stablize.py:239-259"
    },
    "5343": {
        "file_id": 701,
        "content": "This code is updating a list of commands by merging consecutive commands if they are within a certain threshold. It compares the difference between two commands and if it's below a specific value, it updates the list accordingly.",
        "type": "comment"
    },
    "5344": {
        "file_id": 701,
        "content": "                            newTargetSequential[index + 1][0] = commandString\n            # bring this sequential into dict again.\n            answer = sequentialToMergedRanges(newTargetSequential)\n            # print(\"_\"*30, \"ANSWER\",\"_\"*30)\n            # for elem in answer.items():\n            #     print(elem)\n            return answer\n        print(\"[FAILSAFE] SOMEHOW THE CODE SUCKS\")\n        return {}\n    xLeftPoints = data[:, 0, 0]\n    yLeftPoints = data[:, 0, 1]\n    xRightPoints = data[:, 1, 0]\n    yRightPoints = data[:, 1, 1]\n    mPoints = [xLeftPoints, yLeftPoints, xRightPoints, yRightPoints]\n    answers = []\n    for mPoint in mPoints:\n        answer = getSinglePointStableState(mPoint)\n        answers.append(answer)\n        # print(\"_\"*30, \"ANSWER\",\"_\"*30)\n        # for elem in answer.items():\n        #     print(elem)\n    if answers == [{}, {}, {}, {}]:\n        print(\"NO PIP FOUND\")\n        finalCommandDict = {}\n    else:\n        defaultCoord = [0, 0, defaultWidth, defaultHeight]  # deal with it later?",
        "type": "code",
        "location": "/tests/video_detector_tests/pip_meanVariance_stablize.py:260-291"
    },
    "5345": {
        "file_id": 701,
        "content": "This code is performing image analysis and stabilization for PIP (Picture-in-Picture) detection. It first creates newSequential, then converts it back into a dictionary named \"answer.\" The code checks if any PIP was found by comparing the \"answers\" list to four empty dictionaries. If no PIP is detected, it returns an empty dictionary; otherwise, it proceeds further with default coordinates.",
        "type": "comment"
    },
    "5346": {
        "file_id": 701,
        "content": "        defaults = [{str(defaultCoord[index]): [(0, len(data))]} for index in range(4)]\n        for index in range(4):\n            if answers[index] == {}:\n                answers[index] = defaults[index]\n        labels = [\"xleft\", \"yleft\", \"xright\", \"yright\"]\n        commandDict = {}\n        for index, elem in enumerate(answers):\n            label = labels[index]\n            newElem = {\"{}:{}\".format(label, key): elem[key] for key in elem.keys()}\n            commandDict.update(newElem)\n        commandDict = getContinualMappedNonSympyMergeResult(commandDict)\n        commandDictSequential = mergedRangesToSequential(commandDict)\n        def getSpanDuration(span):\n            start, end = span\n            return end - start\n        itemDurationThreshold = 15\n        # print(\"HERE\")\n        # loopCount = 0\n        while True:\n            # print(\"LOOP COUNT:\", loopCount)\n            # loopCount+=1\n            # noAlter = True\n            beforeChange = [item[0] for item in commandDictSequential].copy()\n            for i in range(len(commandDictSequential) - 1):",
        "type": "code",
        "location": "/tests/video_detector_tests/pip_meanVariance_stablize.py:292-318"
    },
    "5347": {
        "file_id": 701,
        "content": "Sets default values for missing answers, converts dictionary format, applies consecutive ranges to sequential format, and enters a while loop that iteratively checks changes in the commandDictSequential list.",
        "type": "comment"
    },
    "5348": {
        "file_id": 701,
        "content": "                currentItem = commandDictSequential[i]\n                nextItem = commandDictSequential[i + 1]\n                currentItemCommand = currentItem[0]\n                currentItemDuration = getSpanDuration(currentItem[1])\n                nextItemCommand = nextItem[0]\n                nextItemDuration = getSpanDuration(nextItem[1])\n                if currentItemDuration < itemDurationThreshold:\n                    if nextItemCommand != currentItemCommand:\n                        # print(\"HERE0\",i, currentItemCommand, nextItemCommand)\n                        commandDictSequential[i][0] = nextItemCommand\n                        # noAlter=False\n                if nextItemDuration < itemDurationThreshold:\n                    if nextItemCommand != currentItemCommand and currentItemDuration >= itemDurationThreshold:\n                        # print(\"HERE1\",i, currentItemCommand, nextItemCommand)\n                        commandDictSequential[i + 1][0] = currentItemCommand\n                        # noAlter=False",
        "type": "code",
        "location": "/tests/video_detector_tests/pip_meanVariance_stablize.py:319-334"
    },
    "5349": {
        "file_id": 701,
        "content": "Checks if current and next commands in commandDictSequential have durations below itemDurationThreshold. If so, adjusts or merges the commands to maintain sequence continuity.",
        "type": "comment"
    },
    "5350": {
        "file_id": 701,
        "content": "            afterChange = [item[0] for item in commandDictSequential].copy()\n            noAlter = beforeChange == afterChange\n            if noAlter:\n                break\n        preFinalCommandDict = sequentialToMergedRanges(commandDictSequential)\n        finalCommandDict = {}\n        for key, elem in preFinalCommandDict.items():\n            # print(key,elem)\n            varNames = [\"xleft\", \"yleft\", \"xright\", \"yright\"]\n            defaultValues = [0, 0, defaultWidth, defaultHeight]\n            for varName, defaultValue in zip(varNames, defaultValues):\n                key = key.replace(\n                    \"{}:empty\".format(varName), \"{}:{}\".format(varName, defaultValue)\n                )\n            # print(key,elem)\n            # breakpoint()\n            import parse\n            formatString = (\n                \"xleft:{xleft:d}|yleft:{yleft:d}|xright:{xright:d}|yright:{yright:d}\"\n            )\n            commandArguments = parse.parse(formatString, key)\n            x, y, w, h = (\n                commandArguments[\"xleft\"],",
        "type": "code",
        "location": "/tests/video_detector_tests/pip_meanVariance_stablize.py:335-358"
    },
    "5351": {
        "file_id": 701,
        "content": "The code checks if the command dictionary remains unchanged after certain operations. If it does not change, the loop is exited. The code then converts the sequential command dictionary to a merged ranges form. It creates an empty final command dictionary and iterates over the pre-final command dictionary items. For each item, it replaces any \"empty\" values with default values for specific variables, such as xleft, yleft, xright, and yright. The code then uses the parse module to parse a format string containing these variable names and their updated values, creating commandArguments that contain the final x, y, w, h values.",
        "type": "comment"
    },
    "5352": {
        "file_id": 701,
        "content": "                commandArguments[\"yleft\"],\n                commandArguments[\"xright\"] - commandArguments[\"xleft\"],\n                commandArguments[\"yright\"] - commandArguments[\"yleft\"],\n            )\n            if w <= 0 or h <= 0:\n                continue\n            cropCommand = \"crop_{}_{}_{}_{}\".format(x, y, w, h)\n            # print(cropCommand)\n            finalCommandDict.update({cropCommand: elem})\n            # print(elem)\n            # the parser shall be in x,y,w,h with keywords.\n            # we might want to parse the command string and reengineer this shit.\n    return finalCommandDict\nobjective = \"discrete\"\n# objective = \"continual\"\n# objective = \"continual_najie\"\nif __name__ == \"__main__\":\n    # better plot this shit.\n    import json\n    if objective == \"continual\":\n        dataDict = json.loads(open(\"pip_meanVariance.json\", \"r\").read())\n    elif objective == 'continual_najie':\n        dataDict = json.loads(open(\"pip_meanVarianceSisterNa.json\", \"r\").read())\n    elif objective == \"discrete\":\n        dataDict = json.loads(open(\"pip_discrete_meanVariance.json\", \"r\").read())",
        "type": "code",
        "location": "/tests/video_detector_tests/pip_meanVariance_stablize.py:359-387"
    },
    "5353": {
        "file_id": 701,
        "content": "This code processes video detector test results and generates a dictionary containing cropped image commands based on the specified objective. It loads data from JSON files depending on the objective (\"continual\", \"continual_najie\", or \"discrete\"). The code then continues to process the resulting data.",
        "type": "comment"
    },
    "5354": {
        "file_id": 701,
        "content": "    else:\n        raise Exception(\"unknown objective: %s\" % objective)\n    # print(len(data)) # 589\n    data = dataDict[\"data\"]\n    defaultWidth, defaultHeight = dataDict[\"width\"], dataDict[\"height\"]\n    if objective in [\"continual\", 'continual_najie']:\n        finalCommandDict = kalmanStablePipRegionExporter(\n            data, defaultWidth, defaultHeight\n        )\n    else:\n        finalCommandDict = sampledStablePipRegionExporter(\n            data, defaultWidth, defaultHeight\n        )\n    import matplotlib.pyplot as plt\n    from matplotlib.patches import Rectangle\n    fig, ax = plt.subplots()\n    def plotRect(ax, x, y, width, height, facecolor):\n        ax.add_patch(\n            Rectangle((x, y), width, height, facecolor=facecolor, fill=True, alpha=0.5)\n        )  # in 0-1\n    ax.plot([[0, 0], [defaultWidth, defaultHeight]])\n    plotRect(ax, 0, 0, defaultWidth, defaultHeight, \"black\")\n    colors = [\"red\", \"yellow\", \"blue\",'orange','white','purple']\n    for index, key in enumerate(finalCommandDict.keys()):\n        import parse",
        "type": "code",
        "location": "/tests/video_detector_tests/pip_meanVariance_stablize.py:388-419"
    },
    "5355": {
        "file_id": 701,
        "content": "This code is handling an objective and preparing a plot for stable PIP region exporter. It raises an exception if the objective is unknown. Depending on the objective, it uses kalmanStablePipRegionExporter or sampledStablePipRegionExporter. It then plots a rectangle on the figure using matplotlib's Rectangle class. Finally, it sets colors for each region in the plot.",
        "type": "comment"
    },
    "5356": {
        "file_id": 701,
        "content": "        commandArguments = parse.parse(\"crop_{x:d}_{y:d}_{w:d}_{h:d}\", key)\n        color = colors[index%len(colors)]\n        rect = [int(commandArguments[name]) for name in [\"x\", \"y\", \"w\", \"h\"]]\n        print(\"RECT\", rect, color, \"SPAN\", finalCommandDict[key])\n        plotRect(ax, *rect, color)\n    # breakpoint()\n    plt.show()",
        "type": "code",
        "location": "/tests/video_detector_tests/pip_meanVariance_stablize.py:421-427"
    },
    "5357": {
        "file_id": 701,
        "content": "The code is creating a rectangular plot using matplotlib. It takes commandArguments as input and uses them to define the x, y, w, and h values of the rectangle. The color of the rectangle is randomly chosen from a list of colors. It then prints the coordinates of the rectangle, the chosen color, and the span of the finalCommandDict[key]. Lastly, it displays the plot using plt.show().",
        "type": "comment"
    },
    "5358": {
        "file_id": 702,
        "content": "/tests/video_detector_tests/motion_gpl.sh",
        "type": "filepath"
    },
    "5359": {
        "file_id": 702,
        "content": "Kills motion process and attempts to capture video from webcam using ffmpeg, then starts motion with the mconfig.conf configuration file. Not suitable for media file processing.",
        "type": "summary"
    },
    "5360": {
        "file_id": 702,
        "content": "killall -s KILL motion\n# ffmpeg -re -i ../../samples/video/LlfeL29BP.mp4 -f v4l2 /dev/video0 &\nmotion -c mconfig.conf\n# to conclude, this is only useful for webcams, not for media file processing.\n# are you sure if you want to capture shits over webcams by this?",
        "type": "code",
        "location": "/tests/video_detector_tests/motion_gpl.sh:1-6"
    },
    "5361": {
        "file_id": 702,
        "content": "Kills motion process and attempts to capture video from webcam using ffmpeg, then starts motion with the mconfig.conf configuration file. Not suitable for media file processing.",
        "type": "comment"
    },
    "5362": {
        "file_id": 703,
        "content": "/tests/video_detector_tests/motion_github.py",
        "type": "filepath"
    },
    "5363": {
        "file_id": 703,
        "content": "This code imports libraries, initializes a motion detector algorithm and sets up video capture. It continuously reads frames from the source, applies an algorithm to create output images, displays them in separate windows, and runs until a frame is not ready or Esc key pressed.",
        "type": "summary"
    },
    "5364": {
        "file_id": 703,
        "content": "# motion detectors are used to track objects. though you may want to separate objects with it.\nimport numpy as np\nimport cv2\nimport pybgs as bgs\n# suspect by static image analysis, and then create bounding box over the thing.\n# check image quality.\nalgorithm = bgs.FrameDifference() # track object we need that.\n# algorithm = bgs.SuBSENSE()\n# video_file = \"../../samples/video/highway_car.avi\"\n# video_file = \"../../samples/video/dog_with_text.mp4\"\nvideo_file = \"../../samples/video/LiEIfnsvn.mp4\" # this one with cropped boundaries. \n# video_file = \"../../samples/video/LlfeL29BP.mp4\"\n# maybe we should consider something else to crop the thing? or not?\n# accumulate the delta over time to see the result?\n# use static detection method.\ncapture = cv2.VideoCapture(video_file)\nwhile not capture.isOpened():\n  capture = cv2.VideoCapture(video_file)\n  cv2.waitKey(1000)\n  print(\"Wait for the header\")\n#pos_frame = capture.get(cv2.cv.CV_CAP_PROP_POS_FRAMES)\n#pos_frame = capture.get(cv2.CV_CAP_PROP_POS_FRAMES)\npos_frame = capture.get(1)",
        "type": "code",
        "location": "/tests/video_detector_tests/motion_github.py:1-29"
    },
    "5365": {
        "file_id": 703,
        "content": "This code imports necessary libraries and initializes a motion detector algorithm (FrameDifference) to track objects in a video. It also defines the video file path and sets up a VideoCapture object. The code waits for the video header, retrieves the current frame position, and is ready to process frames using the motion detection algorithm.",
        "type": "comment"
    },
    "5366": {
        "file_id": 703,
        "content": "while True:\n  flag, frame = capture.read()\n  if flag:\n    cv2.imshow('video', frame)\n    #pos_frame = capture.get(cv2.cv.CV_CAP_PROP_POS_FRAMES)\n    #pos_frame = capture.get(cv2.CV_CAP_PROP_POS_FRAMES)\n    pos_frame = capture.get(1)\n    #print str(pos_frame)+\" frames\"\n    img_output = algorithm.apply(frame)\n    img_bgmodel = algorithm.getBackgroundModel()\n    cv2.imshow('img_output', img_output)\n    cv2.imshow('img_bgmodel', img_bgmodel)\n  else:\n    #capture.set(cv2.cv.CV_CAP_PROP_POS_FRAMES, pos_frame-1)\n    #capture.set(cv2.CV_CAP_PROP_POS_FRAMES, pos_frame-1)\n    #capture.set(1, pos_frame-1)\n    #print \"Frame is not ready\"\n    cv2.waitKey(1000)\n    break\n  if 0xFF & cv2.waitKey(10) == 27:\n    break\n  #if capture.get(cv2.cv.CV_CAP_PROP_POS_FRAMES) == capture.get(cv2.cv.CV_CAP_PROP_FRAME_COUNT):\n  #if capture.get(cv2.CV_CAP_PROP_POS_FRAMES) == capture.get(cv2.CV_CAP_PROP_FRAME_COUNT):\n  #if capture.get(1) == capture.get(cv2.CV_CAP_PROP_FRAME_COUNT):\n    #break\ncv2.destroyAllWindows()",
        "type": "code",
        "location": "/tests/video_detector_tests/motion_github.py:30-62"
    },
    "5367": {
        "file_id": 703,
        "content": "The code continuously reads frames from a video source and displays them. It captures the current frame position, applies an algorithm to create output images, and shows the output and background model images in separate windows. It keeps running until a frame is not ready or the user presses Esc key, closing all windows at the end.",
        "type": "comment"
    },
    "5368": {
        "file_id": 704,
        "content": "/tests/video_detector_tests/mathlib.py",
        "type": "filepath"
    },
    "5369": {
        "file_id": 704,
        "content": "The code uses Sympy to merge overlapping intervals in a list of tuples, creates unified boundaries, and returns final results as merged continual mappings.",
        "type": "summary"
    },
    "5370": {
        "file_id": 704,
        "content": "# not overriding math.\n# do some ranged stuff here...\ndef getContinualNonSympyMergeResult(inputMSetCandidates):\n    # basically the same example.\n    # assume no overlapping here.\n    import sympy\n    def unionToTupleList(myUnion):\n        unionBoundaries = list(myUnion.boundary)\n        unionBoundaries.sort()\n        leftBoundaries = unionBoundaries[::2]\n        rightBoundaries = unionBoundaries[1::2]\n        return list(zip(leftBoundaries, rightBoundaries))\n    def tupleSetToUncertain(mSet):\n        mUncertain = None\n        for start, end in mSet:\n            if mUncertain is None:\n                mUncertain = sympy.Interval(start, end)\n            else:\n                mUncertain += sympy.Interval(start, end)\n        typeUncertain = type(mUncertain)\n        return mUncertain, typeUncertain\n    def mergeOverlappedInIntervalTupleList(intervalTupleList):\n        mUncertain, _ = tupleSetToUncertain(intervalTupleList)\n        mUncertainBoundaryList = list(mUncertain.boundary)\n        mUncertainBoundaryList.sort()",
        "type": "code",
        "location": "/tests/video_detector_tests/mathlib.py:1-28"
    },
    "5371": {
        "file_id": 704,
        "content": "This code defines three functions for set operations involving intervals. The functions are getContinualNonSympyMergeResult, unionToTupleList, and mergeOverlappedInIntervalTupleList. The code uses Sympy library to handle mathematical operations on intervals and merges non-overlapping intervals into a single uncertain variable. It sorts and converts intervals into tuples for further processing.",
        "type": "comment"
    },
    "5372": {
        "file_id": 704,
        "content": "        mergedIntervalTupleList = list(\n            zip(mUncertainBoundaryList[::2], mUncertainBoundaryList[1::2])\n        )\n        return mergedIntervalTupleList\n    # mSet = mergeOverlappedInIntervalTupleList([(0, 1), (2, 3)])\n    # mSet2 = mergeOverlappedInIntervalTupleList([(0.5, 1.5), (1.6, 2.5)])\n    # print(\"MSET\", mSet)\n    # print(\"MSET2\", mSet2)\n    mSetCandidates = [\n        mergeOverlappedInIntervalTupleList(x) for x in inputMSetCandidates\n    ]\n    mSetUnified = [x for y in mSetCandidates for x in y]\n    leftBoundaryList = set([x[0] for x in mSetUnified])\n    rightBoundaryList = set([x[1] for x in mSetUnified])\n    # they may freaking overlap.\n    # if want nearby-merge strategy, simply just expand all intervals, merge them with union and shrink the individual intervals inside union respectively.\n    markers = {\n        \"enter\": {k: [] for k in leftBoundaryList},\n        \"exit\": {k: [] for k in rightBoundaryList},\n    }\n    for index, mSetCandidate in enumerate(mSetCandidates):\n        leftBoundaryListOfCandidate = [x[0] for x in mSetCandidate]",
        "type": "code",
        "location": "/tests/video_detector_tests/mathlib.py:29-55"
    },
    "5373": {
        "file_id": 704,
        "content": "This code defines a function `mergeOverlappedInIntervalTupleList` which takes a list of interval tuples, merges overlapping intervals, and returns the merged list. The main purpose is to unify all the data in the same scope with potential overlap. It first creates a set of left and right boundaries from the unified data, then initializes a `markers` dictionary with \"enter\" and \"exit\" markers for each left boundary. Finally, it iterates over each candidate set, extracting the left boundaries and using them to update the marker dictionary. The final merged interval tuples are not explicitly calculated or returned here, but can be derived from the information in `markers`.",
        "type": "comment"
    },
    "5374": {
        "file_id": 704,
        "content": "        rightBoundaryListOfCandidate = [x[1] for x in mSetCandidate]\n        for leftBoundaryOfCandidate in leftBoundaryListOfCandidate:\n            markers[\"enter\"][leftBoundaryOfCandidate].append(index)  # remap this thing!\n        for rightBoundaryOfCandidate in rightBoundaryListOfCandidate:\n            markers[\"exit\"][rightBoundaryOfCandidate].append(index)  # remap this thing!\n    # now, iterate through the boundaries of mSetUnified.\n    unifiedBoundaryList = leftBoundaryList.union(\n        rightBoundaryList\n    )  # call me a set instead of a list please? now we must sort this thing\n    unifiedBoundaryList = list(unifiedBoundaryList)\n    unifiedBoundaryList.sort()\n    unifiedBoundaryMarks = {}\n    finalMappings = {}\n    # print(\"MARKERS\", markers)\n    # breakpoint()\n    for index, boundary in enumerate(unifiedBoundaryList):\n        previousMark = unifiedBoundaryMarks.get(index - 1, [])\n        enterList = markers[\"enter\"].get(boundary, [])\n        exitList = markers[\"exit\"].get(boundary, [])\n        currentMark = set(previousMark + enterList).difference(set(exitList))",
        "type": "code",
        "location": "/tests/video_detector_tests/mathlib.py:56-77"
    },
    "5375": {
        "file_id": 704,
        "content": "This code creates a set of unified boundaries and maps the markers accordingly. It first gathers the \"enter\" and \"exit\" markers for each boundary, then forms the final mappings by taking the difference between the \"enter\" and \"exit\" lists. The code also sorts the boundaries and retrieves previous marks to form the current mark set.",
        "type": "comment"
    },
    "5376": {
        "file_id": 704,
        "content": "        currentMark = list(currentMark)\n        unifiedBoundaryMarks.update({index: currentMark})\n        # now, handle the change? or not?\n        # let's just deal those empty ones, shall we?\n        if previousMark == []:  # inside it is empty range.\n            # elif currentMark == []:\n            if index == 0:\n                continue  # just the start, no need to note this down.\n            else:\n                finalMappings.update(\n                    {\n                        \"empty\": finalMappings.get(\"empty\", [])\n                        + [(unifiedBoundaryList[index - 1], boundary)]\n                    }\n                )\n            # the end of previous mark! this interval belongs to previousMark\n        else:\n            key = previousMark.copy()\n            key.sort()\n            key = tuple(key)\n            finalMappings.update(\n                {\n                    key: finalMappings.get(key, [])\n                    + [(unifiedBoundaryList[index - 1], boundary)]\n                }\n            )",
        "type": "code",
        "location": "/tests/video_detector_tests/mathlib.py:78-103"
    },
    "5377": {
        "file_id": 704,
        "content": "This code checks if the current mark is empty and updates the finalMappings accordingly. If previousMark is empty, it skips noting down just the start of a range. Otherwise, it sorts and makes a unique key using previousMark, then adds the interval to finalMappings for that key.",
        "type": "comment"
    },
    "5378": {
        "file_id": 704,
        "content": "            # also the end of previous mark! belongs to previousMark.\n    ### NOW THE FINAL OUTPUT ###\n    finalCats = {}\n    for key, value in finalMappings.items():\n        # value is an array containing subInterval tuples.\n        value = mergeOverlappedInIntervalTupleList(value)\n        finalCats.update({key: value})\n    # print(\"______________FINAL CATS______________\")\n    # print(finalCats)\n    return finalCats\ndef getContinualMappedNonSympyMergeResult(mRangesDict, concatSymbol=\"|\", noEmpty=True):\n    mKeyMaps = list(mRangesDict.keys())\n    mSetCandidates = [mRangesDict[key] for key in mKeyMaps]\n    # the next step will automatically merge all overlapped candidates.\n    finalCats = getContinualNonSympyMergeResult(mSetCandidates)\n    finalCatsMapped = {\n        concatSymbol.join([mKeyMaps[k] for k in mTuple]): finalCats[mTuple]\n        for mTuple in finalCats.keys()\n        if type(mTuple) == tuple\n    }\n    if not noEmpty:\n        finalCatsMapped.update(\n            {k: finalCats[k] for k in finalCats.keys() if type(k) != tuple}",
        "type": "code",
        "location": "/tests/video_detector_tests/mathlib.py:104-130"
    },
    "5379": {
        "file_id": 704,
        "content": "This code calculates merged, continual results for a dictionary of sets. It maps the results to a format using a specified concatenation symbol, and allows for empty sets if requested. It uses functions like getContinualNonSympyMergeResult and mergeOverlappedInIntervalTupleList to merge overlapping intervals. The final result is returned as a dictionary of merged continual mappings.",
        "type": "comment"
    },
    "5380": {
        "file_id": 704,
        "content": "        )\n    return finalCatsMapped\n    # default not to output empty set?\ndef getContinualMappedNonSympyMergeResultWithRangedEmpty(\n    mRangesDict, start, end, concatSymbol=\"|\"\n):\n    import uuid\n    emptySetName = str(uuid.uuid4())\n    newRangesDict = mRangesDict.copy()\n    newRangesDict.update({emptySetName: [(start, end)]})\n    newRangesDict = getContinualMappedNonSympyMergeResult(\n        newRangesDict, concatSymbol=\"|\", noEmpty=True\n    )\n    newRangesDict = {\n        key: [\n            (mStart, mEnd)\n            for mStart, mEnd in newRangesDict[key]\n            if mStart >= start and mEnd <= end\n        ]\n        for key in newRangesDict.keys()\n    }\n    newRangesDict = {\n        key: newRangesDict[key]\n        for key in newRangesDict.keys()\n        if newRangesDict[key] != []\n    }\n    finalNewRangesDict = {}\n    for key in newRangesDict.keys():\n        mergedEmptySetName = \"{}{}\".format(concatSymbol, emptySetName)\n        if mergedEmptySetName in key:\n            newKey = key.replace(mergedEmptySetName,\"\")",
        "type": "code",
        "location": "/tests/video_detector_tests/mathlib.py:131-164"
    },
    "5381": {
        "file_id": 704,
        "content": "Function to get a continual mapped non-Sympy merge result with range based on input parameters. It creates a new dictionary with an empty set named UUID, then updates the existing dictionary with this new one. Filters out any ranges that do not fall within the given start and end values. Removes any keys in the newRangesDict dictionary if their corresponding value is an empty list. Finally, iterates over each key in newRangesDict and checks if mergedEmptySetName exists; if it does, it replaces the key with an empty string.",
        "type": "comment"
    },
    "5382": {
        "file_id": 704,
        "content": "            finalNewRangesDict.update({newKey:newRangesDict[key]})\n        elif key == emptySetName:\n            finalNewRangesDict.update({'empty':newRangesDict[key]})\n        else:\n            finalNewRangesDict.update({key:newRangesDict[key]})\n    return finalNewRangesDict\ndef mergedRangesToSequential(renderDict):\n    renderList = []\n    for renderCommandString in renderDict.keys():\n        commandTimeSpans = renderDict[renderCommandString].copy()\n        # commandTimeSpan.sort(key=lambda x: x[0])\n        for commandTimeSpan in commandTimeSpans:\n            renderList.append([renderCommandString, commandTimeSpan].copy())\n    renderList.sort(key=lambda x: x[1][0])\n    return renderList\n    # for renderCommandString, commandTimeSpan in renderList:\n    #     print(renderCommandString, commandTimeSpan)\ndef sequentialToMergedRanges(sequence):\n    mergedRanges = {}\n    for commandString, commandTimeSpan in sequence:\n        mergedRanges.update({commandString: mergedRanges.get(commandString,[])+[commandTimeSpan]})",
        "type": "code",
        "location": "/tests/video_detector_tests/mathlib.py:165-188"
    },
    "5383": {
        "file_id": 704,
        "content": "Function `mergedRangesToSequential` takes a dictionary where keys are commands and values are time spans, sorts them by start time in ascending order, and returns the sorted list of commands with their respective time spans.\n\nFunction `sequentialToMergedRanges` takes a list of command strings and their corresponding start times, groups them by command string, and produces a dictionary with commands as keys and lists of time spans as values.",
        "type": "comment"
    },
    "5384": {
        "file_id": 704,
        "content": "    mergedRanges = getContinualMappedNonSympyMergeResult(mergedRanges)\n    return mergedRanges",
        "type": "code",
        "location": "/tests/video_detector_tests/mathlib.py:189-190"
    },
    "5385": {
        "file_id": 704,
        "content": "This code block retrieves the merged ranges of continuous numbers using getContinualMappedNonSympyMergeResult function and assigns it to variable 'mergedRanges'. Finally, it returns the mergedRanges.",
        "type": "comment"
    },
    "5386": {
        "file_id": 705,
        "content": "/tests/video_detector_tests/rectangle_test.py",
        "type": "filepath"
    },
    "5387": {
        "file_id": 705,
        "content": "This code imports OpenCV libraries, sets up a motion detector algorithm for object tracking and suggests improvements. It reads a video file, applies the algorithm to each frame, finds bounding boxes, handles stability issues, displays images, stores results in JSON format and prints \"DATA DUMPED\" upon successful execution.",
        "type": "summary"
    },
    "5388": {
        "file_id": 705,
        "content": "# motion detectors are used to track objects. though you may want to separate objects with it.\nimport progressbar\nimport json\nimport pybgs as bgs\nimport numpy as np\nimport pathlib\nimport sys\nsite_path = pathlib.Path(\"/usr/local/lib/python3.9/site-packages\")\ncv2_libs_dir = site_path / 'cv2' / \\\n    f'python-{sys.version_info.major}.{sys.version_info.minor}'\nprint(cv2_libs_dir)\ncv2_libs = sorted(cv2_libs_dir.glob(\"*.so\"))\nif len(cv2_libs) == 1:\n    print(\"INSERTING:\", cv2_libs[0].parent)\n    sys.path.insert(1, str(cv2_libs[0].parent))\nimport cv2\n# suspect by static image analysis, and then create bounding box over the thing.\n# check image quality.\n# for donga, you must change the framerate to skip identical frames.\n# also donga have strange things you may dislike, e.g.: when only part of the image changes.\n# algorithm = bgs.FrameDifference() # this is not stable since we have more boundaries. shall we group things?\n# can we use something else?\nalgorithm = bgs.WeightedMovingVariance()\n# this one with cropped boundaries.",
        "type": "code",
        "location": "/tests/video_detector_tests/rectangle_test.py:1-31"
    },
    "5389": {
        "file_id": 705,
        "content": "This code imports necessary libraries, checks the OpenCV library version, and sets up a motion detector algorithm (WeightedMovingVariance) for object tracking. It also suggests possible improvements like grouping boundaries or using a different algorithm if needed.",
        "type": "comment"
    },
    "5390": {
        "file_id": 705,
        "content": "# average shit.\n# video_file = \"../../samples/video/LiEIfnsvn.mp4\"\n# select our \nvideo_file = \"../../samples/video/LiGlReJ4i.mp4\"\n# video_file = \"../../samples/video/LiEIfnsvn.mp4\"\n# denoising, moving average, sampler and  similar merge.\n# moving average span: -20 frame to +20 frame\n# denoising: \ncapture = cv2.VideoCapture(video_file)\nwhile not capture.isOpened():\n    capture = cv2.VideoCapture(video_file)\n    cv2.waitKey(1000)\n    print(\"Wait for the header\")\ndefaultWidth = capture.get(cv2.CAP_PROP_FRAME_WIDTH)\ndefaultHeight = capture.get(cv2.CAP_PROP_FRAME_HEIGHT)\ntotal_frames = capture.get(cv2.CAP_PROP_FRAME_COUNT)\ntotal_frames = int(total_frames)\npipFrames = []\ndefaultRect = [(0,0),(defaultWidth,defaultHeight)]\npos_frame = capture.get(1)\nareaThreshold = int(0.2*0.2*defaultWidth*defaultHeight)\nfor index in progressbar.progressbar(range(total_frames)):\n    # if index % 20 != 0: continue\n    flag, frame = capture.read()\n    if flag:\n        pos_frame = capture.get(1)\n        img_output = algorithm.apply(frame)",
        "type": "code",
        "location": "/tests/video_detector_tests/rectangle_test.py:32-66"
    },
    "5391": {
        "file_id": 705,
        "content": "This code reads a video file and applies an algorithm to each frame. It also retrieves the frame width, height, total frames, and uses a progress bar for looping through each frame. The code has a skipping mechanism for every 20th frame and a threshold for area calculations.",
        "type": "comment"
    },
    "5392": {
        "file_id": 705,
        "content": "        imgThresh = img_output\n        # imgMorph = cv2.GaussianBlur(img_output, (3,3), 0)\n        # _,imgThresh = cv2.threshold(imgMorph, 1, 255, cv2.THRESH_BINARY)\n        # img_bgmodel = algorithm.getBackgroundModel()\n        # _, contours = cv2.findContours(\n        #     imgThresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n        # maybe you should merge all active areas.\n        # if contours is not None:\n            # continue\n            # counted = False\n            # maxArea = 0\n            # for contour in contours:\n        [x, y, w, h] = cv2.boundingRect(img_output) # wtf is this?\n        area = w*h\n        if area > areaThreshold:\n                # #     maxArea = area\n                # if counted==False:\n            min_x, min_y = x, y\n            max_x, max_y = x+w, y+h\n                # else:\n                #     if x<min_x: min_x = x\n                #     if x+w>max_x: max_x = x+w\n                #     if y<min_y: min_y = y\n                #     if y+w>max_y: max_y = y+w\n            currentRect = [(min_x, min_y), (max_x, max_y)]",
        "type": "code",
        "location": "/tests/video_detector_tests/rectangle_test.py:67-91"
    },
    "5393": {
        "file_id": 705,
        "content": "This code segment is responsible for finding the bounding box around objects in an image. The variables `x`, `y`, `w`, and `h` are the coordinates of the rectangle's top left corner, width, and height respectively. It uses a Gaussian blur on the image and applies a binary threshold to isolate objects. It then finds contours, checks if they exceed a certain area threshold, and updates the minimum and maximum coordinates of the bounding box accordingly.",
        "type": "comment"
    },
    "5394": {
        "file_id": 705,
        "content": "            pipFrames.append(currentRect.copy())\n            defaultRect = currentRect.copy()\n        else:\n            pipFrames.append(defaultRect.copy())\n            # how to stablize this shit?\n        # cv2.imshow('video', frame)\n        # cv2.imshow('img_output', img_output)\n        # cv2.imshow('img_bgmodel', img_bgmodel)\n        # cv2.imshow('imgThresh', imgThresh)\n        # cv2.waitKey(100)\n    else:\n        # cv2.waitKey(1000)\n        break\ncv2.destroyAllWindows()\n# we process this shit elsewhere.\nwith open(\"pip_meanVarianceSisterNa.json\", 'w') as f:\n    f.write(json.dumps(\n        {\"data\": pipFrames, \"width\": defaultWidth, \"height\": defaultHeight}))\nprint(\"DATA DUMPED\")",
        "type": "code",
        "location": "/tests/video_detector_tests/rectangle_test.py:92-116"
    },
    "5395": {
        "file_id": 705,
        "content": "Code captures frames, appends rectangles to pipFrames, and handles stability issues. It displays images on various windows using OpenCV. Breaks loop if no changes detected. Closes all windows after processing. Writes pipFrames data to a JSON file named \"pip_meanVarianceSisterNa.json\" with width and height information. Prints \"DATA DUMPED\" upon successful execution.",
        "type": "comment"
    },
    "5396": {
        "file_id": 706,
        "content": "/tests/video_detector_tests/rect_detect.py",
        "type": "filepath"
    },
    "5397": {
        "file_id": 706,
        "content": "The code reads video files, performs object tracking/recognition, and detects lines using OpenCV. It calculates mean differences, updates rectangles, applies edge detection and morphological operations, displays results, and saves as houghlines.jpg.",
        "type": "summary"
    },
    "5398": {
        "file_id": 706,
        "content": "import cv2\nimport numpy as np\nimport itertools\nimport uuid\n# Reading the required image in\n# which operations are to be done.\n# Make sure that the image is in the same\n# directory in which this python program is\n# video_file = \"../../samples/video/LiEIfnsvn.mp4\" # this one with cropped boundaries.\nvideo_file = \"/media/root/help1/pyjom/samples/video/LiGE8vLuX.mp4\"\nvideo = cv2.VideoCapture(video_file)\ndef rectMerge(oldRect, newRect,delta_thresh = 0.1):\n    # if very much alike, we merge these rects.\n    # what about those rect that overlaps? we check exactly those who overlaps.\n    # 1. check all new rects against all old rects. if they overlap, highly alike (or not) then mark it as having_alike_rect (or not) and append to new old rect list. <- after those old rects have been marked with alike sign, one cannot revoke the sign. still remaining new rects will be checked against them.\n    # 2. while checking, if not very alike then append newRect to new rect list.\n    # 3. if one old rect has not yet been ",
        "type": "code",
        "location": "/tests/video_detector_tests/rect_detect.py:1-20"
    },
    "5399": {
        "file_id": 706,
        "content": "The code reads an input video file and defines a function rectMerge that takes two rectangles as parameters and determines if they are highly alike. It checks all new rectangles against all old rectangles to find overlapping ones and marks them as having_alike_rect before proceeding with the remaining new rectangles. The purpose is likely for object tracking or recognition within a video stream.",
        "type": "comment"
    }
}