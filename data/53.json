{
    "5300": {
        "file_id": 687,
        "content": "        \"\"\"\n        功能：识别文本中的命名实体：地名，组织名和机构名\n        参数repead：表示是否进行去重处理 ，默认是不去重\n        参数Entity_dist：表示每个文本，返回的识别后的列表，还是抽取后的实体字典，默认返回的是列表\n        返回值的形式：1.[[['word1',u'O'],['word2',u'O'],['word3',u'O']],[['word2',u'O'],['word3',u'O'],['word4',u'O']],……]\n                        2.[{'person':[],'place':[],'organization':[]},{'person':[],'place':[],'organization':[]},{'person':[],'place':[],'organization':[]},……]\n        \"\"\"\n        words_list, postags_list = self.postagger(input_list, return_words_list=True)\n        entity_text_list = []\n        for words, postags in zip(words_list, postags_list):\n            netags = self.recognizer.recognize(\n                words, postags\n            )  # 命名实体识别 人名（Nh）、地名（Ns）、机构名（Ni）\n            text = list(zip(words, netags))\n            entity_text_list.append(text)\n        if Entity_dist:\n            extract_entity_list = []\n            for words_entity_note_list in entity_text_list:\n                extract_entity_list.append(\n                    self.get_entity_dict(words_entity_note_list, repead)",
        "type": "code",
        "location": "/tests/title_cover_generator/pyltp_server.py:91-113"
    },
    "5301": {
        "file_id": 687,
        "content": "This code snippet is responsible for identifying named entities in a given text, such as person names, place names, and organization names. It uses the postagger to identify words and their parts of speech (POS) and then applies the recognizer to recognize named entities based on these POS tags. If Entity_dist is set to True, it extracts entities into a dictionary format.",
        "type": "comment"
    },
    "5302": {
        "file_id": 687,
        "content": "                )\n            return extract_entity_list\n        else:\n            return entity_text_list\n    def get_entity_dict(self, words_entity_note_list, repead):\n        \"\"\"\n        功能：根据实体识别的标志，统计文本中的命名实体\n        参数repead：表示是否进行去重处理 ，默认是不去重\n        返回值：{'person':[],'place':[],'organization':[]}\n        \"\"\"\n        \"\"\"\n        O：这个词不是NE\n        S：这个词单独构成一个NE\n        B：这个词为一个NE的开始\n        I：这个词为一个NE的中间\n        E：这个词位一个NE的结尾\n        Nh：人名\n        Ni：机构名\n        Ns：地名\n        \"\"\"\n        name_entity_dist = {}\n        # 存储不同实体的列表\n        name_entity_list = []\n        place_entity_list = []\n        organization_entity_list = []\n        ntag_E_Nh = \"\"\n        ntag_E_Ni = \"\"\n        ntag_E_Ns = \"\"\n        for word, ntag in words_entity_note_list:\n            # print word+\"/\"+ntag,\n            if ntag[0] != \"O\":\n                if ntag[0] == \"S\":\n                    if ntag[-2:] == \"Nh\":\n                        name_entity_list.append(word)\n                    elif ntag[-2:] == \"Ni\":\n                        organization_entity_list.append(word)",
        "type": "code",
        "location": "/tests/title_cover_generator/pyltp_server.py:114-151"
    },
    "5303": {
        "file_id": 687,
        "content": "This code segment is part of a class method that identifies and categorizes named entities such as persons, places, and organizations from a given list. The code iterates through the list of words along with their corresponding entity tags (O, S, B, I, E) and adds them to separate lists based on the type of entity they represent. If the repead parameter is True, it performs deduplication on the final result.",
        "type": "comment"
    },
    "5304": {
        "file_id": 687,
        "content": "                    else:\n                        place_entity_list.append(word)\n                elif ntag[0] == \"B\":\n                    if ntag[-2:] == \"Nh\":\n                        ntag_E_Nh = ntag_E_Nh + word\n                    elif ntag[-2:] == \"Ni\":\n                        ntag_E_Ni = ntag_E_Ni + word\n                    else:\n                        ntag_E_Ns = ntag_E_Ns + word\n                elif ntag[0] == \"I\":\n                    if ntag[-2:] == \"Nh\":\n                        ntag_E_Nh = ntag_E_Nh + word\n                    elif ntag[-2:] == \"Ni\":\n                        ntag_E_Ni = ntag_E_Ni + word\n                    else:\n                        ntag_E_Ns = ntag_E_Ns + word\n                else:\n                    if ntag[-2:] == \"Nh\":\n                        ntag_E_Nh = ntag_E_Nh + word\n                        name_entity_list.append(ntag_E_Nh)\n                        ntag_E_Nh = \"\"\n                    elif ntag[-2:] == \"Ni\":\n                        ntag_E_Ni = ntag_E_Ni + word\n                        organization_entity_list.append(ntag_E_Ni)",
        "type": "code",
        "location": "/tests/title_cover_generator/pyltp_server.py:152-175"
    },
    "5305": {
        "file_id": 687,
        "content": "The code is segmenting named entities (name and organization) using the NER (Named Entity Recognition) model. It appends words to separate variables based on their tags, forming name and organization lists when encountering \"Nh\" or \"Ni\". If no entity is detected, it simply adds the word to the place entity list.",
        "type": "comment"
    },
    "5306": {
        "file_id": 687,
        "content": "                        ntag_E_Ni = \"\"\n                    else:\n                        ntag_E_Ns = ntag_E_Ns + word\n                        place_entity_list.append(ntag_E_Ns)\n                        ntag_E_Ns = \"\"\n        if repead:\n            name_entity_dist[\"person\"] = list(set(name_entity_list))\n            name_entity_dist[\"organization\"] = list(set(organization_entity_list))\n            name_entity_dist[\"place\"] = list(set(place_entity_list))\n        else:\n            name_entity_dist[\"person\"] = name_entity_list\n            name_entity_dist[\"organization\"] = organization_entity_list\n            name_entity_dist[\"place\"] = place_entity_list\n        return name_entity_dist\n    def SyntaxParser(self, input_list, return_words_pos=False):\n        \"\"\"\n        # head = parent+1\n        # relation = relate  可以从中间抽取head 和 relation 构成LTP 的标准输出，但是为了根据自己的情况，直接输出返回的全部的信息\n        功能：实现依存句法分析\n        返回值：每个文本的形成一个列表\n        [[{u'relate': u'WP', u'cont': u'\\uff0c', u'id': 4, u'parent': 3, u'pos': u'wp'},{u'relate': u'RAD', u'cont': u'\\u7684', u'id': 1, u'parent': 0, u'pos': u'u'}],……]",
        "type": "code",
        "location": "/tests/title_cover_generator/pyltp_server.py:176-198"
    },
    "5307": {
        "file_id": 687,
        "content": "This code defines a function `name_entity_dist` that handles named entity recognition and extraction. It identifies named entities (person, organization, place) and stores them in separate lists. The function then adds these lists to a dictionary called `name_entity_dist`, which is returned at the end. Additionally, there's another function `SyntaxParser` that performs dependency syntax parsing on the input list and returns a list of parsed relations between words.",
        "type": "comment"
    },
    "5308": {
        "file_id": 687,
        "content": "        \"\"\"\n        words_list, postags_list = self.postagger(input_list, return_words_list=True)\n        syntaxparser_text_list = []\n        for words, postags in zip(words_list, postags_list):\n            arcs = self.parser.parse(words, postags)  # 句法分析\n            # res = [(arc.head, arc.relation) for arc in arcs]\n            res = [arc for arc in arcs] # arguable.\n            # for arc in arcs:\n            #     print(arc)\n            # breakpoint()\n            text = []\n            for i in range(len(words)):\n                tt = {\n                    \"id\": i,\n                    \"cont\": words[i],\n                    \"pos\": postags[i],\n                    \"parent\": res[i][0],\n                    \"relate\": res[i][1],\n                }\n                text.append(tt)\n            syntaxparser_text_list.append(text)\n        if return_words_pos:\n            return words_list, postags_list, syntaxparser_text_list\n        else:\n            return syntaxparser_text_list\n    def triple_extract(self, intput_list):\n        \"\"\"",
        "type": "code",
        "location": "/tests/title_cover_generator/pyltp_server.py:199-229"
    },
    "5309": {
        "file_id": 687,
        "content": "The code is performing syntax parsing using a parser. It takes an input list of words and their corresponding part-of-speech tags, then applies the parser to generate a syntactic parse tree for each word in the input list. The resulting parse trees are stored as a list of dictionaries with information about each word's id, content, part-of-speech tag, parent, and relation. If 'return_words_pos' is True, it returns the words list, postags list, and syntaxparser_text_list. Otherwise, it only returns the syntaxparser_text_list.",
        "type": "comment"
    },
    "5310": {
        "file_id": 687,
        "content": "        功能: 对于给定的句子进行事实三元组抽取\n        Args:\n            sentence: 要处理的语句\n                        形式是：'真实的句子'\n        \"\"\"\n        Subjective_guest = []  # 主谓宾关系(e1,r,e2)\n        Dynamic_relation = []  # 动宾关系\n        Guest = []  # 介宾关系\n        Name_entity_relation = []  # 命名实体之间的关系\n        # 分词后词的列表 words，词性列表 postags，实体标志列表 netags，语法分析列表 arcs\n        words = []\n        postags = []\n        netags = []\n        arcs = []\n        syntaxparser_text_list = self.SyntaxParser(intput_list)\n        entity_list = self.NamedEntityRecognizer(intput_list)\n        for words_property_list in syntaxparser_text_list[0]:\n            words.append(words_property_list[\"cont\"])\n            postags.append(words_property_list[\"pos\"])\n            arcs.append(\n                {\n                    \"head\": words_property_list[\"parent\"],\n                    \"relation\": words_property_list[\"relate\"],\n                }\n            )\n        for words_entity_list in entity_list[0]:\n            netags.append(words_entity_list[1])\n        child_dict_list = self.build_parse_child_dict(words, postags, arcs)",
        "type": "code",
        "location": "/tests/title_cover_generator/pyltp_server.py:230-258"
    },
    "5311": {
        "file_id": 687,
        "content": "This function performs triplet extraction for a given sentence. It initializes various lists for different relationships and then extracts words, postags, arcs (syntax), and netags (named entities) using the input list. Finally, it builds a dictionary of child relationships from the extracted data.",
        "type": "comment"
    },
    "5312": {
        "file_id": 687,
        "content": "        for index in range(len(postags)):\n            # 抽取以谓词为中心的事实三元组\n            if postags[index] == \"v\":\n                child_dict = child_dict_list[index]\n                # 主谓宾\n                if \"SBV\" in child_dict and \"VOB\" in child_dict:\n                    e1 = self.complete_e(\n                        words, postags, child_dict_list, child_dict[\"SBV\"][0]\n                    )\n                    r = words[index]\n                    e2 = self.complete_e(\n                        words, postags, child_dict_list, child_dict[\"VOB\"][0]\n                    )\n                    Subjective_guest.append((e1, r, e2))\n                # 定语后置，动宾关系\n                if arcs[index][\"relation\"] == \"ATT\":\n                    if \"VOB\" in child_dict:\n                        e1 = self.complete_e(\n                            words, postags, child_dict_list, arcs[index][\"head\"] - 1\n                        )\n                        r = words[index]\n                        e2 = self.complete_e(\n                            words, postags, child_dict_list, child_dict[\"VOB\"][0]",
        "type": "code",
        "location": "/tests/title_cover_generator/pyltp_server.py:260-284"
    },
    "5313": {
        "file_id": 687,
        "content": "This code is extracting subject-predicate-object (SPO) triples from a natural language sentence using PyLTP library. It identifies the verb as the center of the triple and checks for two possible structures: \"SBV\" followed by \"VOB\" or \"ATT\" relation after the verb. The code fills in the subject, predicate, and object entities based on the identified positions in the sentence.",
        "type": "comment"
    },
    "5314": {
        "file_id": 687,
        "content": "                        )\n                        temp_string = r + e2\n                        if temp_string == e1[: len(temp_string)]:\n                            e1 = e1[len(temp_string) :]\n                        if temp_string not in e1:\n                            Dynamic_relation.append((e1, r, e2))\n                # 含有介宾关系的主谓动补关系\n                if \"SBV\" in child_dict and \"CMP\" in child_dict:\n                    # e1 = words[child_dict['SBV'][0]]\n                    e1 = self.complete_e(\n                        words, postags, child_dict_list, child_dict[\"SBV\"][0]\n                    )\n                    cmp_index = child_dict[\"CMP\"][0]\n                    r = words[index] + words[cmp_index]\n                    if \"POB\" in child_dict_list[cmp_index]:\n                        e2 = self.complete_e(\n                            words,\n                            postags,\n                            child_dict_list,\n                            child_dict_list[cmp_index][\"POB\"][0],\n                        )",
        "type": "code",
        "location": "/tests/title_cover_generator/pyltp_server.py:285-306"
    },
    "5315": {
        "file_id": 687,
        "content": "This code checks for a specific relationship between the subject, verb, object, and complement in a sentence. It appends the relationship (e1, r, e2) to Dynamic_relation if it meets certain conditions such as not containing an existing temporary string or being part of the original text.",
        "type": "comment"
    },
    "5316": {
        "file_id": 687,
        "content": "                        Guest.append((e1, r, e2))\n            # 尝试抽取命名实体有关的三元组\n            if netags[index][0] == \"S\" or netags[index][0] == \"B\":\n                ni = index\n                if netags[ni][0] == \"B\":\n                    while netags[ni][0] != \"E\":\n                        ni += 1\n                    e1 = \"\".join(words[index : ni + 1])\n                else:\n                    e1 = words[ni]\n                # 上面是抽取实体，没有判断是什么类型的实体。。\n                if (\n                    arcs[ni][\"relation\"] == \"ATT\"\n                    and postags[arcs[ni][\"head\"] - 1] == \"n\"\n                    and netags[arcs[ni][\"head\"] - 1] == \"O\"\n                ):\n                    r = self.complete_e(\n                        words, postags, child_dict_list, arcs[ni][\"head\"] - 1\n                    )\n                    if e1 in r:\n                        r = r[(r.index(e1) + len(e1)) :]\n                    if (\n                        arcs[arcs[ni][\"head\"] - 1][\"relation\"] == \"ATT\"\n                        and netags[arcs[arcs[ni][\"head\"] - 1][\"head\"] - 1] != \"O\"",
        "type": "code",
        "location": "/tests/title_cover_generator/pyltp_server.py:307-331"
    },
    "5317": {
        "file_id": 687,
        "content": "This code attempts to extract named entity triples. It checks if the current tag is a start or begin tag, and then extracts the named entity based on that. If it meets specific conditions involving \"ATT\" relation and certain postags, it completes the entity and checks if the extracted entity is in the result.",
        "type": "comment"
    },
    "5318": {
        "file_id": 687,
        "content": "                    ):\n                        e2 = self.complete_e(\n                            words,\n                            postags,\n                            child_dict_list,\n                            arcs[arcs[ni][\"head\"] - 1][\"head\"] - 1,\n                        )\n                        mi = arcs[arcs[ni][\"head\"] - 1][\"head\"] - 1\n                        li = mi\n                        if netags[mi][0] == \"B\":\n                            while netags[mi][0] != \"E\":\n                                mi += 1\n                            e = \"\".join(words[li + 1 : mi + 1])\n                            e2 += e\n                        if r in e2:\n                            e2 = e2[(e2.index(r) + len(r)) :]\n                        if r + e2 in sentence:\n                            Name_entity_relation.append((e1, r, e2))\n        return Subjective_guest, Dynamic_relation, Guest, Name_entity_relation\n    def build_parse_child_dict(self, words, postags, arcs):\n        \"\"\"\n        功能：为句子中的每个词语维护一个保存句法依存儿子节点的字典",
        "type": "code",
        "location": "/tests/title_cover_generator/pyltp_server.py:332-354"
    },
    "5319": {
        "file_id": 687,
        "content": "The code defines a function called `build_parse_child_dict` which takes in the words, postags, and arcs of a sentence. It creates a dictionary for each word in the sentence that stores its syntactic dependency children. If a relation word exists between two named entities, it is added to the Name_entity_relation list. The function returns four variables: Subjective_guest, Dynamic_relation, Guest, and Name_entity_relation",
        "type": "comment"
    },
    "5320": {
        "file_id": 687,
        "content": "        Args:\n            words: 分词列表\n            postags: 词性列表\n            arcs: 句法依存列表\n        \"\"\"\n        child_dict_list = []\n        for index in range(len(words)):\n            child_dict = dict()\n            for arc_index in range(len(arcs)):\n                if arcs[arc_index][\"head\"] == index + 1:\n                    if arcs[arc_index][\"relation\"] in child_dict:\n                        child_dict[arcs[arc_index][\"relation\"]].append(arc_index)\n                    else:\n                        child_dict[arcs[arc_index][\"relation\"]] = []\n                        child_dict[arcs[arc_index][\"relation\"]].append(arc_index)\n            child_dict_list.append(child_dict)\n        return child_dict_list\n    def complete_e(self, words, postags, child_dict_list, word_index):\n        \"\"\"\n        功能：完善识别的部分实体\n        \"\"\"\n        child_dict = child_dict_list[word_index]\n        prefix = \"\"\n        if \"ATT\" in child_dict:\n            for i in range(len(child_dict[\"ATT\"])):\n                prefix += self.complete_e(\n                    words, postags, child_dict_list, child_dict[\"ATT\"][i]",
        "type": "code",
        "location": "/tests/title_cover_generator/pyltp_server.py:355-383"
    },
    "5321": {
        "file_id": 687,
        "content": "This function takes in a list of words, their respective parts of speech (postags), and syntactic dependency relations (arcs) as input. It organizes the arcs into a dictionary structure for each word in the list, and returns this dictionary list. The next function aims to further refine or \"complete\" part of the identified entities by recursively calling itself with the appropriate parameters.",
        "type": "comment"
    },
    "5322": {
        "file_id": 687,
        "content": "                )\n        postfix = \"\"\n        if postags[word_index] == \"v\":\n            if \"VOB\" in child_dict:\n                postfix += self.complete_e(\n                    words, postags, child_dict_list, child_dict[\"VOB\"][0]\n                )\n            if \"SBV\" in child_dict:\n                prefix = (\n                    self.complete_e(\n                        words, postags, child_dict_list, child_dict[\"SBV\"][0]\n                    )\n                    + prefix\n                )\n        return prefix + words[word_index] + postfix\nif __name__ == \"__main__\":\n    # intput_list = [\"中国自称为炎黄子孙、龙的传人\"]\n    # incorrect name spliters.\n    from commons import sample_data\n    intput_list = sample_data\n    model = LTP_MODEL()\n    input_sentence = \"雅生活服务的物业管理服务。\"\n    # print(model.SplitSentence(input_sentence))\n    # print(model.segment(intput_list))\n    # print(model.postagger(intput_list))\n    # print(model.NamedEntityRecognizer(intput_list, Entity_dist=True))\n    print(model.NamedEntityRecognizer(intput_list))",
        "type": "code",
        "location": "/tests/title_cover_generator/pyltp_server.py:384-414"
    },
    "5323": {
        "file_id": 687,
        "content": "This code segment is a part of the Named Entity Recognizer function in a Chinese language processing model. It takes an input list containing sentences, and based on postags (part-of-speech tags), it identifies named entities within the text and returns them. The code snippet handles verbs with \"VOB\" or \"SBV\" child nodes differently by appending prefixes accordingly, and then combines prefix, word, and postfix to generate the final output.",
        "type": "comment"
    },
    "5324": {
        "file_id": 687,
        "content": "    # print(model.SyntaxParser(intput_list))\n    (\n        Subjective_guest,\n        Dynamic_relation,\n        Guest,\n        Name_entity_relation,\n    ) = model.triple_extract(intput_list)\n    print(\"=\" * 30)\n    print(Subjective_guest, Dynamic_relation, Guest, Name_entity_relation)\n    model.__release__()",
        "type": "code",
        "location": "/tests/title_cover_generator/pyltp_server.py:415-426"
    },
    "5325": {
        "file_id": 687,
        "content": "Extracting triples from input list using the model's triple_extract method, then printing them and releasing resources.",
        "type": "comment"
    },
    "5326": {
        "file_id": 688,
        "content": "/tests/title_cover_generator/pegasus_trainer.py",
        "type": "filepath"
    },
    "5327": {
        "file_id": 688,
        "content": "This code trains a PEGASUS machine translation model using MT5ForConditionalGeneration, loads data, tokenizes text, and saves weights every 5000 updates for 1000 epochs. It includes optimization steps, error handling, and backup saving functionality. The trainer is dissatisfied with the current performance.",
        "type": "summary"
    },
    "5328": {
        "file_id": 688,
        "content": "from commons import load_train_data_core, import_word\nWord = import_word()\n# print(Word)\n# break()\n#importing the PEGASUS Transformer model\nimport torch\nfrom transformers import MT5ForConditionalGeneration\nfrom tokenizer import T5PegasusTokenizer\nmodel_path = \"./pegasus_title_generation/pegasus_1\" # trained on paraphrase tasks.\n# model_name = './t5_pegasus_training/t5_pegasus'\nmodel_name = model_path\nmodel_name_or_path = model_name\ntorch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntokenizer = T5PegasusTokenizer.from_pretrained(model_name)\nmodel = MT5ForConditionalGeneration.from_pretrained(model_name).to(torch_device)\n# import random\n# import progressbar\ndef mydataset(len_threshold = 2,batchsize=1): # train till you fucking die. this almost depleted my VRAM. better train this shit elsewhere.\n    for a,b in load_train_data_core(len_threshold = 2,batchsize=1): yield a,b # freaking shit.\ndef get_train_data(batchsize=2,max_length=1024):",
        "type": "code",
        "location": "/tests/title_cover_generator/pegasus_trainer.py:2-27"
    },
    "5329": {
        "file_id": 688,
        "content": "Code imports necessary libraries for loading training data, tokenizing text with T5PegasusTokenizer and initializing the PEGASUS Transformer model. The MT5ForConditionalGeneration model is loaded from a pre-trained checkpoint located at `model_path` or `model_name`. The code also specifies the device to use for training (either CPU or CUDA-enabled GPU). A function named `mydataset` is defined, which generates a dataset from load_train_data_core with specified batch size and length threshold. Another function `get_train_data` takes in batch size and maximum sequence length as inputs. This code seems to be used for training the PEGASUS model on specific tasks.",
        "type": "comment"
    },
    "5330": {
        "file_id": 688,
        "content": "    for source_sentences, target_sentences in mydataset(batchsize=batchsize):\n        # targetSentence = [\"今天天气不错\",\"你吃了没有\"]\n        batchsize = len(source_sentences)\n        if batchsize >0:\n        # print([source_sentence,target_sentence])\n            input_ids = tokenizer.batch_encode_plus(source_sentences,max_length=max_length,padding=True,truncation=True, return_tensors=\"pt\").input_ids.to(device)\n            labels = tokenizer.batch_encode_plus(target_sentences,return_tensors=\"pt\",padding=True,truncation=True,max_length=max_length,).input_ids.to(device) # what is the freaking max_length?\n            yield input_ids, labels\n# from torch.optim import SGD\n# from torch.optim import ASGD as SGD\nfrom torch.optim import RMSprop as SGD\nbatchsize = 2\n# optimizer = SGD(model.parameters(), momentum=0.9, lr=0.000001*batchsize, weight_decay=0.0001)\noptimizer = SGD(model.parameters(), lr=0.00001*batchsize, weight_decay=0.0001)\nloss_mean = []\nmean_loss_period = 100\nepochs = 1000\nmsaveperiod = 5000 # wtf is 30000\nupdate_period = 1 # hell man.",
        "type": "code",
        "location": "/tests/title_cover_generator/pegasus_trainer.py:28-49"
    },
    "5331": {
        "file_id": 688,
        "content": "This code is training a model with batch size 2. The maximum length of input sentences is not defined. The optimizer is using RMSprop algorithm, with a learning rate adjusted by the batch size. The mean loss over the last 100 batches is stored in 'loss_mean' list. Training continues for 1000 epochs and model weights are saved every 5000 updates. Updates to the model occur once per iteration.",
        "type": "comment"
    },
    "5332": {
        "file_id": 688,
        "content": "#setting up the model\n# def get_response(input_text):\n#   batch = tokenizer.encode(input_text, return_tensors=\"pt\").to(torch_device)\n#   translated = model.generate(batch,decoder_start_token_id=tokenizer.cls_token_id,eos_token_id=tokenizer.sep_token_id,max_length=30).cpu().numpy()[0]\n#   tgt_text = ''.join(tokenizer.decode(translated[1:])).replace(' ', '')\n#   return tgt_text\n# not so bad?\n# can you train this shit?\n# print(get_response(\"你吃了没有\"))\nfor epoch in range(epochs):\n    print(\"STARTING EPOCH {} TOTAL {}\".format(epoch,epochs))\n    for index, (input_ids, labels) in enumerate(get_train_data(batchsize=batchsize)):\n        try:\n            if index%update_period == 0:\n                optimizer.zero_grad()\n            # print([input_ids, labels])\n            outputs = model(input_ids=input_ids, labels=labels)\n            loss = outputs.loss\n            floss = loss.tolist()\n            loss_mean.append(floss)\n            if len(loss_mean) == mean_loss_period:\n                mloss = sum(loss_mean)/mean_loss_period",
        "type": "code",
        "location": "/tests/title_cover_generator/pegasus_trainer.py:52-77"
    },
    "5333": {
        "file_id": 688,
        "content": "This code sets up a model for generating translations, trains it using train data, and calculates the loss. It uses a tokenizer to encode input texts and generate translations, then calculates the mean loss over a specified period.",
        "type": "comment"
    },
    "5334": {
        "file_id": 688,
        "content": "                print(\"EPOCH {} TOTAL {}\".format(epoch,epochs))\n                print(\"MEAN LOSS OVER {} SAMPLES: {}\".format(mean_loss_period,str(mloss)[:5]))\n                loss_mean = []\n            loss.backward()\n            # logits = outputs.logits\n            if index % update_period == 0:\n                optimizer.step() # this is shit. i should run this shit in kaggle.\n        except:\n            import traceback\n            traceback.print_exc()\n            print(\"POSSIBLY OOM\")\n        if index > (msaveperiod - 1) and index%msaveperiod == 0:\n            print(\"SAVING MODEL AT {} SAMPLES\".format(index))\n            model.save_pretrained(model_name_or_path)\n            # shutil.copy(model_name_or_path,model_name_or_path+\"-backup\")\n            model.save_pretrained(model_name_or_path+\"-backup\")\n            ## it is working.",
        "type": "code",
        "location": "/tests/title_cover_generator/pegasus_trainer.py:78-94"
    },
    "5335": {
        "file_id": 688,
        "content": "This code appears to be part of a training loop for a machine learning model. It keeps track of the mean loss over a certain period and saves the model after a set number of samples. The code includes an optimization step, error handling, and backup saving functionality. The trainer seems frustrated with the line \"this is shit. i should run this shit in kaggle.\"",
        "type": "comment"
    },
    "5336": {
        "file_id": 689,
        "content": "/tests/title_cover_generator/paddlenlp_word_label.py",
        "type": "filepath"
    },
    "5337": {
        "file_id": 689,
        "content": "This code imports Taskflow from paddlenlp and commons.sample_data. It iterates through sample data, applying various tasks (word segmentation, NER, POS tagging, etc.) to each element using Taskflow. If the flow is information extraction, it uses a schema for processing. After processing, it deletes the Taskflow object and prints the task name along with the processed data.",
        "type": "summary"
    },
    "5338": {
        "file_id": 689,
        "content": "from paddlenlp import  Taskflow\nfrom commons import sample_data\n# LAC 词语重要性\nfor elem in sample_data:\n    flows = [\"word_segmentation\",\"ner\",\"pos_tagging\",\"dependency_parsing\",\"information_extraction\",\"sentiment_analysis\",\"text_correction\",\"knowledge_mining\"]\n    for flow in flows:\n        if flow !=\"information_extraction\":\n            seg = Taskflow(flow) # need schema for information extraction.\n        else:\n            schema = [\"主语\",\"谓语\",\"宾语\"]\n            seg = Taskflow(flow, schema=schema) # need schema for information extraction\n        data = seg(elem)\n        del seg\n        print(flow,data)",
        "type": "code",
        "location": "/tests/title_cover_generator/paddlenlp_word_label.py:1-16"
    },
    "5339": {
        "file_id": 689,
        "content": "This code imports Taskflow from paddlenlp and commons.sample_data. It iterates through sample data, applying various tasks (word segmentation, NER, POS tagging, etc.) to each element using Taskflow. If the flow is information extraction, it uses a schema for processing. After processing, it deletes the Taskflow object and prints the task name along with the processed data.",
        "type": "comment"
    },
    "5340": {
        "file_id": 690,
        "content": "/tests/title_cover_generator/gpt2_train.sh",
        "type": "filepath"
    },
    "5341": {
        "file_id": 690,
        "content": "This code navigates into the \"GPT2-NewsTitle\" directory and creates a new directory named \"output_dir\". It then runs the Python script \"train.py\" to train a GPT-2 model for generating news titles or covers.",
        "type": "summary"
    },
    "5342": {
        "file_id": 690,
        "content": "cd GPT2-NewsTitle\nmkdir output_dir\npython3 train.py",
        "type": "code",
        "location": "/tests/title_cover_generator/gpt2_train.sh:1-3"
    },
    "5343": {
        "file_id": 690,
        "content": "This code navigates into the \"GPT2-NewsTitle\" directory and creates a new directory named \"output_dir\". It then runs the Python script \"train.py\" to train a GPT-2 model for generating news titles or covers.",
        "type": "comment"
    },
    "5344": {
        "file_id": 691,
        "content": "/tests/title_cover_generator/gpt2_title_data_prep.py",
        "type": "filepath"
    },
    "5345": {
        "file_id": 691,
        "content": "Code copies train data as test data, creates a new data directory if it doesn't exist, writes train data to a JSON file, and then copies the train file to the test file.",
        "type": "summary"
    },
    "5346": {
        "file_id": 691,
        "content": "# simply copy train shit as test shit.\nfrom commons import load_train_data_core, import_word\nWord = import_word()\nimport json\ndata = []\nimport os\ndata_dir = \"/media/root/help/pyjom/tests/title_cover_generator/GPT2-NewsTitle/data_dir\"\nif not os.path.exists(data_dir):\n    os.mkdir(data_dir)\ntrain_file = os.path.join(data_dir,\"train_data.json\")\ntest_file = os.path.join(data_dir,\"test_data.json\")\nfor content, title in load_train_data_core():\n    sample = {\"title\": title[0],\"content\":content[0]}\n    data.append(sample) # is that necessary?\nwith open(train_file,\"w+\",encoding=\"utf8\") as f:\n    f.write(json.dumps(data,ensure_ascii=False,indent=4))\nimport shutil\nshutil.copy(train_file, test_file)",
        "type": "code",
        "location": "/tests/title_cover_generator/gpt2_title_data_prep.py:1-26"
    },
    "5347": {
        "file_id": 691,
        "content": "Code copies train data as test data, creates a new data directory if it doesn't exist, writes train data to a JSON file, and then copies the train file to the test file.",
        "type": "comment"
    },
    "5348": {
        "file_id": 692,
        "content": "/tests/title_cover_generator/commons.py",
        "type": "filepath"
    },
    "5349": {
        "file_id": 692,
        "content": "The code loads and preprocesses training data using load_train_data_core function, iterating through indexes of text chunks, transforming words, and creating Word class instances. It applies shuffle and progress bar for efficient data access.",
        "type": "summary"
    },
    "5350": {
        "file_id": 692,
        "content": "sample_data = [\"【翎伶】world.execute;(me);\", \"【封校日常】沙拉制作\", \"【Blender场景动画】新代 : 城市【VictoryLuode】\", \"历时733天! 圆了挖机梦，我独立造了一台可遥控小型挖机\", \"【难懂的数学】傅里叶、拉普拉斯、卷积、欧拉方程、梯度散度、拉格朗日方程、奈奎斯特采样、虚数等抽象难懂数学一网打尽\", \"这些up主是中学生和大学生的救星啊啊啊啊啊！！！学习方法｜免费课程｜兴趣技能｜生涯规划\", \"【不止游戏】游戏和电影中的M4，究竟有多经典？\", \"Steam++ 新版v2.7发布 新功能介绍\", \"手绘503张！还原数码宝贝OP\", \"好可爱鸭~ summertime\", \"男室友偷偷逛站酷网，毕设惊艳全校！\", \"对不起，我笑得真的很大声！【第一届立直麻将联赛】\", \"在南京每天画画一小时，在家接单养活自己！\", \"没有什么事情是一个纸团解决不了的，如果有那就用很多个\", \"到底是什么让我能在公园大爷面前如此的自信？\", \"欲拔山城寨，先过五虎将\", \"杨侃最下饭｜27 杨毅：经纪人不能太贪心\", \"【深渊的呼唤V】全球总决赛-决赛 Wolves vs SST\", \"【安特卫普MAJOR】亚洲区预选赛 TYLOO vs Renegades\", \"狼队第五人格分部成立两周年啦！\", \"【守望先锋联赛】英雄崛起!准备好迎接2022赛季!\"]\nimport progressbar\nimport random\ndef load_train_data_core(shuffle=True,batchsize=1,len_threshold = 2,no_unk=True):\n    filepath = \"/media/root/help/pyjom/tests/title_cover_generator/DianJing/data/basic_data_80k_v2.pkl\"\n    # warning...\n    import pickle\n    fobj = open(filepath, 'rb')\n    # print([fobj])\n    # breakpoint()\n    class Word:\n        def __init__(self,val,tf,df):\n            self.val = val",
        "type": "code",
        "location": "/tests/title_cover_generator/commons.py:1-17"
    },
    "5351": {
        "file_id": 692,
        "content": "The code imports the progressbar and random libraries, defines a function load_train_data_core which takes optional parameters shuffle, batchsize, len_threshold, and no_unk. The filepath variable stores the path to a pickle file containing data for training. The function opens the file using pickle's open function in read binary mode and does not perform any additional operations on its contents before returning.",
        "type": "comment"
    },
    "5352": {
        "file_id": 692,
        "content": "            self.tf = tf\n            self.df = df\n        def __repr__(self):\n            pass\n    _, word2idx, idx2word, targets, srcs= pickle.load(fobj) # freaking swap.\n    # titles, abstracts\n    # print(titles) # these are not freaking words. numbers.\n    # print(abstracts)\n    for key in idx2word:\n        elem = idx2word[key]\n        if elem.startswith('<') and elem.endswith('>'):\n            elem = elem[1:-1].upper()\n            elem = \"[{}]\".format(elem)\n            idx2word[key] =elem\n    # you can freaking get the data.\n    # title = titles[0]\n    len_indexs = len(targets)\n    # indexs = [x for x in range(indexs)]\n        # random.shuffle(indexs)\n    randomIdx = [x for x in range(len_indexs)]\n    if shuffle:\n        random.shuffle(randomIdx)\n    randomIdx2 = [randomIdx[x*batchsize:(x+1)*batchsize] for x in range(len(randomIdx)//batchsize+1)]\n    len_srcs = len(srcs)\n    len_targets = len(targets)\n    # mfilter = lambda x: x.replace(\" \",\"\").replace(\"\\n\",\"\")\n    for indexs in progressbar.progressbar(randomIdx2):",
        "type": "code",
        "location": "/tests/title_cover_generator/commons.py:18-44"
    },
    "5353": {
        "file_id": 692,
        "content": "The code is loading a pickle file, extracting relevant data including titles and abstracts. It then modifies some elements in the idx2word dictionary by replacing specific characters with formatted strings. The code provides random indexes for accessing the data and applies a shuffle if required. Lastly, it uses a progress bar for iterating over the shuffled indexes to access the data.",
        "type": "comment"
    },
    "5354": {
        "file_id": 692,
        "content": "        src_result=[]\n        target_result=[]\n        for index in indexs:\n            if index < len_srcs and index < len_targets:\n                src, target = srcs[index], targets[index]\n                src, target = [idx2word[x] for x in src], [idx2word[x] for x in target]\n                src, target = \"\".join(src),\"\".join(target)\n                if no_unk:\n                    src, target = src.replace(\"[UNK]\",\"\"), target.replace(\"[UNK]\",\"\")\n                # src, target = mfilter(src), mfilter(target)\n                if max(len(src),len(target)) > len_threshold:\n                    src_result.append(src)\n                    target_result.append(target)\n        if len(src_result) >0:\n            yield src_result,target_result\n    # for index in indexs:\n    #     title = titles[index]\n    #     mytitle = [idx2word[x] for x in title]\n    #     abstract = abstracts[index]\n    #     myabstract = [idx2word[x] for x in abstract]\n    #     if join:\n    #         yield \"\".join(mytitle), \"\".join(myabstract)\n    #     else: yield mytitle, myabstract",
        "type": "code",
        "location": "/tests/title_cover_generator/commons.py:45-67"
    },
    "5355": {
        "file_id": 692,
        "content": "This code is iterating through indexes in two lists of text chunks, transforming them to word form, joining the words into strings, and removing [UNK] tokens if specified. If any resulting string exceeds a certain length threshold, it appends them to two result lists. The code yields these two result lists if there are at least one entry.",
        "type": "comment"
    },
    "5356": {
        "file_id": 692,
        "content": "    # print(mytitle)\n    # breakpoint()\ndef import_word():\n    # if __name__ == \"__main__\":\n    class Word:\n        def __init__(self,val,tf,df):\n            self.val = val\n            self.tf = tf\n            self.df = df\n        def __repr__(self):\n            pass\n    return Word\nif __name__ == '__main__':\n    Word = import_word()\n    for title, abstract in load_train_data_core():\n        print(title)\n        print(abstract) # we have <unk> tokens. how do we freaking deal with it?\n        breakpoint()",
        "type": "code",
        "location": "/tests/title_cover_generator/commons.py:68-87"
    },
    "5357": {
        "file_id": 692,
        "content": "The code defines a function `import_word` that returns a class named Word. The class has attributes `val`, `tf`, and `df`. The code then checks if it is being run as the main program and creates instances of the Word class from loaded data, printing title and abstract. It encounters a breakpoint to debug or inspect the handling of tokens in the code.",
        "type": "comment"
    },
    "5358": {
        "file_id": 693,
        "content": "/tests/tencent_video_recommendation_extraction/requests_html_test.py",
        "type": "filepath"
    },
    "5359": {
        "file_id": 693,
        "content": "Imports pyppeteer and initializes an HTMLSession object, sets the URL to a QQ video page, uses session.get() to fetch the page's content, then iterates through all absolute links in the fetched HTML using r.html.absolute_links. Prints each link.",
        "type": "summary"
    },
    "5360": {
        "file_id": 693,
        "content": "from requests_html import HTMLSession # use pyppeteer.\nsession = HTMLSession()\n# url='https://www.baidu.com/'\nurl = 'http://v.qq.com/x/page/m0847y71q98.html'\nr = session.get(url)\nfor link in r.html.absolute_links:\n    print(link)",
        "type": "code",
        "location": "/tests/tencent_video_recommendation_extraction/requests_html_test.py:1-8"
    },
    "5361": {
        "file_id": 693,
        "content": "Imports pyppeteer and initializes an HTMLSession object, sets the URL to a QQ video page, uses session.get() to fetch the page's content, then iterates through all absolute links in the fetched HTML using r.html.absolute_links. Prints each link.",
        "type": "comment"
    },
    "5362": {
        "file_id": 694,
        "content": "/tests/tencent_video_recommendation_extraction/playwright_with_elinks.sh",
        "type": "filepath"
    },
    "5363": {
        "file_id": 694,
        "content": "This code executes a Python script and an elinks command to extract recommended video IDs from a webpage, searching for them in specific HTML elements.",
        "type": "summary"
    },
    "5364": {
        "file_id": 694,
        "content": "python3 dump_page.py\nelinks -dump dump.html > dump.log\n# please find recommended video id in <div data-vid=\"<vid>\">\n# or in <img src=\"//puui.qpic.cn/vpic_cover/<vid>/<vid>_old_ori.jpg/s640x360?max_age=7776000\">",
        "type": "code",
        "location": "/tests/tencent_video_recommendation_extraction/playwright_with_elinks.sh:1-5"
    },
    "5365": {
        "file_id": 694,
        "content": "This code executes a Python script and an elinks command to extract recommended video IDs from a webpage, searching for them in specific HTML elements.",
        "type": "comment"
    },
    "5366": {
        "file_id": 695,
        "content": "/tests/tencent_video_recommendation_extraction/phantomjs_test.js",
        "type": "filepath"
    },
    "5367": {
        "file_id": 695,
        "content": "This code uses PhantomJS and creates a new page. It opens the specified URL, checks if the status is \"success\", then retrieves the HTML of the page using evaluate() and outputs it to the console before exiting.",
        "type": "summary"
    },
    "5368": {
        "file_id": 695,
        "content": "var page = require('webpage').create();\npage.open('http://v.qq.com/x/page/m0847y71q98.html', function(status) {\n    //console.log(\"Status: \" + status);\n    if (status === \"success\") {\n        //\tpage.render('example.png');\n        result = page.evaluate(() => document.body.innerHTML);\n        console.log(result)\n    }\n    phantom.exit();\n});",
        "type": "code",
        "location": "/tests/tencent_video_recommendation_extraction/phantomjs_test.js:1-10"
    },
    "5369": {
        "file_id": 695,
        "content": "This code uses PhantomJS and creates a new page. It opens the specified URL, checks if the status is \"success\", then retrieves the HTML of the page using evaluate() and outputs it to the console before exiting.",
        "type": "comment"
    },
    "5370": {
        "file_id": 696,
        "content": "/tests/tencent_video_recommendation_extraction/loop_all_tags.py",
        "type": "filepath"
    },
    "5371": {
        "file_id": 696,
        "content": "This code reads the \"dump.html\" file, parses it with BeautifulSoup, and then iterates over all elements to check if their attributes include 'src' or 'href'. If so, it prints the key-value pair for further examination.",
        "type": "summary"
    },
    "5372": {
        "file_id": 696,
        "content": "from bs4 import BeautifulSoup\ndata = open(\"dump.html\",'r').read()\nsoup = BeautifulSoup(data)\nfor elem in soup.find_all():\n    # print(elem.attrs)\n    # for further examination\n    attrs = elem.attrs\n    for key in ['src', 'href']:\n        if key in attrs.keys():\n            print(key, attrs[key])",
        "type": "code",
        "location": "/tests/tencent_video_recommendation_extraction/loop_all_tags.py:1-14"
    },
    "5373": {
        "file_id": 696,
        "content": "This code reads the \"dump.html\" file, parses it with BeautifulSoup, and then iterates over all elements to check if their attributes include 'src' or 'href'. If so, it prints the key-value pair for further examination.",
        "type": "comment"
    },
    "5374": {
        "file_id": 697,
        "content": "/tests/tencent_video_recommendation_extraction/dump_page.py",
        "type": "filepath"
    },
    "5375": {
        "file_id": 697,
        "content": "This code uses Playwright library to launch a headless Chromium browser, navigates to a QQ video page, waits for the DOM content loaded state, retrieves the HTML content, writes it to a \"dump.html\" file, and then closes the browser.",
        "type": "summary"
    },
    "5376": {
        "file_id": 697,
        "content": "from playwright.sync_api import sync_playwright\ndef run(playwright):\n    webkit = playwright.chromium\n    browser = webkit.launch(headless=True)\n    context = browser.new_context()\n    page = context.new_page()\n    page.goto(\"https://v.qq.com/x/page/m0847y71q98.html\")\n    page.wait_for_load_state(\"domcontentloaded\")\n    content = page.content()\n    with open(\"dump.html\", 'w+') as f: f.write(content)\n    print(\"content write to dump.html\")\n    browser.close()\nwith sync_playwright() as playwright:\n    run(playwright)",
        "type": "code",
        "location": "/tests/tencent_video_recommendation_extraction/dump_page.py:1-16"
    },
    "5377": {
        "file_id": 697,
        "content": "This code uses Playwright library to launch a headless Chromium browser, navigates to a QQ video page, waits for the DOM content loaded state, retrieves the HTML content, writes it to a \"dump.html\" file, and then closes the browser.",
        "type": "comment"
    },
    "5378": {
        "file_id": 698,
        "content": "/tests/vapoursynth_linux_test/view_test.py",
        "type": "filepath"
    },
    "5379": {
        "file_id": 698,
        "content": "The code imports VapourSynth library functions, creates a Video object with FFMS2 source and option to transpose, but generating previews isn't working as vspipe uses existing APIs and can only generate raw frame data. OpenCV might help; example at https://github.com/UniversalAl/view.",
        "type": "summary"
    },
    "5380": {
        "file_id": 698,
        "content": "videoPath = \"/root/Desktop/works/pyjom/samples/video/dog_with_text.mp4\"\n# videoPath = \"/Users/jamesbrown/desktop/works/pyjom_remote/samples/video/dog_with_text.mp4\"\n# reference: http://www.vapoursynth.com/doc/pythonreference.html\n# The VideoFrame and AudioFrame classes contains one picture/audio chunk and all the metadata associated with it. It is possible to access the raw data using either get_read_ptr(plane) or get_write_ptr(plane) and get_stride(plane) with ctypes.\n# A more Python friendly wrapping is also available where each plane/channel can be accessed as a Python array using frame[plane/channel].\n# To get a frame simply call get_frame(n) on a clip. Should you desire to get all frames in a clip, use this code:\n# for frame in clip.frames():\n#     # Do stuff with your frame\n#     pass\nfrom vapoursynth import core\nvideo = core.ffms2.Source(source=videoPath)\n# video = core.std.Transpose(video)\n# video.set_output()\n# from viewKali import Preview\n# clip = vs.core.lsmas.LibavSMASHSource('source.mp4')",
        "type": "code",
        "location": "/tests/vapoursynth_linux_test/view_test.py:1-23"
    },
    "5381": {
        "file_id": 698,
        "content": "The code imports the necessary functions from the VapourSynth library and defines a video path. It then creates a Video object using the FFMS2 source and provides an option to transpose the video if needed, but it's currently commented out. Additionally, there is a reference to another file called \"viewKali\" where a Preview function may be used, but it's not implemented yet.",
        "type": "comment"
    },
    "5382": {
        "file_id": 698,
        "content": "# seems not working\n# Preview(video)\n# vspipe is a wrapper around existing apis. vapoursynth can only generate raw frame data so we cannot encode video here alone. maybe we need opencv for this?\n# opencv preview https://github.com/UniversalAl/view",
        "type": "code",
        "location": "/tests/vapoursynth_linux_test/view_test.py:24-28"
    },
    "5383": {
        "file_id": 698,
        "content": "This code appears to attempt creating a preview using VapourSynth, but the functionality isn't working. It suggests that vspipe is a wrapper around existing APIs and VapourSynth can only generate raw frame data, so encoding a video alone might not be possible. OpenCV might help in generating previews, and there's an example at this GitHub link: https://github.com/UniversalAl/view.",
        "type": "comment"
    },
    "5384": {
        "file_id": 699,
        "content": "/tests/vapoursynth_linux_test/test_ffmpeg_docker.sh",
        "type": "filepath"
    },
    "5385": {
        "file_id": 699,
        "content": "Code snippet attempts to download a video file 'flower_cif.y4m' using wget, and then applies various filters with ffmpeg-tensorflow Docker container to upscale the video resolution by 2x and save it as 'flower_cif_2x.mp4'. The code also provides an alias for easier execution of ffmpeg-tensorflow command and specifies video filter complexities within the ffmpeg command.",
        "type": "summary"
    },
    "5386": {
        "file_id": 699,
        "content": "wget https://media.xiph.org/video/derf/y4m/flower_cif.y4m\n# no good for using docker gpu containers.\n# alias ffmpeg-tensorflow='docker run --rm --gpus all -u $(id -u):$(id -g) -v \"$PWD\":/data -w /data -i miratmu/ffmpeg-tensorflow'\n# ffmpeg-tensorflow -i flower_cif.y4m -filter_complex '[0:v] format=pix_fmts=yuv420p, extractplanes=y+u+v [y][u][v]; [y] sr=dnn_backend=tensorflow:scale_factor=2:model=/models/espcn.pb [y_scaled]; [u] scale=iw*2:ih*2 [u_scaled]; [v] scale=iw*2:ih*2 [v_scaled]; [y_scaled][u_scaled][v_scaled] mergeplanes=0x001020:yuv420p [merged]' -map [merged] -sws_flags lanczos -c:v libx264 -crf 17 -c:a copy -y flower_cif_2x.mp4",
        "type": "code",
        "location": "/tests/vapoursynth_linux_test/test_ffmpeg_docker.sh:1-6"
    },
    "5387": {
        "file_id": 699,
        "content": "Code snippet attempts to download a video file 'flower_cif.y4m' using wget, and then applies various filters with ffmpeg-tensorflow Docker container to upscale the video resolution by 2x and save it as 'flower_cif_2x.mp4'. The code also provides an alias for easier execution of ffmpeg-tensorflow command and specifies video filter complexities within the ffmpeg command.",
        "type": "comment"
    },
    "5388": {
        "file_id": 700,
        "content": "/tests/vapoursynth_linux_test/test.sh",
        "type": "filepath"
    },
    "5389": {
        "file_id": 700,
        "content": "This code is running vspipe, a command-line tool for processing video files with VapourSynth script. It takes a .vpy script file as input and uses the -c flag for y4m format output.",
        "type": "summary"
    },
    "5390": {
        "file_id": 700,
        "content": "vspipe -c y4m script.vpy -",
        "type": "code",
        "location": "/tests/vapoursynth_linux_test/test.sh:1-1"
    },
    "5391": {
        "file_id": 700,
        "content": "This code is running vspipe, a command-line tool for processing video files with VapourSynth script. It takes a .vpy script file as input and uses the -c flag for y4m format output.",
        "type": "comment"
    },
    "5392": {
        "file_id": 701,
        "content": "/tests/vapoursynth_linux_test/scene_change_detection.sh",
        "type": "filepath"
    },
    "5393": {
        "file_id": 701,
        "content": "This code is using FFmpeg to process a video file, extracting scenes by selecting frames where the scene change is greater than 0.1 and displaying information about each scene change. The output is redirected to null.",
        "type": "summary"
    },
    "5394": {
        "file_id": 701,
        "content": "ffmpeg -hide_banner -i \"/root/Desktop/works/pyjom/samples/video/LiEIfnsvn.mp4\" -an \\\n-filter:v \"select='gt(scene,0.1)',showinfo\" \\\n-f null \\\n- 2>&1",
        "type": "code",
        "location": "/tests/vapoursynth_linux_test/scene_change_detection.sh:3-6"
    },
    "5395": {
        "file_id": 701,
        "content": "This code is using FFmpeg to process a video file, extracting scenes by selecting frames where the scene change is greater than 0.1 and displaying information about each scene change. The output is redirected to null.",
        "type": "comment"
    },
    "5396": {
        "file_id": 702,
        "content": "/tests/vapoursynth_linux_test/pure_ffmpeg_interpolate_resolution_denoise.sh",
        "type": "filepath"
    },
    "5397": {
        "file_id": 702,
        "content": "This script uses FFmpeg and TensorFlow to process video files, applying Super Resolution and Edge Preserving Blur filters for improved quality. It utilizes Anaconda libraries for CUDA toolkit and CuDNN in a non-real-time processing manner.",
        "type": "summary"
    },
    "5398": {
        "file_id": 702,
        "content": "# ffmpeg -y -i \"/root/Desktop/works/pyjom/tests/random_giphy_gifs/samoyed.gif\" -vf \"minterpolate,scale=w=iw*2:h=ih*2:flags=lanczos,hqdn3d\" -r 60 ffmpeg_samoyed.mp4\n# SRCNN=espcn.pb\n# 5fps or something\n# env LD_LIBRARY_PATH=/root/anaconda3/pkgs/cudatoolkit-10.0.130-0/lib/:/root/anaconda3/pkgs/cudnn-7.6.5-cuda10.0_0/lib/:$LD_LIBRARY_PATH ffmpeg -i \"/root/Desktop/works/pyjom/tests/random_giphy_gifs/samoyed.gif\" -y -vf \"sr=dnn_backend=tensorflow:model=./sr_models/dnn_models/espcn.pb\"  ffmpeg_samoyed_espcn.mp4\n# 9fps or something\n# ffmpeg -i \"/root/Desktop/works/pyjom/tests/random_giphy_gifs/samoyed.gif\" -y -vf \"yaepblur\"  ffmpeg_samoyed_srcnn.mp4\n# strange shit.\n# env LD_LIBRARY_PATH=/root/anaconda3/pkgs/cudatoolkit-10.0.130-0/lib/:/root/anaconda3/pkgs/cudnn-7.6.5-cuda10.0_0/lib/:$LD_LIBRARY_PATH ffmpeg -i \"/root/Desktop/works/pyjom/tests/random_giphy_gifs/samoyed.gif\" -y -vf \"sr=dnn_backend=tensorflow:model=./sr/espcn.pb,yaepblur,hqdn3d\"  ffmpeg_samoyed_srcnn.mp4\n# env LD_LIBRARY_PATH=/root/anaco",
        "type": "code",
        "location": "/tests/vapoursynth_linux_test/pure_ffmpeg_interpolate_resolution_denoise.sh:1-13"
    },
    "5399": {
        "file_id": 702,
        "content": "The script contains FFmpeg commands to resize, denoise and apply different filters on a GIF file. It uses the TensorFlow model \"espcn.pb\" for super-resolution and the \"yaepblur\" filter. The environment variable LD_LIBRARY_PATH is used to specify paths for CUDA toolkit and CUDNN libraries. The final output is saved as \".mp4\" files with different names.",
        "type": "comment"
    }
}