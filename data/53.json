{
    "5300": {
        "file_id": 693,
        "content": "    def Kalman1D(observations, damping=0.2):\n        # To return the smoothed time series data\n        observation_covariance = damping\n        initial_value_guess = observations[0]\n        transition_matrix = 1\n        transition_covariance = 0.1\n        initial_value_guess\n        kf = KalmanFilter(\n            initial_state_mean=initial_value_guess,\n            initial_state_covariance=observation_covariance,\n            observation_covariance=observation_covariance,\n            transition_covariance=transition_covariance,\n            transition_matrices=transition_matrix,\n        )\n        pred_state, state_cov = kf.smooth(observations)\n        return pred_state\n    def getSinglePointStableState(xLeftPoints, signalFilterThreshold=10, commandFloatMergeThreshold = 15, \n        stdThreshold = 1,\n        slopeThreshold = 0.2):\n        xLeftPointsFiltered = Kalman1D(xLeftPoints)\n        xLeftPointsFiltered = xLeftPointsFiltered.reshape(-1)\n        from itertools import groupby\n        def extract_span(mlist, target=0):",
        "type": "code",
        "location": "/tests/video_detector_tests/pip_meanVariance_stablize.py:131-155"
    },
    "5301": {
        "file_id": 693,
        "content": "This code defines two functions: \"Kalman1D\" and \"getSinglePointStableState\". \"Kalman1D\" uses a Kalman filter algorithm to smooth time-series observations, while \"getSinglePointStableState\" filters and processes left points data for single-point stable states. The input parameters allow customization of the filtering and processing thresholds.",
        "type": "comment"
    },
    "5302": {
        "file_id": 693,
        "content": "            counter = 0\n            spanList = []\n            target_list = [(a, len(list(b))) for a, b in groupby(mlist)]\n            for a, b in target_list:\n                nextCounter = counter + b\n                if a == target:\n                    spanList.append((counter, nextCounter))\n                counter = nextCounter\n            return spanList\n        # solve diff.\n        xLeftPointsFilteredDiff = np.diff(xLeftPointsFiltered)\n        # xLeftPointsFilteredDiff3 = np.diff(xLeftPointsFilteredDiff)\n        # import matplotlib.pyplot as plt\n        # plt.plot(xLeftPointsFilteredDiff)\n        # plt.plot(xLeftPointsFiltered)\n        # plt.plot(xLeftPoints)\n        # plt.show()\n        # xLeftPointsFilteredDiff3Filtered = Kalman1D(xLeftPointsFilteredDiff3)\n        derivativeThreshold = 3\n        # derivative3Threshold = 3\n        xLeftPointsSignal = (\n            (abs(xLeftPointsFilteredDiff) < derivativeThreshold)\n            .astype(np.uint8)\n            .tolist()\n        )\n        def signalFilter(signal, threshold=10):",
        "type": "code",
        "location": "/tests/video_detector_tests/pip_meanVariance_stablize.py:156-184"
    },
    "5303": {
        "file_id": 693,
        "content": "The code filters and extracts a list of spans from a given input, likely for text processing or analysis purposes. It uses groupby function to split the input into consecutive repetitions of the same element, then iterates over the resulting list of tuples (element, count) to create a new span list based on a target element. The code also performs differential calculations on the filtered xLeftPoints and applies a derivative threshold to generate a binary signal list likely for further processing or visualization purposes.",
        "type": "comment"
    },
    "5304": {
        "file_id": 693,
        "content": "            newSignal = np.zeros(len(signal))\n            signalFiltered = extract_span(xLeftPointsSignal, target=1)\n            newSignalRanges = []\n            for start, end in signalFiltered:\n                length = end - start\n                if length >= threshold:\n                    newSignalRanges.append((start, end))\n                    newSignal[start : end + 1] = 1\n            return newSignal, newSignalRanges\n        xLeftPointsSignalFiltered, newSignalRanges = signalFilter(xLeftPointsSignal, threshold = signalFilterThreshold)\n        xLeftPointsSignalFiltered *= 255\n        mShrink = 2\n        from sklearn.linear_model import LinearRegression\n        target = []\n        for start, end in newSignalRanges:\n            # could we shrink the boundaries?\n            mStart, mEnd = start + mShrink, end - mShrink\n            if mEnd <= mStart:\n                continue\n            sample = xLeftPointsFiltered[mStart:mEnd]\n            std = np.std(sample)\n            if std > stdThreshold:\n                continue",
        "type": "code",
        "location": "/tests/video_detector_tests/pip_meanVariance_stablize.py:185-210"
    },
    "5305": {
        "file_id": 693,
        "content": "This code segment performs signal filtering and preparation for subsequent analysis. It extracts a filtered signal, selects ranges above a threshold, scales the values to 255, applies boundary shrinking, and checks if the standard deviation exceeds a threshold before passing it on for further processing.",
        "type": "comment"
    },
    "5306": {
        "file_id": 693,
        "content": "            model = LinearRegression()\n            X, y = np.array(range(sample.shape[0])).reshape(-1, 1), sample\n            model.fit(X, y)\n            coef = model.coef_[0]  # careful!\n            if abs(coef) > slopeThreshold:\n                continue\n            meanValue = int(np.mean(sample))\n            target.append({\"range\": (start, end), \"mean\": meanValue})\n            # print((start, end), std, coef)\n        newTarget = {}\n        for elem in target:\n            meanStr = str(elem[\"mean\"])\n            mRange = elem[\"range\"]\n            newTarget.update({meanStr: newTarget.get(meanStr, []) + [mRange]})\n        mStart = 0\n        mEnd = len(xLeftPoints)\n        newTarget = getContinualMappedNonSympyMergeResultWithRangedEmpty(\n            newTarget, mStart, mEnd\n        )\n        newTargetSequential = mergedRangesToSequential(newTarget)\n        if (newTargetSequential) == 1:\n            if newTargetSequential[0][0] == \"empty\":\n                # the whole thing is empty now. no need to investigate.\n                print(\"NO STATIC PIP FOUND HERE.\")",
        "type": "code",
        "location": "/tests/video_detector_tests/pip_meanVariance_stablize.py:211-238"
    },
    "5307": {
        "file_id": 693,
        "content": "Performs linear regression on sample data, if slope is within threshold range, adds mean value and range to target list. Combines target elements into newTarget dictionary, and converts newTarget to sequential format. If entire sequential format is empty, prints \"NO STATIC PIP FOUND HERE.\"",
        "type": "comment"
    },
    "5308": {
        "file_id": 693,
        "content": "                return {}\n        else:\n            # newTargetSequential\n            newTargetSequentialUpdated = []\n            for index in range(len(newTargetSequential) - 1):\n                elem = newTargetSequential[index]\n                commandString, commandTimeSpan = elem\n                nextElem = newTargetSequential[index + 1]\n                nextCommandString, nextCommandTimeSpan = nextElem\n                if commandString == \"empty\":\n                    newTargetSequential[index][0] = nextCommandString\n                else:\n                    if nextCommandString == \"empty\":\n                        newTargetSequential[index + 1][0] = commandString\n                    else:  # compare the two!\n                        commandFloat = float(commandString)\n                        nextCommandFloat = float(nextCommandString)\n                        if (\n                            abs(commandFloat - nextCommandFloat)\n                            < commandFloatMergeThreshold\n                        ):",
        "type": "code",
        "location": "/tests/video_detector_tests/pip_meanVariance_stablize.py:239-259"
    },
    "5309": {
        "file_id": 693,
        "content": "This code is updating a list of commands by merging consecutive commands if they are within a certain threshold. It compares the difference between two commands and if it's below a specific value, it updates the list accordingly.",
        "type": "comment"
    },
    "5310": {
        "file_id": 693,
        "content": "                            newTargetSequential[index + 1][0] = commandString\n            # bring this sequential into dict again.\n            answer = sequentialToMergedRanges(newTargetSequential)\n            # print(\"_\"*30, \"ANSWER\",\"_\"*30)\n            # for elem in answer.items():\n            #     print(elem)\n            return answer\n        print(\"[FAILSAFE] SOMEHOW THE CODE SUCKS\")\n        return {}\n    xLeftPoints = data[:, 0, 0]\n    yLeftPoints = data[:, 0, 1]\n    xRightPoints = data[:, 1, 0]\n    yRightPoints = data[:, 1, 1]\n    mPoints = [xLeftPoints, yLeftPoints, xRightPoints, yRightPoints]\n    answers = []\n    for mPoint in mPoints:\n        answer = getSinglePointStableState(mPoint)\n        answers.append(answer)\n        # print(\"_\"*30, \"ANSWER\",\"_\"*30)\n        # for elem in answer.items():\n        #     print(elem)\n    if answers == [{}, {}, {}, {}]:\n        print(\"NO PIP FOUND\")\n        finalCommandDict = {}\n    else:\n        defaultCoord = [0, 0, defaultWidth, defaultHeight]  # deal with it later?",
        "type": "code",
        "location": "/tests/video_detector_tests/pip_meanVariance_stablize.py:260-291"
    },
    "5311": {
        "file_id": 693,
        "content": "This code is performing image analysis and stabilization for PIP (Picture-in-Picture) detection. It first creates newSequential, then converts it back into a dictionary named \"answer.\" The code checks if any PIP was found by comparing the \"answers\" list to four empty dictionaries. If no PIP is detected, it returns an empty dictionary; otherwise, it proceeds further with default coordinates.",
        "type": "comment"
    },
    "5312": {
        "file_id": 693,
        "content": "        defaults = [{str(defaultCoord[index]): [(0, len(data))]} for index in range(4)]\n        for index in range(4):\n            if answers[index] == {}:\n                answers[index] = defaults[index]\n        labels = [\"xleft\", \"yleft\", \"xright\", \"yright\"]\n        commandDict = {}\n        for index, elem in enumerate(answers):\n            label = labels[index]\n            newElem = {\"{}:{}\".format(label, key): elem[key] for key in elem.keys()}\n            commandDict.update(newElem)\n        commandDict = getContinualMappedNonSympyMergeResult(commandDict)\n        commandDictSequential = mergedRangesToSequential(commandDict)\n        def getSpanDuration(span):\n            start, end = span\n            return end - start\n        itemDurationThreshold = 15\n        # print(\"HERE\")\n        # loopCount = 0\n        while True:\n            # print(\"LOOP COUNT:\", loopCount)\n            # loopCount+=1\n            # noAlter = True\n            beforeChange = [item[0] for item in commandDictSequential].copy()\n            for i in range(len(commandDictSequential) - 1):",
        "type": "code",
        "location": "/tests/video_detector_tests/pip_meanVariance_stablize.py:292-318"
    },
    "5313": {
        "file_id": 693,
        "content": "Sets default values for missing answers, converts dictionary format, applies consecutive ranges to sequential format, and enters a while loop that iteratively checks changes in the commandDictSequential list.",
        "type": "comment"
    },
    "5314": {
        "file_id": 693,
        "content": "                currentItem = commandDictSequential[i]\n                nextItem = commandDictSequential[i + 1]\n                currentItemCommand = currentItem[0]\n                currentItemDuration = getSpanDuration(currentItem[1])\n                nextItemCommand = nextItem[0]\n                nextItemDuration = getSpanDuration(nextItem[1])\n                if currentItemDuration < itemDurationThreshold:\n                    if nextItemCommand != currentItemCommand:\n                        # print(\"HERE0\",i, currentItemCommand, nextItemCommand)\n                        commandDictSequential[i][0] = nextItemCommand\n                        # noAlter=False\n                if nextItemDuration < itemDurationThreshold:\n                    if nextItemCommand != currentItemCommand and currentItemDuration >= itemDurationThreshold:\n                        # print(\"HERE1\",i, currentItemCommand, nextItemCommand)\n                        commandDictSequential[i + 1][0] = currentItemCommand\n                        # noAlter=False",
        "type": "code",
        "location": "/tests/video_detector_tests/pip_meanVariance_stablize.py:319-334"
    },
    "5315": {
        "file_id": 693,
        "content": "Checks if current and next commands in commandDictSequential have durations below itemDurationThreshold. If so, adjusts or merges the commands to maintain sequence continuity.",
        "type": "comment"
    },
    "5316": {
        "file_id": 693,
        "content": "            afterChange = [item[0] for item in commandDictSequential].copy()\n            noAlter = beforeChange == afterChange\n            if noAlter:\n                break\n        preFinalCommandDict = sequentialToMergedRanges(commandDictSequential)\n        finalCommandDict = {}\n        for key, elem in preFinalCommandDict.items():\n            # print(key,elem)\n            varNames = [\"xleft\", \"yleft\", \"xright\", \"yright\"]\n            defaultValues = [0, 0, defaultWidth, defaultHeight]\n            for varName, defaultValue in zip(varNames, defaultValues):\n                key = key.replace(\n                    \"{}:empty\".format(varName), \"{}:{}\".format(varName, defaultValue)\n                )\n            # print(key,elem)\n            # breakpoint()\n            import parse\n            formatString = (\n                \"xleft:{xleft:d}|yleft:{yleft:d}|xright:{xright:d}|yright:{yright:d}\"\n            )\n            commandArguments = parse.parse(formatString, key)\n            x, y, w, h = (\n                commandArguments[\"xleft\"],",
        "type": "code",
        "location": "/tests/video_detector_tests/pip_meanVariance_stablize.py:335-358"
    },
    "5317": {
        "file_id": 693,
        "content": "The code checks if the command dictionary remains unchanged after certain operations. If it does not change, the loop is exited. The code then converts the sequential command dictionary to a merged ranges form. It creates an empty final command dictionary and iterates over the pre-final command dictionary items. For each item, it replaces any \"empty\" values with default values for specific variables, such as xleft, yleft, xright, and yright. The code then uses the parse module to parse a format string containing these variable names and their updated values, creating commandArguments that contain the final x, y, w, h values.",
        "type": "comment"
    },
    "5318": {
        "file_id": 693,
        "content": "                commandArguments[\"yleft\"],\n                commandArguments[\"xright\"] - commandArguments[\"xleft\"],\n                commandArguments[\"yright\"] - commandArguments[\"yleft\"],\n            )\n            if w <= 0 or h <= 0:\n                continue\n            cropCommand = \"crop_{}_{}_{}_{}\".format(x, y, w, h)\n            # print(cropCommand)\n            finalCommandDict.update({cropCommand: elem})\n            # print(elem)\n            # the parser shall be in x,y,w,h with keywords.\n            # we might want to parse the command string and reengineer this shit.\n    return finalCommandDict\nobjective = \"discrete\"\n# objective = \"continual\"\n# objective = \"continual_najie\"\nif __name__ == \"__main__\":\n    # better plot this shit.\n    import json\n    if objective == \"continual\":\n        dataDict = json.loads(open(\"pip_meanVariance.json\", \"r\").read())\n    elif objective == 'continual_najie':\n        dataDict = json.loads(open(\"pip_meanVarianceSisterNa.json\", \"r\").read())\n    elif objective == \"discrete\":\n        dataDict = json.loads(open(\"pip_discrete_meanVariance.json\", \"r\").read())",
        "type": "code",
        "location": "/tests/video_detector_tests/pip_meanVariance_stablize.py:359-387"
    },
    "5319": {
        "file_id": 693,
        "content": "This code processes video detector test results and generates a dictionary containing cropped image commands based on the specified objective. It loads data from JSON files depending on the objective (\"continual\", \"continual_najie\", or \"discrete\"). The code then continues to process the resulting data.",
        "type": "comment"
    },
    "5320": {
        "file_id": 693,
        "content": "    else:\n        raise Exception(\"unknown objective: %s\" % objective)\n    # print(len(data)) # 589\n    data = dataDict[\"data\"]\n    defaultWidth, defaultHeight = dataDict[\"width\"], dataDict[\"height\"]\n    if objective in [\"continual\", 'continual_najie']:\n        finalCommandDict = kalmanStablePipRegionExporter(\n            data, defaultWidth, defaultHeight\n        )\n    else:\n        finalCommandDict = sampledStablePipRegionExporter(\n            data, defaultWidth, defaultHeight\n        )\n    import matplotlib.pyplot as plt\n    from matplotlib.patches import Rectangle\n    fig, ax = plt.subplots()\n    def plotRect(ax, x, y, width, height, facecolor):\n        ax.add_patch(\n            Rectangle((x, y), width, height, facecolor=facecolor, fill=True, alpha=0.5)\n        )  # in 0-1\n    ax.plot([[0, 0], [defaultWidth, defaultHeight]])\n    plotRect(ax, 0, 0, defaultWidth, defaultHeight, \"black\")\n    colors = [\"red\", \"yellow\", \"blue\",'orange','white','purple']\n    for index, key in enumerate(finalCommandDict.keys()):\n        import parse",
        "type": "code",
        "location": "/tests/video_detector_tests/pip_meanVariance_stablize.py:388-419"
    },
    "5321": {
        "file_id": 693,
        "content": "This code is handling an objective and preparing a plot for stable PIP region exporter. It raises an exception if the objective is unknown. Depending on the objective, it uses kalmanStablePipRegionExporter or sampledStablePipRegionExporter. It then plots a rectangle on the figure using matplotlib's Rectangle class. Finally, it sets colors for each region in the plot.",
        "type": "comment"
    },
    "5322": {
        "file_id": 693,
        "content": "        commandArguments = parse.parse(\"crop_{x:d}_{y:d}_{w:d}_{h:d}\", key)\n        color = colors[index%len(colors)]\n        rect = [int(commandArguments[name]) for name in [\"x\", \"y\", \"w\", \"h\"]]\n        print(\"RECT\", rect, color, \"SPAN\", finalCommandDict[key])\n        plotRect(ax, *rect, color)\n    # breakpoint()\n    plt.show()",
        "type": "code",
        "location": "/tests/video_detector_tests/pip_meanVariance_stablize.py:421-427"
    },
    "5323": {
        "file_id": 693,
        "content": "The code is creating a rectangular plot using matplotlib. It takes commandArguments as input and uses them to define the x, y, w, and h values of the rectangle. The color of the rectangle is randomly chosen from a list of colors. It then prints the coordinates of the rectangle, the chosen color, and the span of the finalCommandDict[key]. Lastly, it displays the plot using plt.show().",
        "type": "comment"
    },
    "5324": {
        "file_id": 694,
        "content": "/tests/video_detector_tests/rect_active_rect_frame_difference.py",
        "type": "filepath"
    },
    "5325": {
        "file_id": 694,
        "content": "This code aims to merge frame difference with rectangle detection, calculate the most active rect region, and consider 4 boundary lines as a custom switch.",
        "type": "summary"
    },
    "5326": {
        "file_id": 694,
        "content": "# merge framedifference with rectangle detection.\n# calculate the most active rect region.\n# also you might want to include 4 boundary lines as custom switch.",
        "type": "code",
        "location": "/tests/video_detector_tests/rect_active_rect_frame_difference.py:1-5"
    },
    "5327": {
        "file_id": 694,
        "content": "This code aims to merge frame difference with rectangle detection, calculate the most active rect region, and consider 4 boundary lines as a custom switch.",
        "type": "comment"
    },
    "5328": {
        "file_id": 695,
        "content": "/tests/video_detector_tests/rect_corner_detect_fast.py",
        "type": "filepath"
    },
    "5329": {
        "file_id": 695,
        "content": "This code appears to be exploring various corner detection methods from the OpenCV library (cv2) for potential use with the Canny edge detector. The purpose might be to find the most suitable method for a specific application.",
        "type": "summary"
    },
    "5330": {
        "file_id": 695,
        "content": "# there are some corner detection methods in cv2. not sure what to follow... is it intended to use with canny edge detector or not?",
        "type": "code",
        "location": "/tests/video_detector_tests/rect_corner_detect_fast.py:1-1"
    },
    "5331": {
        "file_id": 695,
        "content": "This code appears to be exploring various corner detection methods from the OpenCV library (cv2) for potential use with the Canny edge detector. The purpose might be to find the most suitable method for a specific application.",
        "type": "comment"
    },
    "5332": {
        "file_id": 696,
        "content": "/tests/video_detector_tests/rect_detect.py",
        "type": "filepath"
    },
    "5333": {
        "file_id": 696,
        "content": "The code reads video files, performs object tracking/recognition, and detects lines using OpenCV. It calculates mean differences, updates rectangles, applies edge detection and morphological operations, displays results, and saves as houghlines.jpg.",
        "type": "summary"
    },
    "5334": {
        "file_id": 696,
        "content": "import cv2\nimport numpy as np\nimport itertools\nimport uuid\n# Reading the required image in\n# which operations are to be done.\n# Make sure that the image is in the same\n# directory in which this python program is\n# video_file = \"../../samples/video/LiEIfnsvn.mp4\" # this one with cropped boundaries.\nvideo_file = \"/media/root/help1/pyjom/samples/video/LiGE8vLuX.mp4\"\nvideo = cv2.VideoCapture(video_file)\ndef rectMerge(oldRect, newRect,delta_thresh = 0.1):\n    # if very much alike, we merge these rects.\n    # what about those rect that overlaps? we check exactly those who overlaps.\n    # 1. check all new rects against all old rects. if they overlap, highly alike (or not) then mark it as having_alike_rect (or not) and append to new old rect list. <- after those old rects have been marked with alike sign, one cannot revoke the sign. still remaining new rects will be checked against them.\n    # 2. while checking, if not very alike then append newRect to new rect list.\n    # 3. if one old rect has not yet been ",
        "type": "code",
        "location": "/tests/video_detector_tests/rect_detect.py:1-20"
    },
    "5335": {
        "file_id": 696,
        "content": "The code reads an input video file and defines a function rectMerge that takes two rectangles as parameters and determines if they are highly alike. It checks all new rectangles against all old rectangles to find overlapping ones and marks them as having_alike_rect before proceeding with the remaining new rectangles. The purpose is likely for object tracking or recognition within a video stream.",
        "type": "comment"
    },
    "5336": {
        "file_id": 696,
        "content": "checked as having_alike_rect then cut its life. otherwise extend its life, though not extend above max_rect_life.\n    (old_x1,old_y1), (old_x2, old_y2) = oldRect\n    (new_x1,new_y1), (new_x2, new_y2) = newRect\n    old_w = old_x2-old_x1\n    old_h = old_y2-old_y1\n    det_x1 = abs(new_x1 - old_x1)/ old_w\n    det_x2 = abs(new_x2 - old_x2)/ old_w\n    det_y1 = abs(new_y1 - old_y1)/ old_h\n    det_y2 = abs(new_y2 - old_y2)/ old_h\n    # print(\"deltas:\",det_x1, det_x2, det_y1, det_y2)\n    having_alike_rect =  (det_x1 < delta_thresh) and (det_y1 < delta_thresh) and (det_x2 < delta_thresh ) and (det_y2 < delta_thresh)\n    myRect = newRect\n    if having_alike_rect:\n        myRect = oldRect\n    return myRect, having_alike_rect\ndef rectSurge(oldRectList, newRectList,delta_thresh = 0.1, min_rect_life = 0, max_rect_life = 6):\n    newToOldDictList = []\n    oldRectDictList = [{\"rect\":x[\"rect\"], \"alike\":False, \"life\":x[\"life\"],\"uuid\":x[\"uuid\"]} for x in oldRectList] # actually they are all dict lists. you can pass an empty list as oldRectList anyway.",
        "type": "code",
        "location": "/tests/video_detector_tests/rect_detect.py:20-43"
    },
    "5337": {
        "file_id": 696,
        "content": "This code calculates the delta between two rectangles and checks if they are alike within a certain threshold. It updates the rectangle list based on this comparison, ensuring rectangles with little change remain unchanged while extending or cutting their life depending on the condition.",
        "type": "comment"
    },
    "5338": {
        "file_id": 696,
        "content": "    # print(\"OLDRECTDICTLIST:\",oldRectDictList)\n    for newRect in newRectList:\n        needAppend = True\n        for index, oldRectDict in enumerate(oldRectDictList):\n            # print(\"ENUMERATING OLD INDEX:\",index)\n            oldRect = oldRectDict[\"rect\"]\n            _, having_alike_rect = rectMerge(oldRect,newRect,delta_thresh=delta_thresh)\n            if having_alike_rect:\n                needAppend = False\n                if not oldRectDict[\"alike\"]:\n                    # print(\"SET ALIKE:\",index,oldRect)\n                    oldRectDictList[index][\"alike\"] = True\n                # ignore myRect.\n        if needAppend:\n            newToOldDictList.append({\"rect\":newRect,\"life\":1,\"uuid\":str(uuid.uuid4())}) # make sure it is not duplicated?\n            # if appended we shall break this loop. but when shall we append?\n    oldToOldDictList = []\n    for oldRectDict in oldRectDictList:\n        alike = oldRectDict[\"alike\"]\n        life = oldRectDict[\"life\"]\n        oldRect = oldRectDict[\"rect\"]\n        myUUID = oldRectDict[\"uuid\"]",
        "type": "code",
        "location": "/tests/video_detector_tests/rect_detect.py:44-65"
    },
    "5339": {
        "file_id": 696,
        "content": "The code is iterating through a list of new rectangles and an existing list of old rectangles. For each new rectangle, it checks if any old rectangle has a similar one using the rectMerge function. If a similar rectangle is found, it updates its \"alike\" flag in the oldRectDictList. If no similar rectangle is found, it appends the new rectangle to the newToOldDictList. Finally, it creates a new list, oldToOldDictList, by iterating through oldRectDictList and excluding any rectangles with \"alike\" set to True.",
        "type": "comment"
    },
    "5340": {
        "file_id": 696,
        "content": "        if not alike:\n            life -=1\n        else:\n            life +=1\n            life = min(max_rect_life, life)\n        if life <= min_rect_life:\n            continue\n        oldToOldDictList.append({\"rect\":oldRect,\"life\":life,\"uuid\":myUUID})\n    return oldToOldDictList + newToOldDictList # a combination.\ndef updateTotalRects(oldTotalRectDict,rectList,currentFrameIndex,diffFrame):\n    for elem in rectList:\n        uuid = elem[\"uuid\"]\n        rect = elem[\"rect\"]\n        if uuid not in oldTotalRectDict.keys():\n            oldTotalRectDict.update({uuid:{\"rect\":rect,\"startFrame\":currentFrameIndex,\"endFrame\":None,\"meanDifference\":None}}) # finally,remove those without endFrame.\n        else:\n            duration = currentFrameIndex - oldTotalRectDict[uuid][\"startFrame\"]\n            (x0,y0),(x1,y1) = rect\n            diff = diffFrame[y0:y1,x0:x1] # this is shit. we need to crop this shit.\n            # grayscale.\n            # std = np.abs(std)\n            # get the total delta over time?\n            # std = np.mean(std,axis=2)",
        "type": "code",
        "location": "/tests/video_detector_tests/rect_detect.py:66-89"
    },
    "5341": {
        "file_id": 696,
        "content": "This function updates a dictionary of rectangles by iterating over a list of rectangles and their properties. It adds new rectangles to the dictionary or updates existing ones with their duration, position, and difference from previous frames. It ignores rectangles that do not change and removes those without an end frame.",
        "type": "comment"
    },
    "5342": {
        "file_id": 696,
        "content": "            diff_x = np.mean(diff.flatten())\n            # std_x = np.std(std,axis=2)\n            # std_x = np.std(std_x,axis=1)\n            # std_x = np.std(std_x,axis=0)\n            std_total = diff_x # later we need to convert this float64.\n            # breakpoint()\n            if duration == 1:\n                oldTotalRectDict[uuid][\"meanDifference\"] = std_total\n            else:\n                dur2 = duration - 1\n                prev_std = oldTotalRectDict[uuid][\"meanDifference\"]\n                new_std = (dur2*prev_std + std_total)/duration # may freaking exceed limit.\n                oldTotalRectDict[uuid][\"meanDifference\"] = new_std\n            oldTotalRectDict[uuid][\"endFrame\"] = currentFrameIndex\n    return oldTotalRectDict\ntotal_rect_dict ={}\nrect_dict_main_list = []\nmin_rect_life_display_thresh = 3 # a filter.\nmode = 1\nline_thresh =  150\nincludeBoundaryLines = True # applied to those cornered crops.\n# this will slow down the process. or maybe?\nframeIndex = -1\nprevFrame = None\nif mode == 1:\n    import pybgs as bgs",
        "type": "code",
        "location": "/tests/video_detector_tests/rect_detect.py:90-117"
    },
    "5343": {
        "file_id": 696,
        "content": "This code calculates the mean difference of rectangles over a certain duration and updates an existing dictionary with this information. It then returns the updated dictionary. The dictionary contains rectangle information such as mean difference, start frame index, end frame index, and duration for each unique ID (uuid). If the duration is 1, it simply stores the current mean difference in the dictionary. Otherwise, it calculates a new mean difference by taking a weighted average of the previous mean difference and the new mean difference. The code also initializes variables and imports a module called pybgs as bgs.",
        "type": "comment"
    },
    "5344": {
        "file_id": 696,
        "content": "    algorithm = (\n    bgs.FrameDifference()\n)  # this\nwhile True:\n    ret, img = video.read()\n    if img is None:\n        if mode == 1:\n            popKeys = []\n            for key in total_rect_dict.keys():\n                elem = total_rect_dict[key]\n                if elem[\"endFrame\"] is None:\n                    popKeys.append(key)\n            for key in popKeys:\n                total_rect_dict.pop(key) # remove premature rectangles.\n        break\n    else: frameIndex+=1\n    if mode == 1:\n        diff_img_output = algorithm.apply(img)\n    # what about the freaking still image?\n    # Convert the img to grayscale\n    # gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n    # no need to use gray image.\n    # Apply edge detection method on the image\n    blurred = cv2.GaussianBlur(img, (5, 5), 0)\n    edges = cv2.Canny(blurred,20,210,apertureSize = 3) # great.\n    # why not applying edges directly to rectangles?\n    # This returns an array of r and theta values\n    # line_thresh =  200\n    # maintain a rectangle list. merge the alikes?",
        "type": "code",
        "location": "/tests/video_detector_tests/rect_detect.py:118-152"
    },
    "5345": {
        "file_id": 696,
        "content": "This code reads frames from a video and applies different algorithms to detect objects. It checks for premature rectangles and removes them if necessary, and applies edge detection methods on the image. The code uses GaussianBlur and Canny functions for edge detection. It maintains a rectangle list and considers merging similar rectangles.",
        "type": "comment"
    },
    "5346": {
        "file_id": 696,
        "content": "    if mode == 1:\n        lines = cv2.HoughLines(edges,1,np.pi/180,line_thresh)\n        angle_error = 0.00003   # this can only detect square things, absolute square.\n        # we need to know horizontal and vertical lines, when they cross we get points.\n        frameHeight, frameWidth = img.shape[:2]\n        # print(\"height: \", frameHeight)\n        # print(\"width: \", frameWidth)\n        mlines = {\"horizontal\":[], \"vertical\":[]}\n        if includeBoundaryLines:\n            originPoint = (0,0)\n            cornerPoint = (frameWidth,frameHeight)\n            mlines[\"horizontal\"].append(originPoint)\n            mlines[\"horizontal\"].append(cornerPoint)\n            mlines[\"vertical\"].append(originPoint)\n            mlines[\"vertical\"].append(cornerPoint)\n        for line in lines:\n            for r_theta in line:\n                # breakpoint()\n                r,theta = r_theta.tolist()\n                # Stores the value of cos(theta) in a\n                # filter detected lines?\n                # theta filter:\n                if not abs(theta % (np.pi/2) )< angle_error:",
        "type": "code",
        "location": "/tests/video_detector_tests/rect_detect.py:154-176"
    },
    "5347": {
        "file_id": 696,
        "content": "The code segment is filtering HoughLines-detected lines from an image, ensuring they are nearly horizontal or vertical. It stores the cosine of line angles in a variable called 'a', and applies an angle error threshold to filter out lines not close to 0 (horizontal) or π/2 (vertical). If includeBoundaryLines is True, it adds four boundary points for both horizontal and vertical lines.",
        "type": "comment"
    },
    "5348": {
        "file_id": 696,
        "content": "                    continue # this is filtering.\n                # print(\"line parameter:\",r,theta)\n                a = np.cos(theta)\n                # Stores the value of sin(theta) in b\n                b = np.sin(theta)\n                # x0 stores the value rcos(theta)\n                x0 = a*r\n                # y0 stores the value rsin(theta)\n                y0 = b*r\n                # x1 stores the rounded off value of (rcos(theta)-1000sin(theta))\n                x1 = int(x0 + 1000*(-b))\n                # y1 stores the rounded off value of (rsin(theta)+1000cos(theta))\n                y1 = int(y0 + 1000*(a))\n                # x2 stores the rounded off value of (rcos(theta)+1000sin(theta))\n                x2 = int(x0 - 1000*(-b))\n                # y2 stores the rounded off value of (rsin(theta)-1000cos(theta))\n                y2 = int(y0 - 1000*(a))\n                # cv2.line draws a line in img from the point(x1,y1) to (x2,y2).\n                # (0,0,255) denotes the colour of the line to be\n                #drawn. In this case, it is red.",
        "type": "code",
        "location": "/tests/video_detector_tests/rect_detect.py:177-204"
    },
    "5349": {
        "file_id": 696,
        "content": "This code calculates line parameters using trigonometry and then draws a red line on the image. The loop filters out certain conditions, and the calculations are based on radius (r), polar angle (theta). Lines are drawn between different points calculated from these variables to create lines in the image.",
        "type": "comment"
    },
    "5350": {
        "file_id": 696,
        "content": "                df_x = abs(x1-x2)\n                df_y = abs(y1-y2)\n                lineType = \"vertical\"\n                if df_x > df_y:\n                    lineType = \"horizontal\"\n                # we just need one single point and lineType.\n                linePoint = (x1,y1)\n                mlines[lineType].append(linePoint)\n                # cv2.line(img,(x1,y1), (x2,y2), (0,0,255),2)\n                # would not draw lines this time. draw found rects instead.\n        # get rectangle points. or just all possible rectangles?\n        # enumerate all possible lines.\n        if len(mlines[\"horizontal\"]) < 2 or len(mlines[\"vertical\"]) < 2:\n            print(\"unable to form rectangles.\")\n            continue\n        else:\n            rects =[] # list of rectangles\n            for line_h1, line_h2 in itertools.combinations(mlines[\"horizontal\"],2):\n                ymin, ymax = list(sorted([line_h1[1],line_h2[1]]))\n                for line_v1, line_v2 in itertools.combinations(mlines[\"vertical\"], 2):\n                    xmin, xmax = list(sorted([line_v1[0],line_v2[0]]))",
        "type": "code",
        "location": "/tests/video_detector_tests/rect_detect.py:205-226"
    },
    "5351": {
        "file_id": 696,
        "content": "Calculating differences in x and y coordinates to determine line type. Appends line points based on line type to a dictionary. If there are less than 2 horizontal or vertical lines, the code cannot form rectangles and skips to the next iteration. Iterates through combinations of horizontal lines to find the upper and lower bounds, then does the same with vertical lines for left and right bounds.",
        "type": "comment"
    },
    "5352": {
        "file_id": 696,
        "content": "                    rect = ((xmin,ymin),(xmax,ymax))\n                    rects.append(rect)\n            rect_dict_main_list = rectSurge(rect_dict_main_list,rects)\n            # print(\"RECT DICT MAIN LIST:\")\n            # print(rect_dict_main_list) # maybe i want this shit?\n            total_rect_dict = updateTotalRects(total_rect_dict,rect_dict_main_list,frameIndex,diff_img_output)\n            mdisplayed_rect_count = 0\n            for rect_dict in rect_dict_main_list:\n                life = rect_dict[\"life\"]\n                if life < min_rect_life_display_thresh:\n                    continue # this is needed.\n                # draw shit now.\n                mdisplayed_rect_count +=1\n                (xmin,ymin),(xmax,ymax) = rect_dict[\"rect\"]\n                cv2.rectangle(img,(xmin,ymin),(xmax,ymax) , (255,0,0), 2)\n            #     (xmin,ymin),(xmax,ymax) = rect\n            #     rect_area = (xmax-xmin) * (ymax-ymin)\n            #     print(\"rect found:\",rect,rect_area)\n            prevFrame = img.copy()",
        "type": "code",
        "location": "/tests/video_detector_tests/rect_detect.py:227-245"
    },
    "5353": {
        "file_id": 696,
        "content": "The code is updating a list of rectangles and iterating over it to draw them on an image. It checks if the rectangle \"life\" is above a certain threshold before drawing, and maintains a count of displayed rectangles. The code also stores the previous frame for comparison in a later step.",
        "type": "comment"
    },
    "5354": {
        "file_id": 696,
        "content": "            # print(\"total rects:\",mdisplayed_rect_count)\n    elif mode == 2:\n        lines = cv2.HoughLinesP(edges,1,np.pi/180,line_thresh,minLineLength=2,maxLineGap=100) # these are not angle filtering.\n        for points in lines:\n      # Extracted points nested in the list\n            x1,y1,x2,y2=points[0]\n            # filter out angle errors?\n            # Draw the lines joing the points\n            # On the original image\n            cv2.line(img,(x1,y1),(x2,y2),(0,255,0),2)\n            # Maintain a simples lookup list for points\n            # lines_list.append([(x1,y1),(x2,y2)])\n    elif mode == 3:\n        # edges = cv2.GaussianBlur(edges, (5, 5), 0)\n        horizontal_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (10,1))\n        detect_horizontal = cv2.morphologyEx(edges, cv2.MORPH_OPEN, horizontal_kernel, iterations=3)\n        vertical_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (1,10))\n        detect_vertical = cv2.morphologyEx(edges, cv2.MORPH_OPEN, vertical_kernel, iterations=3)",
        "type": "code",
        "location": "/tests/video_detector_tests/rect_detect.py:246-264"
    },
    "5355": {
        "file_id": 696,
        "content": "The code applies different modes to detect lines and edges in an image. Mode 2 uses HoughLinesP to detect lines without angle filtering, drawing them on the original image. Mode 3 applies morphological operations using structuring elements for horizontal and vertical lines detection.",
        "type": "comment"
    },
    "5356": {
        "file_id": 696,
        "content": "        cnts_horizontal = cv2.findContours(detect_horizontal, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n        cnts_horizontal = cnts_horizontal[0] if len(cnts_horizontal) == 2 else cnts_horizontal[1]\n        cnts_vertical = cv2.findContours(detect_vertical, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n        cnts_vertical = cnts_vertical[0] if len(cnts_vertical) == 2 else cnts_vertical[1]\n        for c in cnts_horizontal:\n            cv2.drawContours(img, [c], -1, (255,0,0), 3)\n        for c in cnts_vertical:\n            cv2.drawContours(img, [c], -1, (255,0,0), 3)\n    # what the heck?\n    # The below for loop runs till r and theta values\n    # are in the range of the 2d array\n    # why you have middle lines?\n            # how to get the intersections? lines?\n    cv2.imshow('linesDetected.jpg', img)\n    # cv2.imshow(\"edges.jpg\",edges) # not for fun.\n    if cv2.waitKey(20) == ord(\"q\"):\n        print(\"QUIT INTERFACE.\")\n        break\n# All the changes made in the input image are finally\n# written on a new image houghlines.jpg",
        "type": "code",
        "location": "/tests/video_detector_tests/rect_detect.py:266-290"
    },
    "5357": {
        "file_id": 696,
        "content": "This code uses OpenCV to detect horizontal and vertical lines in an image. It finds contours for both line types and draws them on the original image. Then, it displays the image with the detected lines using cv2.imshow(). The code also checks for a 'q' key press to exit the interface and saves the edited image as houghlines.jpg.",
        "type": "comment"
    },
    "5358": {
        "file_id": 696,
        "content": "if mode == 1:\n    print(\"FINAL RESULT:\")\n    for key in total_rect_dict.keys():\n        elem = total_rect_dict[key]\n        print(\"RECT UUID\",key)\n        print(\"RECT CONTENT\",elem)",
        "type": "code",
        "location": "/tests/video_detector_tests/rect_detect.py:291-298"
    },
    "5359": {
        "file_id": 696,
        "content": "This code block checks if mode is 1, then prints the final results. It iterates through each key-value pair in total_rect_dict and outputs the rect UUID and content.",
        "type": "comment"
    },
    "5360": {
        "file_id": 697,
        "content": "/tests/video_detector_tests/rectangle_framedifference.py",
        "type": "filepath"
    },
    "5361": {
        "file_id": 697,
        "content": "The code utilizes a motion detector to continuously capture frames, detecting changes for object detection and tracking. It calculates merged bounding boxes, applies thresholds, updates coordinates based on average values, and displays frames using OpenCV.",
        "type": "summary"
    },
    "5362": {
        "file_id": 697,
        "content": "# motion detectors are used to track objects. though you may want to separate objects with it.\nimport numpy as np\nimport cv2\nimport pybgs as bgs\nimport talib  # wait till all points are stablized. find a way to stream this.\n# suspect by static image analysis, and then create bounding box over the thing.\n# check image quality.\n# this can generate frame borders.\nalgorithm = (\n    bgs.FrameDifference()\n)  # this is not stable since we have more boundaries. shall we group things?\nvideo_file = (\n    \"../../samples/video/dog_with_text.mp4\"  # this is doggy video without borders.\n)\n# video_file = \"../../samples/video/LiEIfnsvn.mp4\" # this one with cropped boundaries.\ncapture = cv2.VideoCapture(video_file)\nwhile not capture.isOpened():\n    capture = cv2.VideoCapture(video_file)\n    # cv2.waitKey(1000)\n    # print(\"Wait for the header\")\npos_frame = capture.get(1)\ndef getAppendArray(mx1, min_x, past_frames=19):\n    return np.append(mx1[-past_frames:], min_x)\ndef getFrameAppend(frameArray, pointArray, past_frames=19):\n    mx1, mx2, my1, my2 = [",
        "type": "code",
        "location": "/tests/video_detector_tests/rectangle_framedifference.py:1-33"
    },
    "5363": {
        "file_id": 697,
        "content": "The code initializes a motion detector using frame difference algorithm to track objects in a video. It reads the video file and prepares two functions: `getAppendArray` for appending array elements, and `getFrameAppend` for processing frame data. These functions are used with specified past frames to analyze the video and possibly group objects. The code also handles potential video file opening issues by retrying if necessary.",
        "type": "comment"
    },
    "5364": {
        "file_id": 697,
        "content": "        getAppendArray(a, b, past_frames=past_frames)\n        for a, b in zip(frameArray, pointArray)\n    ]\n    return mx1, mx2, my1, my2\ndef getStreamAvg(a, timeperiod=10):  # to maintain stability.\n    return talib.stream.EMA(a, timeperiod=timeperiod)\ndef checkChange(frame_x1, val_x1, h, change_threshold=0.2):\n    return (abs(frame_x1 - val_x1) / h) > change_threshold  # really changed.\nmx1, mx2, my1, my2 = [np.array([]) for _ in range(4)]\npast_frames = 19\nperc = 0.03\nframe_num = 0\n# what is the time to update the frame?\nframe_x1, frame_y1, frame_x2, frame_y2 = [None for _ in range(4)]\nreputation = 0\nmax_reputation = 3\nminVariance = 10\nframeDict = {}  # include index, start, end, coords.\nframeIndex = 0\nwhile True:\n    flag, frame = capture.read()\n    frameIndex += 1\n    if flag:\n        pos_frame = capture.get(1)  # this is getting previous frame without read again.\n        img_output = algorithm.apply(frame)\n        img_bgmodel = algorithm.getBackgroundModel()\n        _, contours = cv2.findContours(\n            img_output, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE",
        "type": "code",
        "location": "/tests/video_detector_tests/rectangle_framedifference.py:34-70"
    },
    "5365": {
        "file_id": 697,
        "content": "The code initializes variables for frame coordinates, past frames count, and other parameters. It then enters a loop to continuously capture frames from the camera, apply an algorithm to detect changes, and update relevant variables accordingly. The purpose is likely object detection and tracking using background subtraction or similar techniques.",
        "type": "comment"
    },
    "5366": {
        "file_id": 697,
        "content": "        )\n        # maybe you should merge all active areas.\n        if contours is not None:\n            # continue\n            counted = False\n            for contour in contours:\n                [x, y, w, h] = cv2.boundingRect(img_output)\n                if not counted:\n                    min_x, min_y = x, y\n                    max_x, max_y = x + w, y + h\n                    counted = True\n                else:\n                    min_x = min(min_x, x)\n                    min_y = min(min_y, y)\n                    max_x = max(max_x, x + w)\n                    max_y = max(max_y, y + h)\n                    # only create one single bounding box.\n            # print(\"points:\",min_x, min_y, max_x,max_y)\n            this_w = max_x - min_x\n            this_h = max_y - min_y\n            thresh_x = max(minVariance, int(perc * (this_w)))\n            thresh_y = max(minVariance, int(perc * (this_h)))\n            mx1, mx2, my1, my2 = getFrameAppend(\n                (mx1, mx2, my1, my2), (min_x, max_x, min_y, max_y)\n            )",
        "type": "code",
        "location": "/tests/video_detector_tests/rectangle_framedifference.py:71-95"
    },
    "5367": {
        "file_id": 697,
        "content": "The code finds the bounding box of all detected contours and merges them into a single one. It then calculates the width and height of this merged bounding box, applies thresholds based on its size and percentage, and updates existing frame append values with new min and max coordinates.",
        "type": "comment"
    },
    "5368": {
        "file_id": 697,
        "content": "            val_x1, val_x2, val_y1, val_y2 = [\n                getStreamAvg(a) for a in (mx1, mx2, my1, my2)\n            ]\n            # not a number. float\n            # will return False on any comparison, including equality.\n            if (\n                abs(val_x1 - min_x) < thresh_x\n                and abs(val_x2 - max_x) < thresh_x\n                and abs(val_y1 - min_y) < thresh_y\n                and abs(val_y2 - max_y) < thresh_y\n            ):\n                needChange = False\n                # this will create bounding rect.\n                # this cannot handle multiple active rects.\n                reputation = max_reputation\n                if frame_x1 == None:\n                    needChange = True\n                elif (\n                    checkChange(frame_x1, val_x1, this_w)\n                    or checkChange(frame_x2, val_x2, this_w)\n                    or checkChange(frame_y1, val_y1, this_h)\n                    or checkChange(frame_y2, val_y2, this_h)\n                ):\n                    needChange = True",
        "type": "code",
        "location": "/tests/video_detector_tests/rectangle_framedifference.py:96-119"
    },
    "5369": {
        "file_id": 697,
        "content": "This code calculates the average values of x1, x2, y1, and y2 using getStreamAvg function for variables mx1 to my2. If these averages are within a certain threshold from min_x, max_x, min_y, and max_y, it sets needChange to False and reputation to max_reputation. If frame_x1 is None or there's a change in any of the variables, needChange is set to True.",
        "type": "comment"
    },
    "5370": {
        "file_id": 697,
        "content": "                    # the #2 must be of this reason.\n                if needChange:\n                    frame_x1, frame_y1, frame_x2, frame_y2 = [\n                        int(a) for a in (min_x, min_y, max_x, max_y)\n                    ]\n                    print()\n                    print(\"########FRAME CHANGED########\")\n                    frame_num += 1\n                    frame_area = (frame_x2 - frame_x1) * (frame_y2 - frame_y1)\n                    # update the shit.\n                    coords = ((frame_x1, frame_y1), (frame_x2, frame_y2))\n                    frameDict[frame_num] = {\n                        \"coords\": coords,\n                        \"start\": frameIndex,\n                        \"end\": frameIndex,\n                    }\n                    print(\n                        \"FRAME INDEX: {}\".format(frame_num)\n                    )  # this is the indexable frame. not uuid.\n                    print(\"FRAME AREA: {}\".format(frame_area))\n                    print(\"FRAME COORDS: {}\".format(str(coords)))",
        "type": "code",
        "location": "/tests/video_detector_tests/rectangle_framedifference.py:120-140"
    },
    "5371": {
        "file_id": 697,
        "content": "This code snippet handles frame changes in a video detection process. When a change is detected (needChange), it updates the frame's coordinates, number, and area. It then prints information about the new frame and adds it to the frameDict dictionary with the index as the key.",
        "type": "comment"
    },
    "5372": {
        "file_id": 697,
        "content": "                # allow us to introduce our new frame determinism.\n            else:\n                if reputation > 0:\n                    reputation -= 1\n            if frame_x1 is not None and reputation > 0:\n                # you may choose to keep cutting the frame? with delay though.\n                cv2.rectangle(\n                    frame, (frame_x1, frame_y1), (frame_x2, frame_y2), (255, 0, 0), 2\n                )\n                frameDict[frame_num][\"end\"] = frameIndex\n                # we mark the first and last time to display this frame.\n            # how to stablize this shit?\n        cv2.imshow(\"video\", frame)\n        # just video.\n        # cv2.imshow('img_output', img_output)\n        # cv2.imshow('img_bgmodel', img_bgmodel)\n    else:\n        cv2.waitKey(1000)\n        break\n    if 0xFF & cv2.waitKey(10) == 27:\n        break\ncv2.destroyAllWindows()\nprint(\"FINAL FRAME DETECTIONS:\")\nprint(frameDict)\n# {1: {'coords': ((80, 199), (496, 825)), 'start': 13, 'end': 269}, 2: {'coords': ((80, 381), (483, 644)), 'start': 297, 'end': 601}}",
        "type": "code",
        "location": "/tests/video_detector_tests/rectangle_framedifference.py:141-169"
    },
    "5373": {
        "file_id": 697,
        "content": "This code appears to be part of a video detection and analysis program. It uses OpenCV to display frames from the video and overlay rectangles on frames that have been detected multiple times. The \"frameDict\" stores information about detected frames, including their coordinates, start and end indices in the video, and reputation. The program continues until the user presses ESC or waits too long, then prints the final frame detections.",
        "type": "comment"
    },
    "5374": {
        "file_id": 698,
        "content": "/tests/video_detector_tests/rectangle_test.py",
        "type": "filepath"
    },
    "5375": {
        "file_id": 698,
        "content": "This code imports OpenCV libraries, sets up a motion detector algorithm for object tracking and suggests improvements. It reads a video file, applies the algorithm to each frame, finds bounding boxes, handles stability issues, displays images, stores results in JSON format and prints \"DATA DUMPED\" upon successful execution.",
        "type": "summary"
    },
    "5376": {
        "file_id": 698,
        "content": "# motion detectors are used to track objects. though you may want to separate objects with it.\nimport progressbar\nimport json\nimport pybgs as bgs\nimport numpy as np\nimport pathlib\nimport sys\nsite_path = pathlib.Path(\"/usr/local/lib/python3.9/site-packages\")\ncv2_libs_dir = site_path / 'cv2' / \\\n    f'python-{sys.version_info.major}.{sys.version_info.minor}'\nprint(cv2_libs_dir)\ncv2_libs = sorted(cv2_libs_dir.glob(\"*.so\"))\nif len(cv2_libs) == 1:\n    print(\"INSERTING:\", cv2_libs[0].parent)\n    sys.path.insert(1, str(cv2_libs[0].parent))\nimport cv2\n# suspect by static image analysis, and then create bounding box over the thing.\n# check image quality.\n# for donga, you must change the framerate to skip identical frames.\n# also donga have strange things you may dislike, e.g.: when only part of the image changes.\n# algorithm = bgs.FrameDifference() # this is not stable since we have more boundaries. shall we group things?\n# can we use something else?\nalgorithm = bgs.WeightedMovingVariance()\n# this one with cropped boundaries.",
        "type": "code",
        "location": "/tests/video_detector_tests/rectangle_test.py:1-31"
    },
    "5377": {
        "file_id": 698,
        "content": "This code imports necessary libraries, checks the OpenCV library version, and sets up a motion detector algorithm (WeightedMovingVariance) for object tracking. It also suggests possible improvements like grouping boundaries or using a different algorithm if needed.",
        "type": "comment"
    },
    "5378": {
        "file_id": 698,
        "content": "# average shit.\n# video_file = \"../../samples/video/LiEIfnsvn.mp4\"\n# select our 娜姐驾到\nvideo_file = \"../../samples/video/LiGlReJ4i.mp4\"\n# video_file = \"../../samples/video/LiEIfnsvn.mp4\"\n# denoising, moving average, sampler and  similar merge.\n# moving average span: -20 frame to +20 frame\n# denoising: 选区间之内相似的最多的那种\ncapture = cv2.VideoCapture(video_file)\nwhile not capture.isOpened():\n    capture = cv2.VideoCapture(video_file)\n    cv2.waitKey(1000)\n    print(\"Wait for the header\")\ndefaultWidth = capture.get(cv2.CAP_PROP_FRAME_WIDTH)\ndefaultHeight = capture.get(cv2.CAP_PROP_FRAME_HEIGHT)\ntotal_frames = capture.get(cv2.CAP_PROP_FRAME_COUNT)\ntotal_frames = int(total_frames)\npipFrames = []\ndefaultRect = [(0,0),(defaultWidth,defaultHeight)]\npos_frame = capture.get(1)\nareaThreshold = int(0.2*0.2*defaultWidth*defaultHeight)\nfor index in progressbar.progressbar(range(total_frames)):\n    # if index % 20 != 0: continue\n    flag, frame = capture.read()\n    if flag:\n        pos_frame = capture.get(1)\n        img_output = algorithm.apply(frame)",
        "type": "code",
        "location": "/tests/video_detector_tests/rectangle_test.py:32-66"
    },
    "5379": {
        "file_id": 698,
        "content": "This code reads a video file and applies an algorithm to each frame. It also retrieves the frame width, height, total frames, and uses a progress bar for looping through each frame. The code has a skipping mechanism for every 20th frame and a threshold for area calculations.",
        "type": "comment"
    },
    "5380": {
        "file_id": 698,
        "content": "        imgThresh = img_output\n        # imgMorph = cv2.GaussianBlur(img_output, (3,3), 0)\n        # _,imgThresh = cv2.threshold(imgMorph, 1, 255, cv2.THRESH_BINARY)\n        # img_bgmodel = algorithm.getBackgroundModel()\n        # _, contours = cv2.findContours(\n        #     imgThresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n        # maybe you should merge all active areas.\n        # if contours is not None:\n            # continue\n            # counted = False\n            # maxArea = 0\n            # for contour in contours:\n        [x, y, w, h] = cv2.boundingRect(img_output) # wtf is this?\n        area = w*h\n        if area > areaThreshold:\n                # #     maxArea = area\n                # if counted==False:\n            min_x, min_y = x, y\n            max_x, max_y = x+w, y+h\n                # else:\n                #     if x<min_x: min_x = x\n                #     if x+w>max_x: max_x = x+w\n                #     if y<min_y: min_y = y\n                #     if y+w>max_y: max_y = y+w\n            currentRect = [(min_x, min_y), (max_x, max_y)]",
        "type": "code",
        "location": "/tests/video_detector_tests/rectangle_test.py:67-91"
    },
    "5381": {
        "file_id": 698,
        "content": "This code segment is responsible for finding the bounding box around objects in an image. The variables `x`, `y`, `w`, and `h` are the coordinates of the rectangle's top left corner, width, and height respectively. It uses a Gaussian blur on the image and applies a binary threshold to isolate objects. It then finds contours, checks if they exceed a certain area threshold, and updates the minimum and maximum coordinates of the bounding box accordingly.",
        "type": "comment"
    },
    "5382": {
        "file_id": 698,
        "content": "            pipFrames.append(currentRect.copy())\n            defaultRect = currentRect.copy()\n        else:\n            pipFrames.append(defaultRect.copy())\n            # how to stablize this shit?\n        # cv2.imshow('video', frame)\n        # cv2.imshow('img_output', img_output)\n        # cv2.imshow('img_bgmodel', img_bgmodel)\n        # cv2.imshow('imgThresh', imgThresh)\n        # cv2.waitKey(100)\n    else:\n        # cv2.waitKey(1000)\n        break\ncv2.destroyAllWindows()\n# we process this shit elsewhere.\nwith open(\"pip_meanVarianceSisterNa.json\", 'w') as f:\n    f.write(json.dumps(\n        {\"data\": pipFrames, \"width\": defaultWidth, \"height\": defaultHeight}))\nprint(\"DATA DUMPED\")",
        "type": "code",
        "location": "/tests/video_detector_tests/rectangle_test.py:92-116"
    },
    "5383": {
        "file_id": 698,
        "content": "Code captures frames, appends rectangles to pipFrames, and handles stability issues. It displays images on various windows using OpenCV. Breaks loop if no changes detected. Closes all windows after processing. Writes pipFrames data to a JSON file named \"pip_meanVarianceSisterNa.json\" with width and height information. Prints \"DATA DUMPED\" upon successful execution.",
        "type": "comment"
    },
    "5384": {
        "file_id": 699,
        "content": "/tests/video_detector_tests/siamMask/demo.sh",
        "type": "filepath"
    },
    "5385": {
        "file_id": 699,
        "content": "This script changes directory to \"SiamMask\" and sets environment variables for running a SiamMask demo using Python 3. It resumes from the \"SiamMask_DAVIS.pth\" file with configuration from \"config_davis.json\".",
        "type": "summary"
    },
    "5386": {
        "file_id": 699,
        "content": "cd SiamMask\nexport SiamMask=$PWD\n# cd $SiamMask/experiments/siammask_sharp\n# cd $SiamMask/experiments/siammask_sharp\n# export PYTHONPATH=$PWD:$PYTHONPATH\n# which python3\npython3 -m tools.demo --resume experiments/siammask_sharp/SiamMask_DAVIS.pth --config experiments/siammask_sharp/config_davis.json",
        "type": "code",
        "location": "/tests/video_detector_tests/siamMask/demo.sh:1-7"
    },
    "5387": {
        "file_id": 699,
        "content": "This script changes directory to \"SiamMask\" and sets environment variables for running a SiamMask demo using Python 3. It resumes from the \"SiamMask_DAVIS.pth\" file with configuration from \"config_davis.json\".",
        "type": "comment"
    },
    "5388": {
        "file_id": 700,
        "content": "/tests/video_detector_tests/siamMask/setup.sh",
        "type": "filepath"
    },
    "5389": {
        "file_id": 700,
        "content": "Downloading and setting up SiamMask from GitHub, then retrieving pre-trained model files for VOT and DAVIS datasets.",
        "type": "summary"
    },
    "5390": {
        "file_id": 700,
        "content": "git clone https://github.com/foolwood/SiamMask.git && cd SiamMask\nexport SiamMask=$PWD\ncd $SiamMask/experiments/siammask_sharp\nwget http://www.robots.ox.ac.uk/~qwang/SiamMask_VOT.pth\nwget http://www.robots.ox.ac.uk/~qwang/SiamMask_VOT_LD.pth\nwget http://www.robots.ox.ac.uk/~qwang/SiamMask_DAVIS.pth",
        "type": "code",
        "location": "/tests/video_detector_tests/siamMask/setup.sh:1-6"
    },
    "5391": {
        "file_id": 700,
        "content": "Downloading and setting up SiamMask from GitHub, then retrieving pre-trained model files for VOT and DAVIS datasets.",
        "type": "comment"
    },
    "5392": {
        "file_id": 701,
        "content": "/tests/video_detector_tests/singleTracker.py",
        "type": "filepath"
    },
    "5393": {
        "file_id": 701,
        "content": "This code utilizes YOLOv5 for object detection and a video tracker to monitor dog movement in frames, identifying dogs and providing bounding box coordinates above threshold. Additionally, it closes OpenCV-created video windows.",
        "type": "summary"
    },
    "5394": {
        "file_id": 701,
        "content": "import cv2\n# import imutils #another dependency?\n# tracker = cv2.TrackerCSRT_create() # outdated tracker.\n# i really don't know what is a dog.\nimport torch\n# don't really know how paddleocr recognize chars.\nlocalModelDir = '/root/Desktop/works/pyjom/pyjom/models/yolov5/ultralytics_yolov5_master/'\nimport os\nos.environ[\"YOLOV5_MODEL_DIR\"] = '/root/Desktop/works/pyjom/pyjom/models/yolov5/'\nmodel = torch.hub.load(localModelDir, 'yolov5s',source=\"local\")\ndef getDogBB(frame,thresh=0.7):\n    img = frame[:,:,::-1].transpose((2,0,1))\n    # Inference\n    # reshape this shit.\n    # img = np.reshape()\n    results = model(img) # pass the image through our model\n    df = results.pandas().xyxy[0]\n    print(df)\n    data = []\n    for index,line in df.iterrows():\n        # print(line)\n        left = (line[\"xmin\"],line[\"ymin\"])\n        right = (line[\"xmax\"],line[\"ymax\"])\n        confidence = line[\"confidence\"]\n        class_ = line[\"class\"]\n        name = line[\"name\"]\n        if name == \"dog\" and confidence >= thresh: # better figure out all output names.",
        "type": "code",
        "location": "/tests/video_detector_tests/singleTracker.py:1-31"
    },
    "5395": {
        "file_id": 701,
        "content": "This code imports necessary libraries, sets the local model directory, and loads a YOLOv5 model for object detection. It defines a function to get the bounding box coordinates of a dog in an image, with the option to set a minimum confidence threshold. The code then performs inference using the loaded model on the input frame image and returns the bounding box information if the detected class is \"dog\" and the confidence meets or exceeds the threshold.",
        "type": "comment"
    },
    "5396": {
        "file_id": 701,
        "content": "            data.append({\"location\":[left,right],\"confidence\":confidence,\"identity\":{\"class\":class_,\"name\":name}})\n    print(data)\n    data = list(sorted(data,key=lambda x: -x[\"confidence\"]))\n    if len(data)>0:\n        target= data[0]\n        xmin,ymin = target[\"location\"][0]\n        xmax,ymax = target[\"location\"][1]\n        return int(xmin),int(ymin),int(xmax-xmin),int(ymax-ymin)\ndef checkDog(frame,thresh=0.5):\n    return getDogBB(frame,thresh=thresh) == None # dog missing.\n# better use something else?\n# tracker = cv2.TrackerMIL_create()\ntracker_types = ['MIL', 'GOTURN', 'DaSiamRPN']\ntracker_type = tracker_types[2]\nbasepath = \"./OpenCV-Object-Tracker-Python-Sample\"\nif tracker_type == 'MIL':\n    tracker = cv2.TrackerMIL_create()\nelif tracker_type == 'DaSiamRPN': # deeplearning.\n    # this tracker is slow as hell. really.\n    params = cv2.TrackerDaSiamRPN_Params()\n    params.model = os.path.join(basepath,\"model/DaSiamRPN/dasiamrpn_model.onnx\")\n    params.kernel_r1 = os.path.join(basepath,\"model/DaSiamRPN/dasiamrpn_kernel_r1.onnx\")",
        "type": "code",
        "location": "/tests/video_detector_tests/singleTracker.py:32-56"
    },
    "5397": {
        "file_id": 701,
        "content": "Code initializes a video tracker using different algorithms and prepares parameters for the 'DaSiamRPN' tracker, which is slower but uses deep learning. It then checks if a dog is present in each frame of a video and returns its bounding box coordinates.",
        "type": "comment"
    },
    "5398": {
        "file_id": 701,
        "content": "    params.kernel_cls1 = os.path.join(basepath,\"model/DaSiamRPN/dasiamrpn_kernel_cls1.onnx\")\n    tracker = cv2.TrackerDaSiamRPN_create(params)\n    # tracker = cv2.TrackerDaSiamRPN_create()\nelif tracker_type == 'GOTURN': #also need config file.\n    # this is bad though.\n    params = cv2.TrackerGOTURN_Params()\n    params.modelTxt = os.path.join(basepath,\"model/GOTURN/goturn.prototxt\") # save this shit without BOM.\n    params.modelBin = os.path.join(basepath,\"model/GOTURN/goturn.caffemodel\")\n    tracker = cv2.TrackerGOTURN_create(params)\n    # tracker = cv2.TrackerGOTURN_create()\n# we have to feed dog coordinates into the shit.\nvideo = cv2.VideoCapture(\"../../samples/video/dog_with_text.mp4\")\n_,frame = video.read()\n# frame = imutils.resize(frame,width=720) #why?\nindex = 0\nyoloRate = 10\ntrack_success = False\nupdate_track = 3\nBB = None\ninit=False\nwhile frame is not None:\n    index +=1\n    _, frame = video.read()\n    if frame is None:\n        print(\"VIDEO END.\")\n        break\n    if index%yoloRate == 0:\n        if BB is None:",
        "type": "code",
        "location": "/tests/video_detector_tests/singleTracker.py:57-86"
    },
    "5399": {
        "file_id": 701,
        "content": "This code initializes a tracker using the cv2.TrackerDaSiamRPN or cv2.TrackerGOTURN methods based on the tracker_type variable. It then creates parameters for the GOTURN tracker and creates a video capture object to read a video file. The loop reads frames from the video, checks if a bounding box is None every yoloRate frames, and if it's None, it initializes the BB variable. This code appears to be part of a video tracking application.",
        "type": "comment"
    }
}