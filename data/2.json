{
    "200": {
        "file_id": 12,
        "content": "            line.center,\n            line.middle,\n            line.leadin / 2,\n            line.leadout / 2,\n            line.text,\n        )\n        io.write_line(l)\n        # Random clipped text colorated\n        l.layer = 1\n        for i in range(1, int(line.width / 80)):\n            x_clip = line.left + random.uniform(0, line.width)\n            y_clip = line.top - 5\n            clip = (\n                x_clip,\n                y_clip,\n                x_clip + random.uniform(10, 30),\n                y_clip + line.height + 10,\n            )\n            l.text = \"{\\\\an5\\\\pos(%.3f,%.3f)\\\\fad(%d,%d)\\\\clip(%d,%d,%d,%d)%s}%s\" % (\n                line.center,\n                line.middle,\n                line.leadin / 2,\n                line.leadout / 2,\n                clip[0],\n                clip[1],\n                clip[2],\n                clip[3],\n                colors,\n                line.text,\n            )\n            io.write_line(l)\n    for line in lines:\n        # Generating lines\n        if line.styleref.alignment >= 7:",
        "type": "code",
        "location": "/pyjom/lyrictoolbox.py:725-762"
    },
    "201": {
        "file_id": 12,
        "content": "This code applies random clips and colors to each line of text. It generates lines based on a given set of lines, randomly sets x-coordinates for the clipping rectangle, adjusts y-coordinate, and writes the modified text with applied clip and color to an output file.",
        "type": "comment"
    },
    "202": {
        "file_id": 12,
        "content": "            lineModSource = line.copy()\n            break\n        elif line.styleref.alignment >= 4:  # what is this shit?\n            lineModSource = line.copy()\n            break\n    lineModSourceKanji = lineModSource.copy()\n    lineModSourceKanji.style = \"Kanji\"\n    # from test_pylrc import *\n    # just a test.\n    # censor these lyrics! fucker!\n    newTextArray = textArray\n    for mIndex, elem in enumerate(newTextArray):\n        # if fail then just continue. fuck.\n        try:\n            translatedTuple = translatedList[mIndex]\n            if len(translatedTuple) == 1:\n                sourceText = translatedTuple[0]\n                translatedText = None\n            elif len(translatedTuple) == 2:\n                sourceText, translatedText = translatedTuple\n            else:\n                print(\"Invalid translatedTuple: %s\" % str(translatedTuple))\n                breakpoint()\n            if sourceText is None or sourceText.strip() == \"\":\n                continue\n            else:\n                hasTranslatedText = (",
        "type": "code",
        "location": "/pyjom/lyrictoolbox.py:763-790"
    },
    "203": {
        "file_id": 12,
        "content": "Code snippet checks if the alignment of a line's style reference is greater than or equal to 4. If so, it creates a copy of the line source and assigns it to 'lineModSource'. It also creates a new copy of the line source specifically for Kanji styling. The code then iterates over a list of translated text elements, extracting source and translated texts. If there is no source text or it's empty, the iteration continues; otherwise, it proceeds with further processing.",
        "type": "comment"
    },
    "204": {
        "file_id": 12,
        "content": "                    (translatedText not in [sourceText, None, \"\"])\n                    and type(translatedText) == str\n                    and translatedText.strip() != \"\"\n                )\n            if puncturalRemoval:\n                sourceText = removeUnnecessaryPunctuation(sourceText)\n                if hasTranslatedText:\n                    translatedText = removeUnnecessaryPunctuation(translatedText)\n            if censor:\n                sourceText = censorTextWithTextFilter(sourceText)\n                if hasTranslatedText:\n                    translatedText = censorTextWithTextFilter(translatedText)\n            elem[\"text\"] = sourceText\n            lineMod = lineModSource.copy()\n            lineMod.start_time = max(0, elem[\"start\"] * 1000 - shiftAdjust)\n            lineMod.end_time = elem[\"end\"] * 1000 - shiftAdjust\n            lineMod.duration = lineMod.end_time - lineMod.start_time\n            lineMod.text = elem[\"text\"].strip()\n            while True:\n                if \"  \" in lineMod.text:",
        "type": "code",
        "location": "/pyjom/lyrictoolbox.py:791-811"
    },
    "205": {
        "file_id": 12,
        "content": "Code filters and prepares input text for display by checking if it has been translated, removing unnecessary punctuation, censoring the text if needed, and setting the start and end time of a line. It also removes any double spaces before passing it to the next step.",
        "type": "comment"
    },
    "206": {
        "file_id": 12,
        "content": "                    lineMod.text = lineMod.text.replace(\"  \", \" \")\n                else:\n                    break\n            # print(lineMod)\n            def addSylToLine(\n                lineMod,\n                translateShift=0,\n                charShift=30,  # increase this!\n                CENTER=1600 / 2,\n                mSylYShift=600,\n                mTop=25,\n                mMiddle=49.0,\n                mBottom=73.0,\n                cutOneByOne=False,\n            ):\n                lineMod.center = CENTER  # wtf?\n                if cutOneByOne:\n                    lineMod.words = list(lineMod.text)\n                elif lineMod.text.count(\" \") >= 1:\n                    lineMod.words = lineMod.text.split(\" \")\n                else:\n                    lineMod.words = getJiebaCuttedText(lineMod.text)\n                sylList = []\n                wordCount = len(lineMod.words)\n                if wordCount == 0:  # clean it no matter what.\n                    lineMod.words = [\" \"]\n                    lineMod.text = \" \"",
        "type": "code",
        "location": "/pyjom/lyrictoolbox.py:812-839"
    },
    "207": {
        "file_id": 12,
        "content": "This function takes a line of text, processes it by replacing consecutive spaces with a single space and splitting the line into words based on the count of spaces. It then initializes variables for cutting words, processing them one at a time or in bulk depending on input parameters, and creates a list of syllables from each word. If there are no words, it sets a default single space.",
        "type": "comment"
    },
    "208": {
        "file_id": 12,
        "content": "                    wordCount = 1\n                sylDuration = (lineMod.end_time - lineMod.start_time) / wordCount\n                textLength = len(lineMod.text)\n                absWordCenterShiftList = []\n                prevWordShift = 0\n                wordWidthList = []\n                for word in lineMod.words:\n                    wordWidth = len(word) * charShift\n                    wordWidthList.append(wordWidth)\n                    wordLength = len(word) + 1\n                    wordCenterShift = (charShift * wordLength) / 2\n                    wordShift = charShift * wordLength\n                    absWordCenterShift = (\n                        CENTER\n                        - (textLength * charShift) / 2\n                        + prevWordShift\n                        + wordCenterShift\n                    )\n                    absWordCenterShiftList.append(absWordCenterShift)\n                    prevWordShift += wordShift\n                # CENTER + centerShift*charShift\n                getCenter = lambda index: absWordCenterShiftList[index]",
        "type": "code",
        "location": "/pyjom/lyrictoolbox.py:840-863"
    },
    "209": {
        "file_id": 12,
        "content": "This code calculates the absolute center shift for each word in a line of text, considering the number of characters and the start-end time difference. It stores these values in two lists: 'wordWidthList' and 'absWordCenterShiftList'. The function also includes a lambda getter method to retrieve a specific value from the 'absWordCenterShiftList'.",
        "type": "comment"
    },
    "210": {
        "file_id": 12,
        "content": "                getWidth = lambda index: wordWidthList[index]\n                for index, word in enumerate(lineMod.words):\n                    syl = Syllable()\n                    syl.text = word\n                    syl.i = index\n                    syl.center = getCenter(index)\n                    syl.width = getWidth(index)\n                    syl.top = mTop + mSylYShift + translateShift\n                    syl.inline_fx = \"m2\"\n                    syl.middle = mMiddle + mSylYShift + translateShift\n                    syl.bottom = mBottom + mSylYShift + translateShift\n                    syl.start_time = lineMod.start_time + index * sylDuration\n                    syl.end_time = syl.start_time + sylDuration\n                    syl.duration = sylDuration\n                    sylList.append(syl)\n                # double check here! fucker\n                startSyl = sylList[0]\n                startLine = startSyl.center - startSyl.width / 2\n                endSyl = sylList[-1]\n                endLine = endSyl.center + endSyl.width / 2",
        "type": "code",
        "location": "/pyjom/lyrictoolbox.py:864-883"
    },
    "211": {
        "file_id": 12,
        "content": "The code creates a list of syllables for each line of lyrics. It calculates the position, duration, and timing of each syllable based on the line's words, word positions, and durations. The syllables are then added to a list. Finally, it checks the starting and ending lines by finding the first and last syllable in the list, respectively.",
        "type": "comment"
    },
    "212": {
        "file_id": 12,
        "content": "                currentCenter = (endLine + startLine) / 2\n                # print(startLine, endLine)\n                # print('current center:', currentCenter)\n                centerShift = int(CENTER - currentCenter)\n                # print(\"CENTERSHIFT\", centerShift)\n                for index in range(len(sylList)):\n                    sylList[index].center += centerShift\n                    # import copy\n                    # elem = copy.deepcopy(sylList[index])\n                    # elem.center = sylList[index].center+centerShift\n                    # print(\"CHANGED CENTER\", elem.center, sylList[index].center)\n                    # sylList[index] = copy.deepcopy(elem)\n                # startSyl = sylList[0]\n                # startLine = startSyl.center - startSyl.width/2\n                # endSyl = sylList[-1]\n                # endLine = endSyl.center + endSyl.width/2\n                # currentCenter = (endLine+startLine)/2\n                # print(startLine, endLine)\n                # print('adjusted center:', currentCenter)",
        "type": "code",
        "location": "/pyjom/lyrictoolbox.py:884-902"
    },
    "213": {
        "file_id": 12,
        "content": "Calculates the center point of a lyric by averaging start and end line coordinates, adjusts each syllable's center position based on this calculated value.",
        "type": "comment"
    },
    "214": {
        "file_id": 12,
        "content": "                lineMod.syls = sylList\n            # print(lineMod.syls)\n            # breakpoint()\n            # if translatedText == None:\n            #     addSylToLine(lineMod, charShift = 10)\n            # else:\n            addSylToLine(\n                lineMod,\n                cutOneByOne=styleConfig[\"original\"][\"cutOneByOne\"],\n                charShift=styleConfig[\"original\"][\"charShift\"],\n            )  # this function is to locate this thing.\n            # breakpoint()\n            # pyonfx.ass_core.Syllable\n            lineMod.style = styleConfig[\"original\"][\"style\"]\n            source = lineMod.copy()\n            target = lineMod.copy()\n            methodMapping = {\"kanji\": kanji, \"romaji\": romaji}\n            methodMapping[styleConfig[\"original\"][\"method\"]](\n                source, target\n            )  # writing 'kanji' style to romaji?\n            if hasTranslatedText:\n                # source.style = styleConfig['original']['style']\n                lineMod2 = lineMod.copy()\n                lineMod2.style = styleConfig[\"translated\"][\"style\"]",
        "type": "code",
        "location": "/pyjom/lyrictoolbox.py:904-929"
    },
    "215": {
        "file_id": 12,
        "content": "This code snippet is responsible for adding syllables to a line and handling original and translated text. It first assigns the syllable list, applies styles, copies the source and target lines, and then applies the method based on the style configuration. If there's translated text, it creates a new line with the translated text's style.",
        "type": "comment"
    },
    "216": {
        "file_id": 12,
        "content": "                translatedText = translatedText.replace(\" \", \"\")  # fuck?\n                lineMod2.text = translatedText\n                translateShift = 100\n                addSylToLine(\n                    lineMod2,\n                    translateShift=translateShift,\n                    cutOneByOne=styleConfig[\"translated\"][\"cutOneByOne\"],\n                    charShift=styleConfig[\"translated\"][\"charShift\"],\n                )\n                source = lineMod2.copy()\n                target = lineMod2.copy()\n                # elif line.styleref.alignment >= 4:\n                methodMapping[styleConfig[\"translated\"][\"method\"]](source, target)\n            # breakpoint()\n            # else:\n            #     romaji(source, target)\n        except:\n            # must be some \"start_ms\" <= 0 or some shit.\n            import traceback\n            traceback.print_exc()\n            print(\"Error when generating lyric subtitle line by line.\")\n            continue\n    io.save()\n    print(\"ASS RENDERED AT %s\" % assPath)\n    return assPath",
        "type": "code",
        "location": "/pyjom/lyrictoolbox.py:930-955"
    },
    "217": {
        "file_id": 12,
        "content": "This code is translating and formatting lyrics in an ASS (Advanced Substation Alpha) file. It replaces spaces with empty strings, adjusts the translation based on configuration, and applies different formatting methods depending on alignment. If an error occurs, it prints the traceback and continues to the next line. Finally, it saves and returns the generated ASS file path.",
        "type": "comment"
    },
    "218": {
        "file_id": 12,
        "content": "# do the preview later?\n# # io.open_aegisub()\ndef previewAssWithVideo(sample_video, assPath):\n    # from pyonfx import Ass\n    # io = Ass(assPath)\n    # do not load this shit again unless you want to block the whole shit...\n    print(\"PREVIEWING ASS SCRIPT: %s\" % assPath)\n    # io.open_mpv(video_path=sample_video) # ain't see shit...\n    cmd = \"mpv --sub-file='{}' '{}'\".format(assPath, sample_video)\n    os.system(cmd)\ndef lrcToAnimatedAss(\n    musicPath,\n    lrcPath,\n    assPath,\n    translate=True,\n    translate_method: Literal[\n        \"baidu\", \"deepl\", \"random\"\n    ] = \"baidu\",  # so how the fuck you can use deepl?\n    ass_template_configs={},\n    assStyleConfig={},\n):\n    # already moved to lyrictoolbox\n    # TODO: more styles incoming\n    textArray = lrcToTextArray(musicPath, lrcPath)\n    textList = [elem[\"text\"] for elem in textArray]\n    if translate:\n        translatedList = getTextListTranslated(\n            textList, translate_method=translate_method\n        )  # this is taking long time during test. make it redis lru cached!",
        "type": "code",
        "location": "/pyjom/lyrictoolbox.py:958-988"
    },
    "219": {
        "file_id": 12,
        "content": "This code snippet contains two functions: `previewAssWithVideo()` and `lrcToAnimatedAss()`. The former previews an ASS script with a sample video using the mpv command, while the latter converts LRC to animated ASS format by translating text (if specified) and creating an ASS script file. It also allows for custom ASS styles through configuration.",
        "type": "comment"
    },
    "220": {
        "file_id": 12,
        "content": "    else:\n        translatedList = [\n            (sourceText,) for sourceText in textList\n        ]  # notice, we need to examine this damn list.\n    # so we pass both arguments to the ass generator.\n    return textArrayWithTranslatedListToAss(\n        textArray,\n        translatedList,\n        assPath,\n        ass_template_configs=ass_template_configs,\n        assStyleConfig=assStyleConfig,\n    )\n# lyrictoolbox\ndef getLyricNearbyBpmCandidates(lyric_times, beats):\n    nearbys, remains = [], []\n    mbeats = beats.copy()\n    mbeats = list(set(mbeats))\n    mbeats.sort()\n    for ltime in lyric_times:\n        mbeats.sort(key=lambda x: abs(x - ltime))\n        nearby = mbeats[:2].copy()\n        nearbys += nearby\n        for elem in nearby:\n            mbeats.remove(elem)\n    remains = mbeats\n    return nearbys, remains\n# lyrictoolbox\ndef read_lrc(lrc_path):\n    assert lrc_path.endswith(\".lrc\")\n    with open(lrc_path, \"r\") as f:\n        lrc_string = f.read()\n        subs = pylrc.parse(lrc_string)\n        sublist = []\n        for sub in subs:",
        "type": "code",
        "location": "/pyjom/lyrictoolbox.py:989-1027"
    },
    "221": {
        "file_id": 12,
        "content": "The code is a part of the pyjom module, specifically the lyrictoolbox. It defines functions for processing LRC files and extracting lyrics near BPM candidates. The `textArrayWithTranslatedListToAss` function converts an array of text with translated lists to ASS format. The `getLyricNearbyBpmCandidates` function finds nearby BPM (beats per minute) candidates for given lyric timings and beats, and returns them along with any remaining beats. The `read_lrc` function reads an LRC file, parses its content using pylrc module, and returns a list of subtitles.",
        "type": "comment"
    },
    "222": {
        "file_id": 12,
        "content": "            time_in_secs = sub.time\n            content = sub.text\n            sublist.append({\"time\": time_in_secs, \"content\": content})\n            # another square bracket that could kill me.\n        return sublist\n# mainly for netease, this may change.\ndef cleanLrcFromWeb(\n    lyric_string: str,\n    song_duration: float,\n    min_lines_of_lyrics: int = 5,  # whatever. fuck this.\n    min_total_lines_of_lyrics: int = 10,\n    # potential_forbidden_chars=[], # how to deal with these? just remove the line?\n    removeLinesWithPotentialForbiddenChars: bool = True,\n    potential_forbidden_chars=[\"[\", \"]\", \"【\", \"】\", \"「\", \"」\", \"《\", \"》\", \"/\", \"(\", \")\"],\n    core_forbidden_chars=[\":\", \"：\", \"@\"],\n):\n    import pylrc\n    # you'd better inspect the thing. what is really special about the lyric, which can never appear?\n    def checkLyricText(text):\n        forbidden_chars = core_forbidden_chars\n        return not any([char in text for char in forbidden_chars])\n    # also get the total time covered by lyric.\n    # the time must be long enough, compared to the total time of the song.",
        "type": "code",
        "location": "/pyjom/lyrictoolbox.py:1028-1056"
    },
    "223": {
        "file_id": 12,
        "content": "This function cleans LRC (Lyric Rich Description) files by parsing each subtitle, removing forbidden characters, and ensuring a minimum number of lines to be considered valid. The minimum_lines_of_lyrics and min_total_lines_of_lyrics parameters allow customization of these thresholds. If removeLinesWithPotentialForbiddenChars is True, any line containing potential_forbidden_chars will be removed. The function utilizes the pylrc module and defines a checkLyricText function to inspect each subtitle's text for forbidden characters.",
        "type": "comment"
    },
    "224": {
        "file_id": 12,
        "content": "    lrc_parsed = pylrc.parse(lyric_string)  # this is string, not None!\n    lrc_parsed_list = [line for line in lrc_parsed]\n    lrc_parsed_list.sort(key=lambda line: line.time)\n    begin = False\n    # end = False\n    # line_counter = 0\n    # new_lines = []\n    # lrc_parsed: pylrc.classes.Lyrics\n    flags = []\n    for line in lrc_parsed_list:\n        # print(line)\n        text = line.text.strip()\n        # startTime = line.time\n        if not begin:\n            flag = checkLyricText(text)\n            if not flag:\n                begin = True\n        else:\n            flag = checkLyricText(text)\n            if flag:\n                begin = False\n        flags.append(flag)\n        # breakpoint()\n    # select consecutive spans.\n    # from test_commons import *\n    from pyjom.mathlib import extract_span\n    int_flags = [int(flag) for flag in flags]\n    mySpans = extract_span(int_flags, target=1)\n    print(mySpans)  # this will work.\n    # this span is for the range function. no need to add one to the end.\n    total_length = 0",
        "type": "code",
        "location": "/pyjom/lyrictoolbox.py:1057-1091"
    },
    "225": {
        "file_id": 12,
        "content": "This code is parsing a lyric string using pylrc library, then sorting and iterating over the parsed lines. It checks each line to determine if it contains valid lyrics, appends flags, and extracts consecutive spans from the flags. The extracted spans are then printed. This process ensures that only continuous lyrics sequences are considered for further analysis or processing.",
        "type": "comment"
    },
    "226": {
        "file_id": 12,
        "content": "    new_lyric_list = []\n    for mstart, mend in mySpans:\n        length = mend - mstart\n        total_length += length\n        if length >= min_lines_of_lyrics:\n            # process these lines.\n            for index in range(mstart, mend):\n                line_start_time = lrc_parsed_list[index].time\n                line_text = lrc_parsed_list[index].text\n                if removeLinesWithPotentialForbiddenChars:\n                    if any([char in line_text for char in potential_forbidden_chars]):\n                        line_text = \"\"\n                if line_start_time <= song_duration:\n                    line_end_time = song_duration\n                    if index + 1 < len(lrc_parsed_list):\n                        line_end_time = lrc_parsed_list[index + 1].time\n                        if line_end_time > song_duration:\n                            line_end_time = song_duration\n                    new_lyric_list.append((line_text, line_start_time))\n                    if index == mend - 1:\n                        # append one more thing.",
        "type": "code",
        "location": "/pyjom/lyrictoolbox.py:1093-1113"
    },
    "227": {
        "file_id": 12,
        "content": "This code extracts lyrics from a parsed LRC file. It iterates through each line span, checks if the line meets minimum length requirements, and filters out forbidden characters before appending valid lines to new_lyric_list. Line start and end times are also checked and adjusted based on the song duration.",
        "type": "comment"
    },
    "228": {
        "file_id": 12,
        "content": "                        new_lyric_list.append((\"\", line_end_time))\n                else:\n                    continue\n    # for elem in new_lyric_list:\n    #     print(elem)\n    # exit()\n    if total_length >= min_total_lines_of_lyrics:\n        print(\"LYRIC ACCEPTED.\")\n        new_lrc = pylrc.classes.Lyrics()\n        for text, myTime in new_lyric_list:\n            timecode_min, timecode_sec = divmod(myTime, 60)\n            timecode = \"[{:d}:{:.3f}]\".format(int(timecode_min), timecode_sec)\n            myLine = pylrc.classes.LyricLine(timecode, text)\n            new_lrc.append(myLine)\n        new_lrc_string = new_lrc.toLRC()\n        return new_lrc_string\n        # print(new_lrc_string)\n        # if nothing returned, you know what to do.\n# remerge demanded cut spans.\ndef remergeDemandedCutSpans(demanded_cut_spans:list[tuple[float,float]],min_span=2, max_span=10):\n    new_cut_spans = []  # the last span. could be a problem.\n    checkSpanValid = lambda a, b, c: a <= b and a >= c\n    continue_flag = 0\n    def subdivide_span(",
        "type": "code",
        "location": "/pyjom/lyrictoolbox.py:1114-1141"
    },
    "229": {
        "file_id": 12,
        "content": "The code is checking the total number of lines in a lyric. If it meets the minimum requirement, it formats the lyric with timestamps and appends each line to an instance of Lyrics class. The resulting LRC string is returned for the accepted lyrics. If the total length is not sufficient, the code continues execution without adding any lines. The `remergeDemandedCutSpans` function seems to handle cut spans by merging valid ones within a specified range.",
        "type": "comment"
    },
    "230": {
        "file_id": 12,
        "content": "        span_duration, min_span, max_span, span_start, span_end, new_cut_spans,div = 2,\n    ):\n        while True:\n            subduration = span_duration / div\n            # print(f'div {div} subduation {subduration}')\n            # if div>15:\n            #     breakpoint()\n            if checkSpanValid(subduration, max_span, min_span):\n                myspans = [\n                    (span_start + i * subduration, span_start + (i + 1) * subduration)\n                    for i in range(div)\n                ]\n                myspans[-1] = (myspans[-1][0], span_end)\n                for mspan in myspans:\n                    new_cut_spans.append(mspan)\n                break\n            else:\n                div += 1\n    for index, span in enumerate(demanded_cut_spans):\n        if continue_flag>0:\n            continue_flag -=1\n            continue\n        span_start, span_end = span\n        span_duration = span_end - span_start\n        if checkSpanValid(span_duration, max_span, min_span):\n            new_cut_spans.append((span_start, span_end))",
        "type": "code",
        "location": "/pyjom/lyrictoolbox.py:1142-1169"
    },
    "231": {
        "file_id": 12,
        "content": "This code calculates cut spans for a given lyric by dividing the original span duration into sub-durations, checking if they are within the specified minimum and maximum duration limits. It appends these valid sub-spans to new_cut_spans until it reaches the desired number of cuts or all original spans have been processed.",
        "type": "comment"
    },
    "232": {
        "file_id": 12,
        "content": "        elif span_duration > max_span:\n            subdivide_span(\n                span_duration, min_span, max_span, span_start, span_end, new_cut_spans\n            )\n        elif span_duration < min_span:\n            if len(new_cut_spans) > 0:\n                # merge with the previous span.\n                mynewspan = (new_cut_spans[-1][0], span_end)\n                # cut it in the old way.\n                new_cut_spans.pop(-1)\n            else:\n                mynewspan = None\n                for i0 in range(0,len(demanded_cut_spans)-index-1):\n                    continue_flag +=1\n                    mynewspan = span_start, demanded_cut_spans[index + 1+i0][1]\n                    if mynewspan[1]-mynewspan[0]>min_span:\n                        break\n                if mynewspan is None:\n                    print('Error!\\nMaybe the source cut span list is too small or is some abnormal source cut span.\\nPlease check.')\n                    breakpoint()\n            subdivide_span(\n                mynewspan[1]-mynewspan[0],",
        "type": "code",
        "location": "/pyjom/lyrictoolbox.py:1170-1191"
    },
    "233": {
        "file_id": 12,
        "content": "If the span duration is larger than the maximum allowed span, it subdivides the span. If the span duration is smaller than the minimum allowed span, it merges with the previous span or creates a new one if there's no previous span. If the source cut span list is abnormal, it prints an error message and stops execution.",
        "type": "comment"
    },
    "234": {
        "file_id": 12,
        "content": "                min_span,\n                max_span,\n                mynewspan[0],\n                mynewspan[1],\n                new_cut_spans,\n                div=1\n            )\n                # new_cut_spans.append(mynewspan)\n            # just merge with previous span. if previous span not present, merge with later span.\n    return new_cut_spans",
        "type": "code",
        "location": "/pyjom/lyrictoolbox.py:1192-1201"
    },
    "235": {
        "file_id": 12,
        "content": "The code is returning a list of new_cut_spans after merging, appending to existing spans if present, or merging with later span if previous one is not present.",
        "type": "comment"
    },
    "236": {
        "file_id": 13,
        "content": "/pyjom/modify_package.sh",
        "type": "filepath"
    },
    "237": {
        "file_id": 13,
        "content": "This script searches for Python files (.py), then replaces specific module imports (\"from modules\", \"from main\", etc.) with their new corresponding pyjom paths using sed command.",
        "type": "summary"
    },
    "238": {
        "file_id": 13,
        "content": "find | grep -E \".py\\$\" | xargs -iabc sed -i \"s/from modules/from pyjom.modules/g\" abc\nfind | grep -E \".py\\$\" | xargs -iabc sed -i \"s/from main/from pyjom.main/g\" abc \nfind | grep -E \".py\\$\" | xargs -iabc sed -i \"s/from commons/from pyjom.commons/g\" abc\nfind | grep -E \".py\\$\" | xargs -iabc sed -i \"s/from config/from pyjom.config/g\" abc\nfind | grep -E \".py\\$\" | xargs -iabc sed -i \"s/from medialang/from pyjom.medialang/g\" abc\nfind | grep -E \".py\\$\" | xargs -iabc sed -i \"s/from primitives/from pyjom.primitives/g\" abc",
        "type": "code",
        "location": "/pyjom/modify_package.sh:1-6"
    },
    "239": {
        "file_id": 13,
        "content": "This script searches for Python files (.py), then replaces specific module imports (\"from modules\", \"from main\", etc.) with their new corresponding pyjom paths using sed command.",
        "type": "comment"
    },
    "240": {
        "file_id": 14,
        "content": "/pyjom/main.py",
        "type": "filepath"
    },
    "241": {
        "file_id": 14,
        "content": "The code creates ContentProducer and ContentReviewer classes for content processing, uses dynamic method calling with methodsList dictionary, fixes operations, fetches feedback, optimizes results, and saves data in specified location.",
        "type": "summary"
    },
    "242": {
        "file_id": 14,
        "content": "from types import GeneratorType\nfrom pyjom.commons import *  # really swap this shit?\nfrom pyjom.modules import *\nclass ContentProducer:\n    def __init__(self):\n        self.uuid = dummyId()\n        self.log_location = None\n        self.trash_location = None\n        self.methodsList = {\n            \"topic\": dummyTopic,\n            \"info\": dummyInfo,\n            \"processor\": dummyProcessor,\n            \"producer\": dummyProducer,\n            \"poster\": dummyPoster,\n            # below three all switched to 'auto' mode for iterating generators.\n            \"feedback\": dummyFeedback,\n            \"optimizer\": dummyOptimizer,\n            \"updator\": dummyUpdator,\n            \"identifier\": dummyIdentifier,\n        }\n        self.identifier = self.get_one_identifier(\n            self.uuid\n        )  # make sure occasionly used methods can be regenerated.\n        self.identifier.typeFix(type(self).__name__)\n    def get_one_identifier(self, uuid):\n        return self.methodsList[\"identifier\"](uuid)\n    def get_one_topic(self):",
        "type": "code",
        "location": "/pyjom/main.py:1-31"
    },
    "243": {
        "file_id": 14,
        "content": "This code initializes a ContentProducer class with various methods and identifiers, ensuring regeneration of occasionally used methods. It utilizes dummy functions for different tasks and has the ability to switch modes for certain generators.",
        "type": "comment"
    },
    "244": {
        "file_id": 14,
        "content": "        topic, source = self.methodsList[\"topic\"]()\n        self.identifier.topicFix(source)\n        return topic\n    def get_some_info(self, topic):\n        info, source = self.methodsList[\"info\"](topic)\n        self.identifier.infoFix(source)\n        return info\n    def process_some_info(self, info):\n        method = self.methodsList[\"processor\"]\n        # print(method)\n        # breakpoint()\n        # print(info)\n        processed_info, source = method(info)\n        # print(processed_info,source)\n        # breakpoint()\n        self.identifier.processorFix(source)\n        return processed_info\n    def produce_some_content(self, processed_info):\n        # print(processed_info)\n        # print(self.methodsList['producer'])\n        # breakpoint()\n        content, source = self.methodsList[\"producer\"](processed_info)\n        self.identifier.producerFix(source)\n        return content\n    def post_some_content(self, content):\n        posted_location, source = self.methodsList[\"poster\"](content)\n        self.identifier.posterFix(source)",
        "type": "code",
        "location": "/pyjom/main.py:32-62"
    },
    "245": {
        "file_id": 14,
        "content": "This code defines several methods for processing and producing content. It uses a \"methodsList\" dictionary to dynamically call different methods based on input parameters. Each method is responsible for a specific step in the processing chain, with potential fixing operations performed by \"identifier\" after each step.",
        "type": "comment"
    },
    "246": {
        "file_id": 14,
        "content": "        return posted_location\n    def collect_some_feedback(self, posted_location):\n        feedback, source = self.methodsList[\"feedback\"](posted_location)\n        self.identifier.feedbackFix(source)\n        return feedback\n    def optimize_topic_by_feedback(self, topic, feedback):\n        optimized_result, source = self.methodsList[\"optimizer\"](topic, feedback)\n        self.identifier.optimizerFix(source)\n        return feedback\n    def update_optimized_result(self, optimized_result):\n        update_result, source = self.methodsList[\"updator\"](optimized_result)\n        if type(update_result) == GeneratorType:\n            for _ in update_result:\n                ...  # to fix not iterating bug.\n        self.identifier.updatorFix(source)\n    def main(self):\n        topic = self.get_one_topic()\n        info = self.get_some_info(topic)\n        processed_info = self.process_some_info(info)\n        # print(\"PROCESSED_INFO: %s\" % processed_info)\n        # breakpoint()\n        content = self.produce_some_content(processed_info)",
        "type": "code",
        "location": "/pyjom/main.py:63-88"
    },
    "247": {
        "file_id": 14,
        "content": "This code defines several methods for collecting feedback, optimizing a topic based on feedback, updating an optimized result, and getting one topic. It also includes a main function that retrieves information, processes it, produces content, and possibly prints or debugs processed info.",
        "type": "comment"
    },
    "248": {
        "file_id": 14,
        "content": "        posted_location = self.post_some_content(content)\n        feedback = self.collect_some_feedback(posted_location)\n        optimized_result = self.optimize_topic_by_feedback(topic, feedback)\n        self.update_optimized_result(optimized_result)\nclass ContentReviewer(ContentProducer):\n    def __init__(self):\n        super().__init__()\n        self.identifier.typeFix(type(self).__name__)\n        self.methodsList.update(\n            {\n                \"fetcher\": dummyFetcher,\n                \"reviewer\": dummyReviewer,\n                \"reviewOptimizer\": dummyReviewOptimizer,\n            }\n        )\n    def fetch_some_content(self, topic):\n        (posted_location, content), source = self.methodsList[\"fetcher\"](topic)\n        self.identifier.fetcherFix(source)\n        return posted_location, content\n    def review_content(self, content):\n        review, source = self.methodsList[\"reviewer\"](content)\n        self.identifier.reviewerFix(source)\n        return review\n    def optimize_topic_by_feedback_review(self, topic, feedback, review):",
        "type": "code",
        "location": "/pyjom/main.py:89-117"
    },
    "249": {
        "file_id": 14,
        "content": "This code defines a class called ContentReviewer which inherits from ContentProducer. It has methods for fetching content, reviewing content, and optimizing topics based on feedback and review. The update_optimized_result method is used to store the optimized result. The methodsList dictionary contains predefined functions for fetching, reviewing, and optimizing content.",
        "type": "comment"
    },
    "250": {
        "file_id": 14,
        "content": "        optimized_result, source = dummyReviewOptimizer(topic, feedback, review)\n        return optimized_result  # this with instant feedback.\n    def main(self, skip_review=False):\n        if self.trash_location is not None:\n            print(\"dumping trash at:\\n{}\".format(self.trash_location))\n            dumpTrashDir(self.trash_location)\n        topic = self.get_one_topic()\n        # print(\"fetched topic:\", topic)\n        protocol, content = self.fetch_some_content(topic)  # dummy since here.\n        # print(\"fetched protocol:\", protocol)\n        # print(\"fetched content:\", content)\n        if not skip_review:\n            review = self.review_content(content)  # dummy reviewer.\n            # print(\"reviewed content:\", review)\n        else:\n            review = {key: [] for key in content.keys()}  # test feedback\n            # of course nothing will be there.\n        feedback = self.collect_some_feedback(review)  # instant feedback.\n        # print(\"fetched feedback:\", feedback)\n        # breakpoint()\n        if self.log_location is not None:",
        "type": "code",
        "location": "/pyjom/main.py:118-140"
    },
    "251": {
        "file_id": 14,
        "content": "This code fetches content and feedback for a given topic, performs a review (if skip_review is False), and returns optimized results with instant feedback. The code also handles trash location and log location settings.",
        "type": "comment"
    },
    "252": {
        "file_id": 14,
        "content": "            mtype0, mcontent = jsonPrettyPrint(review)\n            mtype1, mfeedback_content = jsonPrettyPrint(feedback)\n            mtype0 = \"log\" if mtype0 != \"json\" else mtype0\n            mtype1 = \"log\" if mtype0 != \"json\" else mtype1\n            timestamp = getTimestamp()\n            timestamp = str(timestamp).replace(\".\", \"_\")\n            logName = \"{}.{}\".format(timestamp, mtype0)\n            feedback_logName = \"{}_feedback.{}\".format(timestamp, mtype1)\n            writeFileWithPath(\n                self.log_location, logName, mcontent, \"w+\", encoding=\"utf-8\"\n            )\n            writeFileWithPath(\n                self.log_location,\n                feedback_logName,\n                mfeedback_content,\n                \"w+\",\n                encoding=\"utf-8\",\n            )\n        # feedback is non-existant for local files.\n        optimized_result = self.optimize_topic_by_feedback_review(\n            topic, feedback, review\n        )\n        self.update_optimized_result(optimized_result)",
        "type": "code",
        "location": "/pyjom/main.py:141-165"
    },
    "253": {
        "file_id": 14,
        "content": "The code is parsing review and feedback data, converting them to desired types, generating timestamps, creating log file names, writing the data to files at a specified location, and then proceeding with optimization using the parsed data.",
        "type": "comment"
    },
    "254": {
        "file_id": 15,
        "content": "/pyjom/imagetoolbox.py",
        "type": "filepath"
    },
    "255": {
        "file_id": 15,
        "content": "This code provides image processing functions such as cropping, text recognition, and K-means clustering using EasyOCR. It also includes a customizable FastAPI server for detecting animals using PaddlePaddle's ResNet50 classifier and applies preprocessing techniques on images to extract animal objects.",
        "type": "summary"
    },
    "256": {
        "file_id": 15,
        "content": "from pyjom.commons import *\nimport numpy as np\nimport cv2\nfrom functools import lru_cache\nfrom lazero.utils.tools import flattenUnhashableList\nfrom typing import Literal\ndef imageCropWithDiagonalRectangle(\n    image, diagonalRectangle, order: Literal[\"opencv\", \"normal\"] = \"opencv\"\n):\n    # order is opencv.\n    assert order in [\"opencv\", \"normal\"]\n    x0, y0, x1, y1 = flattenUnhashableList(diagonalRectangle)\n    imageShape = image.shape\n    if len(imageShape) == 3:\n        if order == \"opencv\":\n            return image[y0:y1, x0:x1, :]\n        elif order == \"normal\":\n            return image[x0:x1, y0:y1, :]\n    elif len(imageShape) == 2:\n        if order == \"opencv\":\n            return image[y0:y1, x0:x1]\n        elif order == \"normal\":\n            return image[x0:x1, y0:y1]\n    else:\n        raise Exception(\"unknown image shape:\", imageShape)\ndef draw_bounding_box_with_contour(\n    contours, image, area_threshold=20, debug=False\n):  # are you sure?\n    # this is the top-k approach.\n    # Call our function to get the list of contour areas",
        "type": "code",
        "location": "/pyjom/imagetoolbox.py:1-35"
    },
    "257": {
        "file_id": 15,
        "content": "The code defines two functions: `imageCropWithDiagonalRectangle` and `draw_bounding_box_with_contour`. The first function crops an image based on a given diagonal rectangle, considering both opencv and normal orders. The second function draws bounding boxes around contours in an image, using the top-k approach. It takes contours, the image, area threshold, and debug flag as inputs.",
        "type": "comment"
    },
    "258": {
        "file_id": 15,
        "content": "    # cnt_area = contour_area(contours)\n    # Loop through each contour of our image\n    x0, y0, x1, y1 = [None] * 4\n    for i in range(0, len(contours), 1):\n        cnt = contours[i]\n        # Only draw the the largest number of boxes\n        if cv2.contourArea(cnt) > area_threshold:\n            # if (cv2.contourArea(cnt) > cnt_area[number_of_boxes]):\n            # Use OpenCV boundingRect function to get the details of the contour\n            x, y, w, h = cv2.boundingRect(cnt)\n            if x0 == None:\n                x0, y0, x1, y1 = x, y, x + w, y + h\n            if x < x0:\n                x0 = x\n            if y < y0:\n                y0 = y\n            if x + w > x1:\n                x1 = x + w\n            if y + h > y1:\n                y1 = y + h\n            # Draw the bounding box\n    if x0 is not None:\n        if debug:\n            image = cv2.rectangle(image, (x0, y0), (x1, y1), (0, 0, 255), 2)\n            cv2.imshow(\"with_bounding_box\", image)\n            cv2.waitKey(0)\n    if x0 is None:\n        height, width = image.shape[:2]",
        "type": "code",
        "location": "/pyjom/imagetoolbox.py:36-67"
    },
    "259": {
        "file_id": 15,
        "content": "Looping through contours, finding largest boxes above area threshold.",
        "type": "comment"
    },
    "260": {
        "file_id": 15,
        "content": "        x0, y0, x1, y1 = 0, 0, width, height\n    return (x0, y0), (x1, y1)\ndef imageLoader(image):\n    if type(image) == str:\n        if os.path.exists(image):\n            image = cv2.imread(image)\n        elif image.startswith(\"http\"):\n            import requests\n            r = requests.get(image)\n            content = r.content\n            content = np.asarray(bytearray(content), dtype=\"uint8\")\n            image = cv2.imdecode(content, cv2.IMREAD_COLOR)\n        else:\n            raise Exception(\"unknown image link: %s\" % image)\n    return image\ndef getDeltaWidthHeight(defaultWidth, defaultHeight):\n    deltaWidthRatio = 4 + (4 - 3) * (defaultWidth / defaultHeight - 16 / 9) / (\n        16 / 9 - 9 / 16\n    )\n    deltaWidthRatio = makeValueInRange(deltaWidthRatio, 3, 4)\n    deltaHeightRatio = 8 + (8 - 6) * (defaultHeight / defaultWidth - 16 / 9) / (\n        16 / 9 - 9 / 16\n    )\n    deltaHeightRatio = makeValueInRange(deltaHeightRatio, 6, 8)\n    deltaWidth, deltaHeight = int(defaultWidth / deltaWidthRatio), int(",
        "type": "code",
        "location": "/pyjom/imagetoolbox.py:68-97"
    },
    "261": {
        "file_id": 15,
        "content": "The code defines a function `imageLoader` that loads an image based on its type (file path or URL). It checks if the image is a file path and reads it using OpenCV, if it's a URL, it uses requests to download the content and decodes it into an image using OpenCV. If the image is neither a file path nor a valid URL, it raises an exception. The function `getDeltaWidthHeight` calculates the aspect ratio adjustments for resizing images based on default width and height, ensuring they are within specific ranges.",
        "type": "comment"
    },
    "262": {
        "file_id": 15,
        "content": "        defaultHeight / deltaHeightRatio\n    )\n    return deltaWidth, deltaHeight\ndef getFourCorners(x, y, defaultWidth, defaultHeight):\n    deltaWidth, deltaHeight = getDeltaWidthHeight(defaultWidth, defaultHeight)\n    # (x1, y1), (x2, y2)\n    fourCorners = [\n        [(0, 0), (deltaWidth, deltaHeight)],\n        [(defaultWidth - deltaWidth, 0), (defaultWidth, deltaHeight)],\n        [\n            (defaultWidth - deltaWidth, defaultHeight - deltaHeight),\n            (defaultWidth, defaultHeight),\n        ],\n        [(0, defaultHeight - deltaHeight), (deltaWidth, defaultHeight)],\n    ]\n    fourCorners = [[(a + x, b + y), (c + x, d + y)] for [(a, b), (c, d)] in fourCorners]\n    return fourCorners\n@lru_cache(maxsize=1)\ndef getEasyOCRReader(langs: tuple, gpu=True, recognizer=False):\n    import easyocr\n    # no metal? no dbnet18?\n    reader = easyocr.Reader(langs, gpu=gpu, recognizer=recognizer)\n    return reader\n# @lru_cache(maxsize=30)\ndef getImageTextAreaRecognized(\n    image, langs: tuple = (\"en\",), gpu=True, recognizer=False, return_res=False",
        "type": "code",
        "location": "/pyjom/imagetoolbox.py:98-130"
    },
    "263": {
        "file_id": 15,
        "content": "Function 'getFourCorners' takes in x, y, defaultWidth and defaultHeight coordinates and returns the coordinates of four corners of an image by calculating deltaWidth and deltaHeight using 'getDeltaWidthHeight'. It scales the image while maintaining its aspect ratio.\n\nThe function 'getEasyOCRReader' imports the 'easyocr' library and creates a Reader object with specified language (langs), GPU usage (gpu) and recognizer type (recognizer). This function utilizes caching using the decorator '@lru_cache(maxsize=1)' to speed up future calls by storing the last N results.\n\nThe function 'getImageTextAreaRecognized' takes an image, optional language parameter (langs), GPU and recognizer settings, and returns the recognized text from the image using the EasyOCR Reader created in 'getEasyOCRReader'. The results can be optionally returned with their respective coordinates by setting 'return_res=True'.",
        "type": "comment"
    },
    "264": {
        "file_id": 15,
        "content": "):\n    reader = getEasyOCRReader(langs, gpu=gpu, recognizer=recognizer)\n    if type(image) == str:\n        image = cv2.imread(image)\n    frame = image\n    height, width = frame.shape[:2]\n    res = (width, height)\n    detection, recognition = reader.detect(frame)  # not very sure.\n    if return_res:\n        return res, (detection, recognition)\n    else:\n        return detection, recognition\nfrom typing import Literal\ndef partial_blur(image0, mask, kernel=None):\n    # need improvement. malnly the boundary.\n    if kernel is None:\n        height, width = image0.shape[:2]\n        kernel_w = max(int(width / 40), 1) * 4\n        kernel_h = max(int(height / 40), 1) * 4\n    else:\n        kernel_w, kernel_h = kernel\n        kernel_w = max(int(kernel_w / 4), 1) * 4\n        kernel_h = max(int(kernel_h / 4), 1) * 4\n    kernel = (kernel_w, kernel_h)\n    mask_total = mask\n    inv_mask_total = 255 - mask_total\n    # mask0 = mask\n    # mask0 = mask/255\n    # inv_mask0 = inv_mask/255\n    non_blur_image = cv2.bitwise_and(image0, image0, mask=inv_mask_total)",
        "type": "code",
        "location": "/pyjom/imagetoolbox.py:131-165"
    },
    "265": {
        "file_id": 15,
        "content": "This code defines a function that detects and recognizes text in an image using EasyOCR reader. It also includes a partial_blur function for image processing with optional kernel size.",
        "type": "comment"
    },
    "266": {
        "file_id": 15,
        "content": "    blur_image0 = cv2.blur(image0, kernel)  # half quicklier.\n    blur_image0 = cv2.bitwise_and(blur_image0, blur_image0, mask=mask_total)\n    dst0 = blur_image0 + non_blur_image\n    return dst0\ndef imageInpainting(image, mask, method: Literal[\"inpaint\", \"blur\"] = \"inpaint\"):\n    if method == \"inpaint\":\n        return cv2.inpaint(image, mask, 3, cv2.INPAINT_TELEA)\n    elif method == \"blur\":\n        return partial_blur(image, mask)\n    else:\n        raise Exception(\"image inpainting method not supported:\", method)\ndef imageFourCornersInpainting(image, method=\"inpaint\"):\n    if type(image) == str:\n        image = cv2.imread(image)\n    defaultHeight, defaultWidth = image.shape[:2]\n    fourCorners = getFourCorners(0, 0, defaultWidth, defaultHeight)\n    img = np.zeros((defaultHeight, defaultWidth), dtype=np.uint8)\n    for (x1, y1), (x2, y2) in fourCorners:\n        w, h = x2 - x1, y2 - y1\n        x, y = x1, y1\n        cv2.rectangle(img, (x, y), (x + w, y + h), 255, -1)\n    return imageInpainting(image, img, method=method)",
        "type": "code",
        "location": "/pyjom/imagetoolbox.py:166-191"
    },
    "267": {
        "file_id": 15,
        "content": "The code provides three functions: 'partial_blur', 'imageInpainting', and 'imageFourCornersInpainting'. The first function, 'partial_blur', applies blur to an image while preserving certain areas specified by a mask. The second function, 'imageInpainting', takes an image and a mask, and using the OpenCV library's inpaint or blur methods, replaces the specified area in the image with the corresponding color from its surroundings. The last function, 'imageFourCornersInpainting', reads an image from a file or takes a string path of the image, creates a black image of the same size, and then uses the 'getFourCorners' function to draw rectangles around four corners of the image. It finally applies inpainting on the entire image using the 'imageInpainting' function.",
        "type": "comment"
    },
    "268": {
        "file_id": 15,
        "content": "def getImageTextAreaRatio(\n    image,\n    langs: tuple = (\"en\",),\n    gpu=True,\n    recognizer=False,\n    debug=False,\n    inpaint=False,\n    method=\"inpaint\",\n    edgeDetection=False,\n):\n    image_passed = image.copy()\n    if edgeDetection:\n        image_passed = cv2.Canny(image_passed, 20, 210, apertureSize=3)\n    res, (detection, recognition) = getImageTextAreaRecognized(\n        image_passed, langs=langs, gpu=gpu, recognizer=recognizer, return_res=True\n    )\n    width, height = res\n    img = np.zeros((height, width), dtype=np.uint8)\n    if detection == [[]]:\n        diagonalRects = []\n    else:\n        diagonalRects = [LRTBToDiagonal(x) for x in detection[0]]\n    for x1, y1, x2, y2 in diagonalRects:\n        w, h = x2 - x1, y2 - y1\n        x, y = x1, y1\n        cv2.rectangle(img, (x, y), (x + w, y + h), 255, -1)\n    # calculate the portion of the text area.\n    textArea = np.sum(img)\n    textAreaRatio = (textArea / 255) / (width * height)\n    if debug:\n        print(\"text area: {:.2f} %\".format(textAreaRatio))\n        cv2.imshow(\"TEXT AREA\", img)",
        "type": "code",
        "location": "/pyjom/imagetoolbox.py:194-225"
    },
    "269": {
        "file_id": 15,
        "content": "This function calculates the text area ratio of an image by detecting and recognizing the text. If edge detection is enabled, it applies Canny edge detection to the image. It then passes the image through getImageTextAreaRecognized function to get the detection and recognition results. It creates a zero matrix with the same dimensions as the image and draws rectangles around detected text areas on this matrix. Finally, it calculates the text area ratio by summing the pixels in the matrix and dividing it by the total possible value for the matrix multiplied by the image's width and height. If debug is enabled, it prints the text area percentage and displays an image showing the text areas.",
        "type": "comment"
    },
    "270": {
        "file_id": 15,
        "content": "        cv2.waitKey(0)\n    if inpaint:\n        return imageInpainting(image, img, method=method)\n    return textAreaRatio\ndef LRTBToDiagonal(lrtb):\n    left, right, top, bottom = lrtb\n    x0, y0, x1, y1 = left, top, right, bottom\n    return (x0, y0, x1, y1)\ndef imageDenoise(image):\n    shape = image.shape\n    if len(shape) == 3:\n        if shape[2] == 3:\n            return cv2.fastNlMeansDenoisingColored(image, None, 10, 10, 7, 21)\n    return cv2.fastNlMeansDenoising(image, None, 4, 7, 35)\ndef getImageColorCentrality(\n    image,\n    sample_size_limit=5000,\n    epsilon=0.01,  # shit man.\n    shift=2,\n    n_clusters=5,\n    batch_size=45,\n    max_no_improvement=10,\n):\n    # image is of numpy.array\n    # multiple centers.\n    # CENTER: [246.76865924 226.40763256 216.41472476]\n    # POSITIVE COUNT: 95497\n    # SUM: 286491.0 MIN: 0 MAX: 3\n    # NEARBY CENTER PERCENTAGE: 6.74 %\n    # CENTRALITY: 7.32 %\n    import numpy as np\n    # image = cv2.imread(src)\n    shape = image.shape\n    if len(shape) > 3 or len(shape) < 2:\n        print(\"weird image shape for getImageColorCentrality:\", shape)",
        "type": "code",
        "location": "/pyjom/imagetoolbox.py:226-267"
    },
    "271": {
        "file_id": 15,
        "content": "This code contains several image processing functions:\n1. cv2.waitKey(0) displays the window containing an image for a specific time, or until a key is pressed.\n2. imageDenoising uses fastNlMeansDenoisingColored or fastNlMeansDenoising to remove noise from images.\n3. LRTBToDiagonal takes a rectangle (left, right, top, bottom) and returns its diagonal as (x0, y0, x1, y1).\n4. getImageColorCentrality identifies central colors in an image using k-means clustering, considering the percentage of nearby pixels to determine color centrality.",
        "type": "comment"
    },
    "272": {
        "file_id": 15,
        "content": "        breakpoint()\n    if len(shape) == 2:\n        image = image.reshape(-1, -1, 1)\n    # for i in range(3):\n    #     image[:,:,i] = i\n    # col_0, col_1 = shape[:2]\n    # coords = []\n    # for c0 in range(col_0):\n    #     for c1 in range(col_1):\n    #         coords.append((c0,c1))\n    # coords = np.array(coords)\n    # print(image.reshape(-1,3))\n    reshapedImage = image.reshape(-1, 3)  # are you sure about this?\n    length, color_channels = reshapedImage.shape\n    reshapedImageIndexs = np.arange(0, length)\n    # so now it is good.\n    sampleIndexs = np.random.choice(\n        reshapedImageIndexs, size=min(sample_size_limit, length)\n    )\n    # print(sampleIndexs)\n    # print(sampleIndexs.shape)\n    sample = reshapedImageIndexs[sampleIndexs]\n    sample = reshapedImage[sample, :]\n    # print(sample)\n    # print(sample.shape)\n    # breakpoint()\n    # sampleCoords = coords[sampleIndexs]\n    # sample = np.hstack([sample, sampleCoords])\n    # print(sample)\n    # print(sample.shape)\n    # breakpoint()\n    # warning: OOM?",
        "type": "code",
        "location": "/pyjom/imagetoolbox.py:268-307"
    },
    "273": {
        "file_id": 15,
        "content": "This code seems to be performing image reshaping and sampling, potentially for a data processing or analysis task. The code initially reshapes the image into a 2D array of shape (-1, 3), then selects random samples from it based on a specified sample size limit. It also includes debugging print statements and potential breakpoints, indicating some experimentation or debugging process. Overall, the purpose of this section appears to be data processing for further analysis or usage.",
        "type": "comment"
    },
    "274": {
        "file_id": 15,
        "content": "    # now cluster shit shall we?\n    # from sklearn.neighbors import NearestNeighbors\n    # neigh = NearestNeighbors(n_neighbors=5)\n    # X = sample\n    # neigh.fit(X)\n    # A = neigh.kneighbors_graph(X)\n    # A.toarray()\n    # print(A)\n    # print(A.shape) # sparse matrix? wtf?\n    # from sklearn.cluster import MiniBatchKMeans  # better?\n    from sklearn.cluster import KMeans\n    X = sample\n    # kmeans = KMeans(n_clusters=5).fit(X) # not deterministic please?\n    # here we've got issue.\n    # import numpy as np\n    # np.seterr(all='ignore')\n    kmeans_model = KMeans(init=\"k-means++\", n_clusters=n_clusters)\n    kmeans = kmeans_model.fit(X)  # fix this shit\n    # keep popping up error logs.\n    # kmeans = MiniBatchKMeans(\n    #     init=\"k-means++\",\n    #     n_clusters=n_clusters,\n    #     batch_size=batch_size,\n    #     # n_init=10,\n    #     max_no_improvement=max_no_improvement,\n    #     verbose=0,\n    # ).fit(X)\n    # from lazero.utils import inspectObject\n    # inspectObject(kmeans)\n    # breakpoint()\n    # labels = kmeans.labels_",
        "type": "code",
        "location": "/pyjom/imagetoolbox.py:308-340"
    },
    "275": {
        "file_id": 15,
        "content": "Code attempts to cluster the sample data using KMeans algorithm and handles potential error logs by ignoring all errors.",
        "type": "comment"
    },
    "276": {
        "file_id": 15,
        "content": "    cluster_centers = kmeans.cluster_centers_\n    # print(labels)\n    # print(cluster_centers)\n    # sample_size = len(sampleIndexs) # this is the real sample size.\n    # label_percentage = {\n    #     x: np.count_nonzero(labels == x) / sample_size for x in range(n_clusters)\n    # }\n    flagged_image = image.copy()\n    flagged_image[:, :, :] = 1  # every element is 1 now.\n    percents = []\n    for center in cluster_centers:\n        # fetch area nearby given center\n        # center = center5[:3]\n        # center_int = center.astype(np.uint8)\n        # i just don't know what the fuck is going on here.\n        upper = center + shift\n        lower = center - shift\n        mask = cv2.inRange(image, lower, upper)\n        # not image.\n        output = cv2.bitwise_and(flagged_image, flagged_image, mask=mask)\n        # print(output)\n        # print(output.shape)\n        mOutput = output.reshape(-1, 3)\n        mOutput = np.sum(mOutput, axis=1)\n        # breakpoint()\n        positive_count = np.count_nonzero(abs(mOutput - 3) < epsilon)",
        "type": "code",
        "location": "/pyjom/imagetoolbox.py:341-369"
    },
    "277": {
        "file_id": 15,
        "content": "Code performs K-means clustering and calculates the percentage of labeled points in each cluster. It then creates a mask based on the cluster centers and applies it to an image, counting the positive pixels and checking if they are close to 3.",
        "type": "comment"
    },
    "278": {
        "file_id": 15,
        "content": "        percent = positive_count / len(mOutput)\n        # print(mOutput)\n        # print(mOutput.shape)\n        # breakpoint()\n        # print(\"CENTER:\",center)\n        # print('POSITIVE COUNT:', positive_count)\n        # mSum = sum(mOutput)\n        # print(\"SUM:\", mSum, \"MIN:\", min(mOutput), 'MAX:', max(mOutput))\n        # print(\"NEARBY CENTER PERCENTAGE: {:.2f} %\".format(percent*100))\n        percents.append(percent)\n    max_nearby_center_percentage = max(percents)\n    print(\n        \"NEARBY CENTER PERCENTAGE: {:.2f} %\".format(max_nearby_center_percentage * 100)\n    )\n    centrality = sum(percents)\n    print(\"CENTRALITY: {:.2f} %\".format(centrality * 100))\n    del kmeans\n    del kmeans_model\n    return centrality, max_nearby_center_percentage\nimport math\ndef scanImageWithWindowSizeAutoResize(\n    image,\n    width,\n    height,\n    return_direction=False,\n    threshold=0.1,  # minimum 'fresh' area left for scanning\n):  # shall you use torch? no?\n    shape = image.shape\n    assert len(shape) == 3\n    ih, iw, channels = shape",
        "type": "code",
        "location": "/pyjom/imagetoolbox.py:370-403"
    },
    "279": {
        "file_id": 15,
        "content": "The code calculates the centrality and maximum nearby center percentage of image features using K-means clustering. It scans an image with a resizable window, returns centrality and max nearby center percentages, and optionally the direction if specified.",
        "type": "comment"
    },
    "280": {
        "file_id": 15,
        "content": "    targetWidth = max(width, math.floor(iw * height / ih))\n    targetHeight = max(height, math.floor(ih * width / iw))\n    resized = cv2.resize(\n        image, (targetWidth, targetHeight), interpolation=cv2.INTER_CUBIC\n    )\n    # determine scan direction here.\n    imageSeries = []\n    if targetWidth / targetHeight == width / height:\n        imageSeries = [resized]  # as image series.\n        direction = None\n    elif targetWidth / targetHeight < width / height:\n        direction = \"vertical\"\n        # the scanning is along the vertical axis, which is the height.\n        index = 0\n        while True:\n            start, end = height * index, height * (index + 1)\n            if start < targetHeight:\n                if end > targetHeight:\n                    if 1 - (end - targetHeight) / targetHeight >= threshold:\n                        end = targetHeight\n                        start = targetHeight - height\n                    else:\n                        break\n                # other conditions, just fine\n            else:",
        "type": "code",
        "location": "/pyjom/imagetoolbox.py:404-428"
    },
    "281": {
        "file_id": 15,
        "content": "This code resizes an image to a target size while considering the aspect ratio, and then determines the scan direction based on the width-to-height ratio. If the ratios are equal, it adds the resized image as the only element in the image series. Otherwise, if the width-to-height ratio is less than the target ratio, it scans vertically along the height axis, defining a start and end point for each segment based on threshold values.",
        "type": "comment"
    },
    "282": {
        "file_id": 15,
        "content": "                break  # must exit since nothing to scan.\n            cropped = resized[start:end, :, :]  # height, width, channels\n            imageSeries.append(cropped)\n            index += 1\n    else:\n        direction = \"horizontal\"\n        index = 0\n        while True:\n            start, end = width * index, width * (index + 1)\n            if start < targetWidth:\n                if end > targetWidth:\n                    if 1 - (end - targetWidth) / targetWidth >= threshold:\n                        end = targetWidth\n                        start = targetWidth - width\n                    else:\n                        break\n                # other conditions, just fine\n            else:\n                break  # must exit since nothing to scan.\n            cropped = resized[:, start:end, :]  # height, width, channels\n            imageSeries.append(cropped)\n            index += 1\n    if return_direction:\n        return imageSeries, direction\n    else:\n        return imageSeries\nfrom typing import Literal\ndef resizeImageWithPadding(",
        "type": "code",
        "location": "/pyjom/imagetoolbox.py:429-460"
    },
    "283": {
        "file_id": 15,
        "content": "This code snippet is used for cropping an image based on a target width, either vertically or horizontally. If the target width cannot be reached, it stops cropping. It appends each cropped section to a list of images and returns them along with the direction (horizontal or vertical) if specified.",
        "type": "comment"
    },
    "284": {
        "file_id": 15,
        "content": "    image,\n    width: Union[int, None],\n    height: Union[int, None],\n    border_type: Literal[\"constant_black\", \"replicate\"] = \"constant_black\",\n):\n    assert any([type(param) == int for param in [width, height]])\n    shape = image.shape\n    assert len(shape) == 3\n    ih, iw, channels = shape\n    if width is None:\n        width = max(1, math.floor((height / ih) * iw))\n    if height is None:\n        height = max(1, math.floor((width / iw) * ih))\n    targetWidth = min(width, math.floor(iw * height / ih))\n    targetHeight = min(height, math.floor(ih * width / iw))\n    resized = cv2.resize(\n        image, (targetWidth, targetHeight), interpolation=cv2.INTER_CUBIC\n    )\n    BLACK = [0] * channels\n    top = max(0, math.floor((height - targetHeight) / 2))\n    bottom = max(0, height - targetHeight - top)\n    left = max(0, math.floor((width - targetWidth) / 2))\n    right = max(0, width - targetWidth - left)\n    if border_type == \"constant_black\":\n        padded = cv2.copyMakeBorder(\n            resized, top, bottom, left, right, cv2.BORDER_CONSTANT, value=BLACK",
        "type": "code",
        "location": "/pyjom/imagetoolbox.py:461-487"
    },
    "285": {
        "file_id": 15,
        "content": "This code resizes an image while maintaining its aspect ratio, allowing for optional border padding. It first checks if width and height are of type int. If not, it assigns default values based on the provided parameters. Then, calculates targetWidth and targetHeight to ensure proper scaling without distortion. Finally, uses OpenCV's cv2.copyMakeBorder function with a constant black value for border_type to create the final padded image.",
        "type": "comment"
    },
    "286": {
        "file_id": 15,
        "content": "        )\n    elif border_type == \"replicate\":\n        padded = cv2.copyMakeBorder(\n            resized, top, bottom, left, right, cv2.BORDER_REPLICATE, value=BLACK\n        )\n    else:\n        raise Exception(\"unknown border_type: %s\" % border_type)\n    return padded\nimport paddlehub as hub\nfrom functools import lru_cache\n@lru_cache(maxsize=1)\ndef getPaddleResnet50AnimalsClassifier():\n    classifier = hub.Module(name=\"resnet50_vd_animals\")\n    return classifier\n@lru_cache(maxsize=3)\ndef labelFileReader(filename):\n    with open(filename, \"r\") as f:\n        content = f.read()\n        content = content.split(\"\\n\")\n        content = [elem.replace(\"\\n\", \"\").strip() for elem in content]\n        content = [elem for elem in content if len(elem) > 0]\n    return content\nfrom pyjom.mathlib import multiParameterExponentialNetwork\nBEZIER_PADDLE_RESNET50_IMAGE_DOG_CAT_DETECTOR_SERVER_ENDPOINT = \"analyzeImage\"\nBEZIER_PADDLE_RESNET50_IMAGE_DOG_CAT_DETECTOR_SERVER_PORT = 4675\nBEZIER_PADDLE_RESNET50_IMAGE_DOG_CAT_DETECTOR_SERVER_HELLO = (",
        "type": "code",
        "location": "/pyjom/imagetoolbox.py:488-522"
    },
    "287": {
        "file_id": 15,
        "content": "This code defines functions to get a PaddlePaddle ResNet50 animals classifier, read label files, and possibly connects to a BEZIER server for a PaddlePaddle ResNet50 dog/cat detector.",
        "type": "comment"
    },
    "288": {
        "file_id": 15,
        "content": "    \"Bezier PaddleHub Resnet50 Image DogCat Detector Server\"\n)\nfrom pyjom.config.shared import pyjom_config\n# TODO: support serving and with redis lock\nfrom lazero.network.checker import waitForServerUp\ndef bezierPaddleHubResnet50ImageDogCatDetectorServerChecker(\n    port=BEZIER_PADDLE_RESNET50_IMAGE_DOG_CAT_DETECTOR_SERVER_PORT,\n    message=BEZIER_PADDLE_RESNET50_IMAGE_DOG_CAT_DETECTOR_SERVER_HELLO,\n):\n    waitForServerUp(port=port, message=message)\n# fuck?\nif not pyjom_config[\"BEZIER_PADDLE_RESNET50_IMAGE_DOG_CAT_DETECTOR_SERVER_INSTANCE\"]:\n    bezierPaddleHubResnet50ImageDogCatDetectorServerChecker()\ndef bezierPaddleHubResnet50ImageDogCatDetectorClient(\n    image,\n    port=BEZIER_PADDLE_RESNET50_IMAGE_DOG_CAT_DETECTOR_SERVER_PORT,\n    endpoint=BEZIER_PADDLE_RESNET50_IMAGE_DOG_CAT_DETECTOR_SERVER_ENDPOINT,\n    input_bias=0.0830047243746045,\n    skew=-0.4986098769473948,\n    # threshold=0.5,\n    dog_label_file_path=\"/root/Desktop/works/pyjom/tests/animals_paddlehub_classification_resnet/dogs.txt\",\n    cat",
        "type": "code",
        "location": "/pyjom/imagetoolbox.py:523-552"
    },
    "289": {
        "file_id": 15,
        "content": "The code defines a function for the Bezier PaddleHub Resnet50 Image DogCat Detector Server checker, which waits for the server to be up and running. If the server instance is not configured in the pyjom_config, it calls the checker function. The function also includes parameters for image, port, endpoint, input_bias, skew, dog_label_file_path, cat, and other possible parameters.",
        "type": "comment"
    },
    "290": {
        "file_id": 15,
        "content": "_label_file_path=\"/root/Desktop/works/pyjom/tests/animals_paddlehub_classification_resnet/cats.txt\",\n    debug=False,\n):\n    isString = type(image) == str\n    import requests\n    url = \"http://localhost:{}/{}\".format(port, endpoint)\n    import numpy_serializer\n    if type(image) == np.ndarray:\n        image = numpy_serializer.to_bytes(image)\n    data = dict(image=image)\n    params = dict(\n        input_bias=input_bias,\n        isBytes=not isString,\n        skew=skew,\n        dog_label_file_path=dog_label_file_path,\n        cat_label_file_path=cat_label_file_path,\n        debug=debug,\n    )\n    r = requests.post(url, data=data, params=params)\n    # what is the result? fuck?\n    return r.json()\ndef bezierPaddleHubResnet50ImageDogCatDetectorCore(\n    image,\n    input_bias=0.0830047243746045,\n    skew=-0.4986098769473948,\n    # threshold=0.5,\n    dog_label_file_path=\"/root/Desktop/works/pyjom/tests/animals_paddlehub_classification_resnet/dogs.txt\",\n    cat_label_file_path=\"/root/Desktop/works/pyjom/tests/animals_paddlehub_classification_resnet/cats.txt\",",
        "type": "code",
        "location": "/pyjom/imagetoolbox.py:552-583"
    },
    "291": {
        "file_id": 15,
        "content": "This function uses a PaddleHub ResNet50 model to classify images as either cat or dog. It takes an image input, and optionally adjusts the input_bias and skew parameters for fine-tuning. The labels for cats and dogs are specified in separate text files. If the image is not in byte format, it converts it using numpy_serializer before making a POST request to a local server endpoint. It returns the response in JSON format.",
        "type": "comment"
    },
    "292": {
        "file_id": 15,
        "content": "    debug=False,\n    use_gpu=False,\n    dog_suffixs=[\"狗\", \"犬\", \"梗\"],\n    cat_suffixs=[\"猫\"],\n    forbidden_words=[\n        \"灵猫\",\n        \"熊猫\",\n        \"猫狮\",\n        \"猫头鹰\",\n        \"丁丁猫儿\",\n        \"绿猫鸟\",\n        \"猫鼬\",\n        \"猫鱼\",\n        \"玻璃猫\",\n        \"猫眼\",\n        \"猫蛱蝶\",\n    ],\n):\n    curve_function_kwargs = {\n        \"start\": (0, 0),\n        \"end\": (1, 1),\n        \"skew\": skew,\n    }  # maximize the output.\n    if type(image) == str:\n        image = cv2.imread(image)\n    frame = image\n    # ends with this, and not containing forbidden words.\n    dog_labels = labelFileReader(dog_label_file_path)\n    cat_labels = labelFileReader(cat_label_file_path)\n    def dog_cat_name_recognizer(name):\n        if name in dog_labels:\n            return \"dog\"\n        elif name in cat_labels:\n            return \"cat\"\n        elif name not in forbidden_words:\n            for dog_suffix in dog_suffixs:\n                if name.endswith(dog_suffix):\n                    return \"dog\"\n            for cat_suffix in cat_suffixs:\n                if name.endswith(cat_suffix):",
        "type": "code",
        "location": "/pyjom/imagetoolbox.py:584-625"
    },
    "293": {
        "file_id": 15,
        "content": "The code reads image labels from two label files and checks if the given name matches any of them or ends with a specific dog/cat suffix. If it does not contain forbidden words, it categorizes the input as a \"dog\" or \"cat\". The function is used to detect if an image contains a dog or cat based on the provided name.",
        "type": "comment"
    },
    "294": {
        "file_id": 15,
        "content": "                    return \"cat\"\n        return None\n    classifier = getPaddleResnet50AnimalsClassifier()\n    def paddleAnimalDetectionResultToList(result):\n        resultDict = result[0]\n        resultList = [(key, value) for key, value in resultDict.items()]\n        resultList.sort(key=lambda item: -item[1])\n        return resultList\n    def translateResultListToDogCatList(resultList):\n        final_result_list = []\n        for name, confidence in resultList:\n            new_name = dog_cat_name_recognizer(name)\n            final_result_list.append((new_name, confidence))\n        return final_result_list\n    # dataList = []\n    # for frame in getVideoFrameIteratorWithFPS(videoPath, -1, -1, fps=1):\n    padded_resized_frame = resizeImageWithPadding(\n        frame, 224, 224, border_type=\"replicate\"\n    )  # pass the test only if three of these containing 'cats'\n    result = classifier.classification(\n        images=[padded_resized_frame], top_k=3, use_gpu=use_gpu  # cuda oom?\n    )  # check it?\n    resultList = paddleAnimalDetectionResultToList(result)",
        "type": "code",
        "location": "/pyjom/imagetoolbox.py:626-652"
    },
    "295": {
        "file_id": 15,
        "content": "This code segment is responsible for performing animal detection and classifying the detected animals as either \"cat\" or None. It first defines two functions: `paddleAnimalDetectionResultToList()` for converting the detection result into a sorted list of names and confidence values, and `translateResultListToDogCatList()` for translating the list to distinguish between dogs and cats using the `dog_cat_name_recognizer`. The code then initializes a PaddleResnet50AnimalsClassifier and performs animal detection on the provided frame, getting the top 3 classification results. These results are converted into a sorted list of dog/cat names with confidence values, which can be used to determine if there is a cat present in the frame.",
        "type": "comment"
    },
    "296": {
        "file_id": 15,
        "content": "    final_result_list = translateResultListToDogCatList(resultList)\n    if debug:\n        sprint(\"RESULT LIST:\", final_result_list)\n    detections = []\n    for index, (label, confidence) in enumerate(final_result_list):\n        scope = final_result_list[index:]\n        scope_confidences = [elem[1] for elem in scope if elem[0] == label]\n        output = multiParameterExponentialNetwork(\n            *scope_confidences,\n            input_bias=input_bias,\n            curve_function_kwargs=curve_function_kwargs,\n        )\n        # treat each as a separate observation in this frame.\n        detections.append({\"identity\": label, \"confidence\": output})\n    return detections\ndef bezierPaddleHubResnet50ImageDogCatDetector(\n    image,\n    input_bias=0.0830047243746045,\n    skew=-0.4986098769473948,\n    # threshold=0.5,\n    dog_label_file_path=\"/root/Desktop/works/pyjom/tests/animals_paddlehub_classification_resnet/dogs.txt\",\n    cat_label_file_path=\"/root/Desktop/works/pyjom/tests/animals_paddlehub_classification_resnet/cats.txt\",",
        "type": "code",
        "location": "/pyjom/imagetoolbox.py:653-676"
    },
    "297": {
        "file_id": 15,
        "content": "The code translates a result list to dog/cat detection list and applies a multi-parameter exponential network function to each detection's confidence score. The detections are appended to a list with respective identities and confidences, then returned as the final output. It uses PaddleHub ResNet model for dog/cat detection on input images.",
        "type": "comment"
    },
    "298": {
        "file_id": 15,
        "content": "    debug=False,\n    use_gpu=False,\n    as_client=True,  # by default!\n):\n    if as_client:\n        return bezierPaddleHubResnet50ImageDogCatDetectorClient(\n            image,\n            input_bias=input_bias,\n            skew=skew,\n            dog_label_file_path=dog_label_file_path,\n            cat_label_file_path=cat_label_file_path,\n            debug=debug,\n        )\n    # from pyjom.imagetoolbox import resizeImageWithPadding\n    detections = bezierPaddleHubResnet50ImageDogCatDetectorCore(\n        image,\n        input_bias=input_bias,\n        skew=skew,\n        dog_label_file_path=dog_label_file_path,\n        cat_label_file_path=cat_label_file_path,\n        debug=debug,\n        use_gpu=use_gpu,\n    )\n    return detections\n# TODO: create/get a redis based lock when doing image checks.\nimport redis_lock\nimport redis\nfrom pyjom.commons import commonRedisPort\nredis_connection = redis.StrictRedis(port=commonRedisPort)\ndef bezierPaddleHubResnet50ImageDogCatDetectorServer(\n    server_port=BEZIER_PADDLE_RESNET50_IMAGE_DOG_CAT_DETECTOR_SERVER_PORT,",
        "type": "code",
        "location": "/pyjom/imagetoolbox.py:677-712"
    },
    "299": {
        "file_id": 15,
        "content": "This code defines a function `bezierPaddleHubResnet50ImageDogCatDetector` that takes an image and some optional parameters, and returns dog and cat detection results. If the `as_client` parameter is True, it creates a client instance of the detector; otherwise, it calls the core detection function. The code also includes a comment about adding a Redis-based lock for image checks in the future. The function also imports necessary modules and sets up a Redis connection for potential future use.",
        "type": "comment"
    }
}