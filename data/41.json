{
    "4100": {
        "file_id": 503,
        "content": "two main problems, one is to detect identical video files, one is to find 'repeated interval' inside each other.",
        "type": "code",
        "location": "/tests/video_phash_deduplication/README.md:1-1"
    },
    "4101": {
        "file_id": 503,
        "content": "Code detects identical video files and finds repeated intervals within each video file.",
        "type": "comment"
    },
    "4102": {
        "file_id": 504,
        "content": "/tests/video_phash_deduplication/config_milvus.sh",
        "type": "filepath"
    },
    "4103": {
        "file_id": 504,
        "content": "This code sets up Milvus, a vector database, by creating its configuration directory and downloading the necessary server and log configurations before starting the Milvus services.",
        "type": "summary"
    },
    "4104": {
        "file_id": 504,
        "content": "# # Create Milvus file\n# $ mkdir -p /home/$USER/milvus/conf\n# $ cd /home/$USER/milvus/conf\n# $ wget https://raw.githubusercontent.com/milvus-io/milvus/v0.8.0/core/conf/demo/server_config.yaml\n# $ wget https://raw.githubusercontent.com/milvus-io/milvus/v0.8.0/core/conf/demo/log_config.conf\nsudo systemctl start milvus\nsudo systemctl start milvus-etcd\nsudo systemctl start milvus-minio",
        "type": "code",
        "location": "/tests/video_phash_deduplication/config_milvus.sh:1-8"
    },
    "4105": {
        "file_id": 504,
        "content": "This code sets up Milvus, a vector database, by creating its configuration directory and downloading the necessary server and log configurations before starting the Milvus services.",
        "type": "comment"
    },
    "4106": {
        "file_id": 505,
        "content": "/tests/medialang_reference/videolang.mdl",
        "type": "filepath"
    },
    "4107": {
        "file_id": 505,
        "content": "The code defines two media streams: the first one is a video with a .mp4 extension and a speed of 1.5, while the second one is an audio file also with a .mp4 extension located at \"/root/Desktop/works/pyjom/src/samples/video/dog_with_text.mp4\", set to play at 1.5 speed but without video output.",
        "type": "summary"
    },
    "4108": {
        "file_id": 505,
        "content": "(\".mp4\", speed=1.5\n)\n(\"/root/Desktop/works/pyjom/src/samples/video/dog_with_text.mp4\",\n    speed=1.5, video=false\n)",
        "type": "code",
        "location": "/tests/medialang_reference/videolang.mdl:1-6"
    },
    "4109": {
        "file_id": 505,
        "content": "The code defines two media streams: the first one is a video with a .mp4 extension and a speed of 1.5, while the second one is an audio file also with a .mp4 extension located at \"/root/Desktop/works/pyjom/src/samples/video/dog_with_text.mp4\", set to play at 1.5 speed but without video output.",
        "type": "comment"
    },
    "4110": {
        "file_id": 506,
        "content": "/tests/medialang_reference/recursive_audiolang.mdl",
        "type": "filepath"
    },
    "4111": {
        "file_id": 506,
        "content": "This code sets the audio language model and specifies its configuration. It uses a subtitle detector with an input file located at \"/root/Desktop/works/pyjom/src/samples/video/dog_with_text.mp4\". The speed of processing is set to 1.5x, and the output file is generated from the template at \"/root/Desktop/works/pyjom/src/test/audiolang.mdl.j2\" with format arguments \"some_number\": 2 and \"some_text\": \"shit\".",
        "type": "summary"
    },
    "4112": {
        "file_id": 506,
        "content": "(\".mp3\", processor=\"subtitle_detector\"\n)\n(\"/root/Desktop/works/pyjom/src/samples/video/dog_with_text.mp4\",\n    speed=1.5\n)\n(\"/root/Desktop/works/pyjom/src/test/audiolang.mdl.j2\",\n    format_args={\"some_number\": 2, \"some_text\": \"shit\"}\n)",
        "type": "code",
        "location": "/tests/medialang_reference/recursive_audiolang.mdl:1-10"
    },
    "4113": {
        "file_id": 506,
        "content": "This code sets the audio language model and specifies its configuration. It uses a subtitle detector with an input file located at \"/root/Desktop/works/pyjom/src/samples/video/dog_with_text.mp4\". The speed of processing is set to 1.5x, and the output file is generated from the template at \"/root/Desktop/works/pyjom/src/test/audiolang.mdl.j2\" with format arguments \"some_number\": 2 and \"some_text\": \"shit\".",
        "type": "comment"
    },
    "4114": {
        "file_id": 507,
        "content": "/tests/medialang_reference/processor_multi.mdl",
        "type": "filepath"
    },
    "4115": {
        "file_id": 507,
        "content": "The code is creating a pipeline to process a video file and associated .json files. It uses a video file \"dog_with_text.mp4\" at 1.5x speed, followed by subtitle detection and active region detection on the .json files.",
        "type": "summary"
    },
    "4116": {
        "file_id": 507,
        "content": "(\"/root/Desktop/works/pyjom/src/samples/video/dog_with_text.mp4\",\n    speed=1.5\n)\n(\".json\", processor=\"subtitle_detector\"\n)\n(\".json\",\n    processor=\"active_region_detector\"\n)",
        "type": "code",
        "location": "/tests/medialang_reference/processor_multi.mdl:1-10"
    },
    "4117": {
        "file_id": 507,
        "content": "The code is creating a pipeline to process a video file and associated .json files. It uses a video file \"dog_with_text.mp4\" at 1.5x speed, followed by subtitle detection and active region detection on the .json files.",
        "type": "comment"
    },
    "4118": {
        "file_id": 508,
        "content": "/tests/medialang_reference/processor_demo.mdl",
        "type": "filepath"
    },
    "4119": {
        "file_id": 508,
        "content": "The code specifies a video file path and sets the playback speed. The \"subtitle_detector\" processor is applied to a JSON file.",
        "type": "summary"
    },
    "4120": {
        "file_id": 508,
        "content": "(\".json\", processor=\"subtitle_detector\"\n)\n(\"/root/Desktop/works/pyjom/samples/video/dog_with_text.mp4\",\n    speed=1.5\n)",
        "type": "code",
        "location": "/tests/medialang_reference/processor_demo.mdl:1-6"
    },
    "4121": {
        "file_id": 508,
        "content": "The code specifies a video file path and sets the playback speed. The \"subtitle_detector\" processor is applied to a JSON file.",
        "type": "comment"
    },
    "4122": {
        "file_id": 509,
        "content": "/tests/medialang_reference/audiolang.mdl.j2",
        "type": "filepath"
    },
    "4123": {
        "file_id": 509,
        "content": "The code defines a sequence of media items to be played. It includes audio and video files, with options for speed adjustment and text-to-speech conversion. The code also utilizes a loop to create multiple instances of these media items, replacing some variables like 'some_number' and 'some_text'.",
        "type": "summary"
    },
    "4124": {
        "file_id": 509,
        "content": "(\".mp3\", speed=1.5\n)\n(\"/root/Desktop/works/pyjom/src/samples/video/dog_with_text.mp4\",\n    speed=1.5, video=false\n)\n    (\"/root/Desktop/works/pyjom/src/samples/video/IxEQhCslT.mp4\",\n        padding=\"black\"\n    )\n        (\"text://you did a good job\",\n            converter=\"voice\", provider=\"tts_male\",\n            speed=2\n        )\n{% for i in range(some_number) %}\n(\"/root/Desktop/works/pyjom/src/samples/video/dog_with_text.mp4\",\n    speed={{ some_number }}\n)\n    (\"text://{{ some_text }}\",\n        converter=\"voice\"\n    )\n{% endfor %}",
        "type": "code",
        "location": "/tests/medialang_reference/audiolang.mdl.j2:1-23"
    },
    "4125": {
        "file_id": 509,
        "content": "The code defines a sequence of media items to be played. It includes audio and video files, with options for speed adjustment and text-to-speech conversion. The code also utilizes a loop to create multiple instances of these media items, replacing some variables like 'some_number' and 'some_text'.",
        "type": "comment"
    },
    "4126": {
        "file_id": 510,
        "content": "/tests/medialang_reference/audiolang.mdl",
        "type": "filepath"
    },
    "4127": {
        "file_id": 510,
        "content": "The code represents a sequence of media items and text to be processed by the program. It includes audio, video files, and text strings for conversion. The media items are specified with their respective paths and properties like speed and padding. The text strings are to be converted using the \"voice\" converter and may have additional properties like provider and speed.",
        "type": "summary"
    },
    "4128": {
        "file_id": 510,
        "content": "(\".mp3\", speed=1.5\n)\n(\"/root/Desktop/works/pyjom/src/samples/video/dog_with_text.mp4\",\n    speed=1.5, video=false\n)\n    (\"/root/Desktop/works/pyjom/src/samples/video/IxEQhCslT.mp4\",\n        padding=\"black\"\n    )\n        (\"text://you did a good job\",\n            converter=\"voice\", provider=\"tts_male\",\n            speed=2\n        )\n(\"/root/Desktop/works/pyjom/src/samples/video/dog_with_text.mp4\",\n    speed=1.5\n)\n    (\"text://you did a bad job\",\n        converter=\"voice\"\n    )",
        "type": "code",
        "location": "/tests/medialang_reference/audiolang.mdl:1-20"
    },
    "4129": {
        "file_id": 510,
        "content": "The code represents a sequence of media items and text to be processed by the program. It includes audio, video files, and text strings for conversion. The media items are specified with their respective paths and properties like speed and padding. The text strings are to be converted using the \"voice\" converter and may have additional properties like provider and speed.",
        "type": "comment"
    },
    "4130": {
        "file_id": 511,
        "content": "/tests/video_detector_tests/yolo_norfair.py",
        "type": "filepath"
    },
    "4131": {
        "file_id": 511,
        "content": "This code initializes Detectron2 for instance segmentation on COCO dataset, sets up a YOLO object detector for video frames, and tracks their positions across frames. It processes detection information, updates tracked objects using a tracker, draws circles with names based on estimated positions. The code displays a video frame, waits for 'q' key press to break loop, destroys windows, and writes the frame to file if display is not enabled.",
        "type": "summary"
    },
    "4132": {
        "file_id": 511,
        "content": "import cv2\nimport numpy as np\nfrom detectron2.config import get_cfg\nfrom detectron2.engine import DefaultPredictor\nfrom cocoNames import cocoName\nfrom norfair import Detection, Tracker, Video, draw_tracked_objects\ndef euclidean_distance(detection, tracked_object):\n    return np.linalg.norm(detection.points - tracked_object.estimate)\n# Set up Detectron2 object detector\ncfg = get_cfg()\ncfg.merge_from_file(\"norfair/demos/faster_rcnn_R_50_FPN_3x.yaml\")\ncfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5 # looks like it does not recognize dog.\n# cfg.MODEL.WEIGHTS = \"detectron2://COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl\"\ncfg.MODEL.WEIGHTS = \"/root/Desktop/works/pyjom/tests/video_detector_tests/detectron2_models/model_final_f10217.pkl\"\n# it is stored in s3\n# https://dl.fbaipublicfiles.com/detectron2/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl\n# download cache: /root/.torch/iopath_cache/detectron2/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl",
        "type": "code",
        "location": "/tests/video_detector_tests/yolo_norfair.py:1-22"
    },
    "4133": {
        "file_id": 511,
        "content": "The code imports necessary libraries and sets up the Detectron2 object detector using a pre-trained model for instance segmentation on COCO dataset. The model weight file is located at \"/root/Desktop/works/pyjom/tests/video_detector_tests/detectron2\\_models/model\\_final\\_f10217.pkl\".",
        "type": "comment"
    },
    "4134": {
        "file_id": 511,
        "content": "detector = DefaultPredictor(cfg)\n# what are the classes output by the model?\n# Norfair\n# video_path = \"/root/Desktop/works/pyjom/samples/video/dog_with_text.mp4\"\nvideo_path = \"/root/Desktop/works/pyjom/samples/video/LlfeL29BP.mp4\"\nvideo = Video(input_path=video_path)\ntracker = Tracker(distance_function=euclidean_distance, distance_threshold=200,hit_inertia_min=2,hit_inertia_max=20,initialization_delay=1) # what the heck?\n# you should implement standalone tracker function, optical.\n# maybe you can track the object via framedifference?\ntracked_objects = None\ndisplay=True\nfor index, frame in enumerate(video): # we need to speed up.\n    if index%10 == 0:\n        detections = detector(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n        print(\"original detections:\",detections)\n        instances = detections[\"instances\"]\n        print(\"instances:\",instances)\n        # breakpoint()\n        pred_classes = instances.pred_classes\n        if len(pred_classes) == 0:\n            continue\n        detections2=[]\n        for index,class_ in enumerate(pred_classes):",
        "type": "code",
        "location": "/tests/video_detector_tests/yolo_norfair.py:24-48"
    },
    "4135": {
        "file_id": 511,
        "content": "This code is initializing a YOLO object detector, reading a video file, setting up a tracker, and then iterating through each frame of the video. The detector predicts objects in each frame, and if any are detected, it saves the detections and continues to the next frame. If no objects are detected, it skips that frame. The tracker helps keep track of object positions across frames, but the implementation details are unclear.",
        "type": "comment"
    },
    "4136": {
        "file_id": 511,
        "content": "            print(\"index:\",index)\n            class_ = int(class_.cpu().numpy().tolist())\n            print(\"class:\",class_)\n            box = instances.pred_boxes.tensor[index].cpu().numpy().tolist()\n            box = [int(x) for x in box]\n            score = float(instances.scores[index].cpu().numpy().tolist())\n            print('box:',box)\n            print('score:',score)\n            det = Detection(instances.pred_boxes.get_centers()[index].cpu().numpy(),scores=np.array([score]),data={\"box\":box,\"class\":{\"id\":class_,\"name\":cocoRealName[class_]}})\n            detections2.append(det)\n            # breakpoint()\n        # detections = [Detection(p) for p in instances.pred_boxes.get_centers().cpu().numpy()] # what is this instance anyway?\n        # you would lost data you dick!\n        print(\"detections2\",detections2)\n        tracked_objects = tracker.update(detections=detections2)\n        # print(detections)\n        print(\"tracked objects:\",tracked_objects) # you don't track shit?\n    if tracked_objects is not None:",
        "type": "code",
        "location": "/tests/video_detector_tests/yolo_norfair.py:49-66"
    },
    "4137": {
        "file_id": 511,
        "content": "The code is processing and printing detection information, appending detections to a list (detections2), and updating tracked objects using a tracker. The comment criticizes a previous line of code for potentially losing data. The final if statement checks if any tracked_objects were found.",
        "type": "comment"
    },
    "4138": {
        "file_id": 511,
        "content": "        if tracked_objects!=[]:\n            # there is no bounding box avaliable?\n            for obj in tracked_objects:\n                point = obj.estimate[0]\n                position = tuple(point.astype(int))\n                color = (255,0,0)\n                # breakpoint()\n                name = obj.last_detection.data[\"class\"][\"name\"]\n                cv2.circle(\n                        frame,\n                        position,\n                        radius=100,\n                        color=color,\n                        thickness=2,\n                    )\n                cv2.putText(\n                    frame,\n                    \"[{}][{}]\".format(str(obj.id),name),\n                    (position[0]-100,position[1]),\n                    cv2.FONT_HERSHEY_SIMPLEX,\n                    2,\n                    (0,255,0),\n                    3,\n                    cv2.LINE_AA,\n                )\n            # breakpoint()\n        # i want to draw you in a different way.\n        # draw_tracked_objects(frame, tracked_objects,color=(255,0,0))",
        "type": "code",
        "location": "/tests/video_detector_tests/yolo_norfair.py:67-94"
    },
    "4139": {
        "file_id": 511,
        "content": "The code checks if there are any tracked objects. If there are, it estimates the object's position and draws a circle at that point on the frame. The object's name is also displayed near the circle. Finally, the code calls a function to draw the objects differently using a specific color.",
        "type": "comment"
    },
    "4140": {
        "file_id": 511,
        "content": "    if display:\n        cv2.imshow(\"window\",frame)\n        key  =  cv2.waitKey(1) & 0xff\n        if key == ord('q'):\n            break\n        # maybe we shall print this shit somehow.\n    # video.write(frame) # you write what?\nif display:\n    cv2.destroyAllWindows()",
        "type": "code",
        "location": "/tests/video_detector_tests/yolo_norfair.py:95-103"
    },
    "4141": {
        "file_id": 511,
        "content": "This code snippet displays a video frame on the screen, waits for a key press (specifically 'q'), and breaks the loop if 'q' is pressed. It then destroys all windows created. The frame is written to the video file if display is not enabled.",
        "type": "comment"
    },
    "4142": {
        "file_id": 512,
        "content": "/tests/video_detector_tests/yolo.py",
        "type": "filepath"
    },
    "4143": {
        "file_id": 512,
        "content": "The code reads video frames, uses YOLOv5 model for object detection and text extraction, sets model directories and environment variables, performs inference, and stores results in a DataFrame. It detects objects (likely a dog) and displays image with details before exiting upon key press.",
        "type": "summary"
    },
    "4144": {
        "file_id": 512,
        "content": "image_path = \"../../samples/video/dog_with_text.mp4\"\nimport cv2\nvideo = cv2.VideoCapture(image_path)\nfor _ in range(100):\n    ret, frame = video.read() # first frame is blackout!\nimport torch\n# don't really know how paddleocr recognize chars.\nlocalModelDir = '/root/Desktop/works/pyjom/pyjom/models/yolov5/ultralytics_yolov5_master/'\nimport os\nos.environ[\"YOLOV5_MODEL_DIR\"] = '/root/Desktop/works/pyjom/pyjom/models/yolov5/'\nmodel = torch.hub.load(localModelDir, 'yolov5s',source=\"local\") # the yolov5s.pt file is required when loading the model.\n# Image\n# img = 'https://ultralytics.com/images/zidane.jpg'\nimg = frame[:,:,::-1].transpose((2,0,1))\n# Inference\n# reshape this shit.\n# img = np.reshape()\nresults = model(img) # pass the image through our model\ndf = results.pandas().xyxy[0]\nprint(df)\ndata = []\nfor index,line in df.iterrows():\n    # print(line)\n    left = (line[\"xmin\"],line[\"ymin\"])\n    right = (line[\"xmax\"],line[\"ymax\"])\n    confidence = line[\"confidence\"]\n    class_ = line[\"class\"]\n    name = line[\"name\"]\n  ",
        "type": "code",
        "location": "/tests/video_detector_tests/yolo.py:1-37"
    },
    "4145": {
        "file_id": 512,
        "content": "The code reads a video frame by frame, applies object detection using YOLOv5 model, and extracts text from the detected objects. It sets the local model directory and environment variable for YOLOv5 model loading, loads the model, resizes and preprocesses the frame, performs inference, and stores the resultant bounding boxes with associated data (class, confidence, name) into a DataFrame.",
        "type": "comment"
    },
    "4146": {
        "file_id": 512,
        "content": "  data.append({\"location\":[left,right],\"confidence\":confidence,\"identity\":{\"class\":class_,\"name\":name}})\nprint(data)\ncv2.imshow(\"name\",frame)\ncv2.waitKey(0)\n# found the freaking dog!",
        "type": "code",
        "location": "/tests/video_detector_tests/yolo.py:37-41"
    },
    "4147": {
        "file_id": 512,
        "content": "This code detects an object (likely a dog) using YOLO and appends its location, confidence level, and identity details to the \"data\" list. The image is displayed with the name \"name\" and waits for a key press to exit. The comment celebrates finding the desired object.",
        "type": "comment"
    },
    "4148": {
        "file_id": 513,
        "content": "/tests/video_detector_tests/singleTracker.py",
        "type": "filepath"
    },
    "4149": {
        "file_id": 513,
        "content": "This code utilizes YOLOv5 for object detection and a video tracker to monitor dog movement in frames, identifying dogs and providing bounding box coordinates above threshold. Additionally, it closes OpenCV-created video windows.",
        "type": "summary"
    },
    "4150": {
        "file_id": 513,
        "content": "import cv2\n# import imutils #another dependency?\n# tracker = cv2.TrackerCSRT_create() # outdated tracker.\n# i really don't know what is a dog.\nimport torch\n# don't really know how paddleocr recognize chars.\nlocalModelDir = '/root/Desktop/works/pyjom/pyjom/models/yolov5/ultralytics_yolov5_master/'\nimport os\nos.environ[\"YOLOV5_MODEL_DIR\"] = '/root/Desktop/works/pyjom/pyjom/models/yolov5/'\nmodel = torch.hub.load(localModelDir, 'yolov5s',source=\"local\")\ndef getDogBB(frame,thresh=0.7):\n    img = frame[:,:,::-1].transpose((2,0,1))\n    # Inference\n    # reshape this shit.\n    # img = np.reshape()\n    results = model(img) # pass the image through our model\n    df = results.pandas().xyxy[0]\n    print(df)\n    data = []\n    for index,line in df.iterrows():\n        # print(line)\n        left = (line[\"xmin\"],line[\"ymin\"])\n        right = (line[\"xmax\"],line[\"ymax\"])\n        confidence = line[\"confidence\"]\n        class_ = line[\"class\"]\n        name = line[\"name\"]\n        if name == \"dog\" and confidence >= thresh: # better figure out all output names.",
        "type": "code",
        "location": "/tests/video_detector_tests/singleTracker.py:1-31"
    },
    "4151": {
        "file_id": 513,
        "content": "This code imports necessary libraries, sets the local model directory, and loads a YOLOv5 model for object detection. It defines a function to get the bounding box coordinates of a dog in an image, with the option to set a minimum confidence threshold. The code then performs inference using the loaded model on the input frame image and returns the bounding box information if the detected class is \"dog\" and the confidence meets or exceeds the threshold.",
        "type": "comment"
    },
    "4152": {
        "file_id": 513,
        "content": "            data.append({\"location\":[left,right],\"confidence\":confidence,\"identity\":{\"class\":class_,\"name\":name}})\n    print(data)\n    data = list(sorted(data,key=lambda x: -x[\"confidence\"]))\n    if len(data)>0:\n        target= data[0]\n        xmin,ymin = target[\"location\"][0]\n        xmax,ymax = target[\"location\"][1]\n        return int(xmin),int(ymin),int(xmax-xmin),int(ymax-ymin)\ndef checkDog(frame,thresh=0.5):\n    return getDogBB(frame,thresh=thresh) == None # dog missing.\n# better use something else?\n# tracker = cv2.TrackerMIL_create()\ntracker_types = ['MIL', 'GOTURN', 'DaSiamRPN']\ntracker_type = tracker_types[2]\nbasepath = \"./OpenCV-Object-Tracker-Python-Sample\"\nif tracker_type == 'MIL':\n    tracker = cv2.TrackerMIL_create()\nelif tracker_type == 'DaSiamRPN': # deeplearning.\n    # this tracker is slow as hell. really.\n    params = cv2.TrackerDaSiamRPN_Params()\n    params.model = os.path.join(basepath,\"model/DaSiamRPN/dasiamrpn_model.onnx\")\n    params.kernel_r1 = os.path.join(basepath,\"model/DaSiamRPN/dasiamrpn_kernel_r1.onnx\")",
        "type": "code",
        "location": "/tests/video_detector_tests/singleTracker.py:32-56"
    },
    "4153": {
        "file_id": 513,
        "content": "Code initializes a video tracker using different algorithms and prepares parameters for the 'DaSiamRPN' tracker, which is slower but uses deep learning. It then checks if a dog is present in each frame of a video and returns its bounding box coordinates.",
        "type": "comment"
    },
    "4154": {
        "file_id": 513,
        "content": "    params.kernel_cls1 = os.path.join(basepath,\"model/DaSiamRPN/dasiamrpn_kernel_cls1.onnx\")\n    tracker = cv2.TrackerDaSiamRPN_create(params)\n    # tracker = cv2.TrackerDaSiamRPN_create()\nelif tracker_type == 'GOTURN': #also need config file.\n    # this is bad though.\n    params = cv2.TrackerGOTURN_Params()\n    params.modelTxt = os.path.join(basepath,\"model/GOTURN/goturn.prototxt\") # save this shit without BOM.\n    params.modelBin = os.path.join(basepath,\"model/GOTURN/goturn.caffemodel\")\n    tracker = cv2.TrackerGOTURN_create(params)\n    # tracker = cv2.TrackerGOTURN_create()\n# we have to feed dog coordinates into the shit.\nvideo = cv2.VideoCapture(\"../../samples/video/dog_with_text.mp4\")\n_,frame = video.read()\n# frame = imutils.resize(frame,width=720) #why?\nindex = 0\nyoloRate = 10\ntrack_success = False\nupdate_track = 3\nBB = None\ninit=False\nwhile frame is not None:\n    index +=1\n    _, frame = video.read()\n    if frame is None:\n        print(\"VIDEO END.\")\n        break\n    if index%yoloRate == 0:\n        if BB is None:",
        "type": "code",
        "location": "/tests/video_detector_tests/singleTracker.py:57-86"
    },
    "4155": {
        "file_id": 513,
        "content": "This code initializes a tracker using the cv2.TrackerDaSiamRPN or cv2.TrackerGOTURN methods based on the tracker_type variable. It then creates parameters for the GOTURN tracker and creates a video capture object to read a video file. The loop reads frames from the video, checks if a bounding box is None every yoloRate frames, and if it's None, it initializes the BB variable. This code appears to be part of a video tracking application.",
        "type": "comment"
    },
    "4156": {
        "file_id": 513,
        "content": "            BB = getDogBB(frame)\n        else:\n            x, y, w, h = BB\n            if len(frame.shape) == 3:\n                dogFrame = frame[y:y+h,x:x+w,:]\n            else:\n                dogFrame = frame[y:y+h,x:x+w]\n            result = checkDog(dogFrame)\n            if result: # dog gone missing.\n                BB = getDogBB(frame)\n                init=False\n    if BB is not None:\n        if not init:\n            tracker.init(frame, BB) # how to init this shit?\n            init=True\n    # when lost, we know there is no dog inside the bounding box.\n    # frame = imutils.resize(frame,width=720)\n        if index % update_track == 0:\n            track_success,BB = tracker.update(frame)\n        if track_success and BB:\n            top_left = (int(BB[0]),int(BB[1]))\n            bottom_right = (int(BB[0]+BB[2]), int(BB[1]+BB[3]))\n            cv2.rectangle(frame,top_left,bottom_right,(0,255,0),5)\n    cv2.imshow('Output',frame)\n    key  =  cv2.waitKey(1) & 0xff\n    if key == ord('q'):\n        break\nvideo.release()",
        "type": "code",
        "location": "/tests/video_detector_tests/singleTracker.py:87-114"
    },
    "4157": {
        "file_id": 513,
        "content": "This code initializes a tracker and tracks a dog's movement in video frames. It checks if the dog is present in the bounding box and updates the position accordingly. If the dog goes missing, it reinitializes the tracker with new dog's bounding box. The tracked dog's position is displayed on the frame, which is then shown as output.",
        "type": "comment"
    },
    "4158": {
        "file_id": 513,
        "content": "cv2.destroyAllWindows()",
        "type": "code",
        "location": "/tests/video_detector_tests/singleTracker.py:115-115"
    },
    "4159": {
        "file_id": 513,
        "content": "This code is used to close all the video windows created by OpenCV (cv2).",
        "type": "comment"
    },
    "4160": {
        "file_id": 514,
        "content": "/tests/video_detector_tests/rectangle_test.py",
        "type": "filepath"
    },
    "4161": {
        "file_id": 514,
        "content": "This code imports OpenCV libraries, sets up a motion detector algorithm for object tracking and suggests improvements. It reads a video file, applies the algorithm to each frame, finds bounding boxes, handles stability issues, displays images, stores results in JSON format and prints \"DATA DUMPED\" upon successful execution.",
        "type": "summary"
    },
    "4162": {
        "file_id": 514,
        "content": "# motion detectors are used to track objects. though you may want to separate objects with it.\nimport progressbar\nimport json\nimport pybgs as bgs\nimport numpy as np\nimport pathlib\nimport sys\nsite_path = pathlib.Path(\"/usr/local/lib/python3.9/site-packages\")\ncv2_libs_dir = site_path / 'cv2' / \\\n    f'python-{sys.version_info.major}.{sys.version_info.minor}'\nprint(cv2_libs_dir)\ncv2_libs = sorted(cv2_libs_dir.glob(\"*.so\"))\nif len(cv2_libs) == 1:\n    print(\"INSERTING:\", cv2_libs[0].parent)\n    sys.path.insert(1, str(cv2_libs[0].parent))\nimport cv2\n# suspect by static image analysis, and then create bounding box over the thing.\n# check image quality.\n# for donga, you must change the framerate to skip identical frames.\n# also donga have strange things you may dislike, e.g.: when only part of the image changes.\n# algorithm = bgs.FrameDifference() # this is not stable since we have more boundaries. shall we group things?\n# can we use something else?\nalgorithm = bgs.WeightedMovingVariance()\n# this one with cropped boundaries.",
        "type": "code",
        "location": "/tests/video_detector_tests/rectangle_test.py:1-31"
    },
    "4163": {
        "file_id": 514,
        "content": "This code imports necessary libraries, checks the OpenCV library version, and sets up a motion detector algorithm (WeightedMovingVariance) for object tracking. It also suggests possible improvements like grouping boundaries or using a different algorithm if needed.",
        "type": "comment"
    },
    "4164": {
        "file_id": 514,
        "content": "# average shit.\n# video_file = \"../../samples/video/LiEIfnsvn.mp4\"\n# select our 娜姐驾到\nvideo_file = \"../../samples/video/LiGlReJ4i.mp4\"\n# video_file = \"../../samples/video/LiEIfnsvn.mp4\"\n# denoising, moving average, sampler and  similar merge.\n# moving average span: -20 frame to +20 frame\n# denoising: 选区间之内相似的最多的那种\ncapture = cv2.VideoCapture(video_file)\nwhile not capture.isOpened():\n    capture = cv2.VideoCapture(video_file)\n    cv2.waitKey(1000)\n    print(\"Wait for the header\")\ndefaultWidth = capture.get(cv2.CAP_PROP_FRAME_WIDTH)\ndefaultHeight = capture.get(cv2.CAP_PROP_FRAME_HEIGHT)\ntotal_frames = capture.get(cv2.CAP_PROP_FRAME_COUNT)\ntotal_frames = int(total_frames)\npipFrames = []\ndefaultRect = [(0,0),(defaultWidth,defaultHeight)]\npos_frame = capture.get(1)\nareaThreshold = int(0.2*0.2*defaultWidth*defaultHeight)\nfor index in progressbar.progressbar(range(total_frames)):\n    # if index % 20 != 0: continue\n    flag, frame = capture.read()\n    if flag:\n        pos_frame = capture.get(1)\n        img_output = algorithm.apply(frame)",
        "type": "code",
        "location": "/tests/video_detector_tests/rectangle_test.py:32-66"
    },
    "4165": {
        "file_id": 514,
        "content": "This code reads a video file and applies an algorithm to each frame. It also retrieves the frame width, height, total frames, and uses a progress bar for looping through each frame. The code has a skipping mechanism for every 20th frame and a threshold for area calculations.",
        "type": "comment"
    },
    "4166": {
        "file_id": 514,
        "content": "        imgThresh = img_output\n        # imgMorph = cv2.GaussianBlur(img_output, (3,3), 0)\n        # _,imgThresh = cv2.threshold(imgMorph, 1, 255, cv2.THRESH_BINARY)\n        # img_bgmodel = algorithm.getBackgroundModel()\n        # _, contours = cv2.findContours(\n        #     imgThresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n        # maybe you should merge all active areas.\n        # if contours is not None:\n            # continue\n            # counted = False\n            # maxArea = 0\n            # for contour in contours:\n        [x, y, w, h] = cv2.boundingRect(img_output) # wtf is this?\n        area = w*h\n        if area > areaThreshold:\n                # #     maxArea = area\n                # if counted==False:\n            min_x, min_y = x, y\n            max_x, max_y = x+w, y+h\n                # else:\n                #     if x<min_x: min_x = x\n                #     if x+w>max_x: max_x = x+w\n                #     if y<min_y: min_y = y\n                #     if y+w>max_y: max_y = y+w\n            currentRect = [(min_x, min_y), (max_x, max_y)]",
        "type": "code",
        "location": "/tests/video_detector_tests/rectangle_test.py:67-91"
    },
    "4167": {
        "file_id": 514,
        "content": "This code segment is responsible for finding the bounding box around objects in an image. The variables `x`, `y`, `w`, and `h` are the coordinates of the rectangle's top left corner, width, and height respectively. It uses a Gaussian blur on the image and applies a binary threshold to isolate objects. It then finds contours, checks if they exceed a certain area threshold, and updates the minimum and maximum coordinates of the bounding box accordingly.",
        "type": "comment"
    },
    "4168": {
        "file_id": 514,
        "content": "            pipFrames.append(currentRect.copy())\n            defaultRect = currentRect.copy()\n        else:\n            pipFrames.append(defaultRect.copy())\n            # how to stablize this shit?\n        # cv2.imshow('video', frame)\n        # cv2.imshow('img_output', img_output)\n        # cv2.imshow('img_bgmodel', img_bgmodel)\n        # cv2.imshow('imgThresh', imgThresh)\n        # cv2.waitKey(100)\n    else:\n        # cv2.waitKey(1000)\n        break\ncv2.destroyAllWindows()\n# we process this shit elsewhere.\nwith open(\"pip_meanVarianceSisterNa.json\", 'w') as f:\n    f.write(json.dumps(\n        {\"data\": pipFrames, \"width\": defaultWidth, \"height\": defaultHeight}))\nprint(\"DATA DUMPED\")",
        "type": "code",
        "location": "/tests/video_detector_tests/rectangle_test.py:92-116"
    },
    "4169": {
        "file_id": 514,
        "content": "Code captures frames, appends rectangles to pipFrames, and handles stability issues. It displays images on various windows using OpenCV. Breaks loop if no changes detected. Closes all windows after processing. Writes pipFrames data to a JSON file named \"pip_meanVarianceSisterNa.json\" with width and height information. Prints \"DATA DUMPED\" upon successful execution.",
        "type": "comment"
    },
    "4170": {
        "file_id": 515,
        "content": "/tests/video_detector_tests/rect_detect.py",
        "type": "filepath"
    },
    "4171": {
        "file_id": 515,
        "content": "The code reads video files, performs object tracking/recognition, and detects lines using OpenCV. It calculates mean differences, updates rectangles, applies edge detection and morphological operations, displays results, and saves as houghlines.jpg.",
        "type": "summary"
    },
    "4172": {
        "file_id": 515,
        "content": "import cv2\nimport numpy as np\nimport itertools\nimport uuid\n# Reading the required image in\n# which operations are to be done.\n# Make sure that the image is in the same\n# directory in which this python program is\n# video_file = \"../../samples/video/LiEIfnsvn.mp4\" # this one with cropped boundaries.\nvideo_file = \"/media/root/help1/pyjom/samples/video/LiGE8vLuX.mp4\"\nvideo = cv2.VideoCapture(video_file)\ndef rectMerge(oldRect, newRect,delta_thresh = 0.1):\n    # if very much alike, we merge these rects.\n    # what about those rect that overlaps? we check exactly those who overlaps.\n    # 1. check all new rects against all old rects. if they overlap, highly alike (or not) then mark it as having_alike_rect (or not) and append to new old rect list. <- after those old rects have been marked with alike sign, one cannot revoke the sign. still remaining new rects will be checked against them.\n    # 2. while checking, if not very alike then append newRect to new rect list.\n    # 3. if one old rect has not yet been ",
        "type": "code",
        "location": "/tests/video_detector_tests/rect_detect.py:1-20"
    },
    "4173": {
        "file_id": 515,
        "content": "The code reads an input video file and defines a function rectMerge that takes two rectangles as parameters and determines if they are highly alike. It checks all new rectangles against all old rectangles to find overlapping ones and marks them as having_alike_rect before proceeding with the remaining new rectangles. The purpose is likely for object tracking or recognition within a video stream.",
        "type": "comment"
    },
    "4174": {
        "file_id": 515,
        "content": "checked as having_alike_rect then cut its life. otherwise extend its life, though not extend above max_rect_life.\n    (old_x1,old_y1), (old_x2, old_y2) = oldRect\n    (new_x1,new_y1), (new_x2, new_y2) = newRect\n    old_w = old_x2-old_x1\n    old_h = old_y2-old_y1\n    det_x1 = abs(new_x1 - old_x1)/ old_w\n    det_x2 = abs(new_x2 - old_x2)/ old_w\n    det_y1 = abs(new_y1 - old_y1)/ old_h\n    det_y2 = abs(new_y2 - old_y2)/ old_h\n    # print(\"deltas:\",det_x1, det_x2, det_y1, det_y2)\n    having_alike_rect =  (det_x1 < delta_thresh) and (det_y1 < delta_thresh) and (det_x2 < delta_thresh ) and (det_y2 < delta_thresh)\n    myRect = newRect\n    if having_alike_rect:\n        myRect = oldRect\n    return myRect, having_alike_rect\ndef rectSurge(oldRectList, newRectList,delta_thresh = 0.1, min_rect_life = 0, max_rect_life = 6):\n    newToOldDictList = []\n    oldRectDictList = [{\"rect\":x[\"rect\"], \"alike\":False, \"life\":x[\"life\"],\"uuid\":x[\"uuid\"]} for x in oldRectList] # actually they are all dict lists. you can pass an empty list as oldRectList anyway.",
        "type": "code",
        "location": "/tests/video_detector_tests/rect_detect.py:20-43"
    },
    "4175": {
        "file_id": 515,
        "content": "This code calculates the delta between two rectangles and checks if they are alike within a certain threshold. It updates the rectangle list based on this comparison, ensuring rectangles with little change remain unchanged while extending or cutting their life depending on the condition.",
        "type": "comment"
    },
    "4176": {
        "file_id": 515,
        "content": "    # print(\"OLDRECTDICTLIST:\",oldRectDictList)\n    for newRect in newRectList:\n        needAppend = True\n        for index, oldRectDict in enumerate(oldRectDictList):\n            # print(\"ENUMERATING OLD INDEX:\",index)\n            oldRect = oldRectDict[\"rect\"]\n            _, having_alike_rect = rectMerge(oldRect,newRect,delta_thresh=delta_thresh)\n            if having_alike_rect:\n                needAppend = False\n                if not oldRectDict[\"alike\"]:\n                    # print(\"SET ALIKE:\",index,oldRect)\n                    oldRectDictList[index][\"alike\"] = True\n                # ignore myRect.\n        if needAppend:\n            newToOldDictList.append({\"rect\":newRect,\"life\":1,\"uuid\":str(uuid.uuid4())}) # make sure it is not duplicated?\n            # if appended we shall break this loop. but when shall we append?\n    oldToOldDictList = []\n    for oldRectDict in oldRectDictList:\n        alike = oldRectDict[\"alike\"]\n        life = oldRectDict[\"life\"]\n        oldRect = oldRectDict[\"rect\"]\n        myUUID = oldRectDict[\"uuid\"]",
        "type": "code",
        "location": "/tests/video_detector_tests/rect_detect.py:44-65"
    },
    "4177": {
        "file_id": 515,
        "content": "The code is iterating through a list of new rectangles and an existing list of old rectangles. For each new rectangle, it checks if any old rectangle has a similar one using the rectMerge function. If a similar rectangle is found, it updates its \"alike\" flag in the oldRectDictList. If no similar rectangle is found, it appends the new rectangle to the newToOldDictList. Finally, it creates a new list, oldToOldDictList, by iterating through oldRectDictList and excluding any rectangles with \"alike\" set to True.",
        "type": "comment"
    },
    "4178": {
        "file_id": 515,
        "content": "        if not alike:\n            life -=1\n        else:\n            life +=1\n            life = min(max_rect_life, life)\n        if life <= min_rect_life:\n            continue\n        oldToOldDictList.append({\"rect\":oldRect,\"life\":life,\"uuid\":myUUID})\n    return oldToOldDictList + newToOldDictList # a combination.\ndef updateTotalRects(oldTotalRectDict,rectList,currentFrameIndex,diffFrame):\n    for elem in rectList:\n        uuid = elem[\"uuid\"]\n        rect = elem[\"rect\"]\n        if uuid not in oldTotalRectDict.keys():\n            oldTotalRectDict.update({uuid:{\"rect\":rect,\"startFrame\":currentFrameIndex,\"endFrame\":None,\"meanDifference\":None}}) # finally,remove those without endFrame.\n        else:\n            duration = currentFrameIndex - oldTotalRectDict[uuid][\"startFrame\"]\n            (x0,y0),(x1,y1) = rect\n            diff = diffFrame[y0:y1,x0:x1] # this is shit. we need to crop this shit.\n            # grayscale.\n            # std = np.abs(std)\n            # get the total delta over time?\n            # std = np.mean(std,axis=2)",
        "type": "code",
        "location": "/tests/video_detector_tests/rect_detect.py:66-89"
    },
    "4179": {
        "file_id": 515,
        "content": "This function updates a dictionary of rectangles by iterating over a list of rectangles and their properties. It adds new rectangles to the dictionary or updates existing ones with their duration, position, and difference from previous frames. It ignores rectangles that do not change and removes those without an end frame.",
        "type": "comment"
    },
    "4180": {
        "file_id": 515,
        "content": "            diff_x = np.mean(diff.flatten())\n            # std_x = np.std(std,axis=2)\n            # std_x = np.std(std_x,axis=1)\n            # std_x = np.std(std_x,axis=0)\n            std_total = diff_x # later we need to convert this float64.\n            # breakpoint()\n            if duration == 1:\n                oldTotalRectDict[uuid][\"meanDifference\"] = std_total\n            else:\n                dur2 = duration - 1\n                prev_std = oldTotalRectDict[uuid][\"meanDifference\"]\n                new_std = (dur2*prev_std + std_total)/duration # may freaking exceed limit.\n                oldTotalRectDict[uuid][\"meanDifference\"] = new_std\n            oldTotalRectDict[uuid][\"endFrame\"] = currentFrameIndex\n    return oldTotalRectDict\ntotal_rect_dict ={}\nrect_dict_main_list = []\nmin_rect_life_display_thresh = 3 # a filter.\nmode = 1\nline_thresh =  150\nincludeBoundaryLines = True # applied to those cornered crops.\n# this will slow down the process. or maybe?\nframeIndex = -1\nprevFrame = None\nif mode == 1:\n    import pybgs as bgs",
        "type": "code",
        "location": "/tests/video_detector_tests/rect_detect.py:90-117"
    },
    "4181": {
        "file_id": 515,
        "content": "This code calculates the mean difference of rectangles over a certain duration and updates an existing dictionary with this information. It then returns the updated dictionary. The dictionary contains rectangle information such as mean difference, start frame index, end frame index, and duration for each unique ID (uuid). If the duration is 1, it simply stores the current mean difference in the dictionary. Otherwise, it calculates a new mean difference by taking a weighted average of the previous mean difference and the new mean difference. The code also initializes variables and imports a module called pybgs as bgs.",
        "type": "comment"
    },
    "4182": {
        "file_id": 515,
        "content": "    algorithm = (\n    bgs.FrameDifference()\n)  # this\nwhile True:\n    ret, img = video.read()\n    if img is None:\n        if mode == 1:\n            popKeys = []\n            for key in total_rect_dict.keys():\n                elem = total_rect_dict[key]\n                if elem[\"endFrame\"] is None:\n                    popKeys.append(key)\n            for key in popKeys:\n                total_rect_dict.pop(key) # remove premature rectangles.\n        break\n    else: frameIndex+=1\n    if mode == 1:\n        diff_img_output = algorithm.apply(img)\n    # what about the freaking still image?\n    # Convert the img to grayscale\n    # gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n    # no need to use gray image.\n    # Apply edge detection method on the image\n    blurred = cv2.GaussianBlur(img, (5, 5), 0)\n    edges = cv2.Canny(blurred,20,210,apertureSize = 3) # great.\n    # why not applying edges directly to rectangles?\n    # This returns an array of r and theta values\n    # line_thresh =  200\n    # maintain a rectangle list. merge the alikes?",
        "type": "code",
        "location": "/tests/video_detector_tests/rect_detect.py:118-152"
    },
    "4183": {
        "file_id": 515,
        "content": "This code reads frames from a video and applies different algorithms to detect objects. It checks for premature rectangles and removes them if necessary, and applies edge detection methods on the image. The code uses GaussianBlur and Canny functions for edge detection. It maintains a rectangle list and considers merging similar rectangles.",
        "type": "comment"
    },
    "4184": {
        "file_id": 515,
        "content": "    if mode == 1:\n        lines = cv2.HoughLines(edges,1,np.pi/180,line_thresh)\n        angle_error = 0.00003   # this can only detect square things, absolute square.\n        # we need to know horizontal and vertical lines, when they cross we get points.\n        frameHeight, frameWidth = img.shape[:2]\n        # print(\"height: \", frameHeight)\n        # print(\"width: \", frameWidth)\n        mlines = {\"horizontal\":[], \"vertical\":[]}\n        if includeBoundaryLines:\n            originPoint = (0,0)\n            cornerPoint = (frameWidth,frameHeight)\n            mlines[\"horizontal\"].append(originPoint)\n            mlines[\"horizontal\"].append(cornerPoint)\n            mlines[\"vertical\"].append(originPoint)\n            mlines[\"vertical\"].append(cornerPoint)\n        for line in lines:\n            for r_theta in line:\n                # breakpoint()\n                r,theta = r_theta.tolist()\n                # Stores the value of cos(theta) in a\n                # filter detected lines?\n                # theta filter:\n                if not abs(theta % (np.pi/2) )< angle_error:",
        "type": "code",
        "location": "/tests/video_detector_tests/rect_detect.py:154-176"
    },
    "4185": {
        "file_id": 515,
        "content": "The code segment is filtering HoughLines-detected lines from an image, ensuring they are nearly horizontal or vertical. It stores the cosine of line angles in a variable called 'a', and applies an angle error threshold to filter out lines not close to 0 (horizontal) or π/2 (vertical). If includeBoundaryLines is True, it adds four boundary points for both horizontal and vertical lines.",
        "type": "comment"
    },
    "4186": {
        "file_id": 515,
        "content": "                    continue # this is filtering.\n                # print(\"line parameter:\",r,theta)\n                a = np.cos(theta)\n                # Stores the value of sin(theta) in b\n                b = np.sin(theta)\n                # x0 stores the value rcos(theta)\n                x0 = a*r\n                # y0 stores the value rsin(theta)\n                y0 = b*r\n                # x1 stores the rounded off value of (rcos(theta)-1000sin(theta))\n                x1 = int(x0 + 1000*(-b))\n                # y1 stores the rounded off value of (rsin(theta)+1000cos(theta))\n                y1 = int(y0 + 1000*(a))\n                # x2 stores the rounded off value of (rcos(theta)+1000sin(theta))\n                x2 = int(x0 - 1000*(-b))\n                # y2 stores the rounded off value of (rsin(theta)-1000cos(theta))\n                y2 = int(y0 - 1000*(a))\n                # cv2.line draws a line in img from the point(x1,y1) to (x2,y2).\n                # (0,0,255) denotes the colour of the line to be\n                #drawn. In this case, it is red.",
        "type": "code",
        "location": "/tests/video_detector_tests/rect_detect.py:177-204"
    },
    "4187": {
        "file_id": 515,
        "content": "This code calculates line parameters using trigonometry and then draws a red line on the image. The loop filters out certain conditions, and the calculations are based on radius (r), polar angle (theta). Lines are drawn between different points calculated from these variables to create lines in the image.",
        "type": "comment"
    },
    "4188": {
        "file_id": 515,
        "content": "                df_x = abs(x1-x2)\n                df_y = abs(y1-y2)\n                lineType = \"vertical\"\n                if df_x > df_y:\n                    lineType = \"horizontal\"\n                # we just need one single point and lineType.\n                linePoint = (x1,y1)\n                mlines[lineType].append(linePoint)\n                # cv2.line(img,(x1,y1), (x2,y2), (0,0,255),2)\n                # would not draw lines this time. draw found rects instead.\n        # get rectangle points. or just all possible rectangles?\n        # enumerate all possible lines.\n        if len(mlines[\"horizontal\"]) < 2 or len(mlines[\"vertical\"]) < 2:\n            print(\"unable to form rectangles.\")\n            continue\n        else:\n            rects =[] # list of rectangles\n            for line_h1, line_h2 in itertools.combinations(mlines[\"horizontal\"],2):\n                ymin, ymax = list(sorted([line_h1[1],line_h2[1]]))\n                for line_v1, line_v2 in itertools.combinations(mlines[\"vertical\"], 2):\n                    xmin, xmax = list(sorted([line_v1[0],line_v2[0]]))",
        "type": "code",
        "location": "/tests/video_detector_tests/rect_detect.py:205-226"
    },
    "4189": {
        "file_id": 515,
        "content": "Calculating differences in x and y coordinates to determine line type. Appends line points based on line type to a dictionary. If there are less than 2 horizontal or vertical lines, the code cannot form rectangles and skips to the next iteration. Iterates through combinations of horizontal lines to find the upper and lower bounds, then does the same with vertical lines for left and right bounds.",
        "type": "comment"
    },
    "4190": {
        "file_id": 515,
        "content": "                    rect = ((xmin,ymin),(xmax,ymax))\n                    rects.append(rect)\n            rect_dict_main_list = rectSurge(rect_dict_main_list,rects)\n            # print(\"RECT DICT MAIN LIST:\")\n            # print(rect_dict_main_list) # maybe i want this shit?\n            total_rect_dict = updateTotalRects(total_rect_dict,rect_dict_main_list,frameIndex,diff_img_output)\n            mdisplayed_rect_count = 0\n            for rect_dict in rect_dict_main_list:\n                life = rect_dict[\"life\"]\n                if life < min_rect_life_display_thresh:\n                    continue # this is needed.\n                # draw shit now.\n                mdisplayed_rect_count +=1\n                (xmin,ymin),(xmax,ymax) = rect_dict[\"rect\"]\n                cv2.rectangle(img,(xmin,ymin),(xmax,ymax) , (255,0,0), 2)\n            #     (xmin,ymin),(xmax,ymax) = rect\n            #     rect_area = (xmax-xmin) * (ymax-ymin)\n            #     print(\"rect found:\",rect,rect_area)\n            prevFrame = img.copy()",
        "type": "code",
        "location": "/tests/video_detector_tests/rect_detect.py:227-245"
    },
    "4191": {
        "file_id": 515,
        "content": "The code is updating a list of rectangles and iterating over it to draw them on an image. It checks if the rectangle \"life\" is above a certain threshold before drawing, and maintains a count of displayed rectangles. The code also stores the previous frame for comparison in a later step.",
        "type": "comment"
    },
    "4192": {
        "file_id": 515,
        "content": "            # print(\"total rects:\",mdisplayed_rect_count)\n    elif mode == 2:\n        lines = cv2.HoughLinesP(edges,1,np.pi/180,line_thresh,minLineLength=2,maxLineGap=100) # these are not angle filtering.\n        for points in lines:\n      # Extracted points nested in the list\n            x1,y1,x2,y2=points[0]\n            # filter out angle errors?\n            # Draw the lines joing the points\n            # On the original image\n            cv2.line(img,(x1,y1),(x2,y2),(0,255,0),2)\n            # Maintain a simples lookup list for points\n            # lines_list.append([(x1,y1),(x2,y2)])\n    elif mode == 3:\n        # edges = cv2.GaussianBlur(edges, (5, 5), 0)\n        horizontal_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (10,1))\n        detect_horizontal = cv2.morphologyEx(edges, cv2.MORPH_OPEN, horizontal_kernel, iterations=3)\n        vertical_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (1,10))\n        detect_vertical = cv2.morphologyEx(edges, cv2.MORPH_OPEN, vertical_kernel, iterations=3)",
        "type": "code",
        "location": "/tests/video_detector_tests/rect_detect.py:246-264"
    },
    "4193": {
        "file_id": 515,
        "content": "The code applies different modes to detect lines and edges in an image. Mode 2 uses HoughLinesP to detect lines without angle filtering, drawing them on the original image. Mode 3 applies morphological operations using structuring elements for horizontal and vertical lines detection.",
        "type": "comment"
    },
    "4194": {
        "file_id": 515,
        "content": "        cnts_horizontal = cv2.findContours(detect_horizontal, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n        cnts_horizontal = cnts_horizontal[0] if len(cnts_horizontal) == 2 else cnts_horizontal[1]\n        cnts_vertical = cv2.findContours(detect_vertical, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n        cnts_vertical = cnts_vertical[0] if len(cnts_vertical) == 2 else cnts_vertical[1]\n        for c in cnts_horizontal:\n            cv2.drawContours(img, [c], -1, (255,0,0), 3)\n        for c in cnts_vertical:\n            cv2.drawContours(img, [c], -1, (255,0,0), 3)\n    # what the heck?\n    # The below for loop runs till r and theta values\n    # are in the range of the 2d array\n    # why you have middle lines?\n            # how to get the intersections? lines?\n    cv2.imshow('linesDetected.jpg', img)\n    # cv2.imshow(\"edges.jpg\",edges) # not for fun.\n    if cv2.waitKey(20) == ord(\"q\"):\n        print(\"QUIT INTERFACE.\")\n        break\n# All the changes made in the input image are finally\n# written on a new image houghlines.jpg",
        "type": "code",
        "location": "/tests/video_detector_tests/rect_detect.py:266-290"
    },
    "4195": {
        "file_id": 515,
        "content": "This code uses OpenCV to detect horizontal and vertical lines in an image. It finds contours for both line types and draws them on the original image. Then, it displays the image with the detected lines using cv2.imshow(). The code also checks for a 'q' key press to exit the interface and saves the edited image as houghlines.jpg.",
        "type": "comment"
    },
    "4196": {
        "file_id": 515,
        "content": "if mode == 1:\n    print(\"FINAL RESULT:\")\n    for key in total_rect_dict.keys():\n        elem = total_rect_dict[key]\n        print(\"RECT UUID\",key)\n        print(\"RECT CONTENT\",elem)",
        "type": "code",
        "location": "/tests/video_detector_tests/rect_detect.py:291-298"
    },
    "4197": {
        "file_id": 515,
        "content": "This code block checks if mode is 1, then prints the final results. It iterates through each key-value pair in total_rect_dict and outputs the rect UUID and content.",
        "type": "comment"
    },
    "4198": {
        "file_id": 516,
        "content": "/tests/video_detector_tests/rectangle_framedifference.py",
        "type": "filepath"
    },
    "4199": {
        "file_id": 516,
        "content": "The code utilizes a motion detector to continuously capture frames, detecting changes for object detection and tracking. It calculates merged bounding boxes, applies thresholds, updates coordinates based on average values, and displays frames using OpenCV.",
        "type": "summary"
    }
}