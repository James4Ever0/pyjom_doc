{
    "3400": {
        "file_id": 413,
        "content": "/tests/anime1_me_video_download/README.md",
        "type": "filepath"
    },
    "3401": {
        "file_id": 413,
        "content": "This code instructs to extract data from the video API, send it to a specific URL, use the resulting cookie for downloading the video from another URL to avoid errors.",
        "type": "summary"
    },
    "3402": {
        "file_id": 413,
        "content": "the data is hide in the video data-api. unquote it and we will get the info. \npost data to https://v.anime1.me/api, then use responded cookie p,h with the original e(timestamp) for download.\ndownload video from https://shiro.v.anime1.me/(or elsewhere) or somehow we will get it wrong.",
        "type": "code",
        "location": "/tests/anime1_me_video_download/README.md:1-5"
    },
    "3403": {
        "file_id": 413,
        "content": "This code instructs to extract data from the video API, send it to a specific URL, use the resulting cookie for downloading the video from another URL to avoid errors.",
        "type": "comment"
    },
    "3404": {
        "file_id": 414,
        "content": "/tests/anime1_me_video_download/get_best_edm.sh",
        "type": "filepath"
    },
    "3405": {
        "file_id": 414,
        "content": "The code uses FFmpeg to extract segments from the \"edm_super_summit.m4a\" audio file, saving them as \"best_edm_split.mp3\" and \"best_edm_split2.mp3\". The -ss option specifies the start time and -to the end time for each segment.",
        "type": "summary"
    },
    "3406": {
        "file_id": 414,
        "content": "# ffmpeg -y -i edm_super_summit.m4a -ss 00:00:50 -to 00:01:05 best_edm_split.mp3\nffmpeg -y -i edm_super_summit.m4a -ss 00:01:38 -to 00:01:49 best_edm_split2.mp3",
        "type": "code",
        "location": "/tests/anime1_me_video_download/get_best_edm.sh:1-2"
    },
    "3407": {
        "file_id": 414,
        "content": "The code uses FFmpeg to extract segments from the \"edm_super_summit.m4a\" audio file, saving them as \"best_edm_split.mp3\" and \"best_edm_split2.mp3\". The -ss option specifies the start time and -to the end time for each segment.",
        "type": "comment"
    },
    "3408": {
        "file_id": 415,
        "content": "/tests/anime1_me_video_download/api_curl.sh",
        "type": "filepath"
    },
    "3409": {
        "file_id": 415,
        "content": "The code sends a POST request to anime1.me API using cURL, containing video ID, episode number, timestamp, and secret key, likely for interacting with anime videos. It also includes the \"--compressed\" flag for file compression during download, saving storage space and time.",
        "type": "summary"
    },
    "3410": {
        "file_id": 415,
        "content": "curl 'https://v.anime1.me/api' \\\n  -H 'authority: v.anime1.me' \\\n  -H 'accept: */*' \\\n  -H 'accept-language: en-US,en;q=0.9' \\\n  -H 'content-type: application/x-www-form-urlencoded' \\\n  -H 'origin: https://anime1.me' \\\n  -H 'referer: https://anime1.me/' \\\n  --data-raw 'd=%7B%22c%22%3A%221019%22%2C%22e%22%3A%226b%22%2C%22t%22%3A1652431596%2C%22p%22%3A0%2C%22s%22%3A%221bc65800b44a935eb4e5287655c64cb2%22%7D' \\\n  --compressed\n# curl 'https://v.anime1.me/api' \\\n#   -H 'authority: v.anime1.me' \\\n#   -H 'accept: */*' \\\n#   -H 'accept-language: en-US,en;q=0.9' \\\n#   -H 'content-type: application/x-www-form-urlencoded' \\\n#   -H 'origin: https://anime1.me' \\\n#   -H 'referer: https://anime1.me/' \\\n#   -H 'sec-ch-ua: \" Not A;Brand\";v=\"99\", \"Chromium\";v=\"101\"' \\\n#   -H 'sec-ch-ua-mobile: ?1' \\\n#   -H 'sec-ch-ua-platform: \"Android\"' \\\n#   -H 'sec-fetch-dest: empty' \\\n#   -H 'sec-fetch-mode: cors' \\\n#   -H 'sec-fetch-site: same-site' \\\n#   -H 'user-agent: Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.4951.41 Mobile Safari/537.36' \\",
        "type": "code",
        "location": "/tests/anime1_me_video_download/api_curl.sh:1-24"
    },
    "3411": {
        "file_id": 415,
        "content": "This code sends a POST request to 'https://v.anime1.me/api' using cURL, with specified headers and data in the request body. The request includes an API key for authentication and retrieves data from the anime1.me website.",
        "type": "comment"
    },
    "3412": {
        "file_id": 415,
        "content": "#   --data-raw 'd=%7B%22c%22%3A%221019%22%2C%22e%22%3A%226b%22%2C%22t%22%3A1652431596%2C%22p%22%3A0%2C%22s%22%3A%221bc65800b44a935eb4e5287655c64cb2%22%7D' \\\n#   --compressed\n# curl 'https://v.anime1.me/api' \\\n#   -H 'authority: v.anime1.me' \\\n#   -H 'accept: */*' \\\n#   -H 'accept-language: en-US,en;q=0.9' \\\n#   -H 'content-type: application/x-www-form-urlencoded' \\\n#   -H 'cookie: _ga=GA1.2.354375679.1652431604; _gid=GA1.2.1847563412.1652431604; _gat=1' \\\n#   -H 'origin: https://anime1.me' \\\n#   -H 'referer: https://anime1.me/' \\\n#   -H 'sec-ch-ua: \" Not A;Brand\";v=\"99\", \"Chromium\";v=\"101\"' \\\n#   -H 'sec-ch-ua-mobile: ?1' \\\n#   -H 'sec-ch-ua-platform: \"Android\"' \\\n#   -H 'sec-fetch-dest: empty' \\\n#   -H 'sec-fetch-mode: cors' \\\n#   -H 'sec-fetch-site: same-site' \\\n#   -H 'user-agent: Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.4951.41 Mobile Safari/537.36' \\\n#   --data-raw 'd=%7B%22c%22%3A%221019%22%2C%22e%22%3A%226b%22%2C%22t%22%3A1652431596%2C%22p%22%3A0%2C%22s%22%3A%221bc65800b44a935eb4e5287655c64cb2%22%7D' \\",
        "type": "code",
        "location": "/tests/anime1_me_video_download/api_curl.sh:25-43"
    },
    "3413": {
        "file_id": 415,
        "content": "This code is making an API request to 'https://v.anime1.me/api' using curl command with various headers and a data payload in JSON format. The payload contains information such as video ID, episode number, timestamp, and secret key. It seems to be fetching information or performing an action related to an anime video from the anime1.me website.",
        "type": "comment"
    },
    "3414": {
        "file_id": 415,
        "content": "#   --compressed",
        "type": "code",
        "location": "/tests/anime1_me_video_download/api_curl.sh:44-44"
    },
    "3415": {
        "file_id": 415,
        "content": "The code snippet \"--compressed\" is used to compress the file during download, which can save storage space and reduce transfer time.",
        "type": "comment"
    },
    "3416": {
        "file_id": 416,
        "content": "/tests/generator_yield_from_python_extract_element_one_by_one/test.py",
        "type": "filepath"
    },
    "3417": {
        "file_id": 416,
        "content": "This code uses generators to iterate through numbers, cleaning up temporary files after use. It demonstrates using lambda functions for simplified iteration and exception handling for resource management. The code initializes generator2, calls generator3 with generator2 and a tempfile, checks if the file exists, closes the generator, and again checks if the file exists.",
        "type": "summary"
    },
    "3418": {
        "file_id": 416,
        "content": "from lazero.filesystem.temp import tmpfile\nimport pathlib\nimport os\ndef checkFileExists(filePath, debug=False):\n    result = os.path.exists(filePath)\n    if debug:\n        print('exists?', result)\ndef generator(tempfile):\n    # for index in range(12): # 0 to 11 means 12\n    for index in range(11): # what if it is 11? -> StopIteration and shit get cleaned.\n        with tmpfile(tempfile):\n            pathlib.Path(tempfile).touch()\n            yield index\ndef generator2(tempfile):\n    yield from generator(tempfile)  # this is to simplifying the process of iteration.\ndef iterator(lambdaFunction, tempfile):\n    for _ in range(4):\n        result = lambdaFunction()\n        print(result) # cleaned after next FAILED iteration, which is what we need the most.\n        checkFileExists(tempfile, debug=True)\n        # cleaning after 'close' or next iteration.\ndef generator3(myGenerator, tempfile):\n    getNextNumber = lambda: myGenerator.__next__()\n    for _ in range(3):\n        iterator(getNextNumber, tempfile)\n        print(\"_\" * 30)",
        "type": "code",
        "location": "/tests/generator_yield_from_python_extract_element_one_by_one/test.py:1-35"
    },
    "3419": {
        "file_id": 416,
        "content": "This code defines a series of functions that utilize generators to generate and iterate through numbers, while also checking if the temporary file exists and cleaning it up after each iteration. The code demonstrates how generators can be used with lambda functions for simplified iteration, and how exception handling can be employed to clean up resources after use.",
        "type": "comment"
    },
    "3420": {
        "file_id": 416,
        "content": "if __name__ == \"__main__\":\n    tempfile = \"tmp_test\"\n    if os.path.exists(tempfile):\n        os.remove(tempfile)\n    myGenerator = generator2(tempfile)\n    print(type(myGenerator))\n    breakpoint()\n    generator3(myGenerator, tempfile)  # good.\n    # not over yet.\n    checkFileExists(tempfile, debug=True)\n    myGenerator.close() # choose to close this so you would get this result.\n    checkFileExists(tempfile, debug=True)\n    # another test on generator, about tempfiles during iteration.",
        "type": "code",
        "location": "/tests/generator_yield_from_python_extract_element_one_by_one/test.py:38-50"
    },
    "3421": {
        "file_id": 416,
        "content": "Code initializes generator2 with a temporary file name and prints its type. Then, it calls generator3 passing the generator2 and tempfile as arguments. After that, it checks if the temporary file exists using checkFileExists function in debug mode. Finally, it closes the generator and again checks if the temporary file exists.",
        "type": "comment"
    },
    "3422": {
        "file_id": 417,
        "content": "/tests/english_chinese_mixing_spliter/test_tts.py",
        "type": "filepath"
    },
    "3423": {
        "file_id": 417,
        "content": "This code imports TTS module and generates audio from a given text, iterating through analyzed data for each language. English tool is needed as no English option available currently.",
        "type": "summary"
    },
    "3424": {
        "file_id": 417,
        "content": "from paddlebobo_paddletools_tts import TTSExecutor\nfrom english_grepper import analyze_mixed_text\nmtext = \"你这dollar有问题啊\"\n# analyze this shit.\n# you can translate all english into chinese. doesn't hurt.\ntext_analyze_result = analyze_mixed_text(mtext)\n# print(text_analyze_result)\n# breakpoint()\ntts_config = {\"zh\": {\"model_tag\": 'fastspeech2_csmsc-zh',\n                     \"voc_tag\": \"hifigan_csmsc-zh\", \"lang\": \"zh\"}, \"en\": {\"model_tag\": 'fastspeech2_ljspeech-en',\n                                                                          \"voc_tag\": \"hifigan_ljspeech-en\", \"lang\": \"en\"}}\n# tts_config = {\"zh\": {\"model_tag\": 'tacotron2_csmsc-zh',\n#                      \"voc_tag\": \"hifigan_csmsc-zh\", \"lang\": \"zh\"}, \"en\": {\"model_tag\": 'tacotron2_ljspeech-en',\n#                      \"voc_tag\": \"hifigan_ljspeech-en\", \"lang\": \"en\"}}\nfor langid in [\"en\", \"zh\"]:\n    lang_config = tts_config[langid]\n    TTS = TTSExecutor('default.yaml', **lang_config)  # PaddleSpeech语音合成模块\n    # do we need to delete the TTS?\n    for data in text_analyze_result[langid]:",
        "type": "code",
        "location": "/tests/english_chinese_mixing_spliter/test_tts.py:1-25"
    },
    "3425": {
        "file_id": 417,
        "content": "The code imports necessary modules, defines a mixed text string, analyzes the text for English and Chinese segments using 'analyze_mixed_text' function, creates a TTSExecutor object with specified configurations for English (en) and Chinese (zh), and finally, iterates through the analyzed data for each language.",
        "type": "comment"
    },
    "3426": {
        "file_id": 417,
        "content": "        index, text = data[\"index\"], data[\"text\"]\n        wavfile = TTS.run(\n            text=text, output='output_{}_{}.wav'.format(langid, index))  # 合成音频\n    del TTS\n# there is no freaking english shit.\n# we need english tool.\n# you can also translate this shit.",
        "type": "code",
        "location": "/tests/english_chinese_mixing_spliter/test_tts.py:26-32"
    },
    "3427": {
        "file_id": 417,
        "content": "This code is importing the TTS module and running it to generate audio from a given text. The output file name includes the language ID and index, indicating different languages or speakers may be involved. However, an English tool is needed as there currently seems to be no English option available in the existing codebase.",
        "type": "comment"
    },
    "3428": {
        "file_id": 418,
        "content": "/tests/english_chinese_mixing_spliter/sample_strings.txt",
        "type": "filepath"
    },
    "3429": {
        "file_id": 418,
        "content": "The code contains a mix of English and Chinese text, representing a sample of mixed-language strings for testing purposes. It includes phrases such as \"你这dollar有问题啊\" (This dollar has a problem), \"版本号2.1.0alpha\" (Version 2.1.0 alpha), and \"mixed-content warning别说我没提醒你\" (Mixed content warning, I told you not to say anything).",
        "type": "summary"
    },
    "3430": {
        "file_id": 418,
        "content": "你这dollar有问题啊\n2000万巨资！经费燃烧\n版本号2.1.0alpha，但是这个premature state让人担心\nDo not say a word.她睡觉了。mixed-content warning别说我没提醒你",
        "type": "code",
        "location": "/tests/english_chinese_mixing_spliter/sample_strings.txt:1-4"
    },
    "3431": {
        "file_id": 418,
        "content": "The code contains a mix of English and Chinese text, representing a sample of mixed-language strings for testing purposes. It includes phrases such as \"你这dollar有问题啊\" (This dollar has a problem), \"版本号2.1.0alpha\" (Version 2.1.0 alpha), and \"mixed-content warning别说我没提醒你\" (Mixed content warning, I told you not to say anything).",
        "type": "comment"
    },
    "3432": {
        "file_id": 419,
        "content": "/tests/english_chinese_mixing_spliter/paddlebobo_paddletools_tts.py",
        "type": "filepath"
    },
    "3433": {
        "file_id": 419,
        "content": "This code creates a text-to-speech synthesis model, initializes the architecture, and processes English and Chinese models. It concatenates audio files and deletes objects upon deallocation.",
        "type": "summary"
    },
    "3434": {
        "file_id": 419,
        "content": "import os\nimport numpy as np\nimport paddle\nimport soundfile as sf\nimport yaml\nfrom yacs.config import CfgNode\nfrom paddlespeech.cli.utils import download_and_decompress\nfrom paddlespeech.cli.utils import MODEL_HOME\nfrom paddlespeech.t2s.frontend import English\nfrom paddlespeech.s2t.utils.dynamic_import import dynamic_import\nfrom paddlespeech.t2s.frontend.zh_frontend import Frontend\nfrom paddlespeech.t2s.modules.normalizer import ZScore\nfrom paddlespeech.cli.tts.infer import model_alias, pretrained_models\nmodel_alias2 = {\n    # acoustic model\n    \"fastspeech2\": \"paddlespeech.t2s.models.fastspeech2:FastSpeech2\",\n    \"fastspeech2_inference\": \"paddlespeech.t2s.models.fastspeech2:StyleFastSpeech2Inference\",\n    # voc\n    \"pwgan\":\n    \"paddlespeech.t2s.models.parallel_wavegan:PWGGenerator\",\n    \"pwgan_inference\":\n    \"paddlespeech.t2s.models.parallel_wavegan:PWGInference\",\n}\nmodel_alias.update(model_alias2)\n# pretrained_models = {\n#     # fastspeech2\n#     \"fastspeech2_csmsc-zh\": {\n#         'url':\n#         'https://p",
        "type": "code",
        "location": "/tests/english_chinese_mixing_spliter/paddlebobo_paddletools_tts.py:1-35"
    },
    "3435": {
        "file_id": 419,
        "content": "The code is importing necessary libraries and modules, defining model aliases for acoustic models (fastspeech2) and vocoders (pwgan), and potentially updating pretrained_models dictionary.",
        "type": "comment"
    },
    "3436": {
        "file_id": 419,
        "content": "addlespeech.bj.bcebos.com/Parakeet/released_models/fastspeech2/fastspeech2_nosil_baker_ckpt_0.4.zip',\n#         'md5':\n#         '637d28a5e53aa60275612ba4393d5f22',\n#         'config':\n#         'default.yaml',\n#         'ckpt':\n#         'snapshot_iter_76000.pdz',\n#         'speech_stats':\n#         'speech_stats.npy',\n#         'phones_dict':\n#         'phone_id_map.txt',\n#         'pitch_stats':\n#         'pitch_stats.npy',\n#         'energy_stats':\n#         'energy_stats.npy',\n#     },\n#     # pwgan\n#     \"pwgan_csmsc-zh\": {\n#         'url':\n#         'https://paddlespeech.bj.bcebos.com/Parakeet/released_models/pwgan/pwg_baker_ckpt_0.4.zip',\n#         'md5':\n#         '2e481633325b5bdf0a3823c714d2c117',\n#         'config':\n#         'pwg_default.yaml',\n#         'ckpt':\n#         'pwg_snapshot_iter_400000.pdz',\n#         'speech_stats':\n#         'pwg_stats.npy',\n#     },\n# }\nfor k in [\"fastspeech2_csmsc-zh\",\"fastspeech2_ljspeech-en\"]:\n    model_config = {'pitch_stats':\n        'pitch_stats.npy',\n        'energy_stats':",
        "type": "code",
        "location": "/tests/english_chinese_mixing_spliter/paddlebobo_paddletools_tts.py:35-69"
    },
    "3437": {
        "file_id": 419,
        "content": "This code is a dictionary containing two models: \"fastspeech2_csmsc-zh\" and \"fastspeech2_ljspeech-en\". Each model has its URL, MD5, config file, checkpoint file, and optional statistics files. These models seem to be used for speech synthesis, as they require pitch and energy stats.",
        "type": "comment"
    },
    "3438": {
        "file_id": 419,
        "content": "        'energy_stats.npy',}\n    pretrained_models[k].update(model_config)\nclass TTSExecutor():\n    def __init__(self, config,model_tag = 'fastspeech2_csmsc-zh', voc_tag = \"pwgan_csmsc-zh\",lang=\"zh\"):\n        langId1 = model_tag.split(\"-\")[-1]\n        langId2 = voc_tag.split(\"-\")[-1]\n        assert langId1 == langId2\n        assert langId2 == lang\n        assert lang in [\"zh\",\"en\"]\n        self.lang = lang\n        # match the freaking dataset!\n        #FastSpeech2 or something else. we need freaking english!\n        am_res_path = self._get_pretrained_path(model_tag)\n        am_config = os.path.join(am_res_path,pretrained_models[model_tag]['config'])\n        am_ckpt = os.path.join(am_res_path,pretrained_models[model_tag]['ckpt'])\n        am_stat = os.path.join(am_res_path, pretrained_models[model_tag]['speech_stats'])\n        # must have phones_dict in acoustic\n        phones_dict = os.path.join(am_res_path, pretrained_models[model_tag]['phones_dict'])\n        # StyleFastSpeech\n        pitch_stats = os.path.join(am_res_path, pretrained_models[model_tag]['pitch_stats'])",
        "type": "code",
        "location": "/tests/english_chinese_mixing_spliter/paddlebobo_paddletools_tts.py:70-91"
    },
    "3439": {
        "file_id": 419,
        "content": "The code is initializing a TTSExecutor object with config and model_tag parameters. It checks if the model_tag matches the language specified, and then retrieves the necessary paths for the acoustic model, phones dictionary, and pitch statistics using the pretrained_models dictionary.",
        "type": "comment"
    },
    "3440": {
        "file_id": 419,
        "content": "        energy_stats = os.path.join(am_res_path, pretrained_models[model_tag]['energy_stats'])\n        #VOC\n        voc_res_path = self._get_pretrained_path(voc_tag)\n        voc_config = os.path.join(voc_res_path,pretrained_models[voc_tag]['config'])\n        voc_ckpt = os.path.join(voc_res_path,pretrained_models[voc_tag]['ckpt'])\n        voc_stat = os.path.join(voc_res_path, pretrained_models[voc_tag]['speech_stats'])\n        # Init body.\n        with open(am_config) as f:\n            self.am_config = CfgNode(yaml.safe_load(f))\n        with open(voc_config) as f:\n            voc_config = CfgNode(yaml.safe_load(f))\n        with open(config) as f:\n            self.style_config = CfgNode(yaml.safe_load(f))\n        with open(phones_dict, \"r\") as f:\n            phn_id = [line.strip().split() for line in f.readlines()]\n        vocab_size = len(phn_id)\n        #print(\"vocab_size:\", vocab_size)\n        # acoustic model\n        odim = self.am_config.n_mels\n        # wtf?\n        main_name0 = model_tag.split(\"_\")[0]",
        "type": "code",
        "location": "/tests/english_chinese_mixing_spliter/paddlebobo_paddletools_tts.py:92-118"
    },
    "3441": {
        "file_id": 419,
        "content": "This code is loading pre-trained models and configuration files for an automatic speech recognition (ASR) system. It joins different file paths, opens the configuration files to parse them into CfgNodes, and determines the vocabulary size based on a phone ID list. The code seems to be part of a larger ASR system implementation, initializing variables before using the models for prediction or inference tasks.",
        "type": "comment"
    },
    "3442": {
        "file_id": 419,
        "content": "        am_class = dynamic_import(main_name0, model_alias)\n        am_inference_class = dynamic_import('{}_inference'.format(main_name0), model_alias)\n        am = am_class(idim=vocab_size, odim=odim, spk_num=1, **self.am_config[\"model\"])\n        am.set_state_dict(paddle.load(am_ckpt)[\"main_params\"])\n        am.eval()\n        am_mu, am_std = np.load(am_stat)\n        am_mu = paddle.to_tensor(am_mu)\n        am_std = paddle.to_tensor(am_std)\n        am_normalizer = ZScore(am_mu, am_std)\n        if lang == \"en\":\n            self.am_inference = am_inference_class(am_normalizer, am) # you can also try tensorflowTTS, hifigan with high clarity.\n        else:\n            self.am_inference = am_inference_class(am_normalizer, am, pitch_stats, energy_stats)\n        self.am_inference.eval()\n        # vocoder\n        main_name1 = voc_tag.split(\"_\")[0]\n        voc_class = dynamic_import(main_name1, model_alias)\n        voc_inference_class = dynamic_import('{}_inference'.format(main_name1), model_alias)\n        voc = voc_class(**voc_config[\"generator_params\"])",
        "type": "code",
        "location": "/tests/english_chinese_mixing_spliter/paddlebobo_paddletools_tts.py:119-141"
    },
    "3443": {
        "file_id": 419,
        "content": "The code dynamically imports classes based on model aliases and tags, instantiates models for speech synthesis and vocoder, loads model parameters and normalization stats, and sets up the inference environment for both English and Chinese languages.",
        "type": "comment"
    },
    "3444": {
        "file_id": 419,
        "content": "        voc.set_state_dict(paddle.load(voc_ckpt)[\"generator_params\"])\n        voc.remove_weight_norm()\n        voc.eval()\n        voc_mu, voc_std = np.load(voc_stat)\n        voc_mu = paddle.to_tensor(voc_mu)\n        voc_std = paddle.to_tensor(voc_std)\n        voc_normalizer = ZScore(voc_mu, voc_std)\n        self.voc_inference = voc_inference_class(voc_normalizer, voc)\n        self.voc_inference.eval()\n        if lang == \"zh\":\n            self.frontend = Frontend(phone_vocab_path=phones_dict, tone_vocab_path=None)\n        elif lang == \"en\":\n            self.phones_dict = os.path.join(\n                am_res_path, pretrained_models[model_tag]['phones_dict'])\n            self.frontend = English(phone_vocab_path=self.phones_dict)\n        else: raise Exception(\"Unknown language ID: {}\".format(lang))\n    def _get_pretrained_path(self, tag):\n        \"\"\"\n        Download and returns pretrained resources path of current task.\n        \"\"\"\n        assert tag in pretrained_models, 'Can not find pretrained resources of {}.'.format(tag)",
        "type": "code",
        "location": "/tests/english_chinese_mixing_spliter/paddlebobo_paddletools_tts.py:142-164"
    },
    "3445": {
        "file_id": 419,
        "content": "This code sets up a model for text-to-speech (TTS) synthesis. It loads pretrained models and parameters, initializes the model architecture, applies normalization to input features, selects the appropriate frontend for the language (English or Chinese), and provides a method to download pretrained resources.",
        "type": "comment"
    },
    "3446": {
        "file_id": 419,
        "content": "        res_path = os.path.join(MODEL_HOME, tag)\n        decompressed_path = download_and_decompress(pretrained_models[tag],\n                                                    res_path)\n        decompressed_path = os.path.abspath(decompressed_path)\n        return decompressed_path\n    def run(self, text, output):\n        #文本输入\n        sentences = [str(text)]\n        # 长句处理\n        for sentence in sentences:\n            if self.lang == \"zh\":\n                input_ids = self.frontend.get_input_ids(sentence, merge_sentences=False, get_tone_ids=False) # what the heck? no freaking tone?\n            else:\n                input_ids = self.frontend.get_input_ids(sentence, merge_sentences=False) # what the heck? no freaking tone?\n            phone_ids = input_ids[\"phone_ids\"]\n            flags = 0\n            for part_phone_ids in phone_ids:\n                with paddle.no_grad():\n                    if self.lang == \"en\":\n                        mel = self.am_inference(\n                                        part_phone_ids)",
        "type": "code",
        "location": "/tests/english_chinese_mixing_spliter/paddlebobo_paddletools_tts.py:165-187"
    },
    "3447": {
        "file_id": 419,
        "content": "This code is related to a text-to-speech (TTS) system. It first downloads and decompresses the necessary pretrained model files, then processes the input text into phone_ids, which are used to generate speech using the am_inference function. The code handles both English and Chinese languages but seems to be missing tone information for Chinese.",
        "type": "comment"
    },
    "3448": {
        "file_id": 419,
        "content": "                                        # must get the scale using ffmpeg.\n                    elif self.lang == \"zh\":\n                        mel = self.am_inference(\n                                        part_phone_ids,\n                                        durations=None,\n                                        durations_scale = 1 / float(self.style_config['TTS']['SPEED']),\n                                        durations_bias = None,\n                                        pitch = None,\n                                        pitch_scale = float(self.style_config['TTS']['PITCH']),\n                                        pitch_bias = None,\n                                        energy = float(self.style_config['TTS']['ENERGY']),\n                                        energy_scale = None,\n                                        energy_bias = None,\n                                        )\n                    wav = self.voc_inference(mel)\n                if flags == 0:\n                    wav_all = wav",
        "type": "code",
        "location": "/tests/english_chinese_mixing_spliter/paddlebobo_paddletools_tts.py:188-204"
    },
    "3449": {
        "file_id": 419,
        "content": "This code chunk performs text-to-speech (TTS) conversion for Chinese language by first obtaining the Mel Spectrogram using `am_inference` function. The Mel Spectrogram is then converted to a WAV audio file using the `voc_inference` function. If flags equals 0, it assigns the result directly to `wav_all`.",
        "type": "comment"
    },
    "3450": {
        "file_id": 419,
        "content": "                    flags = 1\n                else:\n                    wav_all = paddle.concat([wav_all, wav])\n            sf.write(\n                output,\n                wav_all.numpy(),\n                samplerate=self.am_config.fs)\n        return output\n    # def __del__(self):\n    #     del self.voc_inference\n    #     del self.am_inference\n    #     del self.am_config\n    #     del self.style_config\n    #     del self.frontend\n    #     del self",
        "type": "code",
        "location": "/tests/english_chinese_mixing_spliter/paddlebobo_paddletools_tts.py:205-219"
    },
    "3451": {
        "file_id": 419,
        "content": "This code is concatenating audio files and saving them as a single output file. If the flag is set to 1, it stops concatenation and writes the existing audio file. The __del__ method deletes various objects when the instance is deallocated.",
        "type": "comment"
    },
    "3452": {
        "file_id": 420,
        "content": "/tests/english_chinese_mixing_spliter/english_grepper.py",
        "type": "filepath"
    },
    "3453": {
        "file_id": 420,
        "content": "This code searches, formats number lists, and tokenizes mixed English-Chinese text using regex. It iterates over results, updates language and UUID, sorts finalResult, creates dictionaries of index-text pairs for English and Chinese lists, then returns the result.",
        "type": "summary"
    },
    "3454": {
        "file_id": 420,
        "content": "# target = \"sample_strings.txt\"\n# data = open(target,\"r\",encoding=\"utf-8\").read()\n# data = data.split(\"\\n\")\nfrom zhon.hanzi import characters, radicals, punctuation\nimport re\ndef recursiveCompiledSearch(compiledRegex, pattern,initPos=0,resultTotal = []):\n    result = compiledRegex.search(pattern)\n    if result !=None:\n        match = result[0]\n        span = result.span()\n        realSpan = (span[0]+initPos, span[1]+initPos)\n        # initialSpan = span[0]\n        endSpan = span[1]\n        initPos += endSpan\n        mresult = {\"match\":match, \"span\":realSpan}\n        resultTotal.append(mresult)\n        newPattern = pattern[endSpan:]\n        return recursiveCompiledSearch(compiledRegex,newPattern,initPos,resultTotal)\n    else: return resultTotal\nfrom itertools import groupby, count\ndef set_to_range(numberlist):\n    numberlist = list(sorted(numberlist)) # double safety?\n    gpb = groupby(numberlist, lambda n, c=count(): n-next(c))\n    # Then to finish it off, generate the string from the groups.\n    def as_range(iterable): # not sure how to do this part elegantly",
        "type": "code",
        "location": "/tests/english_chinese_mixing_spliter/english_grepper.py:1-32"
    },
    "3455": {
        "file_id": 420,
        "content": "This code defines two functions for searching and formatting number lists. The first function, `recursiveCompiledSearch`, uses regex to search for patterns in a string, recursively appending matches to the result list. The second function, `set_to_range`, sorts a number list and then groups consecutive numbers together into ranges.",
        "type": "comment"
    },
    "3456": {
        "file_id": 420,
        "content": "        l = list(iterable)\n        if len(l) > 1:\n            return (l[0], l[-1]+1)\n        else:\n            return (l[0], l[0]+1)\n    result = [as_range(g) for _, g in gpb]\n    # result = [as_range(g) for _, g in groupby(numberlist, key=lambda n, c=count(): n-next(c))]\n    return result\n    # '1-3,6-7,10'\nimport uuid\ndef get_myuuid(): return str(uuid.uuid4())\ndef get_chinese_result(line,chineseSet):\n    chineseRanges = set_to_range(chineseSet)\n    result = []\n    for r in chineseRanges:\n        text = line[r[0]:r[1]]\n        data = {\"match\":text,\"span\":r,\"lang\":\"zh\",\"uuid\":get_myuuid()}\n        result.append(data)\n    return result\nall_chinese = characters+radicals+punctuation\nenglish = re.compile(r\"([a-zA-Z]+([ \\-,\\.:;?!]+)?)+\")\n# for line in data:\ndef analyze_mixed_text(line):\n    line = line.replace(\"\\n\",\"\")\n    # if len(line) <=3: continue\n    # shall we analyze this shit line by line?\n    # just a fucking try...\n    print(\"LINE DATA: \" + line)\n    eng_result = recursiveCompiledSearch(english,line,initPos=0,resultTotal = []) # recursive curse.",
        "type": "code",
        "location": "/tests/english_chinese_mixing_spliter/english_grepper.py:33-68"
    },
    "3457": {
        "file_id": 420,
        "content": "This function takes a line of mixed English and Chinese text, tokenizes it into ranges of each language, and returns these ranges in a list. It first converts the Chinese characters to their corresponding ranges and then finds English words using a compiled regular expression. The function ignores lines with less than 3 characters and calls another function recursively for further processing.",
        "type": "comment"
    },
    "3458": {
        "file_id": 420,
        "content": "    engSet = []\n    engResult = []\n    for eng in eng_result:\n        print(\"FOUND ENGLISH: \", eng)\n        span = eng[\"span\"]\n        mword = line[span[0]:span[1]]\n        mrange = list(range(span[0],span[1]))\n        engSet += mrange\n        eng2 = eng\n        eng2.update({\"lang\":\"en\",\"uuid\":get_myuuid()})\n        engResult.append(eng2)\n        print(\"VERIFICATION:\",mword)\n    chineseSet = [x for x in range(len(line)) if x not in engSet]\n    chineseResult = get_chinese_result(line,chineseSet)\n    finalResult = chineseResult+engResult\n    finalResult = sorted(finalResult,key=lambda x:x[\"span\"][0])\n    result = {\"en\":[],\"zh\":[]}\n    for index, data in enumerate(finalResult):\n        lang = data[\"lang\"]\n        text = data[\"match\"]\n        result[lang].append({\"index\":index,\"text\":text})\n    return result",
        "type": "code",
        "location": "/tests/english_chinese_mixing_spliter/english_grepper.py:69-90"
    },
    "3459": {
        "file_id": 420,
        "content": "The code iterates over the English results, appends the range of each word to engSet, updates the language and UUID of each result, and then builds a finalResult list. It sorts the finalResult by span[0] (start index) and creates a dictionary with English (en) and Chinese (zh) lists containing index-text pairs. Finally, it returns the result dictionary.",
        "type": "comment"
    },
    "3460": {
        "file_id": 421,
        "content": "/tests/english_chinese_mixing_spliter/default.yaml",
        "type": "filepath"
    },
    "3461": {
        "file_id": 421,
        "content": "This YAML configuration file sets the input and output paths for a GAN-based driving application. It includes options for image and video files, TTS settings, and save directories.",
        "type": "summary"
    },
    "3462": {
        "file_id": 421,
        "content": "GANDRIVING:\n  FOM_INPUT_IMAGE: './file/input/test.png'\n  FOM_DRIVING_VIDEO: './file/input/zimeng.mp4'\n  FOM_OUTPUT_VIDEO: './file/input/test.mp4'\nTTS:\n  SPEED: 1.0\n  PITCH: 1.0\n  ENERGY: 1.0\nSAVEPATH:\n  VIDEO_SAVE_PATH: './file/output/video/'\n  AUDIO_SAVE_PATH: './file/output/audio/'",
        "type": "code",
        "location": "/tests/english_chinese_mixing_spliter/default.yaml:1-13"
    },
    "3463": {
        "file_id": 421,
        "content": "This YAML configuration file sets the input and output paths for a GAN-based driving application. It includes options for image and video files, TTS settings, and save directories.",
        "type": "comment"
    },
    "3464": {
        "file_id": 422,
        "content": "/tests/voice_detect_extract_split/spleeter/test2.sh",
        "type": "filepath"
    },
    "3465": {
        "file_id": 422,
        "content": "This code downloads an audio example, separates it into two components using Spleeter's 2-stems model, and saves the results in separate files. However, there seems to be an issue with the second separation process.",
        "type": "summary"
    },
    "3466": {
        "file_id": 422,
        "content": "# wget https://github.com/deezer/spleeter/raw/master/audio_example.mp3\n# separate the example audio into two components\npython3 -m spleeter separate -p spleeter:2stems -o output you_got_me.mp3\npython3 -m spleeter separate -p spleeter:2stems -o output tarot_desc.mp3\n# seems not working at all",
        "type": "code",
        "location": "/tests/voice_detect_extract_split/spleeter/test2.sh:1-5"
    },
    "3467": {
        "file_id": 422,
        "content": "This code downloads an audio example, separates it into two components using Spleeter's 2-stems model, and saves the results in separate files. However, there seems to be an issue with the second separation process.",
        "type": "comment"
    },
    "3468": {
        "file_id": 423,
        "content": "/tests/voice_detect_extract_split/spleeter/test.sh",
        "type": "filepath"
    },
    "3469": {
        "file_id": 423,
        "content": "This code retrieves an example audio file, separates it into two components using the Spleeter library, and saves the result in the \"output\" directory. However, it seems to be facing some issues with separation.",
        "type": "summary"
    },
    "3470": {
        "file_id": 423,
        "content": "# wget https://github.com/deezer/spleeter/raw/master/audio_example.mp3\n# separate the example audio into two components\npython3 -m spleeter separate -p spleeter:2stems -o output audio_example.mp3\n# seems not working at all",
        "type": "code",
        "location": "/tests/voice_detect_extract_split/spleeter/test.sh:1-4"
    },
    "3471": {
        "file_id": 423,
        "content": "This code retrieves an example audio file, separates it into two components using the Spleeter library, and saves the result in the \"output\" directory. However, it seems to be facing some issues with separation.",
        "type": "comment"
    },
    "3472": {
        "file_id": 424,
        "content": "/tests/voice_detect_extract_split/spleeter/spleeter_init.sh",
        "type": "filepath"
    },
    "3473": {
        "file_id": 424,
        "content": "Installs spleeter version 2.1.0, downloads spleeter-2.2.2-py3-none-any.whl and installs it, and imports pretrained_models/4stems.",
        "type": "summary"
    },
    "3474": {
        "file_id": 424,
        "content": "# pip3n install spleeter==2.1.0\nwget https://files.pythonhosted.org/packages/fb/2e/5d2cd3d0179d3f749d03eddf0172f1dbababbc371c1b5cbd7fc27d741070/spleeter-2.2.2-py3-none-any.whl\npip3n install spleeter-2.2.2-py3-none-any.whl # why you require specific tensorflow version?\n# https://github.com/deezer/spleeter/releases/download/v1.4.0/4stems.tar.gz\n# at pretrained_models/4stems",
        "type": "code",
        "location": "/tests/voice_detect_extract_split/spleeter/spleeter_init.sh:1-6"
    },
    "3475": {
        "file_id": 424,
        "content": "Installs spleeter version 2.1.0, downloads spleeter-2.2.2-py3-none-any.whl and installs it, and imports pretrained_models/4stems.",
        "type": "comment"
    },
    "3476": {
        "file_id": 425,
        "content": "/tests/voice_detect_extract_split/spleeter/README.md",
        "type": "filepath"
    },
    "3477": {
        "file_id": 425,
        "content": "This code describes using Spleeter, an open-sourced tool, and mentions two model hosts, Hugging Face and Wolfram Neural Network Library. These libraries provide paraphrasing models and can be utilized with the Wolfram Developer Engine respectively.",
        "type": "summary"
    },
    "3478": {
        "file_id": 425,
        "content": "use spleeter which is open-sourced.\nmany model hoster interests me. the most gigantic one is huggingface. providing paraphrasing models and more. another one is wolfram neural network library. can be used freely with wolfram developer engine.",
        "type": "code",
        "location": "/tests/voice_detect_extract_split/spleeter/README.md:1-3"
    },
    "3479": {
        "file_id": 425,
        "content": "This code describes using Spleeter, an open-sourced tool, and mentions two model hosts, Hugging Face and Wolfram Neural Network Library. These libraries provide paraphrasing models and can be utilized with the Wolfram Developer Engine respectively.",
        "type": "comment"
    },
    "3480": {
        "file_id": 426,
        "content": "/tests/voice_detect_extract_split/spleeter/download_models.sh",
        "type": "filepath"
    },
    "3481": {
        "file_id": 426,
        "content": "The code downloads pretrained model files for spleeter, a sound separation tool. It uses curl to retrieve the tarballs from GitHub releases and stores them in \"pretrained_models\" directory. After downloading, it moves the files and changes the directory to execute further tasks related to these models.",
        "type": "summary"
    },
    "3482": {
        "file_id": 426,
        "content": "curl -O -L https://github.com/deezer/spleeter/releases/download/v1.4.0/2stems.tar.gz\ncurl -O -L https://github.com/deezer/spleeter/releases/download/v1.4.0/4stems.tar.gz\ncurl -O -L https://github.com/deezer/spleeter/releases/download/v1.4.0/5stems.tar.gz\nmv {2stems.tar.gz, 4stems.tar.gz, 5stems.tar.gz} pretrained_models\ncd pretrained_models",
        "type": "code",
        "location": "/tests/voice_detect_extract_split/spleeter/download_models.sh:1-6"
    },
    "3483": {
        "file_id": 426,
        "content": "The code downloads pretrained model files for spleeter, a sound separation tool. It uses curl to retrieve the tarballs from GitHub releases and stores them in \"pretrained_models\" directory. After downloading, it moves the files and changes the directory to execute further tasks related to these models.",
        "type": "comment"
    },
    "3484": {
        "file_id": 427,
        "content": "/tests/voice_detect_extract_split/paddlespeech/test.sh",
        "type": "filepath"
    },
    "3485": {
        "file_id": 427,
        "content": "The code exports HTTP and HTTPS proxy variables, runs TTS (Text-to-Speech) to convert text into audio (output.wav), and then performs ASR (Automatic Speech Recognition) in Chinese language using the output audio file. The code is intended for testing purposes with PaddleSpeech deep learning framework.",
        "type": "summary"
    },
    "3486": {
        "file_id": 427,
        "content": "export http_proxy=\"\"\nexport https_proxy=\"\"\n# this voice is great. excellent for my shit.\npaddlespeech tts --input \"你好，欢迎使用飞桨深度学习框架！\" --output output.wav # must download models on the fly.\npaddlespeech asr --lang zh --input output.wav\n# 你好欢迎使用非讲深度学习框架\n# how does it feel to have errors?\n# left and right variables are not the same. what is that?",
        "type": "code",
        "location": "/tests/voice_detect_extract_split/paddlespeech/test.sh:1-12"
    },
    "3487": {
        "file_id": 427,
        "content": "The code exports HTTP and HTTPS proxy variables, runs TTS (Text-to-Speech) to convert text into audio (output.wav), and then performs ASR (Automatic Speech Recognition) in Chinese language using the output audio file. The code is intended for testing purposes with PaddleSpeech deep learning framework.",
        "type": "comment"
    },
    "3488": {
        "file_id": 428,
        "content": "/tests/moviepy_loop_video_till_target/test.py",
        "type": "filepath"
    },
    "3489": {
        "file_id": 428,
        "content": "This code imports the \"main\" function from the \"loop_till_target\" module and sets the target duration of the video to 20 seconds. It uses a GIF file named \"cute_cat_gif\" as input, applies the main function to it, and saves the resulting video as \"cute_cat_gif_20_secs_plus.gif\". The code also checks if the final duration of the video is greater than or equal to the target duration using an assertion statement.",
        "type": "summary"
    },
    "3490": {
        "file_id": 428,
        "content": "from loop_till_target import main\ntarget_secs = 20\nvideo_in = \"/root/Desktop/works/pyjom/samples/video/cute_cat_gif.gif\"\n# no right codec! fuck. GIF not supported?\nvideo_out = f\"/root/Desktop/works/pyjom/samples/video/cute_cat_gif_{target_secs}_secs_plus.gif\"\nfvd = main(video_in, target_secs, f_out=video_out, in_place=False,debug=True)\nassert fvd >= target_secs",
        "type": "code",
        "location": "/tests/moviepy_loop_video_till_target/test.py:1-11"
    },
    "3491": {
        "file_id": 428,
        "content": "This code imports the \"main\" function from the \"loop_till_target\" module and sets the target duration of the video to 20 seconds. It uses a GIF file named \"cute_cat_gif\" as input, applies the main function to it, and saves the resulting video as \"cute_cat_gif_20_secs_plus.gif\". The code also checks if the final duration of the video is greater than or equal to the target duration using an assertion statement.",
        "type": "comment"
    },
    "3492": {
        "file_id": 429,
        "content": "/tests/moviepy_loop_video_till_target/loop_till_target.py",
        "type": "filepath"
    },
    "3493": {
        "file_id": 429,
        "content": "This code uses ffmpeg to create a video clip with repeating segments, splitting the input video into original and reversed parts, and concatenates them based on loop strategy. It replaces sections of the input video until reaching the target duration and saves the output at specified location.",
        "type": "summary"
    },
    "3494": {
        "file_id": 429,
        "content": "import os\n# moviepy's shit.\nfrom moviepy.editor import VideoFileClip  # , concatenate_videoclips\n# import moviepy.video.fx.all as vfx\ndef main(\n    f_in: str,\n    target_secs: float,\n    f_out: str = \"\",\n    in_place: bool = True,\n    debug: bool = False,\n    # accuracy_float:int=4\n    # audio:bool=False, # it will cause trouble?\n):\n    # print(\"___\")\n    # print(\"AUDIO?\",audio)\n    # print(\"IN PLACE?\",in_place)\n    # print(\"___\")\n    assert os.path.exists(f_in)\n    assert target_secs > 0\n    # target_secs_str =(\"{\"+f':.{accuracy_float}f'+\"}\").format(target_secs)\n    targetFilePath = f_out\n    if not in_place:\n        assert f_out != \"\"\n    else:\n        targetFilePath = f_in\n    clip = VideoFileClip(f_in)\n    # if not audio:\n    #     clip = clip.without_audio()\n    # newclip = clip.fx(vfx.time_mirror) # error?\n    # newclip = clip\n    import ffmpeg\n    file_input_split = ffmpeg.input(f_in).filter_multi_output(\n        \"split\"\n    )  # this is infinite split.\n    videoDuration = clip.duration\n    import math\n    import tempfile",
        "type": "code",
        "location": "/tests/moviepy_loop_video_till_target/loop_till_target.py:1-47"
    },
    "3495": {
        "file_id": 429,
        "content": "The code imports necessary libraries and defines a function \"main\" that takes input file, target duration, output file (optional), performs in-place editing (optional), and debug mode (optional). It asserts the existence of the input file and positive target duration. Depending on options, it either splits or mirrors the video using ffmpeg before processing.",
        "type": "comment"
    },
    "3496": {
        "file_id": 429,
        "content": "    import shutil\n    fileExtension = f_in.split(\".\")[-1]\n    assert fileExtension != \"\"\n    loopStrategy = [\n        (-1) ** i for i in range(math.ceil(target_secs / videoDuration))\n    ]  # zero division error?\n    if debug:\n        print(\"Loop strategy:\")\n        print(loopStrategy)\n    clips = []\n    file_input_original = file_input_split[0].filter_multi_output(\"split\")\n    file_input_reverse = (\n        file_input_split[1].filter(\"reverse\").filter_multi_output(\"split\")\n    )\n    for index, signal in enumerate(loopStrategy):\n        mindex = index // 2\n        if signal == 1:\n            file_input = file_input_original[mindex]\n            clips.append(file_input)\n        else:\n            file_input_reverse2 = file_input_reverse[mindex]\n            clips.append(file_input_reverse2)\n    # final = concatenate_videoclips(clips)\n    final = ffmpeg.concat(*clips)\n    finalVideoDuration = len(loopStrategy) * videoDuration\n    with tempfile.NamedTemporaryFile(\n        \"w+\",\n        suffix=f\".{fileExtension}\",\n    ) as f:",
        "type": "code",
        "location": "/tests/moviepy_loop_video_till_target/loop_till_target.py:48-82"
    },
    "3497": {
        "file_id": 429,
        "content": "This code is creating a video clip with repeating segments. It splits the input video into two parts, original and reversed. Then, it loops through a list of loop strategies to determine which segment (original or reversed) should be used for each iteration. The final video is created by concatenating these segments together using ffmpeg. The resulting video's duration will be determined by the length of the loop strategy list multiplied by the original video's duration.",
        "type": "comment"
    },
    "3498": {
        "file_id": 429,
        "content": "        tmpFilePath = f.name\n        # warning! what is the audio shit?\n        # print(\"TMP FILE PATH?\",tmpFilePath)\n        # breakpoint()\n        # final.write_videofile(tmpFilePath, fps=clip.fps)\n        # finalVideoDuration = final.duration\n        final.output(tmpFilePath).run(overwrite_output=True)\n        shutil.copy(tmpFilePath, targetFilePath)\n    return finalVideoDuration\nif __name__ == \"__main__\":\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"-i\", \"--input\", help=\"input file\", required=True, type=str)\n    parser.add_argument(\"-o\", \"--output\", help=\"output file\", default=\"\", type=str)\n    parser.add_argument(\n        \"-r\",\n        \"--replace\",\n        help=\"replace original input file\",\n        action=\"store_true\",\n        default=False,\n    )\n    # parser.add_argument(\n    #     \"-a\",\n    #     \"--audio\",\n    #     help=\"include audio from input\",\n    #     action=\"store_true\",\n    #     default=False,\n    # )\n    parser.add_argument(\n        \"-t\", \"--target\", help=\"target seconds\", required=True, type=float",
        "type": "code",
        "location": "/tests/moviepy_loop_video_till_target/loop_till_target.py:83-116"
    },
    "3499": {
        "file_id": 429,
        "content": "This code takes an input video file and replaces a specific section of the video with another video until a target duration is reached. The final output is saved at the specified output location.",
        "type": "comment"
    }
}