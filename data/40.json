{
    "4000": {
        "file_id": 503,
        "content": "\"\"\"\nInterface for using the chatgpt api online service, without setting up locally.\nThis interface is used for development, not in production.\n\"\"\"\n# Do not treat the machine like people.\n# You need to handle them differently.\nfrom litellm import completion\nimport os\nimport yaml\napi_key_filepath = os.path.join(\n    os.path.expanduser(\"~\"), \".chatgpt_api_key.yaml\")\nif os.path.exists(api_key_filepath):\n    if os.path.isfile(api_key_filepath):\n        # Load YAML file\n        with open(api_key_filepath, 'r') as file:\n            data = yaml.load(file, Loader=yaml.FullLoader)\n            api_key = data['api_key']\n            endpoint = data['endpoint']\n    else:\n        raise Exception(\n            f\"API key path exists but found non-file object at: '{api_key_filepath}'\")\nelse:\n    raise Exception(f\"API key file not found in: '{api_key_filepath}'\")\nos.environ[\"OPENAI_API_KEY\"] = api_key\nos.environ[\"OPENAI_API_BASE\"] = endpoint\nmodel_tag = \"openai/gpt-3.5-turbo\"\ndef get_reply_from_chatgpt(content: str):\n    messages = [{\"content\": content, \"role\": \"user\"}]",
        "type": "code",
        "location": "/tests/chatgpt_multiagent_agent_product_line_multimodal_langchain_experiments/test_chatgpt_cn_api.py:1-37"
    },
    "4001": {
        "file_id": 503,
        "content": "This code provides an interface for using the ChatGPT API online service without setting up locally, specifically designed for development purposes. It handles loading the API key and endpoint from a YAML file, sets environment variables, and defines a function to get a reply from ChatGPT using provided content.",
        "type": "comment"
    },
    "4002": {
        "file_id": 503,
        "content": "    print(\"sending:\")\n    print(messages)\n    # openai call\n    # many info inside. you may want to take a look?\n    response = completion(model_tag, messages)\n    choices = response['choices']\n    reply_content = choices[0]['message']['content']\n    print(\"reply:\")\n    print(reply_content)\n    return reply_content",
        "type": "code",
        "location": "/tests/chatgpt_multiagent_agent_product_line_multimodal_langchain_experiments/test_chatgpt_cn_api.py:38-47"
    },
    "4003": {
        "file_id": 503,
        "content": "This code sends a message and receives a response using OpenAI's completion API. It prints the input messages, processes the response from the API, extracts the reply content, and returns it for further processing.",
        "type": "comment"
    },
    "4004": {
        "file_id": 504,
        "content": "/tests/chatgpt_multiagent_agent_product_line_multimodal_langchain_experiments/test_url_repair_extract_trace_media_source.py",
        "type": "filepath"
    },
    "4005": {
        "file_id": 504,
        "content": "The code defines functions to recover URLs, indexify them, and prompt for YouTube selection with utility functions for repairing content and retrieving selection IDs. It extracts, repairs, and selects YouTube URLs from a list by handling direct/indirect links and improving readability.",
        "type": "summary"
    },
    "4006": {
        "file_id": 504,
        "content": "from typing import Optional\ndef recover_prompt_constructor(info): return f\"\"\"\nPlease recover any URL from the given context. Every URL shall be visitable, starting with \"http://\" or \"https://\".\nContext:\n{info}\nURLs:\n\"\"\"\ndef indexify_string_list(string_list): return [\n    f'[{index}] {url}' for index, url in enumerate(string_list)]\ndef youtube_select_prompt_constructor(url_list): \n    urls_content = '\\n'.join(indexify_string_list(url_list))\n    return f\"\"\"\nPlease select URLs if they are directed to YouTube. I will give you URLs with index in front of them. Give your selection by selected indices in squared brackets separated by space like: [1] [3].\nURLs:\n{urls_content}\nSelected URLs:\n\"\"\"\nfrom test_chatgpt_cn_api import get_reply_from_chatgpt\ndef repair_content_with_url(info):\n    prompt = recover_prompt_constructor(info)\n    content = get_reply_from_chatgpt(prompt)\n    return content\nimport re\ndef get_youtube_selection_ids(urls):\n    prompt = youtube_select_prompt_constructor(urls)\n    response= get_reply_from_chatgpt(prompt)",
        "type": "code",
        "location": "/tests/chatgpt_multiagent_agent_product_line_multimodal_langchain_experiments/test_url_repair_extract_trace_media_source.py:2-42"
    },
    "4007": {
        "file_id": 504,
        "content": "This code defines functions to recover URLs from context, indexify them in a string list, and prompt the user to select YouTube URLs. It also includes utility functions for repairing content with URLs and getting YouTube selection IDs.",
        "type": "comment"
    },
    "4008": {
        "file_id": 504,
        "content": "    numbers = re.findall(r'\\[\\d+\\]', response)\n    indices = []\n    for num in numbers:\n        num = num.strip(\"[\").strip(']').strip()\n        num = int(num)\n        indices.append(num)\n    return indices\ndef select_youtube_urls(url_list, indices):\n    fatal_error:Optional[str]= None\n    selected_urls = []\n    index_errors = 0\n    url_counts = len(url_list)\n    max_index = url_counts-1\n    for index in indices:\n        try:\n            url = url_list[index]\n            print(\"selected:\", url)\n            selected_urls.append(url)\n        except IndexError:\n            index_errors += 1\n            # TODO: handle error by recursively letting the LLM knows the error and querying answer.\n            # TODO: determine if error is fatal (not recoverable in 5 iterations)\n            # TODO: eliminate possibility of external cause of fatal error by inferance\n            print('index not found: %d (max index is %d)' % (index, max_index))\n    print(\"summary\".center(80, \"=\"))\n    print(\"given url counts: %d\" % url_counts)",
        "type": "code",
        "location": "/tests/chatgpt_multiagent_agent_product_line_multimodal_langchain_experiments/test_url_repair_extract_trace_media_source.py:43-69"
    },
    "4009": {
        "file_id": 504,
        "content": "This code defines two functions, `extract_indices` and `select_youtube_urls`. The first function extracts indices from the response string using regular expressions. The second function takes a list of URLs and a list of indices, selects the corresponding URLs based on the provided indices, handles index errors, and returns the selected URLs. It also includes TODO comments for potential error handling and improvement suggestions.",
        "type": "comment"
    },
    "4010": {
        "file_id": 504,
        "content": "    print(\"selected url counts: %d\", len(selected_urls))\n    print(\"index errors: %d\" % index_errors)\n    return selected_urls\nfrom urlextract import URLExtract\nextractor = URLExtract()\ndef extract_url(content):\n    \"\"\"\n    Just extract the url. Do not repair.\n    \"\"\"\n    urls = extractor.find_urls(content)\n    return urls\ndef repair_and_get_repaired_url_list(info):\n    content = repair_content_with_url(info)\n    url_list = extract_url(content)\n    return url_list\nif __name__ == '__main__':\n    info_direct = \"\"\"\nYoutube\n原标题A Thousand Miles-Neco are(FULL VERSION)\nhttps://youtu.be/Ddpx0JLOH6o?si=zZMjAEFj_TOXkQct\n音频下载：\nhttps://wwxa.lanzouj.com/idPN81a5u4ab\n密码:5292\n\"\"\"\n    info_indirect = \"\"\"\n转自Youtube\n/watch?v=mSqRH4WwnnY // By :Encrypted Lobster\n\"\"\"\n    info_list = [info_direct, info_indirect]\n    for i, info in enumerate(info_list):\n        print(f\"processing info #{i}\")\n        print(info)\n        repaired_urls = repair_and_get_repaired_url_list(info)\n        indices = get_youtube_selection_ids(repaired_urls)\n        selected_urls = select_youtube_urls(repaired_urls, indices)",
        "type": "code",
        "location": "/tests/chatgpt_multiagent_agent_product_line_multimodal_langchain_experiments/test_url_repair_extract_trace_media_source.py:70-114"
    },
    "4011": {
        "file_id": 504,
        "content": "This code is extracting and repairing URLs from provided information. It first extracts URLs without repairing them, then repairs the content with any broken URLs and extracts the repaired URLs. The code handles both direct and indirect Youtube links, processes each info in the list, repairs URLs, selects YouTube URLs based on specified selection IDs, and finally returns the selected URLs.",
        "type": "comment"
    },
    "4012": {
        "file_id": 504,
        "content": "        print(\"selected urls:\")\n        for url in selected_urls:\n            print(f'\\t{url}')\n        print()",
        "type": "code",
        "location": "/tests/chatgpt_multiagent_agent_product_line_multimodal_langchain_experiments/test_url_repair_extract_trace_media_source.py:115-118"
    },
    "4013": {
        "file_id": 504,
        "content": "This code segment prints the selected URLs from a list. The 'for' loop iterates through each URL in the list and displays it with a tab character for readability, followed by a newline to separate each URL for better visualization.",
        "type": "comment"
    },
    "4014": {
        "file_id": 505,
        "content": "/tests/chatgpt_multiagent_agent_product_line_multimodal_langchain_experiments/README.md",
        "type": "filepath"
    },
    "4015": {
        "file_id": 505,
        "content": "The code mentions the power of chatgpt-like bots and plans to run one using CPU instead of a powerful GPU. It also suggests utilizing moderation API and openAI API for testing purposes. The bot will start with a simple task involving constructing/extracting URLs from video descriptions.",
        "type": "summary"
    },
    "4016": {
        "file_id": 505,
        "content": "chatgpt-like bots are powerful.\nwe will run one using cpu, since we don't always have a powerful gpu\nwe can also use moderation api & openai api for testing\nfirst we will give the bot a simple task to construct/extract url from video description.",
        "type": "code",
        "location": "/tests/chatgpt_multiagent_agent_product_line_multimodal_langchain_experiments/README.md:1-7"
    },
    "4017": {
        "file_id": 505,
        "content": "The code mentions the power of chatgpt-like bots and plans to run one using CPU instead of a powerful GPU. It also suggests utilizing moderation API and openAI API for testing purposes. The bot will start with a simple task involving constructing/extracting URLs from video descriptions.",
        "type": "comment"
    },
    "4018": {
        "file_id": 506,
        "content": "/tests/split_long_image_into_video/init.sh",
        "type": "filepath"
    },
    "4019": {
        "file_id": 506,
        "content": "The code downloads the background music (bgm) file \"the_happy_troll.mp3\" and an image file \"long_and_funny_image_about_ai_painting.jpg\". It uses curl command with -L flag for redirecting, -O flag for saving output to named file. The music source is recognized by Shazam.",
        "type": "summary"
    },
    "4020": {
        "file_id": 506,
        "content": "# first, let's download the bgm used by many funny videos, recognized by shazam\n# curl -L -o the_happy_troll.mp3 \"https://ge-sycdn.kuwo.cn/a573fcf0d69bd0cd5912bf9a96cff3dc/63b4a35f/resource/n3/1/70/3124049952.mp3\"\ncurl -O \"https://tmpfiles.org/dl/620815/long_and_funny_image_about_ai_painting.jpg\"",
        "type": "code",
        "location": "/tests/split_long_image_into_video/init.sh:1-3"
    },
    "4021": {
        "file_id": 506,
        "content": "The code downloads the background music (bgm) file \"the_happy_troll.mp3\" and an image file \"long_and_funny_image_about_ai_painting.jpg\". It uses curl command with -L flag for redirecting, -O flag for saving output to named file. The music source is recognized by Shazam.",
        "type": "comment"
    },
    "4022": {
        "file_id": 507,
        "content": "/tests/split_long_image_into_video/generate_video.py",
        "type": "filepath"
    },
    "4023": {
        "file_id": 507,
        "content": "This code resizes an image, generates a video, and creates Editly specification files. It utilizes multiple modules for handling file operations and parameter definitions, then writes the script to a file, executes it, and removes temporary files.",
        "type": "summary"
    },
    "4024": {
        "file_id": 507,
        "content": "# to get a proper cover, let's simply crop.\n# to find a proper title for this video, extract keywords, generate title and find the best cover by embeddings.\n# first, get picture aspect.\nimport cv2\ndef getWidthHeight(impath):\n    d = cv2.imread(impath)\n    # print(d.shape)\n    height, width, channels = d.shape\n    return width, height\nim0 = \"long_and_funny_image_about_ai_painting.jpg\"\nim1 = \"intermediate.png\"\n# very high, low width.\n# calculate actual output?\nmheight, mwidth = 1080, 1920\nwidth, height = getWidthHeight(im0)\nimport ffmpeg\nffmpeg.input(im0).filter(\"scale\", w=mwidth, h=-1).output(im1).run(overwrite_output=True)\nwidth0, height0 = getWidthHeight(im1)\npad_total =( mheight-(height0 % mheight)) % mheight\n# print(\"PAD TOTAL?\", pad_total)\n# breakpoint()\nif pad_total != 0:\n    im2 = \"intermediate_0.png\"\n    pad_above = pad_total // 2\n    pad_below = pad_total - pad_above\n    # then you must rewrite this shit.\n    ffmpeg.input(im1).filter(\n        \"pad\", w=\"iw\", h=\"ih+{}\".format(pad_total), x=0, y=pad_above, color=\"white\"",
        "type": "code",
        "location": "/tests/split_long_image_into_video/generate_video.py:1-36"
    },
    "4025": {
        "file_id": 507,
        "content": "This code reads an image, calculates its aspect ratio, scales it to a specific resolution (1920x1080), and saves the result. If there's still some padding needed for the new height, it pads the top with white space before saving again. The goal is to create a properly formatted image for use as video cover.",
        "type": "comment"
    },
    "4026": {
        "file_id": 507,
        "content": "    ).output(im2).run(overwrite_output=True)\nelse:\n    im2 = im1\n# then chop it up.\nimport os\nimport shutil\nmdir = \"output\"\nfout = \"output%d.png\"\nif os.path.exists(mdir):\n    shutil.rmtree(mdir)\nos.mkdir(mdir)\nmfout = os.path.join(mdir, fout)\nimport math\nmh = math.ceil(height0 / mheight)\nmlayout = \"1x{}\".format(mh)\nffmpeg.input(im2).filter(\"untile\", layout=mlayout).output(mfout).run(\n    overwrite_output=True\n)\nmfiles = os.listdir(mdir)\nimport re\noutput_path = \"./output.mp4\"\nmfiles.sort(key=lambda x: int(re.findall(r\"[0-9]+\", x)[0]))\neditly_script = {\n    \"width\": mwidth,\n    \"height\": mheight,\n    \"fps\": 60,\n    \"outPath\": output_path,\n    \"defaults\": {\n        \"transition\": {\n            \"duration\": 0.5,\n            \"name\": \"random\",\n            \"audioOutCurve\": \"tri\",\n            \"audioInCurve\": \"tri\",\n        },\n        \"duration\": 3,\n    },\n    \"clips\": [\n        {\"layers\": [{\"type\": \"image\", \"path\": os.path.join(mdir, mfile)}]}\n        for mfile in mfiles\n    ],\n    \"audioFilePath\": \"the_happy_troll.mp3\",\n}\nimport json5\neditly_spec_file = \"spec_file.json5\"",
        "type": "code",
        "location": "/tests/split_long_image_into_video/generate_video.py:37-89"
    },
    "4027": {
        "file_id": 507,
        "content": "This code generates a video from a long image, chops it into parts, and then creates an editly specification file for further processing. It handles overwriting files if necessary, sorts the output image files, and defines various parameters such as layout, fps, and duration. The code also imports several modules (os, shutil, math, re) to perform operations like creating directories, removing tree structures, sorting files, and manipulating file paths.",
        "type": "comment"
    },
    "4028": {
        "file_id": 507,
        "content": "with open(editly_spec_file, \"w+\") as fp:\n    json5.dump(editly_script, fp)\n# now execute\nimport os\nos.system(\"rm -rf editly-tmp*\")\nos.system(\"xvfb-run editly {}\".format(editly_spec_file))",
        "type": "code",
        "location": "/tests/split_long_image_into_video/generate_video.py:90-97"
    },
    "4029": {
        "file_id": 507,
        "content": "Writing the Editly script to a file, then executing it with temporary environment variables and removing temporary files.",
        "type": "comment"
    },
    "4030": {
        "file_id": 508,
        "content": "/tests/split_long_image_into_video/cleanup.sh",
        "type": "filepath"
    },
    "4031": {
        "file_id": 508,
        "content": "This code is deleting the 'output' and 'editly-tmp\\*' folders to clean up after a process, ensuring no leftover files are present.",
        "type": "summary"
    },
    "4032": {
        "file_id": 508,
        "content": "rm -rf output\nrm -rf editly-tmp*",
        "type": "code",
        "location": "/tests/split_long_image_into_video/cleanup.sh:1-2"
    },
    "4033": {
        "file_id": 508,
        "content": "This code is deleting the 'output' and 'editly-tmp\\*' folders to clean up after a process, ensuring no leftover files are present.",
        "type": "comment"
    },
    "4034": {
        "file_id": 509,
        "content": "/tests/update_progressbar_network/test.yaml",
        "type": "filepath"
    },
    "4035": {
        "file_id": 509,
        "content": "This code configures a tmux session for running tests. The session has two panes: one running Python client.py and another executing test.sh in Bash.",
        "type": "summary"
    },
    "4036": {
        "file_id": 509,
        "content": "session_name: online_dog_cat_generator_test\nstart_directory: /root/Desktop/works/pyjom/tests/update_progressbar_network\nwindows:\n- layout: main-horizontal\n  options:\n    main-pane-height: 30\n  panes:\n  - shell_command:\n    - python3 client.py\n  - shell_command:\n    - bash test.sh\n  window_name: progressbar window",
        "type": "code",
        "location": "/tests/update_progressbar_network/test.yaml:1-12"
    },
    "4037": {
        "file_id": 509,
        "content": "This code configures a tmux session for running tests. The session has two panes: one running Python client.py and another executing test.sh in Bash.",
        "type": "comment"
    },
    "4038": {
        "file_id": 510,
        "content": "/tests/update_progressbar_network/test.sh",
        "type": "filepath"
    },
    "4039": {
        "file_id": 510,
        "content": "This command runs the Uvicorn web server for a Python application, listening on port 8576. It sets the log level to critical and enables auto-reloading of the app when changes are made.",
        "type": "summary"
    },
    "4040": {
        "file_id": 510,
        "content": "python3 -m uvicorn --port 8576  --log-level critical test:app --reload",
        "type": "code",
        "location": "/tests/update_progressbar_network/test.sh:1-1"
    },
    "4041": {
        "file_id": 510,
        "content": "This command runs the Uvicorn web server for a Python application, listening on port 8576. It sets the log level to critical and enables auto-reloading of the app when changes are made.",
        "type": "comment"
    },
    "4042": {
        "file_id": 511,
        "content": "/tests/update_progressbar_network/test.py",
        "type": "filepath"
    },
    "4043": {
        "file_id": 511,
        "content": "This FastAPI server allows progress updates via network, with endpoints for starting, resetting, and updating tqdm progress bars, along with functions for opening and closing the progress bar.",
        "type": "summary"
    },
    "4044": {
        "file_id": 511,
        "content": "# try to update progressbar via network.\nfrom fastapi import FastAPI\napp = FastAPI()\nfrom tqdm import tqdm\nt = None\n@app.get('/')\ndef hello():\n    return 'progressbar server'\n# not routing this to network.\ndef close_progressbar():\n    global t\n    if t is not None:\n        try:\n            t.close()\n            return {'msg':'success'}\n        except:\n            import traceback\n            traceback.print_exc()\n            print('error closing progressbar')\n            return {'msg':'error closing progressbar'}\n@app.get('/reset')\ndef reset(total: int, name:str='random task'): # pass the iteration count\n    global t\n    close_progressbar()\n    print('processing:', name)\n    t = tqdm(total=total)\n    return {'msg':'success'}\n@app.get('/update')\ndef update_progressbar(progress: int=1):\n    global t\n    if t is not None:\n        try:\n            t.clear()\n            t.update(progress)\n            return {'msg':'success'}\n        except:\n            import traceback\n            traceback.print_exc()\n            print(\"error when updating progessbar\")",
        "type": "code",
        "location": "/tests/update_progressbar_network/test.py:1-46"
    },
    "4045": {
        "file_id": 511,
        "content": "This code sets up a FastAPI server to handle progress updates via network. It includes endpoints for starting, resetting, and updating a tqdm progress bar. The server also includes functions for opening and closing the progress bar.",
        "type": "comment"
    },
    "4046": {
        "file_id": 511,
        "content": "            return {'msg':'error when updating progessbar'}\n    else:\n        print('no progressbar available')\n        return {'msg':'no progressbar available'}\n@app.get('/close')\ndef close():\n    close_progressbar()\n    return {'msg':'success'}",
        "type": "code",
        "location": "/tests/update_progressbar_network/test.py:47-56"
    },
    "4047": {
        "file_id": 511,
        "content": "This code handles a GET route for closing the progressbar and returns success or error messages based on the availability of the progressbar.",
        "type": "comment"
    },
    "4048": {
        "file_id": 512,
        "content": "/tests/update_progressbar_network/load_session.sh",
        "type": "filepath"
    },
    "4049": {
        "file_id": 512,
        "content": "The code snippet is using tmux, a terminal multiplexer, to kill an existing session (online_dog_cat_generator_test) and then load a new session from the test.yaml configuration file. This may be used for testing or managing different terminal sessions efficiently.",
        "type": "summary"
    },
    "4050": {
        "file_id": 512,
        "content": "tmux kill-session -t online_dog_cat_generator_test\ntmuxp load test.yaml",
        "type": "code",
        "location": "/tests/update_progressbar_network/load_session.sh:1-2"
    },
    "4051": {
        "file_id": 512,
        "content": "The code snippet is using tmux, a terminal multiplexer, to kill an existing session (online_dog_cat_generator_test) and then load a new session from the test.yaml configuration file. This may be used for testing or managing different terminal sessions efficiently.",
        "type": "comment"
    },
    "4052": {
        "file_id": 513,
        "content": "/tests/update_progressbar_network/client.py",
        "type": "filepath"
    },
    "4053": {
        "file_id": 513,
        "content": "Code defines a class \"netProgressbar\" that establishes a connection to a progress bar server, allows resetting and updating the progress bar through HTTP requests.",
        "type": "summary"
    },
    "4054": {
        "file_id": 513,
        "content": "import requests\nclass netProgressbar:\n    def __init__(self, port = 8576, message = 'progressbar server'):\n        from lazero.network import waitForServerUp\n        self.port = port\n        self.message = message\n        waitForServerUp(port=port, message=message)\n    def reset(self, total:int):\n        requests.get('http://localhost:{}/reset'.format(self.port),proxies=None,params = {'total':total})\n    def update(self,progress:int=1):\n        requests.get('http://localhost:8576/update',proxies=None, params={'progress':progress})",
        "type": "code",
        "location": "/tests/update_progressbar_network/client.py:1-12"
    },
    "4055": {
        "file_id": 513,
        "content": "Code defines a class \"netProgressbar\" that establishes a connection to a progress bar server, allows resetting and updating the progress bar through HTTP requests.",
        "type": "comment"
    },
    "4056": {
        "file_id": 514,
        "content": "/tests/still_watermark_auto_removal/test_auto_video_watermark_detection.sh",
        "type": "filepath"
    },
    "4057": {
        "file_id": 514,
        "content": "cd into the automatic-watermark-detection directory and execute the video_watermark_detection.py script for watermark detection in videos.",
        "type": "summary"
    },
    "4058": {
        "file_id": 514,
        "content": "cd automatic-watermark-detection\npython3 video_watermark_detection.py",
        "type": "code",
        "location": "/tests/still_watermark_auto_removal/test_auto_video_watermark_detection.sh:1-3"
    },
    "4059": {
        "file_id": 514,
        "content": "cd into the automatic-watermark-detection directory and execute the video_watermark_detection.py script for watermark detection in videos.",
        "type": "comment"
    },
    "4060": {
        "file_id": 515,
        "content": "/tests/still_watermark_auto_removal/maxRectangleSolver.py",
        "type": "filepath"
    },
    "4061": {
        "file_id": 515,
        "content": "The code defines checkOverlap for point validation and solves the maximum rectangle problem, iterating through candidate rectangles, detecting overlaps, and displaying the best candidate in red. It also sorts and prints top 5 areas.",
        "type": "summary"
    },
    "4062": {
        "file_id": 515,
        "content": "import sympy\nimport json\ndata = json.loads(open(\"test_special.json\", \"r\").read())\ncanvas = data[\"canvas\"]\nrectangles = data[\"rectangles\"]\ncanvasWidth, canvasHeight = canvas\nxValid = [0, canvasWidth]\nyValid = [0, canvasHeight]\nmRects = []\ndef checkContains(rect, point):\n    xPoints = [p[0] for p in rect]\n    yPoints = [p[1] for p in rect]\n    maxX, minX = max(xPoints), min(xPoints)\n    maxY, minY = max(yPoints), min(yPoints)\n    x, y = point\n    return x > minX and x < maxX and y > minY and y < maxY\n# def checkOverlapAsymmetric(rect0, rect1):\n#     for point in rect0:\n#         if checkContains(rect1, point):\n#             return True\n#         # also check for intersections?\n#     return False\n# Python program to check if rectangles overlap\nclass D2Point:\n    def __init__(self, x, y):\n        self.x = x\n        self.y = y\ndef getRectDiagonalPoints(rect):\n    xPoints = [p[0] for p in rect]\n    yPoints = [p[1] for p in rect]\n    maxX, minX = max(xPoints), min(xPoints)\n    maxY, minY = max(yPoints), min(yPoints)\n    p0, p1 = D2Point(minX, minY), D2Point(maxX, maxY)",
        "type": "code",
        "location": "/tests/still_watermark_auto_removal/maxRectangleSolver.py:1-43"
    },
    "4063": {
        "file_id": 515,
        "content": "Code reads JSON data containing a canvas and rectangles. It validates points, creates a function to check if two rectangles overlap, and defines D2Point class for storing 2D point coordinates.",
        "type": "comment"
    },
    "4064": {
        "file_id": 515,
        "content": "    return p0, p1\ndef do_overlap(l1, r1, l2, r2):\n    # if rectangle has area 0, no overlap\n    if l1.x == r1.x or l1.y == r1.y or r2.x == l2.x or l2.y == r2.y:\n        return False\n    # If one rectangle is on left side of other\n    if l1.x >= r2.x or l2.x >= r1.x:\n        return False\n    if l1.y >= r2.y or l2.y >= r1.y:\n        return False\n    return True\ndef checkOverlap(rect0, rect1):\n    return do_overlap(*getRectDiagonalPoints(rect0),*getRectDiagonalPoints(rect1))\nfor x, y, mWidth, mHeight in rectangles:\n    xValid.append(x)\n    xValid.append(x + mWidth)\n    yValid.append(y)\n    yValid.append(y + mHeight)\n    p0, p1, p2, p3 = (\n        (x, y),\n        (x + mWidth, y),\n        (x + mWidth, y + mHeight),\n        (x, y + mHeight),\n    )\n    # mRectangle = sympy.Polygon(p0,p1,p2,p3)\n    mRectangle = [p0, p1, p2, p3]\n    mRects.append(mRectangle)\ndef purify(xValid):\n    xValid = list(set(xValid))\n    xValid.sort()\n    return xValid\ndef checkOverlapAgainstRectList(rect, rectList):\n    for testRect in rectList:\n        if checkOverlap(rect, testRect):",
        "type": "code",
        "location": "/tests/still_watermark_auto_removal/maxRectangleSolver.py:44-85"
    },
    "4065": {
        "file_id": 515,
        "content": "This code defines a function checkOverlap which takes two rectangles and checks if they overlap. The function do_overlap is used to determine if the rectangles have any area of overlap. It also includes utility functions like getRectDiagonalPoints, purify, and checkOverlapAgainstRectList for manipulating and comparing rectangles. The code creates a list of rectangles and checks for overlaps between each rectangle and others in a separate list.",
        "type": "comment"
    },
    "4066": {
        "file_id": 515,
        "content": "            return True\n    return False\nxValid = purify(xValid)\nyValid = purify(yValid)\ntotalCandidates = []\ndef getRectArea(rect):\n    xPoints = [p[0] for p in rect]\n    yPoints = [p[1] for p in rect]\n    maxX, minX = max(xPoints), min(xPoints)\n    maxY, minY = max(yPoints), min(yPoints)\n    return (maxX - minX) * (maxY - minY)\nbestCandidate = None\nbestArea = 0\nfor ix0 in range(0, len(xValid)-1):\n    for ix1 in range(ix0+1, len(xValid)):\n        for iy0 in range(0, len(yValid)-1):\n            for iy1 in range(iy0+1, len(yValid)):\n                x0, x1, y0, y1 = xValid[ix0], xValid[ix1], yValid[iy0], yValid[iy1]\n                x, y = x0, y0\n                mWidth, mHeight = x1 - x, y1 - y\n                p0, p1, p2, p3 = (\n                    (x, y),\n                    (x + mWidth, y),\n                    (x + mWidth, y + mHeight),\n                    (x, y + mHeight),\n                )\n                rectCandidate = [p0, p1, p2, p3]\n                area = getRectArea(rectCandidate)\n                if area <= bestArea:",
        "type": "code",
        "location": "/tests/still_watermark_auto_removal/maxRectangleSolver.py:86-120"
    },
    "4067": {
        "file_id": 515,
        "content": "The code is implementing a maximum rectangle solver algorithm. It iterates over the x and y validated points to generate all possible rectangles, calculating their areas using the getRectArea function. The best candidate with the highest area is stored.",
        "type": "comment"
    },
    "4068": {
        "file_id": 515,
        "content": "                    continue\n                if checkOverlapAgainstRectList(rectCandidate, mRects):\n                    break\n                bestCandidate = rectCandidate.copy()\n                bestArea = area\n                # print(\"UPDATING:\",bestCandidate)\n                # print('AREA:', bestArea)\n                # totalCandidates.append(rectCandidate.copy())\nprint(\"final candidate:\", bestCandidate)\n# plot this?\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Rectangle\nfig, ax = plt.subplots()\nax.plot([canvasWidth, canvasHeight])\n# add rectangle to plot\ndef plotRect(ax, x, y, width, height, facecolor):\n    ax.add_patch(Rectangle((x, y), width, height, facecolor=facecolor, fill=True))\ndef rectToXYWH(rect):\n    xPoints = [p[0] for p in rect]\n    yPoints = [p[1] for p in rect]\n    maxX, minX = max(xPoints), min(xPoints)\n    maxY, minY = max(yPoints), min(yPoints)\n    x, y = minX, minY\n    width, height = (maxX - minX), (maxY - minY)\n    return x, y, width, height\nplotRect(ax,0,0,canvasWidth, canvasHeight,'black')",
        "type": "code",
        "location": "/tests/still_watermark_auto_removal/maxRectangleSolver.py:121-151"
    },
    "4069": {
        "file_id": 515,
        "content": "This code finds the maximum rectangle by iterating through candidate rectangles and checking for overlaps. It updates the best candidate and area if a better one is found, then plots the final candidate rectangle on a plot.",
        "type": "comment"
    },
    "4070": {
        "file_id": 515,
        "content": "for rect in mRects:\n    x,y, width, height = rectToXYWH(rect)\n    plotRect(ax,x,y,width,height,'white')\nplotRect(ax,*rectToXYWH(bestCandidate),'red')\n# display plot\nplt.show()\n# totalCandidates.sort(key = lambda rect: -getRectArea(rect))\n# for rect in totalCandidates[:5]:\n#     print(rect)",
        "type": "code",
        "location": "/tests/still_watermark_auto_removal/maxRectangleSolver.py:153-161"
    },
    "4071": {
        "file_id": 515,
        "content": "The code generates and plots rectangles using given coordinates and dimensions, with the best candidate rectangle displayed in red. It also sorts the total list of candidate rectangles by area and prints the top 5 areas.",
        "type": "comment"
    },
    "4072": {
        "file_id": 516,
        "content": "/tests/still_watermark_auto_removal/EAST-Detector-for-text-detection-using-OpenCV-master/README.md",
        "type": "filepath"
    },
    "4073": {
        "file_id": 516,
        "content": "EAST Detector code in OpenCV enables real-time, accurate text detection in natural scenes, addressing challenges like angles, lighting, and non-ideal surfaces. The project welcomes improvements via pull requests and is licensed under MIT.",
        "type": "summary"
    },
    "4074": {
        "file_id": 516,
        "content": "# EAST Detector for Text Detection\nOpenCV’s EAST(Efficient and Accurate Scene Text Detection ) text detector is a deep learning model, based on a novel architecture and training pattern. It is capable of \n- running at near real-time at 13 FPS on 720p images and \n- obtains state-of-the-art text detection accuracy.\n[Link to paper](https://arxiv.org/pdf/1704.03155.pdf)\nOpenCV’s text detector implementation of EAST is quite robust, capable of localizing text even when it’s blurred, reflective, or partially obscured.\nThere are many natural scene text detection challenges that have been described by Celine Mancas-Thillou and Bernard Gosselin in their excellent 2017 paper, [Natural Scene Text Understanding](https://www.tcts.fpms.ac.be/publications/regpapers/2007/VS_cmtbg2007.pdf) below:\n- **Image/sensor noise**: Sensor noise from a handheld camera is typically higher than that of a traditional scanner. Additionally, low-priced cameras will typically interpolate the pixels of raw sensors to produce real colors.",
        "type": "code",
        "location": "/tests/still_watermark_auto_removal/EAST-Detector-for-text-detection-using-OpenCV-master/README.md:1-13"
    },
    "4075": {
        "file_id": 516,
        "content": "Code for OpenCV's EAST Detector implementation for text detection, capable of real-time performance and high accuracy. Based on a novel architecture and training pattern, it can detect text even when blurred, reflective or partially obscured. The code addresses challenges like sensor noise from handheld cameras.",
        "type": "comment"
    },
    "4076": {
        "file_id": 516,
        "content": "- **Viewing angles**: Natural scene text can naturally have viewing angles that are not parallel to the text, making the text harder to recognize.\nBlurring: Uncontrolled environments tend to have blur, especially if the end user is utilizing a smartphone that does not have some form of stabilization.\n- **Lighting conditions**: We cannot make any assumptions regarding our lighting conditions in natural scene images. It may be near dark, the flash on the camera may be on, or the sun may be shining brightly, saturating the entire image.\n- **Resolution**: Not all cameras are created equal — we may be dealing with cameras with sub-par resolution.\n- **Non-paper objects**: Most, but not all, paper is not reflective (at least in context of paper you are trying to scan). Text in natural scenes may be reflective, including logos, signs, etc.\n- **Non-planar objects**: Consider what happens when you wrap text around a bottle — the text on the surface becomes distorted and deformed. While humans may sti",
        "type": "code",
        "location": "/tests/still_watermark_auto_removal/EAST-Detector-for-text-detection-using-OpenCV-master/README.md:15-24"
    },
    "4077": {
        "file_id": 516,
        "content": "The code discusses various challenges in natural scene text detection, such as viewing angles, lighting conditions, resolution, non-paper objects (like reflective surfaces), and non-planar objects (such as distorted text on curved surfaces). These factors make it difficult to recognize and extract text from natural scenes.",
        "type": "comment"
    },
    "4078": {
        "file_id": 516,
        "content": "ll be able to easily “detect” and read the text, our algorithms will struggle. We need to be able to handle such use cases.\n- **Unknown layout**: We cannot use any a priori information to give our algorithms “clues” as to where the text resides.\n## Contributing\nPull requests are welcome. For major changes, please open an issue first to discuss what you would like to change.\n### Thanks to [Adrian's Blog](https://www.pyimagesearch.com/2018/08/20/opencv-text-detection-east-text-detector/) for a comprehensive blog on EAST Detector.\n## License\n[MIT](https://choosealicense.com/licenses/mit/)",
        "type": "code",
        "location": "/tests/still_watermark_auto_removal/EAST-Detector-for-text-detection-using-OpenCV-master/README.md:24-35"
    },
    "4079": {
        "file_id": 516,
        "content": "This code is from the README file of a project that utilizes EAST Detector, an OpenCV-based text detection algorithm. The project aims to improve its algorithms' ability to handle complex text layouts and use cases. It welcomes pull requests for major changes and is licensed under MIT.",
        "type": "comment"
    },
    "4080": {
        "file_id": 517,
        "content": "/tests/still_watermark_auto_removal/EAST-Detector-for-text-detection-using-OpenCV-master/opencv_text_detection_image.py",
        "type": "filepath"
    },
    "4081": {
        "file_id": 517,
        "content": "The code loads an input image, preprocesses it, resizes using argparse, detects text with OpenCV and EAST Detector, calculates prediction time, filters low-confidence detections, extracts scores & geometrical data, applies non-maxima suppression, scales coordinates, draws bounding boxes on the original image, and displays it.",
        "type": "summary"
    },
    "4082": {
        "file_id": 517,
        "content": "# USAGE\n# python3 opencv_text_detection_image.py --image images/lebron_james.jpg --east frozen_east_text_detection.pb\n# import the necessary packages\nfrom imutils.object_detection import non_max_suppression\nimport numpy as np\nimport argparse\nimport time\nimport cv2\n# construct the argument parser and parse the arguments\nap = argparse.ArgumentParser()\nap.add_argument(\"-i\", \"--image\", type=str,\n                help=\"path to input image\")\nap.add_argument(\"-east\", \"--east\", type=str,\n                help=\"path to input EAST text detector\")\nap.add_argument(\"-c\", \"--min-confidence\", type=float, default=0.5,\n                help=\"minimum probability required to inspect a region\")\nap.add_argument(\"-w\", \"--width\", type=int, default=320,\n                help=\"resized image width (should be multiple of 32)\")\nap.add_argument(\"-e\", \"--height\", type=int, default=320,\n                help=\"resized image height (should be multiple of 32)\")\nargs = vars(ap.parse_args())\n# load the input image and grab the image dimensions\nimage = cv2.imread(args[\"image\"])",
        "type": "code",
        "location": "/tests/still_watermark_auto_removal/EAST-Detector-for-text-detection-using-OpenCV-master/opencv_text_detection_image.py:1-26"
    },
    "4083": {
        "file_id": 517,
        "content": "This code loads an input image and applies preprocessing steps. It utilizes the argparse module to accept command-line arguments, allowing users to specify the input image path and the East text detector model file. It also sets default values for confidence threshold and resized image dimensions, which can be overridden by command-line options.",
        "type": "comment"
    },
    "4084": {
        "file_id": 517,
        "content": "orig = image.copy()\n(H, W) = image.shape[:2]\n# set the new width and height and then determine the ratio in change\n# for both the width and height\n(newW, newH) = (args[\"width\"], args[\"height\"])\nrW = W / float(newW)\nrH = H / float(newH)\n# resize the image and grab the new image dimensions\nimage = cv2.resize(image, (newW, newH))\n(H, W) = image.shape[:2]\n# define the two output layer names for the EAST detector model that\n# we are interested -- the first is the output probabilities and the\n# second can be used to derive the bounding box coordinates of text\nlayerNames = [\n    \"feature_fusion/Conv_7/Sigmoid\",\n    \"feature_fusion/concat_3\"]\n# load the pre-trained EAST text detector\nprint(\"[INFO] loading EAST text detector...\")\nnet = cv2.dnn.readNet(args[\"east\"])\n# construct a blob from the image and then perform a forward pass of\n# the model to obtain the two output layer sets\nblob = cv2.dnn.blobFromImage(image, 1.0, (W, H),\n                             (123.68, 116.78, 103.94), swapRB=True, crop=False)\nstart = time.time()",
        "type": "code",
        "location": "/tests/still_watermark_auto_removal/EAST-Detector-for-text-detection-using-OpenCV-master/opencv_text_detection_image.py:27-55"
    },
    "4085": {
        "file_id": 517,
        "content": "This code performs image preprocessing, resizing and loads the EAST text detector model for text detection. It sets the original image copy, calculates new width and height based on arguments, resizes the image, gets updated dimensions, defines output layer names for the model, loads the pre-trained EAST text detector model, constructs a blob from the image, performs a forward pass of the model to obtain two output layers.",
        "type": "comment"
    },
    "4086": {
        "file_id": 517,
        "content": "net.setInput(blob)\n(scores, geometry) = net.forward(layerNames)\nend = time.time()\n# show timing information on text prediction\nprint(\"[INFO] text detection took {:.6f} seconds\".format(end - start))\n# grab the number of rows and columns from the scores volume, then\n# initialize our set of bounding box rectangles and corresponding\n# confidence scores\n(numRows, numCols) = scores.shape[2:4]\nrects = []\nconfidences = []\n# loop over the number of rows\nfor y in range(0, numRows):\n    # extract the scores (probabilities), followed by the geometrical\n    # data used to derive potential bounding box coordinates that\n    # surround text\n    scoresData = scores[0, 0, y]\n    xData0 = geometry[0, 0, y]\n    xData1 = geometry[0, 1, y]\n    xData2 = geometry[0, 2, y]\n    xData3 = geometry[0, 3, y]\n    anglesData = geometry[0, 4, y]\n    # loop over the number of columns\n    for x in range(0, numCols):\n        # if our score does not have sufficient probability, ignore it\n        if scoresData[x] < args[\"min_confidence\"]:\n            continue",
        "type": "code",
        "location": "/tests/still_watermark_auto_removal/EAST-Detector-for-text-detection-using-OpenCV-master/opencv_text_detection_image.py:56-86"
    },
    "4087": {
        "file_id": 517,
        "content": "Code performs text detection using OpenCV and EAST Detector. It calculates the time taken for text prediction, extracts scores and geometrical data, filters out low-confidence detections, and stores bounding box rectangles and corresponding confidence scores in lists.",
        "type": "comment"
    },
    "4088": {
        "file_id": 517,
        "content": "        # compute the offset factor as our resulting feature maps will\n        # be 4x smaller than the input image\n        (offsetX, offsetY) = (x * 4.0, y * 4.0)\n        # extract the rotation angle for the prediction and then\n        # compute the sin and cosine\n        angle = anglesData[x]\n        cos = np.cos(angle)\n        sin = np.sin(angle)\n        # use the geometry volume to derive the width and height of\n        # the bounding box\n        h = xData0[x] + xData2[x]\n        w = xData1[x] + xData3[x]\n        # compute both the starting and ending (x, y)-coordinates for\n        # the text prediction bounding box\n        endX = int(offsetX + (cos * xData1[x]) + (sin * xData2[x]))\n        endY = int(offsetY - (sin * xData1[x]) + (cos * xData2[x]))\n        startX = int(endX - w)\n        startY = int(endY - h)\n        # add the bounding box coordinates and probability score to\n        # our respective lists\n        rects.append((startX, startY, endX, endY))\n        confidences.append(scoresData[x])\n# apply non-maxima suppression to suppress weak, overlapping bounding",
        "type": "code",
        "location": "/tests/still_watermark_auto_removal/EAST-Detector-for-text-detection-using-OpenCV-master/opencv_text_detection_image.py:88-115"
    },
    "4089": {
        "file_id": 517,
        "content": "This code computes the bounding box coordinates and confidence scores for text predictions, using input data such as angles, offsets, xData values. It then applies non-maxima suppression to suppress weak overlapping bounding boxes, likely for further processing or object detection purposes.",
        "type": "comment"
    },
    "4090": {
        "file_id": 517,
        "content": "# boxes\nboxes = non_max_suppression(np.array(rects), probs=confidences)\n# loop over the bounding boxes\nfor (startX, startY, endX, endY) in boxes:\n    # scale the bounding box coordinates based on the respective\n    # ratios\n    startX = int(startX * rW)\n    startY = int(startY * rH)\n    endX = int(endX * rW)\n    endY = int(endY * rH)\n    # draw the bounding box on the image\n    cv2.rectangle(orig, (startX, startY), (endX, endY), (0, 255, 0), 2)\n# show the output image\ncv2.imshow(\"Text Detection\", orig)\ncv2.waitKey(0)",
        "type": "code",
        "location": "/tests/still_watermark_auto_removal/EAST-Detector-for-text-detection-using-OpenCV-master/opencv_text_detection_image.py:116-133"
    },
    "4091": {
        "file_id": 517,
        "content": "This code performs non-maximum suppression on bounding box coordinates, scales the coordinates based on image ratios, draws bounding boxes on the original image using OpenCV, and displays the resulting image.",
        "type": "comment"
    },
    "4092": {
        "file_id": 518,
        "content": "/tests/still_watermark_auto_removal/automatic-watermark-detection/video_watermark_detection.py",
        "type": "filepath"
    },
    "4093": {
        "file_id": 518,
        "content": "The code employs OpenCV and Deep Learning for watermark detection, estimation, and removal. It detects video watermarks using adaptive thresholding, applies Gaussian blur, scales, and draws boxes before saving the information to a JSON file.",
        "type": "summary"
    },
    "4094": {
        "file_id": 518,
        "content": "# sample few images from a video.\nimport random\n## we import our version of cv2 here? or uninstall and reinstall opencv-python with custom things?\nimport pathlib\nimport sys\nsite_path = pathlib.Path(\"/usr/local/lib/python3.9/site-packages\")\ncv2_libs_dir = site_path / 'cv2' / f'python-{sys.version_info.major}.{sys.version_info.minor}'\nprint(cv2_libs_dir)\ncv2_libs = sorted(cv2_libs_dir.glob(\"*.so\"))\nif len(cv2_libs) == 1:\n    print(\"INSERTING:\",cv2_libs[0].parent)\n    sys.path.insert(1, str(cv2_libs[0].parent))\nimport cv2\nimport progressbar as pb\nvideoPaths = [\n    \"/root/Desktop/works/pyjom/tests/still_watermark_auto_removal/kunfu_cat.mp4\", # bilibili animal video compilation\n    \"/root/Desktop/works/pyjom/tests/bilibili_practices/bilibili_video_translate/japan_day.webm\", # youtube animation with watermark\n    \"/root/Desktop/works/pyjom/samples/video/LiGHT3ZCi.mp4\", # animal video compilation with pip and large area of watermark\n]  # his watermark. scorpa.\nvideo_path = videoPaths[2]\n# will change this shit.\n# shall we downscale this thing?",
        "type": "code",
        "location": "/tests/still_watermark_auto_removal/automatic-watermark-detection/video_watermark_detection.py:1-25"
    },
    "4095": {
        "file_id": 518,
        "content": "The code imports necessary libraries, checks and inserts custom OpenCV library paths, defines a list of video paths, and selects the third video for processing.",
        "type": "comment"
    },
    "4096": {
        "file_id": 518,
        "content": "# video = cv2.\n# video_path = \"\"\n# long loading time since we are backing up.\nsample_count = 60\nvideo_cap = cv2.VideoCapture(video_path)\nfps = video_cap.get(cv2.CAP_PROP_FPS)  # 60.\nframe_count = int(video_cap.get(cv2.CAP_PROP_FRAME_COUNT))\nprint(frame_count)\nsample_indexs = [x for x in range(frame_count)]\nsample_indexs = random.sample(sample_indexs, sample_count)\n# import copy\nimageSet = []\nfor frame_index_counter in pb.progressbar(range(frame_count)):  # are you sure?\n    success, frame = video_cap.read()\n    if not success:\n        break\n    if frame_index_counter in sample_indexs:\n        imageSet.append(frame.copy())\nfrom src import *\ngx, gy, gxlist, gylist = estimate_watermark_imgSet(imageSet)\n# print(len(imageSet))\ncropped_gx, cropped_gy, watermark_location = crop_watermark(gx, gy, location=True)\nW_m = poisson_reconstruct(cropped_gx, cropped_gy)\nW_full = poisson_reconstruct(gx, gy)\nprint(cropped_gx.shape, cropped_gy.shape, W_m.shape)  # (50, 137, 3) may vary.\nprint(watermark_location)  # ((1022, 21), (1072, 158)) inverted x,y! hell.",
        "type": "code",
        "location": "/tests/still_watermark_auto_removal/automatic-watermark-detection/video_watermark_detection.py:27-62"
    },
    "4097": {
        "file_id": 518,
        "content": "This code reads frames from a video, randomly selects some frames to analyze for watermark detection, and then estimates the watermark using poisson reconstruction. The code also outputs the shape of the detected watermark and its location on the frames. It may not use progress bar properly and has an issue with inverted x and y coordinates for watermark location.",
        "type": "comment"
    },
    "4098": {
        "file_id": 518,
        "content": "# cv2.imshow(\"WATERMARK\",W_m)\n# cv2.imshow(\"WATERMARK_FULL\",W_full)\n# # remove the freaking watermark please?\n# cv2.waitKey(0)\n# east_net = \"/media/root/help/pyjom/tests/still_watermark_auto_removal/EAST-Detector-for-text-detection-using-OpenCV-master/frozen_east_text_detection.pb\"\n# net = cv2.dnn.readNet(east_net)\n# H,W = W_full.shape[:2]\n# newH = (H//32)*32\n# newW = (W//32)*32\n# rH, rW = H/float(newH), W/float(newW)\n# W_full = cv2.resize(W_full,(newW,newH))\nmaxval, minval = np.max(W_full), np.min(W_full)\nW_full = (W_full - minval) * (255 / (maxval - minval))  # is that necessary?\n# # print(,W_full.shape,W_full.dtype)\nW_full = W_full.astype(np.uint8)\n# # breakpoint()\n# newH,newW = W_full.shape[:2]\n# # 14.122540090957173 -17.575702620638673 (1080, 1920, 3) float64\n# # you even have negative values. what the fuck?\n# blob = cv2.dnn.blobFromImage(W_full, 1.0, (newW, newH), (123.68, 116.78, 103.94), swapRB=True, crop=False)\n# # start = time.time()\n# net.setInput(blob)\n# layerNames = [\n# \t\"feature_fusion/Conv_7/Sigmoid\",",
        "type": "code",
        "location": "/tests/still_watermark_auto_removal/automatic-watermark-detection/video_watermark_detection.py:64-91"
    },
    "4099": {
        "file_id": 518,
        "content": "The code is using OpenCV and TensorFlow to detect and remove a watermark from an input image. It resizes the input image, normalizes pixel values, preprocesses the image with a DNN (Deep Neural Network), and then sets the input for the network's feature extraction layer.",
        "type": "comment"
    }
}