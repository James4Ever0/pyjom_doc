{
    "2000": {
        "file_id": 199,
        "content": "        diagonalRects = []\n    else:\n        diagonalRects = [LRTBToDiagonal(x) for x in detection[0]]\n    for x1, y1, x2, y2 in diagonalRects:\n        w, h = x2 - x1, y2 - y1\n        x, y = x1, y1\n        cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), -1)\n    # calculate the portion of the text area.\n    textArea = np.sum(img)\n    textAreaRatio = (textArea / 255) / (width * height)\n    print(\"text area: {:.2f} %\".format(textAreaRatio))\n    cv2.imshow(\"img\", img)\n    cv2.waitKey(0)",
        "type": "code",
        "location": "/tests/unittest_ocr_filter_large_area_of_text.py:38-50"
    },
    "2001": {
        "file_id": 199,
        "content": "This code calculates the text area percentage of an image and displays it. It first determines diagonal rectangles from detection data, then draws rectangles around the detected text areas using OpenCV's rectangle function. The total text area is calculated by summing pixel values in the image, which is then normalized to a percentage of the image's total area. Finally, the image with drawn rectangles and text area ratio is displayed.",
        "type": "comment"
    },
    "2002": {
        "file_id": 200,
        "content": "/tests/unittest_online_topic_generator_giphy.py",
        "type": "filepath"
    },
    "2003": {
        "file_id": 200,
        "content": "This code generates online topics, downloads assets, filters videos based on duration, FPS, color centrality, and processes them sequentially through specified filters. It checks validity using various functions, skips/deletes invalid or abandoned files.",
        "type": "summary"
    },
    "2004": {
        "file_id": 200,
        "content": "from test_commons import *\nfrom pyjom.modules.topicGenerator import OnlineTopicGenerator\nfrom pyjom.modules.informationGathering import OnlineFetcher\nfrom lazero.utils import sprint\nfrom lazero.network import download, waitForServerUp\nfrom lazero.filesystem import tmpdir\nclash_refresher_port = 8677\nclash_refresher_url = \"http://127.0.0.1:{}\".format(clash_refresher_port)\nwaitForServerUp(clash_refresher_port, \"clash update controller\")\nelems, function_label = OnlineTopicGenerator()\nsprint(\"FUNCTION LABEL:\", function_label)\n# # 'pyjom.commons.OnlineTopicGenerator'\n# breakpoint()\ntmpPath = \"/dev/shm/medialang/online_test\"\nimport os\nproxy_url = \"http://127.0.0.1:8381\"\ndef set_proxy():\n    os.environ[\"http_proxy\"] = proxy_url\n    os.environ[\"https_proxy\"] = proxy_url\nflag = \"topic_with_fetcher\"\nwith tmpdir(path=tmpPath) as testDir:\n    print(\"TESTDIR:\", testDir)\n    if flag == \"only_topic_generator\":\n        # print(\"HERE??\",1)\n        for asset_id, meta in elems:\n            print(\"X\", asset_id, meta)\n            url = meta[\"url\"]",
        "type": "code",
        "location": "/tests/unittest_online_topic_generator_giphy.py:1-36"
    },
    "2005": {
        "file_id": 200,
        "content": "This code sets up an online topic generator and uses it to generate elements. It also defines a function to set a proxy, creates a temporary directory for testing, and checks if only the topic generator is needed. It then iterates through the generated elements, printing their IDs and URLs.",
        "type": "comment"
    },
    "2006": {
        "file_id": 200,
        "content": "            extension = url.split(\"?\")[0].split(\".\")[-1]\n            basename = \".\".join([asset_id, extension])\n            download_path = os.path.join(tmpPath, basename)\n            try:\n                download(\n                    url,\n                    download_path,\n                    threads=6,\n                    size_filter={\"min\": 0.4, \"max\": 50},\n                    use_multithread=True,\n                )\n            except:\n                print(\"Error when download file\")\n            # X ('sr8jYZVVsCmxddga8w', {'height': 480, 'width': 474, 'url': 'https://media0.giphy.com/media/sr8jYZVVsCmxddga8w/giphy.gif'})\n            # breakpoint()\n            # seems good. now we check the cat/dog.\n    elif flag == \"topic_with_fetcher\":\n        sprint(\"checking online fetcher\")\n        # print(\"HERE??\",2)\n        set_proxy()\n        newElems, label = OnlineFetcher(\n            elems, tempdir=tmpPath\n        )  # infinite video generators.\n        for elem in newElems:\n            waitForServerUp(clash_refresher_port, \"clash update controller\")",
        "type": "code",
        "location": "/tests/unittest_online_topic_generator_giphy.py:37-61"
    },
    "2007": {
        "file_id": 200,
        "content": "This code block is downloading an asset from a specified URL, with options for size and multithreading. If any error occurs during the download process, it prints an error message. Then, it checks the topic of an online generator using OnlineFetcher, setting a proxy before executing it. This involves creating new elements and waiting for the server to be updated. The purpose seems to be related to topic-based online generation and video fetching.",
        "type": "comment"
    },
    "2008": {
        "file_id": 200,
        "content": "            sprint(elem)\n            (item_id, local_video_location) = elem\n            # what is the freaking response?\n            from caer.video.frames_and_fps import get_duration, get_fps_float\n            # duration = get_duration(local_video_location)\n            from pyjom.commons import checkMinMaxDict\n            duration_filter = {\"min\": 0.6, \"max\": 9}\n            fps_filter = {\"min\": 7, \"max\": 60}\n            # fps_float = get_fps_float(local_video_location)\n            # duration_valid = checkMinMaxDict(duration,duration_filter)\n            # fps_valid = checkMinMaxDict(fps_float,fps_filter)\n            from pyjom.videotoolbox import (\n                getVideoColorCentrality,\n                checkVideoColorCentrality,\n                getEffectiveFPS,\n                NSFWVideoFilter,\n                yolov5_bezier_paddlehub_resnet50_dog_cat_video_filter,\n                dummyFilterFunction,  # just for dog and cat, no other animals.\n            )\n            video_color_filter = {\n                \"centrality\": {\"max\": 0.30},",
        "type": "code",
        "location": "/tests/unittest_online_topic_generator_giphy.py:62-85"
    },
    "2009": {
        "file_id": 200,
        "content": "This code snippet is filtering video elements based on their duration, FPS (frames per second), and color centrality. It uses the `get_duration`, `get_fps_float`, `checkMinMaxDict` functions from different modules. Additionally, it imports video processing tools like `getVideoColorCentrality`, `checkVideoColorCentrality`, `getEffectiveFPS`, and some specific filters such as `NSFWVideoFilter`, `yolov5_bezier_paddlehub_resnet50_dog_cat_video_filter`. It defines a threshold for the video's color centrality (\"max\": 0.30).",
        "type": "comment"
    },
    "2010": {
        "file_id": 200,
        "content": "                \"max_nearby_center_percentage\": {\"max\": 0.20},\n            }\n            video_effective_fps_filter = {\"min\": 7}\n            valid = True\n            mList = [\n                [get_duration, duration_filter, checkMinMaxDict, \"duration\"],\n                [get_fps_float, fps_filter, checkMinMaxDict, \"fps\"],\n                [\n                    yolov5_bezier_paddlehub_resnet50_dog_cat_video_filter,\n                    None,\n                    dummyFilterFunction,\n                    \"DogCat\",\n                ],\n                [\n                    getVideoColorCentrality,\n                    video_color_filter,\n                    checkVideoColorCentrality,\n                    \"video_color_centrality\",\n                ],\n                [\n                    getEffectiveFPS,\n                    video_effective_fps_filter,\n                    checkMinMaxDict,\n                    \"EffectiveFPS\",\n                ],  # also, the dog/cat detector! fuck.\n                [NSFWVideoFilter, None, dummyFilterFunction, \"NSFW\"],",
        "type": "code",
        "location": "/tests/unittest_online_topic_generator_giphy.py:86-111"
    },
    "2011": {
        "file_id": 200,
        "content": "This code defines a list of filters and their parameters for processing video clips. Each filter is applied sequentially, checking if the clip meets specific criteria such as duration, FPS, color centrality, effectiveness, and whether it contains explicit content. The filters are specified by functions, with optional minimum/maximum thresholds or additional checks.",
        "type": "comment"
    },
    "2012": {
        "file_id": 200,
        "content": "            ]\n            for function, mFilter, filterFunction, flag in mList:\n                mValue = function(local_video_location)\n                valid = filterFunction(mValue, mFilter)\n                if not valid:\n                    print(\"skipping due to invalid %s: %s\" % (flag, mValue))\n                    print(\"%s filter:\" % flag, mFilter)\n                    break\n            if not valid:\n                print(\"abandon video:\", item_id)\n            breakpoint()\n            if not valid:\n                if os.path.exists(local_video_location):\n                    print(\"removing abandoned video:\", local_video_location)\n                    os.remove(local_video_location)\n                # if you abandon that, better delete it!\n            # do time duration check, effective fps check, color centrality check, then the dog/cat check\n            # what's next? find some audio files? or just use one audio?\n    # print(\"HERE??\",3)\n    # print('flag', flag)",
        "type": "code",
        "location": "/tests/unittest_online_topic_generator_giphy.py:112-131"
    },
    "2013": {
        "file_id": 200,
        "content": "This code is filtering a video based on various flags and conditions. It checks for validity by using different functions and filters, and skips or deletes the file if it's invalid or abandoned. The process involves duration, FPS, color centrality, dog/cat detection, and potentially audio files.",
        "type": "comment"
    },
    "2014": {
        "file_id": 201,
        "content": "/tests/unittest_lazero_external_downloader.py",
        "type": "filepath"
    },
    "2015": {
        "file_id": 201,
        "content": "This code downloads a file from a given URL and saves it to a specified path. It checks if the file already exists at the target location, and if so, deletes it before initiating the download. The success or failure of the download is reported using print statement.",
        "type": "summary"
    },
    "2016": {
        "file_id": 201,
        "content": "from lazero.network.downloader import download\nurl = \"https://media3.giphy.com/media/wTrXRamYhQzsY/giphy.gif?cid=dda24d502m79hkss38jzsxteewhs4e3ocd3iqext2285a3cq&rid=giphy.gif&ct=g\"\npath = \"/dev/shm/medialang/test.gif\"\nimport os\nif os.path.exists(path):\n    os.remove(path)\nreport = download(url, path)\nprint(\"download success?\", report)",
        "type": "code",
        "location": "/tests/unittest_lazero_external_downloader.py:1-14"
    },
    "2017": {
        "file_id": 201,
        "content": "This code downloads a file from a given URL and saves it to a specified path. It checks if the file already exists at the target location, and if so, deletes it before initiating the download. The success or failure of the download is reported using print statement.",
        "type": "comment"
    },
    "2018": {
        "file_id": 202,
        "content": "/tests/unittest_property_decorator.py",
        "type": "filepath"
    },
    "2019": {
        "file_id": 202,
        "content": "This code defines a class with a property that increments its value each time accessed. It creates an object of the class, stores the property in a list twice, and prints the property's value three times, showing its dynamic nature.",
        "type": "summary"
    },
    "2020": {
        "file_id": 202,
        "content": "# a dynamic property in set\nclass Obj:\n    def __init__(self):\n        self.val = 0\n    @property\n    def prop(self):\n        self.val += 1\n        return self.val\nobj = Obj()\n# mproperty = obj.prop\nmyData = [{\"a\": lambda: obj.prop}] * 2\nfor d in myData:\n    val = d[\"a\"]()\n    print(val)\n# for _ in range(3):\n#     print(obj.prop) # strange.",
        "type": "code",
        "location": "/tests/unittest_property_decorator.py:1-22"
    },
    "2021": {
        "file_id": 202,
        "content": "This code defines a class with a property that increments its value each time accessed. It creates an object of the class, stores the property in a list twice, and prints the property's value three times, showing its dynamic nature.",
        "type": "comment"
    },
    "2022": {
        "file_id": 203,
        "content": "/tests/unittest_musictoolbox_netease_music_lyric.py",
        "type": "filepath"
    },
    "2023": {
        "file_id": 203,
        "content": "This code tests the functionality of getting music and lyrics using NeteaseMusic API from the pyjom library. It checks for similarity in keywords and prints the result, but encounters an issue when no lyrics are found. It then plans to test a specific music ID for lyrics retrieval and shows preparedness in case it fails.",
        "type": "summary"
    },
    "2024": {
        "file_id": 203,
        "content": "from test_commons import *\nfrom pyjom.musictoolbox import neteaseMusic\nNMClient = neteaseMusic()\n# import random\nquery = \"linkin park numb\"\nfor sim in [False, True]:\n    result = NMClient.getMusicAndLyricWithKeywords(query, similar=sim, debug=True)\n    print(\"similar?\", sim)\n    # no lyrics! wtf??\n    breakpoint()\n# now let's test something surely will get lyrics.\n# music_id = 497572729\n# lyric_string = NMClient.getMusicLyricFromNetease(music_id)\n# print(\"LYRIC STRING:\",lyric_string)\n# in case we don't get the lyric, you should be prepared.\n# it works.",
        "type": "code",
        "location": "/tests/unittest_musictoolbox_netease_music_lyric.py:1-17"
    },
    "2025": {
        "file_id": 203,
        "content": "This code tests the functionality of getting music and lyrics using NeteaseMusic API from the pyjom library. It checks for similarity in keywords and prints the result, but encounters an issue when no lyrics are found. It then plans to test a specific music ID for lyrics retrieval and shows preparedness in case it fails.",
        "type": "comment"
    },
    "2026": {
        "file_id": 204,
        "content": "/tests/unittest_nsfw_video_score.py",
        "type": "filepath"
    },
    "2027": {
        "file_id": 204,
        "content": "The code utilizes a trained model to detect NSFW content in videos and images, ensuring compliance by posting non-sexual content through an API. It stores classification probabilities and handles exceptions for unknown test_flags. However, only GIFs can be posted currently with caution about picture stretching.",
        "type": "summary"
    },
    "2028": {
        "file_id": 204,
        "content": "# we take max for the concerned ones, and take mean for the unconcerned ones.\nfrom test_commons import *\nimport requests\nfrom lazero.network.checker import waitForServerUp\nfrom pyjom.videotoolbox import getVideoFrameIteratorWithFPS\nfrom typing import Literal\ngateway = \"http://localhost:8511/\"\nfrom pyjom.mathlib import superMean, superMax\nfrom lazero.utils.importers import cv2_custom_build_init\ncv2_custom_build_init()\nimport cv2\n# suggest you not to use this shit.\n# import math\nfrom pyjom.imagetoolbox import resizeImageWithPadding, scanImageWithWindowSizeAutoResize\nfrom lazero.filesystem import tmpdir, tmpfile\ntmpdirPath = \"/dev/shm/medialang/nsfw\"\nimport uuid\nwaitForServerUp(8511, \"nsfw nodejs server\")\nimport os\ntest_flag = \"nsfw_video\"\n# test_flag = \"nsfw_image\"\n# test_flag = \"scanning\"\n# test_flag = \"paddinging\"\nsource = \"/root/Desktop/works/pyjom/samples/video/cute_cat_gif.gif\"\nimport numpy as np\ndef processNSFWServerImageReply(reply):\n    mDict = {}\n    for elem in reply:\n        className, probability = elem[\"className\"], elem[\"probability\"]",
        "type": "code",
        "location": "/tests/unittest_nsfw_video_score.py:1-44"
    },
    "2029": {
        "file_id": 204,
        "content": "The code imports necessary libraries, initializes certain functions and variables, and defines the processNSFWServerImageReply function which processes image classification reply from the server. It is for testing NSFW detection in videos or images, with options to test different aspects such as scanning, padding, etc. Note that it may not be recommended to use some parts of the code.",
        "type": "comment"
    },
    "2030": {
        "file_id": 204,
        "content": "        mDict.update({className: probability})\n    return mDict\ndef processNSFWReportArray(\n    NSFWReportArray,\n    average_classes=[\"Neutral\"],\n    get_max_classes=[\"Drawing\", \"Porn\", \"Sexy\", \"Hentai\"],\n):\n    assert set(average_classes).intersection(set(get_max_classes)) == set()\n    NSFWReport = {}\n    for element in NSFWReportArray:\n        for key in element.keys():\n            NSFWReport[key] = NSFWReport.get(key, []) + [element[key]]\n    for average_class in average_classes:\n        NSFWReport[average_class] = superMean(NSFWReport.get(average_class, [0]))\n    for get_max_class in get_max_classes:\n        NSFWReport[get_max_class] = superMax(NSFWReport.get(get_max_class, [0]))\n    return NSFWReport\nfrom pyjom.commons import checkMinMaxDict\n# you can reuse this, really.\ndef NSFWFilter(\n    NSFWReport,\n    filter_dict={\n        \"Neutral\": {\"min\": 0.5},\n        \"Sexy\": {\"max\": 0.5},\n        \"Porn\": {\"max\": 0.5},\n        \"Hentai\": {\"max\": 0.5},\n        \"Drawing\": {\"max\": 0.5},\n    },\n    debug=False,\n):\n    for key in filter_dict:",
        "type": "code",
        "location": "/tests/unittest_nsfw_video_score.py:45-80"
    },
    "2031": {
        "file_id": 204,
        "content": "This code processes an NSFW report array and returns a filtered dictionary. It updates the dictionary with class names as keys and their corresponding probabilities. Then, it calculates the average and maximum scores for certain classes. Lastly, it applies filters to the resulting dictionary based on specified minimum or maximum threshold values for each class.",
        "type": "comment"
    },
    "2032": {
        "file_id": 204,
        "content": "        value = NSFWReport.get(key, 0)\n        key_filter = filter_dict[key]\n        result = checkMinMaxDict(value, key_filter)\n        if not result:\n            if debug:\n                print(\"not passing NSFW filter: %s\" % key)\n                print(\"value: %s\" % value)\n                print(\"filter: %s\" % str(key_filter))\n            return False\n    return True\nif test_flag == \"padding\":\n    for frame in getVideoFrameIteratorWithFPS(source, -1, -1, fps=1):\n        image = resizeImageWithPadding(frame, 1280, 720, border_type=\"replicate\")\n        # i'd like to view this.\n        cv2.imshow(\"PADDED\", image)\n        cv2.waitKey(0)\nelif test_flag == \"scanning\":\n    for frame in getVideoFrameIteratorWithFPS(source, -1, -1, fps=1):\n        scanned_array = scanImageWithWindowSizeAutoResize(\n            frame, 1280, 720, threshold=0.3\n        )\n        for index, image in enumerate(scanned_array):\n            cv2.imshow(\"SCANNED %d\" % index, image)\n            cv2.waitKey(0)\nelif test_flag == \"nsfw_video\":\n    # use another source?",
        "type": "code",
        "location": "/tests/unittest_nsfw_video_score.py:81-108"
    },
    "2033": {
        "file_id": 204,
        "content": "The code snippet checks if a video passes the NSFW filter based on certain key values, and then displays the video frames in different scenarios: when testing for padding, it shows each frame with padding; when testing for scanning, it displays each frame after scanning with a specified threshold; and if test_flag is set to \"nsfw_video\", it processes another source.",
        "type": "comment"
    },
    "2034": {
        "file_id": 204,
        "content": "    with tmpdir(path=tmpdirPath) as T:\n        responses = []\n        for frame in getVideoFrameIteratorWithFPS(source, -1, -1, fps=1):\n            padded_resized_frame = resizeImageWithPadding(\n                frame, 224, 224, border_type=\"replicate\"\n            )\n            # i'd like to view this.\n            basename = \"{}.jpg\".format(uuid.uuid4())\n            jpg_path = os.path.join(tmpdirPath, basename)\n            with tmpfile(path=jpg_path) as TF:\n                cv2.imwrite(jpg_path, padded_resized_frame)\n                files = {\"image\": (basename, open(jpg_path, \"rb\"), \"image/jpeg\")}\n                r = requests.post(\n                    gateway + \"nsfw\", files=files\n                )  # post gif? or just jpg?\n                try:\n                    response_json = r.json()\n                    response_json = processNSFWServerImageReply(response_json)\n                    # breakpoint()\n                    # print(\"RESPONSE:\", response_json)\n                    responses.append(\n                        response_json  # it contain 'messages'",
        "type": "code",
        "location": "/tests/unittest_nsfw_video_score.py:109-130"
    },
    "2035": {
        "file_id": 204,
        "content": "This code is looping through video frames, resizing and saving them as JPEGs in a temporary directory. It then posts each image to an API endpoint for NSFW content classification and appends the response JSON to a list of responses. The breakpoint and print statement are optional for debugging purposes.",
        "type": "comment"
    },
    "2036": {
        "file_id": 204,
        "content": "                    )  # there must be at least one response, i suppose?\n                except:\n                    import traceback\n                    traceback.print_exc()\n                    print(\"error when processing NSFW server response\")\n        NSFWReport = processNSFWReportArray(responses)\n        # print(NSFWReport)\n        # breakpoint()\n        result = NSFWFilter(NSFWReport)\n        if result:\n            print(\"NSFW test passed.\")\n            print(\"source %s\" % source)\n# we don't want drawing dogs.\n# [{'className': 'Neutral', 'probability': 0.9995943903923035}, {'className': 'Drawing', 'probability': 0.00019544694805517793}, {'className': 'Porn', 'probability': 0.00013213469355832785}, {'className': 'Sexy', 'probability': 6.839347042841837e-05}, {'className': 'Hentai', 'probability': 9.632151886762585e-06}]\nelif test_flag == \"nsfw_image\":\n    source = \"/root/Desktop/works/pyjom/samples/image/kitty_flash.bmp\"\n    # RESPONSE: [{'className': 'Neutral', 'probability': 0.9997681975364685}",
        "type": "code",
        "location": "/tests/unittest_nsfw_video_score.py:131-149"
    },
    "2037": {
        "file_id": 204,
        "content": "The code processes a server response for NSFW content classification and checks if the test passed. It uses the processNSFWReportArray function to analyze the responses and stores the result in the variable NSFWReport. If there's at least one response, it proceeds with the NSFWFilter function to evaluate the report. If the result is true, it prints \"NSFW test passed\" and source information. The code includes a case for the NSFW_IMAGE test flag and specifies a source file path.",
        "type": "comment"
    },
    "2038": {
        "file_id": 204,
        "content": ", {'className': 'Drawing', 'probability': 0.0002115015813615173}, {'className': 'Porn', 'probability': 1.3146535820851568e-05}, {'className': 'Hentai', 'probability': 4.075543984072283e-06}, {'className': 'Sexy', 'probability': 3.15313491228153e-06}]\n    # source = '/root/Desktop/works/pyjom/samples/image/pig_really.bmp'\n    # RESPONSE: [{'className': 'Neutral', 'probability': 0.9634107351303101}, {'className': 'Porn', 'probability': 0.0244674663990736}, {'className': 'Drawing', 'probability': 0.006115634460002184}, {'className': 'Hentai', 'probability': 0.003590137232095003}, {'className': 'Sexy', 'probability': 0.002416097791865468}]\n    # source = \"/root/Desktop/works/pyjom/samples/image/dog_with_text.bmp\"\n    # source = '/root/Desktop/works/pyjom/samples/image/dick2.jpeg'\n    # [{'className': 'Porn', 'probability': 0.7400921583175659}, {'className': 'Hentai', 'probability': 0.2109236866235733}, {'className': 'Sexy', 'probability': 0.04403943940997124}, {'className': 'Neutral', 'probability': 0.0034419416915625334}, {'className': 'Drawing', 'probability': 0.0015027812914922833}]",
        "type": "code",
        "location": "/tests/unittest_nsfw_video_score.py:149-154"
    },
    "2039": {
        "file_id": 204,
        "content": "This code demonstrates the results of a classification model for detecting different content categories in images. The provided examples show how the model predicts various probabilities for classes like 'Porn', 'Drawing', 'Hentai', and others, given specific image sources.",
        "type": "comment"
    },
    "2040": {
        "file_id": 204,
        "content": "    # source = '/root/Desktop/works/pyjom/samples/image/dick4.jpeg'\n    # RESPONSE: [{'className': 'Porn', 'probability': 0.8319052457809448}, {'className': 'Hentai', 'probability': 0.16578854620456696}, {'className': 'Sexy', 'probability': 0.002254955470561981}, {'className': 'Neutral', 'probability': 3.2827374525368214e-05}, {'className': 'Drawing', 'probability': 1.8473130694474094e-05}]\n    # source = '/root/Desktop/works/pyjom/samples/image/porn_shemale.jpeg'\n    # no good for this one. this is definitely some unacceptable shit, with just cloth wearing.\n    # RESPONSE: [{'className': 'Neutral', 'probability': 0.6256022453308105}, {'className': 'Hentai', 'probability': 0.1276213526725769}, {'className': 'Porn', 'probability': 0.09777139872312546}, {'className': 'Sexy', 'probability': 0.09318379312753677}, {'className': 'Drawing', 'probability': 0.05582122132182121}]\n    # source ='/root/Desktop/works/pyjom/samples/image/dick3.jpeg'\n    # [{'className': 'Porn', 'probability': 0.9784200787",
        "type": "code",
        "location": "/tests/unittest_nsfw_video_score.py:155-161"
    },
    "2041": {
        "file_id": 204,
        "content": "This code is testing the classification accuracy of an image classification model for NSFW content. The comments describe three test cases with different images and the corresponding classifications provided by the model, highlighting the need for improving the model's ability to accurately identify NSFW content.",
        "type": "comment"
    },
    "2042": {
        "file_id": 204,
        "content": "54425}, {'className': 'Hentai', 'probability': 0.01346961222589016}, {'className': 'Sexy', 'probability': 0.006554164923727512}, {'className': 'Neutral', 'probability': 0.0015426197787746787}, {'className': 'Drawing', 'probability': 1.354961841570912e-05}]\n    # a known source causing unwanted shits.\n    image = cv2.imread(source)\n    basename = \"{}.jpg\".format(uuid.uuid4())\n    jpg_path = os.path.join(tmpdirPath, basename)\n    with tmpfile(path=jpg_path) as TF:\n        # black padding will lower the probability of being porn.\n        padded_resized_frame = resizeImageWithPadding(image, 224, 224)\n        # RESPONSE: [{'className': 'Neutral', 'probability': 0.6441782116889954}, {'className': 'Porn', 'probability': 0.3301379978656769}, {'className': 'Sexy', 'probability': 0.010329035110771656}, {'className': 'Hentai', 'probability': 0.010134727694094181}, {'className': 'Drawing', 'probability': 0.005219993181526661}]\n        # padded_resized_frame = resizeImageWithPadding(image, 224, 224,border_type='replicate')",
        "type": "code",
        "location": "/tests/unittest_nsfw_video_score.py:161-170"
    },
    "2043": {
        "file_id": 204,
        "content": "This code reads an image from a known source, generates a unique filename, saves it temporarily, pads and resizes the image for classification, and then passes the processed image to the model for probability prediction. The goal is to lower the probability of being classified as porn by adding black padding around the image before processing.",
        "type": "comment"
    },
    "2044": {
        "file_id": 204,
        "content": "        # RESPONSE: [{'className': 'Neutral', 'probability': 0.6340386867523193}, {'className': 'Porn', 'probability': 0.3443007171154022}, {'className': 'Sexy', 'probability': 0.011606302112340927}, {'className': 'Hentai', 'probability': 0.006618513725697994}, {'className': 'Drawing', 'probability': 0.0034359097480773926}]\n        # neutral again? try porn!\n        cv2.imwrite(jpg_path, padded_resized_frame)\n        files = {\"image\": (basename, open(jpg_path, \"rb\"), \"image/jpeg\")}\n        r = requests.post(gateway + \"nsfw\", files=files)  # post gif? or just jpg?\n        print(\"RESPONSE:\", r.json())\nelse:\n    raise Exception(\"unknown test_flag: %s\" % test_flag)\n# you can only post gif now, or you want to post some other formats?\n# if you post shit, you know it will strentch your picture and produce unwanted shits.",
        "type": "code",
        "location": "/tests/unittest_nsfw_video_score.py:171-180"
    },
    "2045": {
        "file_id": 204,
        "content": "Code snippet is performing the following actions: \n1. Storing response from API containing classification probabilities for video.\n2. Writing frame to JPG format and posting it to gateway as non-sexual content using requests.\n3. If unknown test_flag, raising exception.\n4. Note mentions that only GIF can be posted now and caution about stretching pictures.",
        "type": "comment"
    },
    "2046": {
        "file_id": 205,
        "content": "/tests/unittest_get_subtid_name_and_majortid_name.py",
        "type": "filepath"
    },
    "2047": {
        "file_id": 205,
        "content": "The function `getMajorMinorTopicMappings` retrieves major and minor topic IDs and names, storing them in the `majorMinorMappings` dictionary. The code uses this function to get the associated topics for a given tid, formats them into tags, and prints the result along with the tid for topic ID 1.",
        "type": "summary"
    },
    "2048": {
        "file_id": 205,
        "content": "from bilibili_api import search\nBSP = search.bilibiliSearchParams\ndef getMajorMinorTopicMappings(debug: bool = False):\n    majorMinorMappings = {}\n    for key, value in BSP.all.tids.__dict__.items():\n        try:\n            major_tid = value.tid\n            if debug:\n                print(\"MAJOR\", key, major_tid)\n            content = {\"major\": {\"tid\": major_tid, \"name\": key}}\n            majorMinorMappings.update(\n                {major_tid: content, key: content, str(major_tid): content}\n            )\n            for subkey, subvalue in value.__dict__.items():\n                if subkey != \"tid\" and type(subvalue) == int:\n                    if debug:\n                        print(\"MINOR\", subkey, subvalue)\n                    content = {\n                        \"major\": {\"tid\": major_tid, \"name\": key},\n                        \"minor\": {\"tid\": subvalue, \"name\": subkey},\n                    }\n                    majorMinorMappings.update(\n                        {subvalue: content, subkey: content, str(subvalue): content}",
        "type": "code",
        "location": "/tests/unittest_get_subtid_name_and_majortid_name.py:1-26"
    },
    "2049": {
        "file_id": 205,
        "content": "This function `getMajorMinorTopicMappings` retrieves major and minor topic IDs and names from `BSP.all.tids` dictionary, storing them in `majorMinorMappings` dictionary for further use. It also prints the major and minor topics if debug mode is enabled.",
        "type": "comment"
    },
    "2050": {
        "file_id": 205,
        "content": "                    )\n        except:\n            pass\n    return majorMinorMappings\ndef getTagStringFromTid(tid):\n    majorMinorTopicMappings = getMajorMinorTopicMappings()\n    topic = majorMinorTopicMappings.get(tid, None)\n    tags = []\n    if topic:\n        majorTopic = topic.get(\"major\", {}).get(\"name\", None)\n        minorTopic = topic.get(\"minor\", {}).get(\"name\", None)\n        if majorTopic:\n            tags.append(majorTopic)\n            if minorTopic:\n                tags.append(minorTopic)\n    return \",\".join(tags)\ntid = 1\ntagString = getTagStringFromTid(tid)\nprint(tid, tagString)",
        "type": "code",
        "location": "/tests/unittest_get_subtid_name_and_majortid_name.py:27-49"
    },
    "2051": {
        "file_id": 205,
        "content": "This code retrieves the major and minor topics associated with a given topic ID (tid) using the getMajorMinorTopicMappings() function. It then formats these topics into a comma-separated string of tags. If there are both a major and minor topic, they are concatenated in that order, else if only one exists, it is printed alone. Finally, the tid and associated tagString are printed to the console for the given topic ID 1.",
        "type": "comment"
    },
    "2052": {
        "file_id": 206,
        "content": "/tests/unittest_translate_lyrictoolbox.py",
        "type": "filepath"
    },
    "2053": {
        "file_id": 206,
        "content": "The code imports necessary modules, defines sources as a list of strings, and uses the translate function from pyjom.lyrictoolbox with deepl backend to obtain translations for each source in the list. The results are printed out using sprint.",
        "type": "summary"
    },
    "2054": {
        "file_id": 206,
        "content": "from test_commons import *\nfrom pyjom.lyrictoolbox import translate\nfrom lazero.utils.logger import sprint\nsources = [\"are you ok\", \"are you happy\", \"are you good\", \"are you satisfied\"]\nfor source in sources:\n    result = translate(\n        source, backend=\"deepl\"\n    )  # this is cached. no matter what backend you use.\n    print(\"source:\", source)\n    sprint(\"result:\", result)",
        "type": "code",
        "location": "/tests/unittest_translate_lyrictoolbox.py:1-11"
    },
    "2055": {
        "file_id": 206,
        "content": "The code imports necessary modules, defines sources as a list of strings, and uses the translate function from pyjom.lyrictoolbox with deepl backend to obtain translations for each source in the list. The results are printed out using sprint.",
        "type": "comment"
    },
    "2056": {
        "file_id": 207,
        "content": "/tests/unittest_video_cover_extraction_dog_cat_detections.py",
        "type": "filepath"
    },
    "2057": {
        "file_id": 207,
        "content": "This script initializes a YOLOv5 model for object detection, focusing on dogs and cats, using image processing techniques to enhance accuracy. It may have difficulties categorizing certain objects and displays \"NO COVER FOUND.\" if no suitable cover is detected.",
        "type": "summary"
    },
    "2058": {
        "file_id": 207,
        "content": "import torch\nimport os\nfrom lazero.utils.importers import cv2_custom_build_init\n# order:\n# detect if dog/cat is there, satisfying the qualification\n# remove watermark, remove text, remove potential watermark around corners using inpainting\n# use ffmpeg cropdetect, if has significant area change then no further processing\n# if no significant area change, use this blur detection to get the main area\n# remove watermark again?? around corners?\n# then reuse the dog detection and get the crop from processed/cropped image.\ncv2_custom_build_init()\nimport cv2\nos.environ[\"http_proxy\"] = \"\"\nos.environ[\"https_proxy\"] = \"\"\n# Model\n# localModelDir = (\n#     \"/root/Desktop/works/pyjom/pyjom/models/yolov5/ultralytics_yolov5_master/\"\n# )\n# # import os\n# os.environ[\n#     \"YOLOV5_MODEL_DIR\"\n# ] = \"/root/Desktop/works/pyjom/pyjom/models/yolov5/\"  # this is strange. must be a hack in the localModelDir\n# model = torch.hub.load(\n#     localModelDir, \"yolov5s\", source=\"local\"\n# )  # or yolov5m, yolov5l, yolov5x, custom\nfrom test_commons import *",
        "type": "code",
        "location": "/tests/unittest_video_cover_extraction_dog_cat_detections.py:1-31"
    },
    "2059": {
        "file_id": 207,
        "content": "This code is initializing a YOLOv5 model for object detection, specifically detecting dogs and cats. It also sets environment variables to disable proxies, imports necessary libraries, and defines the model path. The code aims to remove watermarks, text, and potentially blurred corners from images, crop detected animals, and possibly re-detect them to get the crop from the processed image.",
        "type": "comment"
    },
    "2060": {
        "file_id": 207,
        "content": "from pyjom.commons import configYolov5\nmodel = configYolov5()\ndog_or_cat = \"dog\"\n# Images\n# img = '/media/root/help/pyjom/samples/image/miku_on_green.png'  # or file, Path, PIL, OpenCV, numpy, list\n# img = \"/root/Desktop/works/pyjom/samples/image/dog_with_text.jpg\"\nimgPath = \"/root/Desktop/works/pyjom/samples/image/dog_blue_sky.png\"\nimg = cv2.imread(imgPath)\ndefaultHeight, defaultWidth = img.shape[:2]\ntotal_area = defaultHeight * defaultWidth\n# Inference\nresults = model(img)\n# print(results)\n# # Results\n# breakpoint()\nanimal_detection_dataframe = results.pandas().xyxy[0]\n# results.show()\n# # results.print() # or .show(),\narea = (animal_detection_dataframe[\"xmax\"] - animal_detection_dataframe[\"xmin\"]) * (\n    animal_detection_dataframe[\"ymax\"] - animal_detection_dataframe[\"ymin\"]\n)\nanimal_detection_dataframe[\"area_ratio\"] = area / total_area\narea_threshold = 0.08  # min area?\nconfidence_threshold = 0.7  # this is image quality maybe.\ny_expansion_rate = 0.03  # to make the starting point on y axis less \"headless\"",
        "type": "code",
        "location": "/tests/unittest_video_cover_extraction_dog_cat_detections.py:32-67"
    },
    "2061": {
        "file_id": 207,
        "content": "The code imports a YOLOv5 configuration, sets the target animal to \"dog\", and reads an image file. It then performs inference using the model on the image and stores the results in the `results` variable. The code extracts the object detection data from `results`, calculates the area ratio for each detected object, and applies thresholds to filter out objects with low confidence or small areas.",
        "type": "comment"
    },
    "2062": {
        "file_id": 207,
        "content": "df = animal_detection_dataframe\nnew_df = df.loc[\n    (df[\"area_ratio\"] >= area_threshold)\n    & (df[\"confidence\"] >= confidence_threshold)\n    & (df[\"name\"] == dog_or_cat)\n].sort_values(\n    by=[\"confidence\"]\n)  # this one is for 0.13\n# count = new_df.count(axis=0)\ncount = len(new_df)\n# print(\"COUNT: %d\" % count)\ndefaultCropWidth, defaultCropHeight = 1920, 1080\n# this is just to maintain the ratio.\n# you shall find the code elsewhere?\nallowedHeight = min(int(defaultWidth / defaultCropWidth * defaultHeight), defaultHeight)\nif count >= 1:\n    selected_col = new_df.iloc[0]  # it is a dict-like object.\n    # print(new_df)\n    # breakpoint()\n    selected_col_dict = dict(selected_col)\n    # these are floating point shits.\n    # {'xmin': 1149.520263671875, 'ymin': 331.6445007324219, 'xmax': 1752.586181640625, 'ymax': 1082.3826904296875, 'confidence': 0.9185908436775208, 'class': 16, 'name': 'dog', 'area_ratio': 0.13691652620239364}\n    x0, y0, x1, y1 = [\n        int(selected_col[key]) for key in [\"xmin\", \"ymin\", \"xmax\", \"ymax\"]",
        "type": "code",
        "location": "/tests/unittest_video_cover_extraction_dog_cat_detections.py:69-98"
    },
    "2063": {
        "file_id": 207,
        "content": "The code filters the animal detection dataframe based on area ratio, confidence threshold, and dog or cat name. It then sorts the filtered dataframe by confidence. If there is at least one row in the filtered dataframe, it selects the first row as 'selected_col'. The selected column contains values for xmin, ymin, xmax, ymax, confidence, class (dog or cat), and area_ratio. These values are used to extract a region of interest from an image by converting them into coordinates and then cropping the image accordingly.",
        "type": "comment"
    },
    "2064": {
        "file_id": 207,
        "content": "    ]\n    y0_altered = max(int(y0 - (y1 - y0) * y_expansion_rate), 0)\n    height_current = min((y1 - y0_altered), allowedHeight)  # reasonable?\n    width_current = min(\n        int((height_current / defaultCropHeight) * defaultCropWidth), defaultWidth\n    )  # just for safety. not for mathematical accuracy.\n    # height_current = min(allowedHeight, int((width_current/defaultCropWidth)*defaultCropHeight))\n    # (x1+x0)/2-width_current/2\n    import random\n    x0_framework = random.randint(\n        max((x1 - width_current), 0), min((x0 + width_current), defaultWidth)\n    )\n    framework_XYWH = (x0_framework, y0_altered, width_current, height_current)\n    x_f, y_f, w_f, h_f = framework_XYWH\n    croppedImageCover = img[y_f : y_f + h_f, x_f : x_f + w_f, :]\n    # breakpoint()\n    # resize image\n    croppedImageCoverResized = cv2.resize(\n        croppedImageCover, (defaultCropWidth, defaultCropHeight)\n    )\n    cv2.imshow(\"CROPPED IMAGE COVER\", croppedImageCover)\n    cv2.imshow(\"CROPPED IMAGE COVER RESIZED\", croppedImageCoverResized)",
        "type": "code",
        "location": "/tests/unittest_video_cover_extraction_dog_cat_detections.py:99-122"
    },
    "2065": {
        "file_id": 207,
        "content": "This code snippet is responsible for cropping an image and resizing it. It adjusts the cropping parameters to ensure a reasonable aspect ratio while maintaining mathematical safety. The random x0_framework value ensures that the cropped image falls within the allowed width, avoiding any potential out-of-bounds errors. The resulting images are then displayed using OpenCV's imshow function.",
        "type": "comment"
    },
    "2066": {
        "file_id": 207,
        "content": "    # print(selected_col_dict)\n    # print(count)\n    # breakpoint()\n    cv2.waitKey(0)\nelse:\n    print(\"NO COVER FOUND.\")\n# # results.save()\n# # # print(type(results),dir(results))\n# breakpoint()\n# import cv2\n# image = cv2.imread(\"runs/detect/exp3/miku_on_green.jpg\")\n# cv2.imshow(\"NONE\",image)\n# # results.print()  # or .show(),\n# # hold it.\n# # image 1/1: 720x1280 1 bird # what the fuck is a bird?\n# # os.system(\"pause\")\n# # input()\n# this shit has been detected but not in the right category.",
        "type": "code",
        "location": "/tests/unittest_video_cover_extraction_dog_cat_detections.py:123-141"
    },
    "2067": {
        "file_id": 207,
        "content": "The code seems to be a part of an image processing script. It checks for the detection of objects (dog and cat) in images, potentially for cover extraction purposes. The code might have issues with the categorization of certain detected objects. If no suitable cover is found, it displays a \"NO COVER FOUND.\" message.",
        "type": "comment"
    },
    "2068": {
        "file_id": 208,
        "content": "/tests/unittest_tempfile_generator.py",
        "type": "filepath"
    },
    "2069": {
        "file_id": 208,
        "content": "This code generates temporary files named with a \".data\" suffix and yields their names. The generated file paths are then printed, data is written to the files, and the content of these files is read and printed. Finally, it closes all temporary files.",
        "type": "summary"
    },
    "2070": {
        "file_id": 208,
        "content": "import tempfile\ndef generateFile():\n    data = b\"abc\"\n    while True:\n        with tempfile.NamedTemporaryFile(\"wb\", suffix=\".data\") as f:\n            name = f.name\n            print(\"tempfile name:\", name)\n            f.write(data)\n            f.seek(0)  # strange.\n            # what the fuck?\n            # f.close()\n            yield name\nif __name__ == \"__main__\":\n    grt = generateFile()\n    filepath = grt.__next__()\n    for _ in range(2):\n        # good?\n        with open(filepath, \"rb\") as f:\n            content = f.read()\n            print(\"content in {}:\".format(filepath), content)",
        "type": "code",
        "location": "/tests/unittest_tempfile_generator.py:1-24"
    },
    "2071": {
        "file_id": 208,
        "content": "This code generates temporary files named with a \".data\" suffix and yields their names. The generated file paths are then printed, data is written to the files, and the content of these files is read and printed. Finally, it closes all temporary files.",
        "type": "comment"
    },
    "2072": {
        "file_id": 209,
        "content": "/tests/unittest_video_sampler.py",
        "type": "filepath"
    },
    "2073": {
        "file_id": 209,
        "content": "The code imports necessary modules, defines the video path and parameters for sampling frames, and initializes an image set using the getVideoFrameSampler function. It then prints the image set and its type without actually executing it due to the presence of a commented out \"breakpoint()\" call.",
        "type": "summary"
    },
    "2074": {
        "file_id": 209,
        "content": "from test_commons import *\nfrom pyjom.videotoolbox import getVideoFrameSampler\nvideoPath = \"/root/Desktop/works/pyjom/samples/video/LkS8UkiLL.mp4\"\nimageSet = getVideoFrameSampler(videoPath, 0, 5, 60)\n# print(imageSet)\n# print(type(imageSet))\n# # breakpoint()",
        "type": "code",
        "location": "/tests/unittest_video_sampler.py:1-9"
    },
    "2075": {
        "file_id": 209,
        "content": "The code imports necessary modules, defines the video path and parameters for sampling frames, and initializes an image set using the getVideoFrameSampler function. It then prints the image set and its type without actually executing it due to the presence of a commented out \"breakpoint()\" call.",
        "type": "comment"
    },
    "2076": {
        "file_id": 210,
        "content": "/tests/unittest_yolov5_dog_cat_filter_filesystemreviewer.py",
        "type": "filepath"
    },
    "2077": {
        "file_id": 210,
        "content": "This function collects detection data, calculates confidence, applies filters, generates reports, and detects cats and dogs in videos using PaddleResnet50AnimalsClassifier and YOLOv5 model. It iterates through video paths, checks filters, and performs Bezier Curve and Resnet50 detector if needed.",
        "type": "summary"
    },
    "2078": {
        "file_id": 210,
        "content": "from test_commons import *\nfrom pyjom.modules.contentReviewer import filesystemReviewer\nfrom pyjom.commons import keywordDecorator\nfrom lazero.utils.logger import sprint\nfrom pyjom.mathlib import superMean, superMax\ndef extractYolov5DetectionData(detectionData, mimetype=\"video\", debug=False):\n    # plan to get some calculations!\n    filepath, review_data = detectionData[\"review\"][\"review\"]\n    timeseries_data = review_data[\"yolov5_detector\"][\"yolov5\"][\"yolov5_detector\"]\n    data_dict = {}\n    if mimetype == \"video\":\n        dataList = []\n        for frameData in timeseries_data:\n            timestamp, frameNumber, frameDetectionData = [\n                frameData[key] for key in [\"time\", \"frame\", \"yolov5_detector\"]\n            ]\n            if debug:\n                sprint(\"timestamp:\", timestamp)\n            current_shot_detections = []\n            for elem in frameDetectionData:\n                location, confidence, identity = [\n                    elem[key] for key in [\"location\", \"confidence\", \"identity\"]",
        "type": "code",
        "location": "/tests/unittest_yolov5_dog_cat_filter_filesystemreviewer.py:1-26"
    },
    "2079": {
        "file_id": 210,
        "content": "The code defines a function called \"extractYolov5DetectionData\" which takes in detection data and mimetype as parameters. It extracts the filepath, review_data, and timeseries_data from the detection data. If the mimetype is video, it iterates through the frame data, collecting timestamp, frameNumber, and frameDetectionData for further processing. Debug messages are printed if necessary.",
        "type": "comment"
    },
    "2080": {
        "file_id": 210,
        "content": "                ]\n                identity = identity[\"name\"]\n                if debug:\n                    print(\"location:\", location)\n                    print(\"confidence:\", confidence)\n                    sprint(\n                        \"identity:\", identity\n                    )  # we should use the identity name, instead of the identity dict, which is the original identity object.\n                current_shot_detections.append(\n                    {\n                        \"location\": location,\n                        \"confidence\": confidence,\n                        \"identity\": identity,\n                    }\n                )\n            dataList.append(\n                {\"timestamp\": timestamp, \"detections\": current_shot_detections}\n            )\n        data_dict.update({\"data\": dataList})\n    else:\n        frameDetectionData = timeseries_data\n        current_shot_detections = []\n        for elem in frameDetectionData:\n            location, confidence, identity = [\n                elem[key] for key in [\"location\", \"confidence\", \"identity\"]",
        "type": "code",
        "location": "/tests/unittest_yolov5_dog_cat_filter_filesystemreviewer.py:27-51"
    },
    "2081": {
        "file_id": 210,
        "content": "This code appears to be part of a larger program that detects objects in frames, identifies them based on their location and confidence levels, and then stores the data in a list for each timestamp. If there is no existing frame detection data for the current timestamp, it updates with previous timeseries_data. This process repeats for each element in the frameDetectionData list, appending relevant information to current_shot_detections and then adding the full detections data to a final data dictionary under the \"data\" key.",
        "type": "comment"
    },
    "2082": {
        "file_id": 210,
        "content": "            ]\n            identity = identity[\"name\"]\n            if debug:\n                print(\"location:\", location)\n                print(\"confidence:\", confidence)\n                sprint(\"identity:\", identity)\n        data_dict.update(\n            {\"data\": current_shot_detections}\n        )  # just detections, not a list in time series order\n    data_dict.update({\"path\": filepath, \"type\": mimetype})\n    return data_dict\ndef calculateVideoMaxDetectionConfidence(\n    dataList, identities=[\"dog\", \"cat\"]\n):  # does it have a dog?\n    report = {identity: 0 for identity in identities}\n    for elem in dataList:\n        detections = elem[\"detections\"]\n        for detection in detections:\n            identity = detection[\"identity\"]\n            if identity in identities:\n                if report[identity] < detection[\"confidence\"]:\n                    report[identity] = detection[\"confidence\"]\n    return report\nfrom typing import Literal\nimport numpy as np\ndef calculateVideoMeanDetectionConfidence(\n    dataList: list,",
        "type": "code",
        "location": "/tests/unittest_yolov5_dog_cat_filter_filesystemreviewer.py:52-84"
    },
    "2083": {
        "file_id": 210,
        "content": "The code contains a function to calculate the maximum and mean detection confidence for specified identities in a video's data. It defines functions to update a dictionary with detections, identify elements by type and path, and determine the maximum and mean detection confidence for specified identity labels.",
        "type": "comment"
    },
    "2084": {
        "file_id": 210,
        "content": "    identities=[\"dog\", \"cat\"],\n    framewise_strategy: Literal[\"mean\", \"max\"] = \"max\",\n    timespan_strategy: Literal[\"max\", \"mean\", \"mean_no_missing\"] = \"mean_no_missing\",\n):\n    report = {identity: [] for identity in identities}\n    # report = {}\n    for elem in dataList:  # iterate through selected frames\n        # sprint(\"ELEM\")\n        # sprint(elem)\n        # breakpoint()\n        detections = elem[\"detections\"]\n        frame_detection_dict_source = {}\n        # frame_detection_dict = {key:[] for key in identities}\n        for (\n            detection\n        ) in detections:  # in the same frame, iterate through different detections\n            identity = detection[\"identity\"]\n            if identity in identities:\n                frame_detection_dict_source[identity] = frame_detection_dict_source.get(\n                    identity, []\n                ) + [detection[\"confidence\"]]\n        frame_detection_dict = {}\n        for key in identities:\n            valueList = frame_detection_dict_source.get(key, [0])",
        "type": "code",
        "location": "/tests/unittest_yolov5_dog_cat_filter_filesystemreviewer.py:85-108"
    },
    "2085": {
        "file_id": 210,
        "content": "The code iterates through selected frames, collects detections for specified identities (dog and cat), and populates a report with their respective confidence values. It also provides default options for frame-wise and timespan strategies.",
        "type": "comment"
    },
    "2086": {
        "file_id": 210,
        "content": "            if framewise_strategy == \"mean\":\n                frame_detection_dict.update({key: superMean(valueList)})\n            elif framewise_strategy == \"max\":\n                frame_detection_dict.update({key: superMax(valueList)})\n        # now update the report dict.\n        for identity in identities:\n            value = frame_detection_dict.get(identity, 0)\n            if timespan_strategy == \"mean_no_missing\":\n                if value == 0:\n                    continue\n            report[identity].append(value)\n    final_report = {}\n    for identity in identities:\n        valueList = report.get(identity, [0])\n        if timespan_strategy in [\"mean_no_missing\", \"mean\"]:\n            final_report[identity] = superMean(valueList)\n        else:\n            final_report[identity] = superMax(valueList)\n    return final_report\nfrom pyjom.commons import checkMinMaxDict\ndef detectionConfidenceFilter(\n    detectionConfidence: dict,\n    filter_dict={\n        \"dog\": {\"min\": 0.5},\n        \"cat\": {\"min\": 0.5},\n    },  # both have certainty of 0.69 or something. consider to change this value higher?",
        "type": "code",
        "location": "/tests/unittest_yolov5_dog_cat_filter_filesystemreviewer.py:109-138"
    },
    "2087": {
        "file_id": 210,
        "content": "This function calculates the detection confidence for various categories (e.g., dog, cat) and applies filtering based on user-defined thresholds. It uses either mean or maximum strategies for aggregating detection results over time and handles missing values appropriately. The function returns a final report with the filtered results.",
        "type": "comment"
    },
    "2088": {
        "file_id": 210,
        "content": "    logic: Literal[\"AND\", \"OR\"] = \"OR\",\n):  # what is the logic here? and? or?\n    assert logic in [\"AND\", \"OR\"]\n    for identity in filter_dict.keys():\n        value = detectionConfidence.get(identity, 0)\n        key_filter = filter_dict[identity]\n        result = checkMinMaxDict(value, key_filter)\n        if result:\n            if logic == \"OR\":\n                return True\n        else:\n            if logic == \"AND\":\n                return False\n    if logic == \"AND\":\n        return True  # for 'AND' this will be True, but for 'OR' this will be False\n    elif logic == \"OR\":\n        return False\n    else:\n        raise Exception(\"Invalid logic: %s\" % logic)\ndef yolov5VideoDogCatDetector(\n    videoPath,\n    debug=False,\n    filter_dict={\n        \"dog\": {\"min\": 0.5},\n        \"cat\": {\"min\": 0.5},\n    },\n    logic: Literal[\"AND\", \"OR\"] = \"OR\",\n):\n    autoArgs = {\n        \"subtitle_detector\": {\"timestep\": 0.2},\n        \"yolov5_detector\": {\"model\": \"yolov5x\"},  # will this run? no OOM?\n    }  # threshold: 0.4\n    template_names = [\"yolov5_detector.mdl.j2\"]",
        "type": "code",
        "location": "/tests/unittest_yolov5_dog_cat_filter_filesystemreviewer.py:139-175"
    },
    "2089": {
        "file_id": 210,
        "content": "The function checks if the given logic is either 'AND' or 'OR'. It then iterates through a filter dictionary, extracting values and comparing them to a key_filter using the checkMinMaxDict() function. Depending on the logic, it returns True or False based on whether any of the filters pass for 'OR' or all of them pass for 'AND', respectively. The yolov5VideoDogCatDetector function initializes an autoArgs dictionary and template names list to be used in detecting dogs and cats from a videoPath, with an optional logic parameter set to \"OR\" by default.",
        "type": "comment"
    },
    "2090": {
        "file_id": 210,
        "content": "    semiauto = False\n    dummy_auto = False\n    reviewer = keywordDecorator(\n        filesystemReviewer,\n        auto=True,\n        semiauto=semiauto,\n        dummy_auto=dummy_auto,\n        template_names=template_names,\n        args={\"autoArgs\": autoArgs},\n    )\n    # videoPath = \"/root/Desktop/works/pyjom/samples/image/dog_with_text2.png\"\n    # fileList = [{\"type\": \"image\", \"path\": videoPath}]\n    fileList = [{\"type\": \"video\", \"path\": videoPath}]\n    # fileList = [{\"type\": \"video\", \"path\": videoPath} for videoPath in videoPaths]\n    # resultGenerator, function_id = reviewer(\n    #     fileList, generator=True, debug=False\n    # )  # or at least a generator?\n    resultList, function_id = reviewer(\n        fileList, generator=False, debug=False\n    )  # or at least a generator?\n    result = resultList[0]\n    detectionData = extractYolov5DetectionData(result, mimetype=fileList[0][\"type\"])\n    # sprint(\"DETECTION DATA:\")\n    # sprint(detectionData)\n    filepath = detectionData[\"path\"]\n    if debug:\n        sprint(\"FILEPATH: %s\" % filepath)",
        "type": "code",
        "location": "/tests/unittest_yolov5_dog_cat_filter_filesystemreviewer.py:176-207"
    },
    "2091": {
        "file_id": 210,
        "content": "This code initializes a reviewer function using the filesystemReviewer class and sets parameters such as auto, semiauto, dummy_auto, template_names, and args. It then uses this reviewer on a fileList (which could contain image or video paths) to generate resultList and function_id. The first result from resultList is extracted for further processing using extractYolov5DetectionData function, which takes the result and mimetype as parameters. The resulting detectionData is then processed further based on the debug setting.",
        "type": "comment"
    },
    "2092": {
        "file_id": 210,
        "content": "    filetype = detectionData[\"type\"]\n    dataList = detectionData[\"data\"]\n    detectionConfidence = calculateVideoMeanDetectionConfidence(dataList)\n    if debug:\n        sprint(\"DETECTION CONFIDENCE:\", detectionConfidence)\n    filter_result = detectionConfidenceFilter(\n        detectionConfidence, filter_dict=filter_dict, logic=logic\n    )\n    return filter_result\nimport paddlehub as hub\nfrom functools import lru_cache\n@lru_cache(maxsize=1)\ndef getPaddleResnet50AnimalsClassifier():\n    classifier = hub.Module(name=\"resnet50_vd_animals\")\n    return classifier\n@lru_cache(maxsize=3)\ndef labelFileReader(filename):\n    with open(filename, \"r\") as f:\n        content = f.read()\n        content = content.split(\"\\n\")\n        content = [elem.replace(\"\\n\", \"\").strip() for elem in content]\n        content = [elem for elem in content if len(elem) > 0]\n    return content\nfrom pyjom.mathlib import multiParameterExponentialNetwork\n# {'input_bias': 0.0830047243746045, 'skew': -0.4986098769473948}\ndef bezierPaddleHubResnet50VideoDogCatDetector(",
        "type": "code",
        "location": "/tests/unittest_yolov5_dog_cat_filter_filesystemreviewer.py:208-242"
    },
    "2093": {
        "file_id": 210,
        "content": "This function takes detection data and applies a confidence filter based on the video's mean detection confidence. It uses PaddleHub's ResNet50 animals classifier and label file reader to obtain classification results and labels for a video file, respectively. The code also imports multiParameterExponentialNetwork from mathlib and defines a bezierPaddleHubResnet50VideoDogCatDetector function.",
        "type": "comment"
    },
    "2094": {
        "file_id": 210,
        "content": "    videoPath,\n    input_bias=0.0830047243746045,\n    skew=-0.4986098769473948,\n    threshold=0.5,\n    debug=False,\n    logic: Literal[\"AND\", \"OR\"] = \"OR\",\n):\n    filter_dict = {\n        \"dog\": {\"min\": threshold},\n        \"cat\": {\"min\": threshold},\n    }\n    curve_function_kwargs = {\n        \"start\": (0, 0),\n        \"end\": (1, 1),\n        \"skew\": skew,\n    }  # maximize the output.\n    from pyjom.videotoolbox import getVideoFrameIteratorWithFPS\n    from pyjom.imagetoolbox import resizeImageWithPadding\n    dog_suffixs = [\"狗\", \"犬\", \"梗\"]\n    cat_suffixs = [\"猫\"]  # ends with this, and not containing forbidden words.\n    dog_labels = labelFileReader(\n        \"/root/Desktop/works/pyjom/tests/animals_paddlehub_classification_resnet/dogs.txt\"\n    )\n    cat_labels = labelFileReader(\n        \"/root/Desktop/works/pyjom/tests/animals_paddlehub_classification_resnet/cats.txt\"\n    )\n    forbidden_words = [\n        \"灵猫\",\n        \"熊猫\",\n        \"猫狮\",\n        \"猫头鹰\",\n        \"丁丁猫儿\",\n        \"绿猫鸟\",\n        \"猫鼬\",\n        \"猫鱼\",\n        \"玻璃猫\",",
        "type": "code",
        "location": "/tests/unittest_yolov5_dog_cat_filter_filesystemreviewer.py:243-280"
    },
    "2095": {
        "file_id": 210,
        "content": "This code snippet defines a function that filters and processes video frames, detecting both dogs and cats. It takes in the path of the video file, input bias, skew value, threshold for detection, debug mode flag, and logic type (AND or OR). It applies different filters for dog and cat detection based on thresholds, and defines a curve function with given parameters. The code also imports necessary modules and reads label files for dog and cat detection.",
        "type": "comment"
    },
    "2096": {
        "file_id": 210,
        "content": "        \"猫眼\",\n        \"猫蛱蝶\",\n    ]\n    def dog_cat_name_recognizer(name):\n        if name in dog_labels:\n            return \"dog\"\n        elif name in cat_labels:\n            return \"cat\"\n        elif name not in forbidden_words:\n            for dog_suffix in dog_suffixs:\n                if name.endswith(dog_suffix):\n                    return \"dog\"\n            for cat_suffix in cat_suffixs:\n                if name.endswith(cat_suffix):\n                    return \"cat\"\n        return None\n    classifier = getPaddleResnet50AnimalsClassifier()\n    def paddleAnimalDetectionResultToList(result):\n        resultDict = result[0]\n        resultList = [(key, value) for key, value in resultDict.items()]\n        resultList.sort(key=lambda item: -item[1])\n        return resultList\n    def translateResultListToDogCatList(resultList):\n        final_result_list = []\n        for name, confidence in resultList:\n            new_name = dog_cat_name_recognizer(name)\n            final_result_list.append((new_name, confidence))\n        return final_result_list",
        "type": "code",
        "location": "/tests/unittest_yolov5_dog_cat_filter_filesystemreviewer.py:281-312"
    },
    "2097": {
        "file_id": 210,
        "content": "The code defines a function `dog_cat_name_recognizer` that identifies if the given name belongs to a dog or cat. It also initializes a PaddleResnet50AnimalsClassifier, creates functions `paddleAnimalDetectionResultToList`, and `translateResultListToDogCatList` for processing detection results into a sorted list of names with confidence scores and then translates the result to a dog or cat.",
        "type": "comment"
    },
    "2098": {
        "file_id": 210,
        "content": "    dataList = []\n    for frame in getVideoFrameIteratorWithFPS(videoPath, -1, -1, fps=1):\n        padded_resized_frame = resizeImageWithPadding(\n            frame, 224, 224, border_type=\"replicate\"\n        )  # pass the test only if three of these containing 'cats'\n        result = classifier.classification(\n            images=[padded_resized_frame], top_k=3, use_gpu=False\n        )  # check it?\n        resultList = paddleAnimalDetectionResultToList(result)\n        final_result_list = translateResultListToDogCatList(resultList)\n        if debug:\n            sprint(\"RESULT LIST:\", final_result_list)\n        detections = []\n        for index, (label, confidence) in enumerate(final_result_list):\n            scope = final_result_list[index:]\n            scope_confidences = [elem[1] for elem in scope if elem[0] == label]\n            output = multiParameterExponentialNetwork(\n                *scope_confidences,\n                input_bias=input_bias,\n                curve_function_kwargs=curve_function_kwargs\n            )",
        "type": "code",
        "location": "/tests/unittest_yolov5_dog_cat_filter_filesystemreviewer.py:314-334"
    },
    "2099": {
        "file_id": 210,
        "content": "This code extracts frames from a video file, performs object detection using a classifier to identify cats and dogs in each frame, and calculates a score for each label based on the detections. The resulting list of dog and cat detections is then processed by a function called `multiParameterExponentialNetwork`. This code appears to be part of an image classification process for identifying animals in video frames.",
        "type": "comment"
    }
}