{
    "2500": {
        "file_id": 266,
        "content": "# get video metadata first. we may filter unwanted videos by metadata.\n# let's just view here:\n# https://github.com/SocialSisterYi/bilibili-API-collect\n# i found new format of video shortlink:\n# https://b23.tv/BV1zW4y1p7RT\n# https://b23.tv/<bvid>\nvideoLinks = [\n    \"https://www.bilibili.com/video/BV1e54y1y7qy\",  # 女攻男受 emm\n    \"https://www.bilibili.com/video/BV1P441197oV\",  # in which you shall never find anything interesting. no related video.\n    \"https://www.bilibili.com/video/BV1Fs411k7e9\", # multiple chapters, you shall not find this interesting.\n    \"https://www.bilibili.com/video/av5842509\" # aid version of video link.\n]\n# import fake_useragent\n# ua = fake_useragent.UserAgent()\nimport re\nfrom pymaybe import maybe\nimport requests\nfrom urllib.parse import urlencode\ndef extractBVID(chars:str):\n    bvid = maybe(re.findall(r\"/(BV[a-zA-Z0-9]+)\",chars))[0]\n    return bvid\ndef extractAID(chars:str):\n    aid = maybe(re.findall(r\"/av([0-9]+)\",chars))[0]\n    return aid\n## remember the video is always scrapable via av id.",
        "type": "code",
        "location": "/tests/anime_highlight_cuts/theme_collector/anime_compilation_video_metadata.py:1-31"
    },
    "2501": {
        "file_id": 266,
        "content": "This code retrieves video metadata from bilibili, a Chinese video hosting platform. It uses various video link formats and extracts unique identifiers (BVID or AID) to obtain metadata for further filtering unwanted videos. The code also imports relevant libraries and defines two functions to extract BVID and AID from the video links.",
        "type": "comment"
    },
    "2502": {
        "file_id": 266,
        "content": "## av5842509\n# https://api.bilibili.com/x/web-interface/view?aid=<AID>\n# https://api.bilibili.com/x/web-interface/view?bvid=<BVID>\n# videoDownloadPath = \"\"\n# shit!\n# why i need to download whole damn video? i need to cut it into bite-sized video!\n# for some video there's no possibility to determine the source.\n# let's see the video metadata.\n# import os\n# os.system(f'yt-dlp --dump-metadata --output metadata.json \"{videoLinks[0]}\"') # working?\n# bullshit. we shall get the video metadata first.\nurl = \"https://api.bilibili.com/x/web-interface/view\"\ntags_url = \"https://api.bilibili.com/x/tag/archive/tags\"\nrelated_url = \"https://api.bilibili.com/x/web-interface/archive/related\"\nfor videoLink in videoLinks:\n    bvid = extractBVID(videoLink)\n    if bvid:\n        params = {\"bvid\": bvid}\n    else:\n        aid = extractAID(videoLink)\n        if aid:\n            params = {\"aid\": aid}\n        else:\n            print(\"no valid bilibili video id found.\")\n            print(\"skipping video link:\", videoLink)\n            continue",
        "type": "code",
        "location": "/tests/anime_highlight_cuts/theme_collector/anime_compilation_video_metadata.py:32-66"
    },
    "2503": {
        "file_id": 266,
        "content": "The code is trying to collect metadata for Bilibili videos using APIs and may download the video if needed. It first checks if there is a valid BVID or AID, then retrieves the video's metadata and possibly related tags. However, it encounters issues with determining the source and currently has a workaround using yt-dlp to get metadata but notes that this method isn't working properly.",
        "type": "comment"
    },
    "2504": {
        "file_id": 266,
        "content": "    # print(\"PARAMS?\",params)\n    # shit.\n    r = requests.get(f\"{url}?{urlencode(params)}\") # why? what the fuck?\n    r_tags = requests.get(f\"{tags_url}?{urlencode(params)}\")\n    # r = requests.get(url,data=params,headers={\"User-Agent\":ua.random})\n    r_related = requests.get(f'{related_url}?{urlencode(params)}')\n    # r = requests.get(\"https://api.bilibili.com/x/web-interface/view?bvid=BV1e54y1y7qy\")\n    r.raise_for_status()\n    r_tags.raise_for_status()\n    r_related.raise_for_status()\n    # \"need_jump_bv\":false\n    # bvid only?\n    response_json = r.json()\n    response_tags_json = r_tags.json()\n    response_related_json = r_related.json()\n    # it must be json.\n    import rich\n    # rich.print(response_json)\n    assert response_json['code'] == 0\n    assert response_tags_json['code'] == 0\n    assert response_related_json['code'] == 0\n    data = response_json['data']\n    tags_data = response_tags_json['data']\n    related_data = response_related_json['data']\n    ## parsing video stats.\n    title = data['title']",
        "type": "code",
        "location": "/tests/anime_highlight_cuts/theme_collector/anime_compilation_video_metadata.py:68-97"
    },
    "2505": {
        "file_id": 266,
        "content": "This code retrieves video metadata from Bilibili API using Python's `requests` library. It makes three separate requests for the video, tags, and related videos data. The responses are then parsed to extract relevant information such as video title, and the JSON responses are validated to ensure a successful response with status code 0.",
        "type": "comment"
    },
    "2506": {
        "file_id": 266,
        "content": "    pic = data['pic']\n    tid,tname = data['tid'],data['tname']\n    # 27, \"综合\"\n    # 253, \"动漫杂谈\"\n    dynamic = data['dynamic'] # we can copy that.\n    desc = data['desc']\n    owner_mid = data['owner']['mid']\n    state = data['state']\n    assert state == 0 # make sure this video is downloadable.\n    stat =  data['stat']\n    view  = stat['view']\n    reply = stat['reply']\n    danmaku = stat['danmaku']\n    favorite = stat['favorite']\n    coin  = stat['coin']\n    share = stat['share']\n    like  = stat['like']\n    pages = data['pages']\n    page_count = len(pages) # data['videos']\n    for page in pages:\n        page_index = page['page']\n        page_name = page['part']\n        page_dimension = page['dimension']\n        page_width, page_height, page_rotate = page_dimension['width'], page_dimension['height'], page_dimension['rotate']\n        page_duration = page['duration']\n    # subtitle = data['subtitle']\n    # let's just skip.\n    ## parsing tags info.\n    for tag in tags_data:\n        tag_id = tag['tag_id']\n        tag_name = tag['tag_name']",
        "type": "code",
        "location": "/tests/anime_highlight_cuts/theme_collector/anime_compilation_video_metadata.py:98-135"
    },
    "2507": {
        "file_id": 266,
        "content": "This code is fetching data from an API and extracting various information such as video ID, title, thumbnail URLs, description, owner's mid, video statistics (views, replies, danmaku, favorites, coins, shares, likes), and page dimensions. It also skips parsing subtitle info and processes tags data. The code checks if the state is 0 to ensure the video can be downloaded.",
        "type": "comment"
    },
    "2508": {
        "file_id": 266,
        "content": "        tag_used = tag['count']['use']\n        tag_attention = tag['count']['atten']\n        # introduction of tag.\n        tag_content = tag['content']\n        tag_short_content = tag['short_content']\n    ## extract related video info.\n    related_video_counts = len(related_data)\n    for related_video in related_data:\n        related_aid = related_video['aid']\n        related_bvid = related_video['bvid']\n        related_tid = related_video['tid']\n        related_tname = related_video['tname']\n        related_pic = related_video['pic']\n        related_title = related_video['title']\n        related_page_count = related_video['videos'] # make sure this is 1?\n        related_desc = related_video['desc']\n        related_state = related_video['state']\n        if related_state != 0: continue\n        related_duration = related_video['duration']\n        related_owner_mid = related_video['owner']['mid']\n        related_stat = related_video['stat']\n        related_dynamic = related_video['dynamic']\n        # well, we've got non-standard dimensions.",
        "type": "code",
        "location": "/tests/anime_highlight_cuts/theme_collector/anime_compilation_video_metadata.py:136-160"
    },
    "2509": {
        "file_id": 266,
        "content": "This code section is gathering data related to a video, including tag information, count of times the tags were used or caught attention, and details about related videos. It checks if the related state is not 0 before proceeding to gather more information like duration, owner's mid, statistics, etc.",
        "type": "comment"
    },
    "2510": {
        "file_id": 266,
        "content": "        related_dimension = related_video['dimension']\n        # no tag here? you might want more!\n    breakpoint()",
        "type": "code",
        "location": "/tests/anime_highlight_cuts/theme_collector/anime_compilation_video_metadata.py:161-163"
    },
    "2511": {
        "file_id": 266,
        "content": "Checking video dimension and no tag found, further processing or error handling might be required.",
        "type": "comment"
    },
    "2512": {
        "file_id": 267,
        "content": "/tests/anime_highlight_cuts/theme_collector/anilist_to_anidb.py",
        "type": "filepath"
    },
    "2513": {
        "file_id": 267,
        "content": "The code is fetching anime information from Anilist using the AnilistPython API and storing anilist_ids for specific anime. It then prints the anime details, including name_romaji and name_english, and separates them with a horizontal line. The code also mentions a potential step to search for the same anime in anidb but it is not implemented in this snippet.",
        "type": "summary"
    },
    "2514": {
        "file_id": 267,
        "content": "anilist_ids = [\n    112788,  # 海边的异邦人\n    14813,  # Yahari Ore no Seishun Love Come wa Machigatteiru.\n]\n# first let's get name.\nfrom AnilistPython import Anilist\nanilist = Anilist()\nfor anilist_id in anilist_ids:\n    anime = anilist.get_anime_with_id(anilist_id)\n    # what about alias?\n    print(anime)\n    print(\"=\" * 20)\n    romaji = anime.get(\"name_romaji\", None)\n    english = anime.get(\"name_english\", None)\n    # genres = anime.get(\"genres\", []) # not so important. we don't have understanding.\n    # and you will search again.\n    # what is this manual select?\n    # anime2 = anilist.get_anime(romaji) # shit?\n    # print(anime2) # it will just be the same. no shit.\n    # print(\"=\" * 20)\n    # well let's search in anidb. get different names.",
        "type": "code",
        "location": "/tests/anime_highlight_cuts/theme_collector/anilist_to_anidb.py:1-25"
    },
    "2515": {
        "file_id": 267,
        "content": "The code is fetching anime information from Anilist using the AnilistPython API and storing anilist_ids for specific anime. It then prints the anime details, including name_romaji and name_english, and separates them with a horizontal line. The code also mentions a potential step to search for the same anime in anidb but it is not implemented in this snippet.",
        "type": "comment"
    },
    "2516": {
        "file_id": 268,
        "content": "/tests/anime_highlight_cuts/theme_collector/anidb_search_parse.py",
        "type": "filepath"
    },
    "2517": {
        "file_id": 268,
        "content": "This code searches AniDB for anime using a specified query and fake user agent, extracting title and link from the resulting HTML table. It then uses pandas to convert the table data into a DataFrame, retrieves video data as dictionaries, and prints keys of each dictionary.",
        "type": "summary"
    },
    "2518": {
        "file_id": 268,
        "content": "url = \"https://anidb.net/anime/\"\n# query = \"Yahari Ore no Seishun Lovecome wa Machigatte Iru.\"\nquery = \"Yahari Ore no Seishun Love Come wa Machigatteiru.\"  # this will guide you to something different.\nparams = {\"adb.search\": query, \"do.update\": \"Search\", \"noalias\": 1}\nimport pandas\nimport requests\nimport fake_useragent\nua = fake_useragent.UserAgent()\nr = requests.get(\n    url, params=params, headers={\"User-Agent\": ua.random}\n)  # beautiful. really?\nstatus_code = r.status_code\nprint(\"STATUS CODE?\", status_code)\nassert status_code == 200\ntext = r.text\nfrom bs4 import BeautifulSoup\nsoup = BeautifulSoup(text, \"html.parser\")\n# print(soup) # forbidden? wtf?\n# breakpoint()\nimport pandas\n# table = soup.find('table')\ntable = soup.find(\"table\", attrs={\"class\": \"animelist\"})\nif not table:\n    print(\"table not found.\")\n    # you may want to change user agent.\n    breakpoint()\n    # or it is just a page jump. directly to your anime.\nelse:\n    table_str = str(table)\n    # ['No', 'Image', 'Title', 'Award', 'Type', 'Eps', 'Rating', 'Average', 'Reviews', 'User', 'Aired', 'Ended']",
        "type": "code",
        "location": "/tests/anime_highlight_cuts/theme_collector/anidb_search_parse.py:1-38"
    },
    "2519": {
        "file_id": 268,
        "content": "This code is searching for anime on AniDB using the specified query. It makes a GET request with the query and fake user agent to ensure an accurate search result. The code checks if a table containing the search results is found, and if not, it may suggest changing the user agent or the page could be a page jump directly to the anime. The code uses BeautifulSoup to parse the HTML content of the response.",
        "type": "comment"
    },
    "2520": {
        "file_id": 268,
        "content": "    # where is the damn link?\n    for title in table.find_all(\"td\", attrs={\"data-label\": \"Title\"}):\n        title_ref = table.find(\"a\")\n        title_text = title_ref.text\n        title_link = title_ref[\"href\"]\n        print(f\"[{title_link}] - {title_text}\")\n    data = pandas.read_html(table_str)[0]  # must be the first table.\n    # now you have it. sorted?\n    # print(data)\n    # breakpoint()\n    for index, videoDataFrame in data.iterrows():\n        videoData = videoDataFrame.to_dict()\n        print(videoData.keys())\n        # Main Title?\n        breakpoint()\n        # title = videoData['Title']\n        # # where's the damn link? we don't need such thing.\n        # aired, ended = videoData['Aired'], videoData['Ended']\n        # print(f'[{index}] - {title}')",
        "type": "code",
        "location": "/tests/anime_highlight_cuts/theme_collector/anidb_search_parse.py:39-57"
    },
    "2521": {
        "file_id": 268,
        "content": "The code is searching for a specific table on an anime website, extracting the title and link from each row. It then reads the HTML table into a pandas DataFrame, iterates over the rows to obtain the video data as a dictionary, and finally prints the keys of each video's data dictionary.",
        "type": "comment"
    },
    "2522": {
        "file_id": 269,
        "content": "/tests/anime_highlight_cuts/theme_collector/anidb_anime_parse.py",
        "type": "filepath"
    },
    "2523": {
        "file_id": 269,
        "content": "Code imports libraries and functions, uses BeautifulSoup to parse AniDB webpage for specific elements, handles potential null values with 'maybe' function, and reads data into a DataFrame using pandas.",
        "type": "summary"
    },
    "2524": {
        "file_id": 269,
        "content": "# -*- parsing: pep505 -*-\n# import pep505\n# pep505.activate()\n# shit?\nurl = \"https://anidb.net/anime/9310\"\n# from pymonad.maybe import Nothing, Just\n# https://github.com/acaos/python-pep505\nfrom pymaybe import maybe\n# def checkNothing(value):\n#     if value in [None, 0, -1, [], {}, ()]:\n#         return Nothing\n#     return Just(value)\nimport requests\nimport fake_useragent\nua = fake_useragent.UserAgent()\n# r = requests.get(url, headers={\"User-Agent\": ua.random})\n# r.raise_for_status()\n# # assert r.status_code == 200\n# text = r.text\n# with open(\"anidb_info.html\", \"w+\") as f:\n#     f.write(text)\nwith open(\"anidb_info.html\", \"r\") as f:\n    text = f.read()\nfrom bs4 import BeautifulSoup\nsoup = BeautifulSoup(text, \"html.parser\")\n# must be non-empty.\nsimilarAnime = soup.find(attrs={\"id\": \"similaranime\"})\nindirectRelated = soup.find(attrs={\"id\": \"relations_indirect\"})\ndirectRelated = soup.find(attrs={\"id\": \"relations_direct\"})  # it could be none.\ntables = soup.find_all(\"table\")  # shit.\n# null safety?\n# pep 505:\n# https://peps.python.org/pep-0505/",
        "type": "code",
        "location": "/tests/anime_highlight_cuts/theme_collector/anidb_anime_parse.py:1-43"
    },
    "2525": {
        "file_id": 269,
        "content": "The code is importing necessary libraries and functions, parsing a webpage's HTML using BeautifulSoup, and finding specific elements on the page. It appears to be scraping data from AniDB for similar or related anime information. The use of 'maybe' might indicate null safety measures are being implemented, possibly for handling potential None values in the data.",
        "type": "comment"
    },
    "2526": {
        "file_id": 269,
        "content": "# videoInfo = checkNothing(soup.find(\"div\", attrs={\"class\": [\"pane\", \"info\"]})).maybe(\n#     Nothing, lambda x: x.find(\"table\")\n# )\nvideoInfo = maybe(soup.find(\"div\", attrs={\"class\": [\"pane\", \"info\"]})).find(\"table\")\n# if videoInfo:\n# videoInfo = videoInfo.find('table')\n# videoTitles = checkNothing(soup.find(\"div\", attrs={\"class\": [\"pane\", \"titles\"]})).maybe(\n#     Nothing, lambda x: x.find(\"table\")\n# )\nvideoTitles = maybe(soup.find(\"div\", attrs={\"class\": [\"pane\", \"titles\"]})).find(\"table\")\n# if videoTitles:\n# videoTitles = videoTitles.find('table')\n# i think monad is good.\n# import pandas\n# SAData = pandas.read_html(similarAnime)\n# print(SAData)\nbreakpoint()",
        "type": "code",
        "location": "/tests/anime_highlight_cuts/theme_collector/anidb_anime_parse.py:45-67"
    },
    "2527": {
        "file_id": 269,
        "content": "Code snippet is parsing HTML using BeautifulSoup to find specific elements (videoInfo and videoTitles) from a webpage. It uses maybe() function for handling potential missing or null values, and it imports pandas library to potentially read HTML data into a DataFrame.",
        "type": "comment"
    },
    "2528": {
        "file_id": 270,
        "content": "/tests/anime_highlight_cuts/bittorrent_downloader/webtorrent_streaming_test_cut_partial_download.mjs",
        "type": "filepath"
    },
    "2529": {
        "file_id": 270,
        "content": "The code downloads videos using Webtorrent, handles temporary directories and exceptions but has string concatenation issues. It suggests Unix domain sockets for performance improvement, uses FFmpeg to download segments, handles progress/errors and terminates upon torrent completion, though unpipe is unused and readstream may show progress issues.",
        "type": "summary"
    },
    "2530": {
        "file_id": 270,
        "content": "// webtorrent@^1.5.8\n// version mismatch?\n// nope. check how webtorrent-cli works. your code sucks.\n// now: 2.0.1\n// you make countdowns. you use managed temporary directories. you use port within range.\n// you might want a single, unified server instance. in that case you will manage resources within server, which could be error prone?\nvar torrentPath=\"/Users/jamesbrown/Downloads/anime_download/[Kamigami&VCB-Studio] Yahari Ore no Seishun Lovecome wa Machigatte Iru. [Ma10p_1080p].torrent\"\nvar selectedFilePath=\"[Kamigami&VCB-Studio] Yahari Ore no Seishun Lovecome wa Machigatte Iru. [Ma10p_1080p]/SPs/[Kamigami&VCB-Studio] Yahari Ore no Seishun Lovecome wa Machigatte Iru. [CM01][Ma10p_1080p][x265_flac].mkv\" // this is the goddamnly short mkv.\n// var selectedFilePath=\"[Kamigami&VCB-Studio] Yahari Ore no Seishun Lovecome wa Machigatte Iru. [Ma10p_1080p]/[Kamigami&VCB-Studio] Yahari Ore no Seishun Lovecome wa Machigatte Iru. [OVA][Ma10p_1080p][x265_flac].mkv\" // this is long\n// require_esm = require('esm')(module)",
        "type": "code",
        "location": "/tests/anime_highlight_cuts/bittorrent_downloader/webtorrent_streaming_test_cut_partial_download.mjs:2-18"
    },
    "2531": {
        "file_id": 270,
        "content": "This code sets the torrent path and selected file path for a video download using Webtorrent. It uses managed temporary directories and considers using a single server instance, which could potentially manage resources within the server and lead to error-prone situations.",
        "type": "comment"
    },
    "2532": {
        "file_id": 270,
        "content": "// const{WebTorrent} = require_esm('webtorrent').default\n// console.log('IMPORT PATH?',process.env.NODE_PATH)\n// this system sucks. it does not support string concatenation.\n// maybe you can execute command to symlink global node_modules automatically? nope in javascript but in shell script, or it will not run as expected, since the import statements are running before anything would. \nimport ffmpeg from 'fluent-ffmpeg'\nimport fs from 'fs'\n// try {\nfs.rmdirSync('./[Kamigami&VCB-Studio] Yahari Ore no Seishun Lovecome wa Machigatte Iru. [Ma10p_1080p]',{recursive: true})\n// maybe we shall not catch this exception? handle it yourself!\n// }\n// catch(e) { // you can omit the (e)\n//     // console.log(\"GIVEN DIRECTORY DOES NOT EXIST\")\n//     // it will execute even if the directory does not exist.\n//     console.log(\"UNKNOWN ERROR WHILE REMOVING DIRECTORY:\")\n//     console.log(e)\n// }\n// fuck it. let's symlink the NODE_PATH to here.\n// https://github.com/nodejs/node/issues/38687\n// https://nodejs.org/api/esm.html#esm_no_node_path",
        "type": "code",
        "location": "/tests/anime_highlight_cuts/bittorrent_downloader/webtorrent_streaming_test_cut_partial_download.mjs:19-41"
    },
    "2533": {
        "file_id": 270,
        "content": "This code attempts to import `WebTorrent` and `fluent-ffmpeg`, remove a directory, and symlink the NODE_PATH to the current location. It seems to be encountering issues with the system not supporting string concatenation in imports and handling exceptions.",
        "type": "comment"
    },
    "2534": {
        "file_id": 270,
        "content": "// https://nodejs.org/api/esm.html\n// no template string available. shit.\n// import { Readable } from 'stream'\nimport WebTorrent from 'webtorrent'\n// // const WebTorrent = await import('webtorrent')\nconsole.log(\"WEBTORRENT OBJECT?\",WebTorrent)\nconst client=new WebTorrent({dht: true}) // nothing reading out. guess this is fucked.\n// please cache files under some KNOWN directories. otherwise, i will be fucked.\nconst serverPort=8970\nconst instance=client.createServer()\ninstance.server.listen(serverPort) // not random port? not zero? \nconst config={}\n// https://github.com/webtorrent/webtorrent/blob/master/docs/api.md#clientaddtorrentid-opts-function-ontorrent-torrent-\nconfig.path=process.cwd() // download to current directory?\n// pass different temp directory name for different torrents to prevent name clash? but what about the streaming URL?\n// default=`/tmp/webtorrent/`\n// now i fucking got you!\n// add trackers?\n// config.announce=[\"\"]\nclient.add(torrentPath,config,(torrent) => {\n    var selectedFile=torrent.files.find(file => {",
        "type": "code",
        "location": "/tests/anime_highlight_cuts/bittorrent_downloader/webtorrent_streaming_test_cut_partial_download.mjs:42-72"
    },
    "2535": {
        "file_id": 270,
        "content": "Code imports WebTorrent, creates a new client with DHT enabled, starts a server on port 8970, adds a torrent from the specified path using the default configuration, and searches for the desired file in the torrent's files.",
        "type": "comment"
    },
    "2536": {
        "file_id": 270,
        "content": "        // console.log(\"FILENAME?\", file.name)\n        // it will only select the first file matching the criterion.\n        // return file.name.endsWith('.mkv')\n        return file.path==selectedFilePath\n    })\n    // console.log(\"SELECTED FILE?\")\n    // console.log(selectedFile)\n    // exit here?\n    // process.exit()\n    // now pass to fluent-ffmpeg.\n    // https://github.com/leeroybrun/webtorrent-transcode\n    setInterval(() => {console.log(\"SPEED?\",client.downloadSpeed)},2000) // why speed is zero now? wtf? are you finished?\n    // *******************READSTREAM RELATED*******************\n    // https://github.com/webtorrent/webtorrent/issues/2464\n    // const stream = Readable.from(selectedFile) // are you sure?\n    // this sucks. pipe is not seekable. consider something else? (like unix domain socket)\n    // var stream=selectedFile.createReadStream() // not working! fuck.\n    // // // var stream = fs.createReadStream(\"/Users/jamesbrown/Downloads/anime_download/[Sakurato] Onii-chan wa Oshimai! [01][AVC-8bit 1080p AAC][CHT].mp4\")",
        "type": "code",
        "location": "/tests/anime_highlight_cuts/bittorrent_downloader/webtorrent_streaming_test_cut_partial_download.mjs:73-95"
    },
    "2537": {
        "file_id": 270,
        "content": "The code is filtering files based on their name or path to match a predetermined file. It then logs the download speed periodically and attempts to create a readable stream from the selected file. The comments indicate frustration with non-working solutions, suggesting alternative approaches like Unix domain sockets or other methods for better performance.",
        "type": "comment"
    },
    "2538": {
        "file_id": 270,
        "content": "    // stream.unpipe=(nodeStream) => { } //doing nothing?\n    // stream.on('error',function(err) {\n    //     console.log('STREAM ERROR?',err);\n    //     // just ignore it?\n    // })\n    // console.log(\"STREAM?\",stream)\n    // while(true) {\n    //     var buffer=stream.read(200)\n    //     console.log(\"READING:\",buffer)\n    // }\n    // var reading=false\n    // stream.on('readable',function() {\n    //     if(!reading) {\n    //         reading=true\n    //         console.log(\"STREAM READABLE\")\n    //         ffmpeg(stream).ffprobe((err,data) => {\n    //             if(err) {\n    //                 console.log(\"FFPROBE ERROR:\",err)\n    //             } else {\n    //                 console.log(\"FFPROBE METADATA:\",data)\n    //             }\n    //             process.exit()\n    //         })\n    //     }\n    // })\n    // duration is fake.\n    // ffmpeg(stream).ffprobe((err,data) => {\n    //     if(err) {\n    //         console.log(\"FFPROBE ERROR:\",err)\n    //     } else {\n    //         console.log(\"FFPROBE METADATA:\",data)",
        "type": "code",
        "location": "/tests/anime_highlight_cuts/bittorrent_downloader/webtorrent_streaming_test_cut_partial_download.mjs:96-130"
    },
    "2539": {
        "file_id": 270,
        "content": "This code appears to be attempting to read a stream using ffmpeg and retrieve metadata. It handles potential errors, but the unpipe function seems unused, and it might be designed for testing purposes or handling partial downloads. The while loop for continuous reading may not be functional as well.",
        "type": "comment"
    },
    "2540": {
        "file_id": 270,
        "content": "    //     }\n    //     // process.exit()\n    // })\n    // ffmpeg(stream).seekInput('0:10').duration(\"0:15\").on('progress',function(progress) {\n    //     // why not showing progress?\n    //     console.log('FFmpeg Processing: '+progress.percent+'% done');\n    // }).on('end',() => {\n    //     console.log(\"FFMPEG EXECUTION COMPLETE?\")\n    //     // let's rerun.\n    //     // instance.close()\n    //     client.destroy()\n    //     process.exit()\n    //     // the time range simply does not exist.\n    // }).outputOptions(['-c copy','-y']).output('output.mkv').run() // still not working?\n    // *******************READSTREAM RELATED*******************\n    // how about let's use url?\n    // how to urlencode?\n    // var urlSuffix = encodeURIComponent(selectedFilePath)\n    var fileRequestUrl=`http://localhost:${serverPort}`+selectedFile.streamURL\n    // console.log(\"STREAMING URL?\",fileRequestUrl)\n    // http://localhost:8970/webtorrent/421d78cadb5e1bb4fc1fec9dc2d6680e810c13c2/%5BKamigami&VCB-Studio%5D%20Yahari%",
        "type": "code",
        "location": "/tests/anime_highlight_cuts/bittorrent_downloader/webtorrent_streaming_test_cut_partial_download.mjs:131-158"
    },
    "2541": {
        "file_id": 270,
        "content": "This code snippet attempts to download a video file, process it using FFmpeg and stream it over Webtorrent. It encodes the streaming URL for the video and logs progress during the FFmpeg processing. The code may have issues with the FFmpeg processing not showing progress and potential problems in the readstream implementation.",
        "type": "comment"
    },
    "2542": {
        "file_id": 270,
        "content": "20Ore%20no%20Seishun%20Lovecome%20wa%20Machigatte%20Iru.%20%5BMa10p_1080p%5D/SPs/%5BKamigami&VCB-Studio%5D%20Yahari%20Ore%20no%20Seishun%20Lovecome%20wa%20Machigatte%20Iru.%20%5BCM01%5D%5BMa10p_1080p%5D%5Bx265_flac%5D.mkv\n    //shit?\n    // ffmpeg(fileRequestUrl).ffprobe((err,data) => {\n    //     if(err) {\n    //         console.log(\"FFPROBE ERROR:\",err)\n    //     } else {\n    //         console.log(\"FFPROBE METADATA:\",data)\n    //         var duration=data.format.duration\n    //         console.log(\"VIDEO DURATION?\",duration)\n    //         // you'd better read this. you fuck!\n    //         // i ask for 10 secs.\n    //         // output still contains metadata. but do we have subtitles?\n    //         // seeking is not so accurate but in minutes? easy.\n    //         // for file under 1 minute, please do not seek ok? (seek locally?)\n    //         // do not seek for segments that are too short. seek larger segments!\n    ffmpeg(fileRequestUrl).seekInput('0:10').duration(\"0:15\").on('progress',function(progress) {",
        "type": "code",
        "location": "/tests/anime_highlight_cuts/bittorrent_downloader/webtorrent_streaming_test_cut_partial_download.mjs:158-176"
    },
    "2543": {
        "file_id": 270,
        "content": "This code uses FFmpeg to seek and download a video segment from the given URL. It seeks to 10 seconds, sets the duration to 15 seconds, and handles progress updates.",
        "type": "comment"
    },
    "2544": {
        "file_id": 270,
        "content": "        console.log('FFmpeg Processing: '+progress.percent+'% done');\n    }).on('end',() => {\n        console.log(\"FFMPEG EXECUTION COMPLETE?\")\n        // let's rerun.\n        instance.close()\n        client.destroy()\n        process.exit()\n        // the time range simply does not exist.\n    }).outputOptions(['-c copy',\n        '-y']).output('output.mkv').run()\n    // not top-level function or async function. fuck.\n})",
        "type": "code",
        "location": "/tests/anime_highlight_cuts/bittorrent_downloader/webtorrent_streaming_test_cut_partial_download.mjs:177-188"
    },
    "2545": {
        "file_id": 270,
        "content": "The code is closing the FFmpeg instance and destroying the client after a torrent download completes. It also logs progress updates during the download process and ends the program execution upon completion.",
        "type": "comment"
    },
    "2546": {
        "file_id": 271,
        "content": "/tests/anime_highlight_cuts/bittorrent_downloader/torrent_analyzer.py",
        "type": "filepath"
    },
    "2547": {
        "file_id": 271,
        "content": "The code snippet imports libraries, defines paths and analyzes a torrent file using torrent_parser. It prints the data, checks for multiple files, stores their names and lengths (if applicable), formats size, prints file details, writes filenames to JSON, and prepares the file for further processing.",
        "type": "summary"
    },
    "2548": {
        "file_id": 271,
        "content": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\nimport os\n# single file.\n# torrent_path = \"[桜都字幕組] 不當哥哥了！ _ Onii-chan wa Oshimai! [01][1080p][繁體內嵌].torrent\"\ntorrent_path = \"[Kamigami&VCB-Studio] Yahari Ore no Seishun Lovecome wa Machigatte Iru. [Ma10p_1080p].torrent\"\nbasepath = \"/Users/jamesbrown/Downloads/anime_download\"\ntorrent_path = os.path.join(basepath, torrent_path)\n# analyze this torrent file.\nimport torrent_parser as tp\ndata = tp.parse_torrent_file(torrent_path)\nimport rich\nrich.print(data)\n# will be complete name later?\nsingle_file = not('files' in data['info'].keys())\n# data['info']['name'] \n# length will be total length?\n# data['info']['length']\n# breakpoint()\n# does it preserve the order?\n# import humanize\n# well.\nfnames=[]\nimport json\nfrom humanfriendly import format_size\nif not single_file:\n    for index, fileInfo in enumerate(data['info']['files']):\n        aria2c_index = index+1\n        length = fileInfo['length']\n        path = fileInfo['path'] # multiple strings in a list\n        joined_path = \"/\".join(path)",
        "type": "code",
        "location": "/tests/anime_highlight_cuts/bittorrent_downloader/torrent_analyzer.py:1-41"
    },
    "2549": {
        "file_id": 271,
        "content": "Code snippet imports necessary libraries, defines a torrent file path and basepath for downloads, joins the paths, analyzes the torrent file using torrent_parser, prints the parsed data, checks if the torrent contains multiple files or not, stores the name and length of each file (if applicable), converts file paths to single string format, and finally prepares the file for further processing.",
        "type": "comment"
    },
    "2550": {
        "file_id": 271,
        "content": "        filesize_human_readable = format_size(length)\n        print(f\"[{aria2c_index}] ** [{filesize_human_readable}] ** {path[-1]}\")\n        # the index is right.\n        fnames.append(path[-1])\n        print(f\"FULLPATH: {joined_path}\")\nwith open(\"test_filenames.json\",'w+') as f:\n    f.write(json.dumps(fnames))",
        "type": "code",
        "location": "/tests/anime_highlight_cuts/bittorrent_downloader/torrent_analyzer.py:42-49"
    },
    "2551": {
        "file_id": 271,
        "content": "This code snippet formats the file size in human-readable format and prints it along with the aria2c index, file name, and full path. It then stores the filenames in a list and writes them to a JSON file named \"test_filenames.json\".",
        "type": "comment"
    },
    "2552": {
        "file_id": 272,
        "content": "/tests/anime_highlight_cuts/bittorrent_downloader/subtitle_extractor.py",
        "type": "filepath"
    },
    "2553": {
        "file_id": 272,
        "content": "This code references three different tools for extracting subtitles from a video file: mkvextract, ffmpeg, and optical character recognition (OCR). It suggests using the appropriate tool based on the source being processed, with the assumption that it's easier to extract subtitles from fixed locations in Bangumi videos.",
        "type": "summary"
    },
    "2554": {
        "file_id": 272,
        "content": "# use mkvextract:\n# https://github.com/jorti/extract-subs/blob/master/extract-subs.py\n# use ffmpeg:\n# https://github.com/fdenivac/ffextract-subtitles/blob/master/ffextract-subtitles.py\n# use ocr to extract subtitles. since this is bangumi, it is easy to extract from fixed location.",
        "type": "code",
        "location": "/tests/anime_highlight_cuts/bittorrent_downloader/subtitle_extractor.py:1-7"
    },
    "2555": {
        "file_id": 272,
        "content": "This code references three different tools for extracting subtitles from a video file: mkvextract, ffmpeg, and optical character recognition (OCR). It suggests using the appropriate tool based on the source being processed, with the assumption that it's easier to extract subtitles from fixed locations in Bangumi videos.",
        "type": "comment"
    },
    "2556": {
        "file_id": 273,
        "content": "/tests/anime_highlight_cuts/bittorrent_downloader/nyaa_torrent_file_list.py",
        "type": "filepath"
    },
    "2557": {
        "file_id": 273,
        "content": "This code fetches data from the Nyaa.si website using requests library, parses it using NyaaPy's utils and torrent modules, and prints the parsed JSON data and the corresponding data class object.",
        "type": "summary"
    },
    "2558": {
        "file_id": 273,
        "content": "url = \"https://nyaa.si/view/1627038\"\nimport requests\nfrom NyaaPy import utils, torrent\nr = requests.get(url)\nSITE = utils.TorrentSite.NYAASI\njson_data = utils.parse_single(request_text=r.text, site=SITE)\ndata_class = torrent.json_to_class(json_data)\nimport rich\n# json_data['seeders']\n# json_data['title']\n# json_data['files']\nrich.print(json_data)\nprint()\nprint(\"_\"*20)\nprint()\nrich.print(data_class)\nbreakpoint()",
        "type": "code",
        "location": "/tests/anime_highlight_cuts/bittorrent_downloader/nyaa_torrent_file_list.py:1-25"
    },
    "2559": {
        "file_id": 273,
        "content": "This code fetches data from the Nyaa.si website using requests library, parses it using NyaaPy's utils and torrent modules, and prints the parsed JSON data and the corresponding data class object.",
        "type": "comment"
    },
    "2560": {
        "file_id": 274,
        "content": "/tests/anime_highlight_cuts/bittorrent_downloader/nyaa_api_connector.py",
        "type": "filepath"
    },
    "2561": {
        "file_id": 274,
        "content": "The Python script uses requests and BeautifulSoup to search the Nyaa torrent site for anime with 7+ seeders, retrieves results, stores in \"output.html\", and checks if more pages exist using a template and NyaaPy library for torrent handling.",
        "type": "summary"
    },
    "2562": {
        "file_id": 274,
        "content": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\nimport requests\nurl = \"https://nyaa.si\" # change this to mirror sites.\nMIN_SEEDERS=7 # must be greater than this.\nquery = \"oniichan wa oshimai! 01\"\nsort_term = \"seeders\"\nanime_categories = {\n    \"Anime\": \"1_0\",\n    \"Anime - Anime Music Video\": \"1_1\",\n    \"Anime - English-translated\": \"1_2\",\n    \"Anime - Non-English-translated\": \"1_3\",\n    \"Anime - Raw\": \"1_4\",\n}\ncategory_code = anime_categories[\"Anime\"]  # anime\npage = 1  # start page: 1\nend_of_page = False\n# better not to use rss version since it will not sort terms.\nparams = dict(f=0, c=category_code, q=query, s=sort_term, o=\"desc\", p=page)\n# better parse it yourself first huh?\n# r = requests.get(url, params=params)\n# assert r.code == 200\n# text = r.text\nwith open(\"output.html\", \"r\") as f:\n    text = f.read()\nfrom bs4 import BeautifulSoup\n# with open(\"output.html\",'w+') as f:\n#    f.write(text)\nsoup = BeautifulSoup(text, \"html.parser\")\n# breakpoint()\nimport parse\ntemplate = \"Displaying results {start:d}-{end:d} out of {total:d} results.\"",
        "type": "code",
        "location": "/tests/anime_highlight_cuts/bittorrent_downloader/nyaa_api_connector.py:1-48"
    },
    "2563": {
        "file_id": 274,
        "content": "The code is a Python script that uses the requests library to make an API request to the Nyaa torrent site. It searches for a specific anime with 7 or more seeders, retrieves the results, and stores them in a file named \"output.html\". The BeautifulSoup library is used to parse the HTML response, and the parse module seems to be utilized for further processing.",
        "type": "comment"
    },
    "2564": {
        "file_id": 274,
        "content": "banner = soup.find(\"div\", class_=\"pagination-page-info\").text\npagination_info = banner.split(\"\\n\")[0]\npagination_info_result = parse.parse(template, pagination_info)\nif pagination_info_result:\n    if pagination_info_result[\"total\"] == pagination_info_result[\"end\"]:\n        print(\"Reached end of page.\")\n        end_of_page = True\nfrom NyaaPy import utils\nSITE = utils.TorrentSite.NYAASI\njson_info = utils.parse_nyaa(request_text=text, limit=None, site=SITE)\nimport rich\nrich.print(json_info)\n# breakpoint()\nfor videoInfo in json_info:\n    seeders = int(videoInfo['seeders'])\n    seeders_enough = seeders>=MIN_SEEDERS\n    print('seeders?',seeders)\n    print(\"seeders enough?\", seeders_enough)\n    # videoInfo['id'] -> \"https://nyaa.si/view/{}\"\n# you can also download torrent file for only file info.",
        "type": "code",
        "location": "/tests/anime_highlight_cuts/bittorrent_downloader/nyaa_api_connector.py:50-77"
    },
    "2565": {
        "file_id": 274,
        "content": "The code retrieves the banner from a webpage, extracts pagination information, and checks if it has reached the end of the page. It then parses the response using a template and determines if there are enough seeders for each video info. The code prints the number of seeders and whether they are enough based on a minimum seeders threshold. The code uses the NyaaPy library for site-specific torrent handling.",
        "type": "comment"
    },
    "2566": {
        "file_id": 275,
        "content": "/tests/anime_highlight_cuts/bittorrent_downloader/name_resolution_parsing_chapter_recognition.py",
        "type": "filepath"
    },
    "2567": {
        "file_id": 275,
        "content": "The code utilizes ffmpeg to extract subtitles, sets anime series constants, filters series names, and reads filenames from a JSON. It checks for bangume names, identifies episode index location, compares with expected position, and prints the episode index or displays \"EPISODE?\" if not recognized.",
        "type": "summary"
    },
    "2568": {
        "file_id": 275,
        "content": "subtitle_types = [\"ass\", \"srt\"]\nvideo_types = [\n    \"mkv\",\n    \"mov\",\n    \"mp4\",\n    \"flv\",\n    \"avi\",\n    \"ogv\",\n    \"webm\",\n    \"ts\",\n    \"wmv\",\n    \"webm\",\n    \"m4v\",\n    \"3gp\",\n]\n# use ffmpeg for subtitle extraction?\nfiletypes = {\"subtitle\": subtitle_types, \"video\": video_types}\nBangumi_Name = \"Yahari Ore no Seishun Lovecome wa Machigatte Iru.\".strip()\nepisodeIndex = 3\nchinese_simplified_sub_types = [\"chs\", \"简体\", \"简日\"]\nchinese_traditional_sub_types = [\"繁日\", \"繁体\", \"繁體\", \"cht\"]\nimport json\n# replace non-alphanumeric charcters.\nepisode_formatter = lambda episode_index: str(episode_index).zfill(2)\nimport re\n# also replace all double spaces.\ndef double_space_replacer(chars: str):\n    if \"  \" in chars:\n        chars = chars.replace(\"  \", \" \")\n        return double_space_replacer(chars)\n    else:\n        return chars\nalphanumeric_filter = lambda chars: double_space_replacer(\n    re.sub(r\"[^a-z0-9]\", \" \", chars)\n)\nbangume_name_lower_alphanumeric = alphanumeric_filter(Bangumi_Name.lower())\nwith open(\"test_filenames.json\", \"r\") as f:",
        "type": "code",
        "location": "/tests/anime_highlight_cuts/bittorrent_downloader/name_resolution_parsing_chapter_recognition.py:1-46"
    },
    "2569": {
        "file_id": 275,
        "content": "This code defines file types for subtitles and videos, uses ffmpeg for subtitle extraction, defines constants related to a specific anime series, applies an alphanumeric filter to the anime name, and reads filenames from a JSON file.",
        "type": "comment"
    },
    "2570": {
        "file_id": 275,
        "content": "    fnames = json.loads(f.read())\nfor fname in fnames:\n    fname_lower = fname.lower()\n    fname_lower_alphanumeric = alphanumeric_filter(fname_lower)\n    file_extension = fname_lower.split(\".\")[-1]\n    current_file_type = \"unknown\"\n    for filetype, file_extensions in filetypes.items():\n        if file_extension in file_extensions:\n            current_file_type = filetype\n            break\n    print(f\"<{current_file_type}> {fname}\")\n    print(fname_lower_alphanumeric)\n    substring_location_start = fname_lower_alphanumeric.find(\n        bangume_name_lower_alphanumeric\n    )\n    if substring_location_start!=-1:\n        substring_location_end = substring_location_start + len(\n        bangume_name_lower_alphanumeric\n    )\n        assert fname_lower_alphanumeric[substring_location_start: substring_location_end] == bangume_name_lower_alphanumeric\n        episodeIndexLocation = fname_lower_alphanumeric.find(f\" {episode_formatter(episodeIndex)} \")\n        if episodeIndexLocation!=-1:\n            if episodeIndexLocation+1>=substring_location_end:",
        "type": "code",
        "location": "/tests/anime_highlight_cuts/bittorrent_downloader/name_resolution_parsing_chapter_recognition.py:47-71"
    },
    "2571": {
        "file_id": 275,
        "content": "Reading file names from a JSON, filtering, and determining their types. Checking if the bangume name substring is present in the filename. Identifying the episode index location and comparing it with the expected position.",
        "type": "comment"
    },
    "2572": {
        "file_id": 275,
        "content": "                print(\"EPISODE?\") # this is the index we want\n                print(episodeIndex)",
        "type": "code",
        "location": "/tests/anime_highlight_cuts/bittorrent_downloader/name_resolution_parsing_chapter_recognition.py:72-73"
    },
    "2573": {
        "file_id": 275,
        "content": "Code snippet checks the episode index and prints it. If the desired index is not recognized, it displays \"EPISODE?\" for clarification.",
        "type": "comment"
    },
    "2574": {
        "file_id": 276,
        "content": "/tests/anime_highlight_cuts/bittorrent_downloader/name_fuzzy.py",
        "type": "filepath"
    },
    "2575": {
        "file_id": 276,
        "content": "This code initializes two empty lists, 'filenames' and 'bangumi_names'. 'bangumi_names' contains two string values representing anime titles.",
        "type": "summary"
    },
    "2576": {
        "file_id": 276,
        "content": "filenames = []\nbangumi_names= [\"Yahari Ore no Seishun Lovecome wa Machigatte Iru.\",\"Yahari Ore no Seishun Love Come wa Machigatteiru.\"]",
        "type": "code",
        "location": "/tests/anime_highlight_cuts/bittorrent_downloader/name_fuzzy.py:1-3"
    },
    "2577": {
        "file_id": 276,
        "content": "This code initializes two empty lists, 'filenames' and 'bangumi_names'. 'bangumi_names' contains two string values representing anime titles.",
        "type": "comment"
    },
    "2578": {
        "file_id": 277,
        "content": "/tests/anime_highlight_cuts/bittorrent_downloader/make_node_symlink.sh",
        "type": "filepath"
    },
    "2579": {
        "file_id": 277,
        "content": "This script creates a symbolic link named \"node_modules\" pointing to the $NODE_PATH, presumably to resolve or fix an issue with file locations.",
        "type": "summary"
    },
    "2580": {
        "file_id": 277,
        "content": "ln -s $NODE_PATH node_modules # to fix shit.",
        "type": "code",
        "location": "/tests/anime_highlight_cuts/bittorrent_downloader/make_node_symlink.sh:1-1"
    },
    "2581": {
        "file_id": 277,
        "content": "This script creates a symbolic link named \"node_modules\" pointing to the $NODE_PATH, presumably to resolve or fix an issue with file locations.",
        "type": "comment"
    },
    "2582": {
        "file_id": 278,
        "content": "/tests/anime_highlight_cuts/bittorrent_downloader/kill_aria2c.sh",
        "type": "filepath"
    },
    "2583": {
        "file_id": 278,
        "content": "This command is killing the aria2c process with a SIGINT signal, specifically targeting the specified anime episode titled 'Yahari Ore no Seishun Lovecome wa Machigatte Iru. [Ma10p_1080p]' from the Kamigami & VCB-Studio group.",
        "type": "summary"
    },
    "2584": {
        "file_id": 278,
        "content": "ps aux | grep '[Kamigami&VCB-Studio] Yahari Ore no Seishun Lovecome wa Machigatte Iru. [Ma10p_1080p]' | grep -v grep | awk '{print $1}' | xargs -Iabc kill -s INT abc",
        "type": "code",
        "location": "/tests/anime_highlight_cuts/bittorrent_downloader/kill_aria2c.sh:1-1"
    },
    "2585": {
        "file_id": 278,
        "content": "This command is killing the aria2c process with a SIGINT signal, specifically targeting the specified anime episode titled 'Yahari Ore no Seishun Lovecome wa Machigatte Iru. [Ma10p_1080p]' from the Kamigami & VCB-Studio group.",
        "type": "comment"
    },
    "2586": {
        "file_id": 279,
        "content": "/tests/anime_highlight_cuts/bittorrent_downloader/dynamic_import.mjs",
        "type": "filepath"
    },
    "2587": {
        "file_id": 279,
        "content": "Code imports and initializes two modules, FfmpegCommand and WebTorrent, using dynamic import. The code checks the data types of the imported functions, with FfmpegCommand being a function and WebTorrent appearing as a class in the console despite its data type being \"function\".",
        "type": "summary"
    },
    "2588": {
        "file_id": 279,
        "content": "const FfmpegCommand = (await import(`${process.env.NODE_PATH}/fluent-ffmpeg/index.js`)).default \nconst WebTorrent = (await import(`${process.env.NODE_PATH}/webtorrent/index.js`)).default \n// promise!\n// shit this ESM can directly use await statements.\nconsole.log(FfmpegCommand)\nconsole.log(typeof(FfmpegCommand)) // \"function\", with default name.\nconsole.log(WebTorrent)\nconsole.log(typeof(WebTorrent)) // \"function\"? why i see \"class\" in console.log?\n// this syntax is not recommended. autocompletion will not work.",
        "type": "code",
        "location": "/tests/anime_highlight_cuts/bittorrent_downloader/dynamic_import.mjs:1-12"
    },
    "2589": {
        "file_id": 279,
        "content": "Code imports and initializes two modules, FfmpegCommand and WebTorrent, using dynamic import. The code checks the data types of the imported functions, with FfmpegCommand being a function and WebTorrent appearing as a class in the console despite its data type being \"function\".",
        "type": "comment"
    },
    "2590": {
        "file_id": 280,
        "content": "/tests/anime_highlight_cuts/bittorrent_downloader/download_given_file_to_given_name.sh",
        "type": "filepath"
    },
    "2591": {
        "file_id": 280,
        "content": "This script downloads a torrent file using aria2c and removes temporary files once finished. The script sets the base path, torrent name, and file ID for the download. It also includes two different command variations for stopping the download after completion with timeout options. The commands use kill and grep to end the aria2c process with a signal and remove temporary files.",
        "type": "summary"
    },
    "2592": {
        "file_id": 280,
        "content": "# how to end downloading when finished?\n# using some command?\nBASE_PATH=\"/Users/jamesbrown/Downloads/anime_download\"\n# DOWNLOAD_FILE_PATH=\"$BASE_PATH/sample.webp\"\nTORRENT_NAME=\"[Kamigami&VCB-Studio] Yahari Ore no Seishun Lovecome wa Machigatte Iru. [Ma10p_1080p]\"\n# torrent name might be different.\nTORRENT_PATH=\"$BASE_PATH/$TORRENT_NAME.torrent\"\n# echo \"ps aux | grep '$TORRENT_NAME' | grep -v grep | awk '{print \\$1}' | xargs -Iabc kill -s INT abc\" > kill_aria2c.sh\nFILE_ID=\"117\"\n# timeout set to what?\n# rm \"$DOWNLOAD_FILE_PATH\"\nrm -rf \"$TORRENT_NAME\"\nrm -rf \"$TORRENT_NAME.aria2\"\n# this will be ignored.\n# change directory to our temp directory.\n# this speed shall be precalculated.\n# \n# you may check integrity.\n# just count seeders.\n# aria2c -x 16 --select-file=\"$FILE_ID\" --seed-time=0 --file-allocation=none \"$TORRENT_PATH\"\n# aria2c -x 16 --select-file=\"$FILE_ID\" --seed-time=0 --file-allocation=none --lowest-speed-limit=300K --bt-stop-timeout=60 \"$TORRENT_PATH\"",
        "type": "code",
        "location": "/tests/anime_highlight_cuts/bittorrent_downloader/download_given_file_to_given_name.sh:1-27"
    },
    "2593": {
        "file_id": 280,
        "content": "This script downloads a torrent file using aria2c and removes temporary files once finished. The script sets the base path, torrent name, and file ID for the download. It also includes two different command variations for stopping the download after completion with timeout options. The commands use kill and grep to end the aria2c process with a signal and remove temporary files.",
        "type": "comment"
    },
    "2594": {
        "file_id": 281,
        "content": "/tests/conversation_talk_apis/api_tests.py",
        "type": "filepath"
    },
    "2595": {
        "file_id": 281,
        "content": "This code imports modules, disables proxies, and uses requests library to send POST requests to Weibo API's direct messaging endpoint. It creates and sends messages, retrieves responses in JSON format, interacts with Weibo and OwnThink APIs, checks user messages against responses, performs API tests using checkApi function for different chatbot instances.",
        "type": "summary"
    },
    "2596": {
        "file_id": 281,
        "content": "import urllib.parse\nimport requests\n# disable all proxies.\nimport os\nimport time\nos.environ[\"http_proxy\"]=\"\"\nos.environ[\"https_proxy\"]=\"\"\n# do not use freaking proxy, otherwise QingYunKe will not respond.\ndef checkApi(func,message,name):\n    response_message = func(message)\n    if response_message!=None:\n        print(\"{} RESPONSE:\".format(name), response_message)\ndef chatAtri(msg: str, BASE='http://api.nekomimi.icu/v1/'):\n    url = BASE + 'chat?msg=%s' % msg\n    response = requests.get(url)\n    if response.status_code == 200:\n        data = response.json()\n        if data['status'] == 'success':\n            return data['message']\n    # return None\n    # nothing is returned if have error.\n    print(\"ATRI ERROR:\", response.status_code, response.json())\n# import subprocess\n# import json\ndef chatQingKeYun(msg: str, url=\"http://api.qingyunke.com/api.php?key=free&appid=0&msg=\"):\n    msg = urllib.parse.quote(msg)\n    myUrl = url+msg\n    # print(myUrl)\n    # output = subprocess.check_output([\"curl\", myUrl])\n    # data = json.loads(output.decode(\"utf-8\"))",
        "type": "code",
        "location": "/tests/conversation_talk_apis/api_tests.py:1-37"
    },
    "2597": {
        "file_id": 281,
        "content": "Code imports necessary modules, disables proxies, defines a function to check API responses, and includes two chat functions: 'chatAtri' for chatting with Atri using a Chinese language processing API, and 'chatQingKeYun' for chatting with QingYunKe using a free API key. The code also has a commented section that appears to be testing the use of subprocess and json modules but is not implemented yet.",
        "type": "comment"
    },
    "2598": {
        "file_id": 281,
        "content": "    # import requests\n    data = requests.get(myUrl)\n    data = data.json()\n    print(data)\n    result = data['result']\n    assert result == 0  # 202 -> busy\n    content = data['content']\n    return content\n    # breakpoint()\ndef xiaobing(msg):\n    # 其实是新浪微博群发器 微博群发的逻辑类似于b站群发\n    # 刚关注的只能发一条消息\n    uid = '5175429989'\n    source = '209678993'\n    SUB = '_2A25PyitTDeRhGeBG7VAS8y_MwjmIHXVsvhubrDV8PUNbmtANLRfTkW9NRhxXNiVv6Qwut5wwnc8rys3cbJFAxVdX'\n    url_send = 'https://api.weibo.com/webim/2/direct_messages/new.json'\n    data = {\n        'text': msg,\n        'uid': uid,\n        'source': source\n    }\n    headers = {\n        'cookie': 'SUB='+SUB,\n        'Content-Type': 'application/x-www-form-urlencoded',\n        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.130 Safari/537.36',\n        'Referer': 'https://api.weibo.com/chat/'\n    }\n    response = requests.post(url_send, data=data, headers=headers).json()\n    sendMsg = response['text']\n    time.sleep(1)\n    while True:",
        "type": "code",
        "location": "/tests/conversation_talk_apis/api_tests.py:38-69"
    },
    "2599": {
        "file_id": 281,
        "content": "This code is using the requests library to send a POST request to the Weibo API's direct messaging endpoint. It creates a new message with the provided text, sends it to a specified user (uid), and retrieves the response from the API. The script includes necessary headers and uses JSON format for the data payload.",
        "type": "comment"
    }
}