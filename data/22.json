{
    "2200": {
        "file_id": 223,
        "content": "chatgpt-like bots are powerful.\nwe will run one using cpu, since we don't always have a powerful gpu\nwe can also use moderation api & openai api for testing\nfirst we will give the bot a simple task to construct/extract url from video description.",
        "type": "code",
        "location": "/tests/chatgpt_multiagent_agent_product_line_multimodal_langchain_experiments/README.md:1-7"
    },
    "2201": {
        "file_id": 223,
        "content": "The code mentions the power of chatgpt-like bots and plans to run one using CPU instead of a powerful GPU. It also suggests utilizing moderation API and openAI API for testing purposes. The bot will start with a simple task involving constructing/extracting URLs from video descriptions.",
        "type": "comment"
    },
    "2202": {
        "file_id": 224,
        "content": "/tests/blur_image_detection_mask/test_blur_detection.py",
        "type": "filepath"
    },
    "2203": {
        "file_id": 224,
        "content": "This code imports necessary modules, reads an image, applies blur detection algorithm using blur_detector.detectBlur(img), displays the original image and blur map using cv2.imshow(), and waits for user input before closing the windows with cv2.waitKey(0).",
        "type": "summary"
    },
    "2204": {
        "file_id": 224,
        "content": "from lazero.utils.importers import cv2_custom_build_init\ncv2_custom_build_init()\nimport blur_detector\nimport cv2\nimagePath = \"/root/Desktop/works/pyjom/samples/image/dog_blue_sky_split_line.png\"\nif __name__ == '__main__':\n    img = cv2.imread(imagePath,0)\n    blur_map = blur_detector.detectBlur(img, downsampling_factor=4, num_scales=4, scale_start=2, num_iterations_RF_filter=3)\n    cv2.imshow('ori_img', img)\n    cv2.imshow('blur_map', blur_map)\n    cv2.waitKey(0)",
        "type": "code",
        "location": "/tests/blur_image_detection_mask/test_blur_detection.py:1-12"
    },
    "2205": {
        "file_id": 224,
        "content": "This code imports necessary modules, reads an image, applies blur detection algorithm using blur_detector.detectBlur(img), displays the original image and blur map using cv2.imshow(), and waits for user input before closing the windows with cv2.waitKey(0).",
        "type": "comment"
    },
    "2206": {
        "file_id": 225,
        "content": "/tests/blur_image_detection_mask/BlurDetection_install/test.py",
        "type": "filepath"
    },
    "2207": {
        "file_id": 225,
        "content": "The code detects and removes watermarks, crops images, performs inpainting, adjusts text area ratios, and displays images. It identifies contours and draws bounding boxes for detection.",
        "type": "summary"
    },
    "2208": {
        "file_id": 225,
        "content": "# order:\n# detect if dog/cat is there, satisfying the qualification\n# remove watermark, remove text, remove potential watermark around corners using inpainting\n# use ffmpeg cropdetect, if has significant area change then no further processing\n# if no significant area change, use this blur detection to get the main area\n# remove watermark again?? around corners?\n# then reuse the dog detection and get the crop from processed/cropped image.\nimport os\n# from cv2 import waitKey\nfrom lazero.utils.importers import cv2_custom_build_init\ncv2_custom_build_init()\nimport cv2\nimport numpy\n# import logger\nimport BlurDetection\n# img_path = raw_input(\"Please Enter Image Path: \")\n# img_path = \"/root/Desktop/works/pyjom/samples/image/dog_blue_sky_split_line.png\"\n# img_path = \"/root/Desktop/works/pyjom/samples/image/blur_sample.webp\"\nimg_path = \"/root/Desktop/works/pyjom/samples/image/blur_sample_2.webp\"\n# img_path = \"/root/Desktop/works/pyjom/samples/image/dog_with_black_borders.png\"\n# ffmpeg -loop 1 -i /root/Desktop/works/pyjom/samples/image/dog_with_black_borders.png -t 15 -vf cropdetect -f null -",
        "type": "code",
        "location": "/tests/blur_image_detection_mask/BlurDetection_install/test.py:1-28"
    },
    "2209": {
        "file_id": 225,
        "content": "The code imports necessary libraries, initializes OpenCV, sets the image path, and starts by detecting if a dog or cat is present in the image. It then proceeds to remove watermarks, potentially using inpainting for corners, and checks for significant area changes with ffmpeg cropdetect. If no change, it uses blur detection for the main area. Finally, it may remove watermarks around corners again and reuses dog detection to get the final cropped image.",
        "type": "comment"
    },
    "2210": {
        "file_id": 225,
        "content": "# img_path=\"/root/Desktop/works/pyjom/samples/image/husky_cry.png\"\nassert os.path.exists(img_path), \"img_path does not exists\"\nimg = cv2.imread(img_path)\nimport sys\nsys.path.append(\"/root/Desktop/works/pyjom/\")\nfrom pyjom.imagetoolbox import imageFourCornersInpainting, getImageTextAreaRatio\nimg = imageFourCornersInpainting(img)\nimg = getImageTextAreaRatio(img, inpaint=True, edgeDetection=True)\nimg_fft, val, blurry = BlurDetection.blur_detector(img)\nprint(\"this image {0} blurry\".format([\"isn't\", \"is\"][blurry]))\nmsk, result, blurry = BlurDetection.blur_mask(img, max_thresh=120)\ninv_msk = 255 - msk\n# import numpy as np\n# print(np.max(msk), np.min(msk))\n# print(msk.shape)\n# breakpoint()\ndef display(title, img, max_size=200000):\n    assert isinstance(img, numpy.ndarray), \"img must be a numpy array\"\n    assert isinstance(title, str), \"title must be a string\"\n    scale = numpy.sqrt(min(1.0, float(max_size) / (img.shape[0] * img.shape[1])))\n    print(\"image is being scaled by a factor of {0}\".format(scale))\n    shape = (int(scale * img.shape[1]), int(scale * img.shape[0]))",
        "type": "code",
        "location": "/tests/blur_image_detection_mask/BlurDetection_install/test.py:29-57"
    },
    "2211": {
        "file_id": 225,
        "content": "This code performs blur detection and inpainting on an image. It first checks if the image path exists, reads the image using OpenCV, appends the necessary directory to the system path, applies four-corners inpainting and text area ratio adjustment, determines the blurriness of the image, and then uses the BlurDetection class for blur detection and mask generation. Finally, it displays the image with optional scaling and prints the maximum and minimum values of the mask.",
        "type": "comment"
    },
    "2212": {
        "file_id": 225,
        "content": "    img = cv2.resize(img, shape)\n    cv2.imshow(title, img)\n# BlurDetection.scripts.display('img', img)\ndisplay(\"img\", img)\n# display(\"msk\", msk)\ndisplay(\"inv_msk\", inv_msk)\n# Generate contours based on our mask\n# This function allows us to create a descending sorted list of contour areas.\n# def contour_area(contours):\n#     # create an empty list\n#     cnt_area = []\n#     # loop through all the contours\n#     for i in range(0, len(contours), 1):\n#         # for each contour, use OpenCV to calculate the area of the contour\n#         cnt_area.append(cv2.contourArea(contours[i]))\n#     # Sort our list of contour areas in descending order\n#     list.sort(cnt_area, reverse=True)\n#     return cnt_area\ndef draw_bounding_box_with_contour(\n    contours, image, area_threshold=20, debug=False\n):  # are you sure?\n    # this is the top-k approach.\n    # Call our function to get the list of contour areas\n    # cnt_area = contour_area(contours)\n    # Loop through each contour of our image\n    x0, y0, x1, y1 = [None] * 4\n    for i in range(0, len(contours), 1):",
        "type": "code",
        "location": "/tests/blur_image_detection_mask/BlurDetection_install/test.py:58-93"
    },
    "2213": {
        "file_id": 225,
        "content": "Resizes image, displays it with cv2.imshow, and calls the display function for other images. Defines a contour_area function to calculate and sort contour areas in descending order. Draws bounding boxes around the largest contours with the draw_bounding_box_with_contour function.",
        "type": "comment"
    },
    "2214": {
        "file_id": 225,
        "content": "        cnt = contours[i]\n        # Only draw the the largest number of boxes\n        if cv2.contourArea(cnt) > area_threshold:\n            # if (cv2.contourArea(cnt) > cnt_area[number_of_boxes]):\n            # Use OpenCV boundingRect function to get the details of the contour\n            x, y, w, h = cv2.boundingRect(cnt)\n            if x0 == None:\n                x0, y0, x1, y1 = x, y, x + w, y + h\n            if x < x0:\n                x0 = x\n            if y < y0:\n                y0 = y\n            if x + w > x1:\n                x1 = x + w\n            if y + h > y1:\n                y1 = y + h\n            # Draw the bounding box\n    if x0 is not None:\n        if debug:\n            image = cv2.rectangle(image, (x0, y0), (x1, y1), (0, 0, 255), 2)\n            cv2.imshow(\"with_bounding_box\", image)\n            cv2.waitKey(0)\n    if x0 is None:\n        height, width = image.shape[:2]\n        x0, y0, x1, y1 = 0, 0, width, height\n    return (x0, y0), (x1, y1)\n# BlurDetection.scripts.display('msk', msk)\ncontours, hierarchy = cv2.findContours(inv_msk, 1, 2)",
        "type": "code",
        "location": "/tests/blur_image_detection_mask/BlurDetection_install/test.py:94-126"
    },
    "2215": {
        "file_id": 225,
        "content": "This code finds contours in an image, selects the largest one based on area threshold, and calculates the bounding box coordinates. It then draws a rectangle around the detected contour (if debug is enabled) and returns the bounding box coordinates. The code also initializes the bounding box parameters if they are None.",
        "type": "comment"
    },
    "2216": {
        "file_id": 225,
        "content": "rectangle_boundingbox = draw_bounding_box_with_contour(contours, img, debug=True)\n# cv2.waitKey(0)",
        "type": "code",
        "location": "/tests/blur_image_detection_mask/BlurDetection_install/test.py:127-128"
    },
    "2217": {
        "file_id": 225,
        "content": "The code snippet detects contours and draws a bounding box around them using the function draw_bounding_box_with_contour. It also displays an image window with cv2.waitKey(0) but it is commented out, so it's not currently being executed.",
        "type": "comment"
    },
    "2218": {
        "file_id": 226,
        "content": "/tests/black_autopep8_ast_parser_formatter_skipexception/test.py",
        "type": "filepath"
    },
    "2219": {
        "file_id": 226,
        "content": "This code appears to be a mix of unrelated or incomplete fragments. It imports various modules but lacks cohesive structure, making it difficult to determine its purpose or functionality.",
        "type": "summary"
    },
    "2220": {
        "file_id": 226,
        "content": "# may be illegal.\n# use autopep8?\n# first, autopep8, next, black\n# both with 'unlimited' line of code.\n# finally, throw it to our dearly 'skipException'\n    from lib2to3.pgen2.pgen import DFAState\nfrom mimetypes import suffix_map\nfrom os import SCHED_FIFO\nfrom socket import _SendableFile\nfrom xml.dom.pulldom    import \\\n    SAX2DOM\n    print('aaa'\n        ) # there is no repairing on this bracket for autopep8\n# about the dog_or_cat recognition of our cover:\n# 1. throw away unqualified ones (using pop?)\n# 2. lower the threshold of yolo\n# 3. downscale picture before passing to yolo\n# we can go wild here.\n@redisLRUCache(dfsji,\nasdif[dfk,DFAState,\nsdfkg])\ndef shit(aaa, bbb,\nccc,ddd):\n    dd = 2314\n    ee = suffix_map[SAX2DOM,\n    df23, ddd][sdf,\n    sdf,sdf]\n    ss = efldife.dfief(_SendableFile,\n    saif,SCHED_FIFO,\n    asdif[fjisd,\n    sfdsif,sdf])",
        "type": "code",
        "location": "/tests/black_autopep8_ast_parser_formatter_skipexception/test.py:1-36"
    },
    "2221": {
        "file_id": 226,
        "content": "This code appears to be a mix of unrelated or incomplete fragments. It imports various modules but lacks cohesive structure, making it difficult to determine its purpose or functionality.",
        "type": "comment"
    },
    "2222": {
        "file_id": 227,
        "content": "/tests/black_autopep8_ast_parser_formatter_skipexception/format_test.sh",
        "type": "filepath"
    },
    "2223": {
        "file_id": 227,
        "content": "This code is formatting Python files, using autopep8 to enforce maximum line length and Black to ensure consistent formatting. It prevents changes on the original content by redirecting output to a view-only mode.",
        "type": "summary"
    },
    "2224": {
        "file_id": 227,
        "content": "# view only. no change on original content.\n# of course, for lines with long content, we will have trouble.\nMAXINT=1000000000\ncat test.py | autopep8 --max-line-length $MAXINT - | black -l $MAXINT -C - 2>/dev/null",
        "type": "code",
        "location": "/tests/black_autopep8_ast_parser_formatter_skipexception/format_test.sh:1-4"
    },
    "2225": {
        "file_id": 227,
        "content": "This code is formatting Python files, using autopep8 to enforce maximum line length and Black to ensure consistent formatting. It prevents changes on the original content by redirecting output to a view-only mode.",
        "type": "comment"
    },
    "2226": {
        "file_id": 228,
        "content": "/tests/black_autopep8_ast_parser_formatter_skipexception/format_functional.py",
        "type": "filepath"
    },
    "2227": {
        "file_id": 228,
        "content": "This code reads a Python file, encodes it, and then runs it through \"autopep8\" and \"Black\" formatting tools to ensure code follows PEP 8 style guide and is aesthetically pleasing. It also handles exceptions and prints the formatted code for further use.",
        "type": "summary"
    },
    "2228": {
        "file_id": 228,
        "content": "with open(\"test.py\", \"r\") as f:\n    code = f.read()\n# need binary data.\ncode_encoded = code.encode(\"utf-8\")\nimport subprocess\nMAXINT = 10000000000\ncommand = \"autopep8 --max-line-length {MAXINT} - | black -l {MAXINT} -C -\".format(\n    MAXINT=MAXINT\n)\ncommandLine = [\"bash\", \"-c\", command]\nresult = subprocess.run(commandLine, input=code_encoded, capture_output=True)\ntry:\n    assert result.returncode == 0\n    code_formatted = result.stdout.decode(\"utf-8\")\nexcept:\n    import traceback\n    traceback.print_exc()\n    print(\"STDOUT\", result.stdout)\n    print(\"STDERR\", result.stderr)\n    code_formatted = code\nprint(code_formatted)",
        "type": "code",
        "location": "/tests/black_autopep8_ast_parser_formatter_skipexception/format_functional.py:1-26"
    },
    "2229": {
        "file_id": 228,
        "content": "This code reads a Python file, encodes it, and then runs it through \"autopep8\" and \"Black\" formatting tools to ensure code follows PEP 8 style guide and is aesthetically pleasing. It also handles exceptions and prints the formatted code for further use.",
        "type": "comment"
    },
    "2230": {
        "file_id": 229,
        "content": "/tests/youtube_shorts_heuristic_search/README.md",
        "type": "filepath"
    },
    "2231": {
        "file_id": 229,
        "content": "Code contains information about search and downloadability of YouTube Shorts, as well as mentioning the limitations of current video feed acquisition from various platforms. It also provides a link to a Python library for YouTube search and details on a simple bilibili search API.",
        "type": "summary"
    },
    "2232": {
        "file_id": 229,
        "content": "turned out youtube shorts are searchable, downloadable.\nbut the video feed is not yet acquired, just like all other video feeds from youtube, twitch, reddit, qq小世界, bilibili, 抖音, tiktok and the trending ones.\nthe youtube advanced filter is embedded in the search results. you can only jump to one embedded link at a time\nto get the next page on youtube:\nthe key seems to be the unified unlimited api key for youtube.\nPOST https://www.youtube.com/youtubei/v1/search?key=AIzaSyAO_FJ2SlqU8Q4STEHLGCilw_Y9_11qcW8&prettyPrint=false with a lot of headaching parameters.\nyou can bypass them. here are some basic parameters without lots of combinations. if you want to combine them, better figure it out yourself.\nsome of them might already fail to work.\nhttps://github.com/alexmercerind/youtube-search-python/blob/fc12c05747f1f7bd89d71699403762b86b523da5/youtubesearchpython/core/constants.py#L45\nbilibili search api is currently simple:\nhttps://search.bilibili.com/video?keyword=%E6%B1%AA%E6%B1%AA&from_source=webtop_search&spm_id_from=333.1007&search_source=3&tids=219&order=dm&duration=2",
        "type": "code",
        "location": "/tests/youtube_shorts_heuristic_search/README.md:1-21"
    },
    "2233": {
        "file_id": 229,
        "content": "Code contains information about search and downloadability of YouTube Shorts, as well as mentioning the limitations of current video feed acquisition from various platforms. It also provides a link to a Python library for YouTube search and details on a simple bilibili search API.",
        "type": "comment"
    },
    "2234": {
        "file_id": 230,
        "content": "/tests/youtube_shorts_heuristic_search/heuristic_model.py",
        "type": "filepath"
    },
    "2235": {
        "file_id": 230,
        "content": "The code defines a function called \"getSearchQueryFromHeuristicSpace\" which takes a seed as input and returns a search query. This model aims to reduce repetition and increase the likelihood of finding relevant videos by utilizing heuristics. The application is intended to be server-based.",
        "type": "summary"
    },
    "2236": {
        "file_id": 230,
        "content": "seed = 'dog cute'\ndef getSearchQueryFromHeuristicSpace(seed):\n    # less likely to repeat, and more possibility to get needed videos.\n    return searchQuery\n# this heuristic search model shall be a server based application.",
        "type": "code",
        "location": "/tests/youtube_shorts_heuristic_search/heuristic_model.py:2-8"
    },
    "2237": {
        "file_id": 230,
        "content": "The code defines a function called \"getSearchQueryFromHeuristicSpace\" which takes a seed as input and returns a search query. This model aims to reduce repetition and increase the likelihood of finding relevant videos by utilizing heuristics. The application is intended to be server-based.",
        "type": "comment"
    },
    "2238": {
        "file_id": 231,
        "content": "/tests/voice_detect_extract_split/spleeter/test2.sh",
        "type": "filepath"
    },
    "2239": {
        "file_id": 231,
        "content": "This code downloads an audio example, separates it into two components using Spleeter's 2-stems model, and saves the results in separate files. However, there seems to be an issue with the second separation process.",
        "type": "summary"
    },
    "2240": {
        "file_id": 231,
        "content": "# wget https://github.com/deezer/spleeter/raw/master/audio_example.mp3\n# separate the example audio into two components\npython3 -m spleeter separate -p spleeter:2stems -o output you_got_me.mp3\npython3 -m spleeter separate -p spleeter:2stems -o output tarot_desc.mp3\n# seems not working at all",
        "type": "code",
        "location": "/tests/voice_detect_extract_split/spleeter/test2.sh:1-5"
    },
    "2241": {
        "file_id": 231,
        "content": "This code downloads an audio example, separates it into two components using Spleeter's 2-stems model, and saves the results in separate files. However, there seems to be an issue with the second separation process.",
        "type": "comment"
    },
    "2242": {
        "file_id": 232,
        "content": "/tests/voice_detect_extract_split/spleeter/test.sh",
        "type": "filepath"
    },
    "2243": {
        "file_id": 232,
        "content": "This code retrieves an example audio file, separates it into two components using the Spleeter library, and saves the result in the \"output\" directory. However, it seems to be facing some issues with separation.",
        "type": "summary"
    },
    "2244": {
        "file_id": 232,
        "content": "# wget https://github.com/deezer/spleeter/raw/master/audio_example.mp3\n# separate the example audio into two components\npython3 -m spleeter separate -p spleeter:2stems -o output audio_example.mp3\n# seems not working at all",
        "type": "code",
        "location": "/tests/voice_detect_extract_split/spleeter/test.sh:1-4"
    },
    "2245": {
        "file_id": 232,
        "content": "This code retrieves an example audio file, separates it into two components using the Spleeter library, and saves the result in the \"output\" directory. However, it seems to be facing some issues with separation.",
        "type": "comment"
    },
    "2246": {
        "file_id": 233,
        "content": "/tests/voice_detect_extract_split/spleeter/spleeter_init.sh",
        "type": "filepath"
    },
    "2247": {
        "file_id": 233,
        "content": "Installs spleeter version 2.1.0, downloads spleeter-2.2.2-py3-none-any.whl and installs it, and imports pretrained_models/4stems.",
        "type": "summary"
    },
    "2248": {
        "file_id": 233,
        "content": "# pip3n install spleeter==2.1.0\nwget https://files.pythonhosted.org/packages/fb/2e/5d2cd3d0179d3f749d03eddf0172f1dbababbc371c1b5cbd7fc27d741070/spleeter-2.2.2-py3-none-any.whl\npip3n install spleeter-2.2.2-py3-none-any.whl # why you require specific tensorflow version?\n# https://github.com/deezer/spleeter/releases/download/v1.4.0/4stems.tar.gz\n# at pretrained_models/4stems",
        "type": "code",
        "location": "/tests/voice_detect_extract_split/spleeter/spleeter_init.sh:1-6"
    },
    "2249": {
        "file_id": 233,
        "content": "Installs spleeter version 2.1.0, downloads spleeter-2.2.2-py3-none-any.whl and installs it, and imports pretrained_models/4stems.",
        "type": "comment"
    },
    "2250": {
        "file_id": 234,
        "content": "/tests/voice_detect_extract_split/spleeter/README.md",
        "type": "filepath"
    },
    "2251": {
        "file_id": 234,
        "content": "This code describes using Spleeter, an open-sourced tool, and mentions two model hosts, Hugging Face and Wolfram Neural Network Library. These libraries provide paraphrasing models and can be utilized with the Wolfram Developer Engine respectively.",
        "type": "summary"
    },
    "2252": {
        "file_id": 234,
        "content": "use spleeter which is open-sourced.\nmany model hoster interests me. the most gigantic one is huggingface. providing paraphrasing models and more. another one is wolfram neural network library. can be used freely with wolfram developer engine.",
        "type": "code",
        "location": "/tests/voice_detect_extract_split/spleeter/README.md:1-3"
    },
    "2253": {
        "file_id": 234,
        "content": "This code describes using Spleeter, an open-sourced tool, and mentions two model hosts, Hugging Face and Wolfram Neural Network Library. These libraries provide paraphrasing models and can be utilized with the Wolfram Developer Engine respectively.",
        "type": "comment"
    },
    "2254": {
        "file_id": 235,
        "content": "/tests/voice_detect_extract_split/spleeter/download_models.sh",
        "type": "filepath"
    },
    "2255": {
        "file_id": 235,
        "content": "The code downloads pretrained model files for spleeter, a sound separation tool. It uses curl to retrieve the tarballs from GitHub releases and stores them in \"pretrained_models\" directory. After downloading, it moves the files and changes the directory to execute further tasks related to these models.",
        "type": "summary"
    },
    "2256": {
        "file_id": 235,
        "content": "curl -O -L https://github.com/deezer/spleeter/releases/download/v1.4.0/2stems.tar.gz\ncurl -O -L https://github.com/deezer/spleeter/releases/download/v1.4.0/4stems.tar.gz\ncurl -O -L https://github.com/deezer/spleeter/releases/download/v1.4.0/5stems.tar.gz\nmv {2stems.tar.gz, 4stems.tar.gz, 5stems.tar.gz} pretrained_models\ncd pretrained_models",
        "type": "code",
        "location": "/tests/voice_detect_extract_split/spleeter/download_models.sh:1-6"
    },
    "2257": {
        "file_id": 235,
        "content": "The code downloads pretrained model files for spleeter, a sound separation tool. It uses curl to retrieve the tarballs from GitHub releases and stores them in \"pretrained_models\" directory. After downloading, it moves the files and changes the directory to execute further tasks related to these models.",
        "type": "comment"
    },
    "2258": {
        "file_id": 236,
        "content": "/tests/voice_detect_extract_split/paddlespeech/test.sh",
        "type": "filepath"
    },
    "2259": {
        "file_id": 236,
        "content": "The code exports HTTP and HTTPS proxy variables, runs TTS (Text-to-Speech) to convert text into audio (output.wav), and then performs ASR (Automatic Speech Recognition) in Chinese language using the output audio file. The code is intended for testing purposes with PaddleSpeech deep learning framework.",
        "type": "summary"
    },
    "2260": {
        "file_id": 236,
        "content": "export http_proxy=\"\"\nexport https_proxy=\"\"\n# this voice is great. excellent for my shit.\npaddlespeech tts --input \"你好，欢迎使用飞桨深度学习框架！\" --output output.wav # must download models on the fly.\npaddlespeech asr --lang zh --input output.wav\n# 你好欢迎使用非讲深度学习框架\n# how does it feel to have errors?\n# left and right variables are not the same. what is that?",
        "type": "code",
        "location": "/tests/voice_detect_extract_split/paddlespeech/test.sh:1-12"
    },
    "2261": {
        "file_id": 236,
        "content": "The code exports HTTP and HTTPS proxy variables, runs TTS (Text-to-Speech) to convert text into audio (output.wav), and then performs ASR (Automatic Speech Recognition) in Chinese language using the output audio file. The code is intended for testing purposes with PaddleSpeech deep learning framework.",
        "type": "comment"
    },
    "2262": {
        "file_id": 237,
        "content": "/tests/basic_pitch_multi_midi_conversion/test.sh",
        "type": "filepath"
    },
    "2263": {
        "file_id": 237,
        "content": "Creates a new folder called \"output_path\", then uses the 'basic-pitch' command to convert MIDI files from \"/media/root/help/pyjom/tests/bilibili_practices/bilibili_tarot/some_bgm.mp3\" into audio format, saving them in the newly created folder.",
        "type": "summary"
    },
    "2264": {
        "file_id": 237,
        "content": "mkdir output_path\nbasic-pitch --sonify-midi output_path /media/root/help/pyjom/tests/bilibili_practices/bilibili_tarot/some_bgm.mp3",
        "type": "code",
        "location": "/tests/basic_pitch_multi_midi_conversion/test.sh:1-2"
    },
    "2265": {
        "file_id": 237,
        "content": "Creates a new folder called \"output_path\", then uses the 'basic-pitch' command to convert MIDI files from \"/media/root/help/pyjom/tests/bilibili_practices/bilibili_tarot/some_bgm.mp3\" into audio format, saving them in the newly created folder.",
        "type": "comment"
    },
    "2266": {
        "file_id": 238,
        "content": "/tests/audio_volume_meter/test_volume_meter.py",
        "type": "filepath"
    },
    "2267": {
        "file_id": 238,
        "content": "This code calculates audio parameters, generates vocal slices, and clusters segments using KMeans for labeling. It merges adjacent segments with similar labels and stores the updated labels.",
        "type": "summary"
    },
    "2268": {
        "file_id": 238,
        "content": "# usually yelling is not always funny. but we can do speech to text. taking longer time though... pinpoint the cue time.\n# often some exclamation attempts like repetation or louder sounds.\naudio_src = \"/media/root/help/pyjom/samples/audio/dog_with_text/vocals.wav\"\n# heard of dog woooling.\n# import audioop\nimport pydub\ntimestep = 0.1  # my time setting.\naudiofile = pydub.AudioSegment.from_wav(audio_src)\nframe_rate = audiofile.frame_rate\nseconds = audiofile.duration_seconds\nprint(frame_rate)  # 44100.\nprint(seconds)  # sample length\nimport math\nimport numpy as np\nfrom talib import stream\n# frame_rate2 = frame_rate *timestep\nmilistep = 1000 * timestep\nma_step = 10  # one second of buffer size. or more. timeperiod=ma_step\nstd_arr, maxval_arr, abs_nonzero_arr = [], [], []\ndef getPaddingMovingAverage(myarray, timeperiod=10):\n    lt = math.ceil(timeperiod / 2)\n    rt = timeperiod - lt\n    len_myarray = len(myarray)\n    max_index = len_myarray - 1\n    result_array = []\n    for i in range(len_myarray):\n        start_index = i - lt",
        "type": "code",
        "location": "/tests/audio_volume_meter/test_volume_meter.py:1-37"
    },
    "2269": {
        "file_id": 238,
        "content": "Code imports PyDub, sets timestep and frame rate variables from audio file duration and frame rate. Imports math, numpy and talib.stream. Defines function getPaddingMovingAverage to calculate moving average with padding, taking an array and time period as parameters. Initializes std_arr, maxval_arr and abs_nonzero_arr lists for further calculations.",
        "type": "comment"
    },
    "2270": {
        "file_id": 238,
        "content": "        start_index = max(0, start_index)\n        end_index = i + rt\n        end_index = min(end_index, max_index)\n        array_slice = myarray[start_index:end_index]\n        arr_slice_length = end_index - start_index\n        val = sum(array_slice) / arr_slice_length\n        # val = np.median(array_slice)\n        result_array.append(val)\n    return result_array\nmsteps = math.ceil(seconds / timestep)\nfor i in range(msteps):\n    # print(frame_rate2)\n    # probably in miliseconds.\n    segment = audiofile[i * milistep : (i + 1) * milistep]\n    data = segment.get_array_of_samples()\n    # containes two channels. 4410*2\n    darray = np.array(data)\n    print(darray.shape)\n    std = np.std(darray)\n    abs_darray = abs(darray)\n    maxval = np.max(abs_darray)\n    abs_nonzero = np.average(abs_darray)\n    print(\"STD:{} MAX:{} AVG:{}\".format(std, maxval, abs_nonzero))\n    std_arr.append(std)\n    # ma_std = stream.SMA(np.array(std_arr[-ma_step:]).astype(np.float64))\n    maxval_arr.append(maxval)\n    # ma_maxval = stream.SMA(np.array(maxval_arr[-ma_step:]).astype(np.float64))",
        "type": "code",
        "location": "/tests/audio_volume_meter/test_volume_meter.py:38-67"
    },
    "2271": {
        "file_id": 238,
        "content": "This code calculates the standard deviation, maximum value, and average of absolute values for a given audio segment. It appends the calculated values to respective lists and potentially calculates moving averages. The code utilizes numpy functions for array processing and the SMA function from the stream module (possibly) for calculating moving averages.",
        "type": "comment"
    },
    "2272": {
        "file_id": 238,
        "content": "    abs_nonzero_arr.append(abs_nonzero)\n    # ma_abs_nonzero = stream.SMA(np.array(abs_nonzero_arr[-ma_step:]).astype(np.float64))\n    # breakpoint()\n    # print(\"MA_STD:{} MA_MAX:{} MA_AVG:{}\".format(ma_std,ma_maxval,ma_abs_nonzero))\n    # print(data)\n    # breakpoint()\n    # maxAudioValue =audioop.max(data,2)\n    # print(\"STEP:\",i,\"VOLUME:\",maxAudioValue)\nstd_arr0 = getPaddingMovingAverage(std_arr, timeperiod=20)\nmaxval_arr0 = getPaddingMovingAverage(maxval_arr, timeperiod=20)\nabs_nonzero_arr0 = getPaddingMovingAverage(abs_nonzero_arr, timeperiod=20)\nma_std_arr = getPaddingMovingAverage(std_arr, timeperiod=60)\nma_maxval_arr = getPaddingMovingAverage(maxval_arr, timeperiod=60)\nma_abs_nonzero_arr = getPaddingMovingAverage(abs_nonzero_arr, timeperiod=60)\n# just use one freaking example as my conclusion.\nstatus = \"end\"\nvocal_slices = []\nvocal_slice = []\nfinal_index = msteps - 1\n# could you use clustering.\n# like time versus duration.\navg_std = []\nfor i in range(msteps):\n    a, b, c = std_arr0[i], maxval_arr0[i], abs_nonzero_arr0[i]",
        "type": "code",
        "location": "/tests/audio_volume_meter/test_volume_meter.py:68-92"
    },
    "2273": {
        "file_id": 238,
        "content": "This code calculates the moving average for various audio parameters (std_arr, maxval_arr, and abs_nonzero_arr) over different time periods. It then generates a vocal slice based on these moving averages for each step in the range of msteps. The final index is set to be one less than the total number of steps, and an average list is created.",
        "type": "comment"
    },
    "2274": {
        "file_id": 238,
        "content": "    a0, b0, c0 = ma_std_arr[i], ma_maxval_arr[i], ma_abs_nonzero_arr[i]\n    if status == \"end\":\n        # startpoint = a0 < a\n        startpoint = a0 < a or b0 < b or c0 < c\n        if startpoint:\n            vocal_slice.append(i)\n            avg_std.append(a)\n            status = \"start\"\n    else:\n        avg_std.append(a)\n        # endpoint = a0 > a\n        endpoint = a0 > a and b0 > b and c0 > c\n        if endpoint:\n            vocal_slice.append(i)\n            # vocal_slice[1] = i\n            status = \"end\"\n            vocal_slices.append([vocal_slice, np.average(avg_std)])\n            vocal_slice = []\n            avg_std = []\nif len(vocal_slice) == 1:\n    vocal_slice.append(final_index)\n    vocal_slices.append([vocal_slice, np.average(avg_std)])\ntime_rate = timestep\ntimed_vocal_slices = [\n    [[x[0][0] * time_rate, x[0][1] * time_rate], x[1]] for x in vocal_slices\n]\nd2_data = []\nd1_data = []\nfor slice_vocal in timed_vocal_slices:\n    print(slice_vocal)  # it could be two dimentional. both for length and volume?",
        "type": "code",
        "location": "/tests/audio_volume_meter/test_volume_meter.py:93-123"
    },
    "2275": {
        "file_id": 238,
        "content": "The code is iterating through an array of data and dividing it into segments based on threshold values for average, maximum, and absolute non-zero values. These segments are classified as either \"start\" or \"end\", and the indices of the start and end points are stored in separate lists. If a segment only has one point, it is added to the list of vocal slices along with the average of the threshold values. The code then calculates the time rate and creates two-dimensional lists of timed vocal slices (segment start and end times), and data for d1 and d2. Finally, the code prints the timed vocal slices, which could be in a two-dimensional format representing length and volume.",
        "type": "comment"
    },
    "2276": {
        "file_id": 238,
        "content": "    # to find best shit you need grouping.\n    a, b = slice_vocal[0]\n    length = b - a\n    d2_data.append([length, slice_vocal[1]])\n    d1_data.append([slice_vocal[1]])\nfrom sklearn.cluster import KMeans\nkmeans = KMeans(n_clusters=2)\nkm = kmeans.fit(d1_data)\nlabels = km.labels_\nlabel_indexs = {i: labels[i] for i in range(len(labels))}\n# print(label_index)\nnew_labels = []\nmergeTimeGap = 0.5\nlb_new = 0\nlast_elem = None\nfor index, data in enumerate(timed_vocal_slices):\n    # data = timed_vocal_slices\n    [start, end], std = data\n    label = label_indexs[index]\n    if last_elem == None:\n        last_elem = [[start, end], label]\n    else:\n        [[last_start, last_end], last_label] = last_elem\n        if start - last_end < mergeTimeGap and last_label == label:\n            pass\n            # last_elem = [[start,end],label]\n        else:\n            lb_new += 1\n        last_elem = [[start, end], label]\n    new_labels.append(lb_new)\n    print(\"DATA:\", data, \"LABEL:\", label, \"NEW_LABEL:\", lb_new)",
        "type": "code",
        "location": "/tests/audio_volume_meter/test_volume_meter.py:124-157"
    },
    "2277": {
        "file_id": 238,
        "content": "This code is grouping vocal segments based on their start and end timestamps. It uses KMeans clustering from sklearn to assign labels to each segment, then merges adjacent segments with the same label if they are less than a certain time gap apart. The new_labels list stores the updated labels for each segment.",
        "type": "comment"
    },
    "2278": {
        "file_id": 239,
        "content": "/tests/post_numpy_array/server.py",
        "type": "filepath"
    },
    "2279": {
        "file_id": 239,
        "content": "The code sets up a FastAPI server on port 5463, defines an endpoint that receives an image and returns \"good\", and runs a non-blocking Uvicorn server.",
        "type": "summary"
    },
    "2280": {
        "file_id": 239,
        "content": "SERVER_PORT=5463\nif __name__ == '__main__':\n    # from pydantic import BaseModel\n    # import numpy as np\n    import numpy_serializer\n    # from typing import Union\n    # class Image(BaseModel):\n    #     image:Union[str,bytes]\n    from fastapi import FastAPI, Body\n    app = FastAPI()\n    @app.post(\"/\")\n    def receiveImage(image:bytes=Body(default=None),\n        isBytes:bool =False,\n    encoding:str='utf-8', debug:bool=False):\n        # return book\n        # print('image type:',type(image))\n        # print(image)\n        import urllib.parse\n        image = image.removeprefix(b'image=') # fuck man.\n        image = urllib.parse.unquote_to_bytes(image)\n        if debug:\n            print(\"isBytes:\",isBytes)\n        if not isBytes:\n            image = image.decode(encoding) #fuck?\n            # read image from path, url\n        else:\n            image = numpy_serializer.from_bytes(image)\n        if debug:\n            print('shape?',image.shape)\n            print('image?',image)\n        return \"good\"\n    import uvicorn\n ",
        "type": "code",
        "location": "/tests/post_numpy_array/server.py:2-39"
    },
    "2281": {
        "file_id": 239,
        "content": "This code is setting up a FastAPI server on port 5463. It defines an endpoint at the root (\"/\") that receives an image either in bytes or as a string, and returns \"good\" as a response. The image data can be decoded from bytes using numpy_serializer or read from a file or URL if it's received as a string.",
        "type": "comment"
    },
    "2282": {
        "file_id": 239,
        "content": "   # checking: https://9to5answer.com/python-how-to-use-fastapi-and-uvicorn-run-without-blocking-the-thread\n    def run(host='0.0.0.0',port=SERVER_PORT): \n        \"\"\"\n        This function to run configured uvicorn server.\n        \"\"\"\n        uvicorn.run(app=app, host=host, port=port)\n    run()",
        "type": "code",
        "location": "/tests/post_numpy_array/server.py:39-46"
    },
    "2283": {
        "file_id": 239,
        "content": "This function runs a configured Uvicorn server non-blocking, allowing concurrent tasks.",
        "type": "comment"
    },
    "2284": {
        "file_id": 240,
        "content": "/tests/post_numpy_array/client.py",
        "type": "filepath"
    },
    "2285": {
        "file_id": 240,
        "content": "Importing numpy, requests, and numpy_serializer; using SERVER_PORT from server module; creating a test image array; converting the image to bytes using numpy_serializer; sending the image data as a POST request to localhost; printing the response received. Includes a malformatted docstring function with textwrap usage.",
        "type": "summary"
    },
    "2286": {
        "file_id": 240,
        "content": "import numpy as np\nimport requests\nimport numpy_serializer\n# this is pure magic. shit.\nfrom server import SERVER_PORT\nimage = np.array([1,2,3])\nimage_bytes = numpy_serializer.to_bytes(image)\ndata = {'image':image_bytes}\nprint(\"BYTES?\", image_bytes)\nr = requests.post(\"http://localhost:{}\".format(SERVER_PORT),data=data,params={'isBytes':True,'debug':True})\nprint('RESPONSE?',r.text)\ndef docstring(): # malformat\n    import textwrap\n    a =\"\"\"\n    lmn\n    abcdefg \n    hijk\n    \"\"\"\n    print(a)\n    print()\n    print(textwrap.dedent(a))\n    # inspect.cleandoc\n    # https://9to5answer.com/how-to-remove-extra-indentation-of-python-triple-quoted-multi-line-strings\ndocstring()",
        "type": "code",
        "location": "/tests/post_numpy_array/client.py:1-28"
    },
    "2287": {
        "file_id": 240,
        "content": "Importing numpy, requests, and numpy_serializer; using SERVER_PORT from server module; creating a test image array; converting the image to bytes using numpy_serializer; sending the image data as a POST request to localhost; printing the response received. Includes a malformatted docstring function with textwrap usage.",
        "type": "comment"
    },
    "2288": {
        "file_id": 241,
        "content": "/tests/patch_requests_timeout/server.py",
        "type": "filepath"
    },
    "2289": {
        "file_id": 241,
        "content": "This code sets up a FastAPI server using uvicorn, listens on port 9341, and has a single route (\"/\") that returns \"hello world\" after a 10-second delay. The run() function is used to start the configured uvicorn server.",
        "type": "summary"
    },
    "2290": {
        "file_id": 241,
        "content": "SERVER_PORT = 9341\nif __name__ == \"__main__\":\n    from fastapi import FastAPI\n    app = FastAPI()\n    import time\n    @app.get(\"/\")\n    def receiveImage():\n        time.sleep(10)\n        return \"hello world\"\n    import uvicorn\n    # checking: https://9to5answer.com/python-how-to-use-fastapi-and-uvicorn-run-without-blocking-the-thread\n    def run(host='0.0.0.0',port=SERVER_PORT): \n        \"\"\"\n        This function to run configured uvicorn server.\n        \"\"\"\n        uvicorn.run(app=app, host=host, port=port)\n    run()",
        "type": "code",
        "location": "/tests/patch_requests_timeout/server.py:1-21"
    },
    "2291": {
        "file_id": 241,
        "content": "This code sets up a FastAPI server using uvicorn, listens on port 9341, and has a single route (\"/\") that returns \"hello world\" after a 10-second delay. The run() function is used to start the configured uvicorn server.",
        "type": "comment"
    },
    "2292": {
        "file_id": 242,
        "content": "/tests/patch_requests_timeout/client.py",
        "type": "filepath"
    },
    "2293": {
        "file_id": 242,
        "content": "This code patches the requests library to set a default timeout for all requests made using it. It uses patchy module to modify the HTTPAdapter's send method, checking if a timeout is provided and setting it to REQUESTS_TIMEOUT (3 seconds) if none is given. The patch is then applied using the patchy module.",
        "type": "summary"
    },
    "2294": {
        "file_id": 242,
        "content": "import patchy\nfrom requests.adapters import HTTPAdapter\nREQUESTS_TIMEOUT=3 # working! great.\ndef patch_requests_default_timeout() -> None:\n    \"\"\"\n    Set a default timeout for all requests made with “requests”.\n    Upstream is waiting on this longstanding issue:\n    https://github.com/psf/requests/issues/3070\n    \"\"\"\n    patchy.patch(\n        HTTPAdapter.send,\n        f\"\"\"\\\n        @@ -14,6 +14,8 @@\n             :param proxies: (optional) The proxies dictionary to apply to the request.\n             :rtype: requests.Response\n             \\\"\"\"\n        +    if timeout is None:\n        +        timeout = {REQUESTS_TIMEOUT}\n             try:\n                 conn = self.get_connection(request.url, proxies)\n        \"\"\",\n    )\npatch_requests_default_timeout()\nimport requests\nfrom server import SERVER_PORT\nr = requests.get(f\"http://localhost:{SERVER_PORT}\")",
        "type": "code",
        "location": "/tests/patch_requests_timeout/client.py:2-36"
    },
    "2295": {
        "file_id": 242,
        "content": "This code patches the requests library to set a default timeout for all requests made using it. It uses patchy module to modify the HTTPAdapter's send method, checking if a timeout is provided and setting it to REQUESTS_TIMEOUT (3 seconds) if none is given. The patch is then applied using the patchy module.",
        "type": "comment"
    },
    "2296": {
        "file_id": 243,
        "content": "/tests/bilibili_practices/bilibili_video_translate/web_translator.py",
        "type": "filepath"
    },
    "2297": {
        "file_id": 243,
        "content": "This code imports a translators module and defines a function for translating Chinese to English. It randomly selects a translator, fixes line endings if needed, and returns the result. The code also prints exception tracebacks in case of errors.",
        "type": "summary"
    },
    "2298": {
        "file_id": 243,
        "content": "import translators as ts\n# translator = \n# mtranslators = [ts.sogou] #this is pure shit.\n# mtranslators = [ts.baidu,ts.sogou]\n# mtranslators = [ts.baidu,ts.sogou,ts.iciba]\nmtranslators = [ts.youdao,ts.baidu,ts.alibaba] # no yandex, tencent, sogou.\n# mtranslators = [ts.baidu,ts.iciba]\nimport random\ndef fixline(line):\n    notEndings = [\"。\",\"，\"]\n    for x in notEndings:\n        if line.endswith(x): return line[:-1]\n    return line\ndef zh_to_en_translator(text,needFixLine=True):\n    randomLang = [\"zh\",\"zh-CHS\"]\n    from_language = \"en\"\n    # lang = random.choice(randomLang)\n    while True:\n        t = random.choice(mtranslators)\n        # print(type(translator))\n        for rl in randomLang:\n            try:\n                result = t(text,from_language=from_language,to_language=rl)\n                # if len(result) < 3:\n                #     print(t)\n                #     breakpoint()\n                if needFixLine:\n                    result = fixline(result)\n                return result\n            except:\n                import traceback",
        "type": "code",
        "location": "/tests/bilibili_practices/bilibili_video_translate/web_translator.py:1-34"
    },
    "2299": {
        "file_id": 243,
        "content": "This code imports a translators module and defines a function for translating Chinese (zh) to English (en). It randomly selects a translator from a list of options and attempts to translate the text. If needed, it fixes the line endings before returning the result.",
        "type": "comment"
    }
}