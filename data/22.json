{
    "2200": {
        "file_id": 224,
        "content": "import urllib.parse\nimport requests\n# disable all proxies.\nimport os\nimport time\nos.environ[\"http_proxy\"]=\"\"\nos.environ[\"https_proxy\"]=\"\"\n# do not use freaking proxy, otherwise QingYunKe will not respond.\ndef checkApi(func,message,name):\n    response_message = func(message)\n    if response_message!=None:\n        print(\"{} RESPONSE:\".format(name), response_message)\ndef chatAtri(msg: str, BASE='http://api.nekomimi.icu/v1/'):\n    url = BASE + 'chat?msg=%s' % msg\n    response = requests.get(url)\n    if response.status_code == 200:\n        data = response.json()\n        if data['status'] == 'success':\n            return data['message']\n    # return None\n    # nothing is returned if have error.\n    print(\"ATRI ERROR:\", response.status_code, response.json())\n# import subprocess\n# import json\ndef chatQingKeYun(msg: str, url=\"http://api.qingyunke.com/api.php?key=free&appid=0&msg=\"):\n    msg = urllib.parse.quote(msg)\n    myUrl = url+msg\n    # print(myUrl)\n    # output = subprocess.check_output([\"curl\", myUrl])\n    # data = json.loads(output.decode(\"utf-8\"))",
        "type": "code",
        "location": "/tests/conversation_talk_apis/api_tests.py:1-37"
    },
    "2201": {
        "file_id": 224,
        "content": "Code imports necessary modules, disables proxies, defines a function to check API responses, and includes two chat functions: 'chatAtri' for chatting with Atri using a Chinese language processing API, and 'chatQingKeYun' for chatting with QingYunKe using a free API key. The code also has a commented section that appears to be testing the use of subprocess and json modules but is not implemented yet.",
        "type": "comment"
    },
    "2202": {
        "file_id": 224,
        "content": "    # import requests\n    data = requests.get(myUrl)\n    data = data.json()\n    print(data)\n    result = data['result']\n    assert result == 0  # 202 -> busy\n    content = data['content']\n    return content\n    # breakpoint()\ndef xiaobing(msg):\n    # 其实是新浪微博群发器 微博群发的逻辑类似于b站群发\n    # 刚关注的只能发一条消息\n    uid = '5175429989'\n    source = '209678993'\n    SUB = '_2A25PyitTDeRhGeBG7VAS8y_MwjmIHXVsvhubrDV8PUNbmtANLRfTkW9NRhxXNiVv6Qwut5wwnc8rys3cbJFAxVdX'\n    url_send = 'https://api.weibo.com/webim/2/direct_messages/new.json'\n    data = {\n        'text': msg,\n        'uid': uid,\n        'source': source\n    }\n    headers = {\n        'cookie': 'SUB='+SUB,\n        'Content-Type': 'application/x-www-form-urlencoded',\n        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.130 Safari/537.36',\n        'Referer': 'https://api.weibo.com/chat/'\n    }\n    response = requests.post(url_send, data=data, headers=headers).json()\n    sendMsg = response['text']\n    time.sleep(1)\n    while True:",
        "type": "code",
        "location": "/tests/conversation_talk_apis/api_tests.py:38-69"
    },
    "2203": {
        "file_id": 224,
        "content": "This code is using the requests library to send a POST request to the Weibo API's direct messaging endpoint. It creates a new message with the provided text, sends it to a specified user (uid), and retrieves the response from the API. The script includes necessary headers and uses JSON format for the data payload.",
        "type": "comment"
    },
    "2204": {
        "file_id": 224,
        "content": "        print(\"RETRYING\")\n        url_get = 'https://api.weibo.com/webim/2/direct_messages/conversation.json?uid={}&source={}'.format(uid, source)\n        response = requests.get(url_get, headers=headers).json()\n        getMsg = response['direct_messages'][0]['text']\n        if sendMsg == getMsg:\n            time.sleep(1)\n        else:\n            return getMsg\ndef chatOwnThink(msg:str):\n    url = \"https://api.ownthink.com/bot?appid=xiaosi&userid=user&spoken=\"\n    msg = urllib.parse.quote(msg)\n    myUrl = url+msg\n    data = requests.get(myUrl)\n    data = data.json()\n    # output = subprocess.check_output([\"curl\", myUrl])\n    # data = json.loads(output.decode(\"utf-8\"))\n    if data[\"message\"] == \"success\":\n        if data[\"data\"][\"type\"] == 5000:\n            return data[\"data\"][\"info\"][\"text\"]\n    # print(data)\n    # breakpoint()\n    # result = data['result']\n    # assert result == 0  # 202 -> busy\n    # content = data['content']\n    # return content\nif __name__ == '__main__':\n    # execute my tests.\n    message = \"你好\"",
        "type": "code",
        "location": "/tests/conversation_talk_apis/api_tests.py:70-99"
    },
    "2205": {
        "file_id": 224,
        "content": "The code is attempting to interact with two APIs - Weibo and OwnThink. It first checks if the message from the user matches the response received from the Weibo API conversation. If it's a match, the code waits for a second before rechecking. If there's no match, it sends the message to the OwnThink API to get a response. The response is then checked for success and if the type of response is 5000, the text information from the response is returned.",
        "type": "comment"
    },
    "2206": {
        "file_id": 224,
        "content": "    # checkApi(chatAtri, message, \"ATRI\")\n    # checkApi(xiaobing, message, \"XIAOBING\")\n    # checkApi(chatOwnThink, message, \"OWNTHINK\")\n    checkApi(chatQingKeYun, message, \"QINGYUNKE\")",
        "type": "code",
        "location": "/tests/conversation_talk_apis/api_tests.py:100-103"
    },
    "2207": {
        "file_id": 224,
        "content": "This code is calling the checkApi function with different parameters for each chatbot instance: chatAtri, xiaobing, chatOwnThink, and chatQingKeYun. The function call passes a message and specific identifier to perform an API test on each chatbot.",
        "type": "comment"
    },
    "2208": {
        "file_id": 225,
        "content": "/tests/chatgpt_multiagent_agent_product_line_multimodal_langchain_experiments/test_chatgpt_cn_api.py",
        "type": "filepath"
    },
    "2209": {
        "file_id": 225,
        "content": "The code offers an online interface for ChatGPT API, streamlining development by loading API key and endpoint from a YAML file, setting environment variables, and using the OpenAI completion API to send messages, receive responses, and return replies.",
        "type": "summary"
    },
    "2210": {
        "file_id": 225,
        "content": "\"\"\"\nInterface for using the chatgpt api online service, without setting up locally.\nThis interface is used for development, not in production.\n\"\"\"\n# Do not treat the machine like people.\n# You need to handle them differently.\nfrom litellm import completion\nimport os\nimport yaml\napi_key_filepath = os.path.join(\n    os.path.expanduser(\"~\"), \".chatgpt_api_key.yaml\")\nif os.path.exists(api_key_filepath):\n    if os.path.isfile(api_key_filepath):\n        # Load YAML file\n        with open(api_key_filepath, 'r') as file:\n            data = yaml.load(file, Loader=yaml.FullLoader)\n            api_key = data['api_key']\n            endpoint = data['endpoint']\n    else:\n        raise Exception(\n            f\"API key path exists but found non-file object at: '{api_key_filepath}'\")\nelse:\n    raise Exception(f\"API key file not found in: '{api_key_filepath}'\")\nos.environ[\"OPENAI_API_KEY\"] = api_key\nos.environ[\"OPENAI_API_BASE\"] = endpoint\nmodel_tag = \"openai/gpt-3.5-turbo\"\ndef get_reply_from_chatgpt(content: str):\n    messages = [{\"content\": content, \"role\": \"user\"}]",
        "type": "code",
        "location": "/tests/chatgpt_multiagent_agent_product_line_multimodal_langchain_experiments/test_chatgpt_cn_api.py:1-37"
    },
    "2211": {
        "file_id": 225,
        "content": "This code provides an interface for using the ChatGPT API online service without setting up locally, specifically designed for development purposes. It handles loading the API key and endpoint from a YAML file, sets environment variables, and defines a function to get a reply from ChatGPT using provided content.",
        "type": "comment"
    },
    "2212": {
        "file_id": 225,
        "content": "    print(\"sending:\")\n    print(messages)\n    # openai call\n    # many info inside. you may want to take a look?\n    response = completion(model_tag, messages)\n    choices = response['choices']\n    reply_content = choices[0]['message']['content']\n    print(\"reply:\")\n    print(reply_content)\n    return reply_content",
        "type": "code",
        "location": "/tests/chatgpt_multiagent_agent_product_line_multimodal_langchain_experiments/test_chatgpt_cn_api.py:38-47"
    },
    "2213": {
        "file_id": 225,
        "content": "This code sends a message and receives a response using OpenAI's completion API. It prints the input messages, processes the response from the API, extracts the reply content, and returns it for further processing.",
        "type": "comment"
    },
    "2214": {
        "file_id": 226,
        "content": "/tests/chatgpt_multiagent_agent_product_line_multimodal_langchain_experiments/test_url_repair_extract_trace_media_source.py",
        "type": "filepath"
    },
    "2215": {
        "file_id": 226,
        "content": "The code defines functions to recover URLs, indexify them, and prompt for YouTube selection with utility functions for repairing content and retrieving selection IDs. It extracts, repairs, and selects YouTube URLs from a list by handling direct/indirect links and improving readability.",
        "type": "summary"
    },
    "2216": {
        "file_id": 226,
        "content": "from typing import Optional\ndef recover_prompt_constructor(info): return f\"\"\"\nPlease recover any URL from the given context. Every URL shall be visitable, starting with \"http://\" or \"https://\".\nContext:\n{info}\nURLs:\n\"\"\"\ndef indexify_string_list(string_list): return [\n    f'[{index}] {url}' for index, url in enumerate(string_list)]\ndef youtube_select_prompt_constructor(url_list): \n    urls_content = '\\n'.join(indexify_string_list(url_list))\n    return f\"\"\"\nPlease select URLs if they are directed to YouTube. I will give you URLs with index in front of them. Give your selection by selected indices in squared brackets separated by space like: [1] [3].\nURLs:\n{urls_content}\nSelected URLs:\n\"\"\"\nfrom test_chatgpt_cn_api import get_reply_from_chatgpt\ndef repair_content_with_url(info):\n    prompt = recover_prompt_constructor(info)\n    content = get_reply_from_chatgpt(prompt)\n    return content\nimport re\ndef get_youtube_selection_ids(urls):\n    prompt = youtube_select_prompt_constructor(urls)\n    response= get_reply_from_chatgpt(prompt)",
        "type": "code",
        "location": "/tests/chatgpt_multiagent_agent_product_line_multimodal_langchain_experiments/test_url_repair_extract_trace_media_source.py:2-42"
    },
    "2217": {
        "file_id": 226,
        "content": "This code defines functions to recover URLs from context, indexify them in a string list, and prompt the user to select YouTube URLs. It also includes utility functions for repairing content with URLs and getting YouTube selection IDs.",
        "type": "comment"
    },
    "2218": {
        "file_id": 226,
        "content": "    numbers = re.findall(r'\\[\\d+\\]', response)\n    indices = []\n    for num in numbers:\n        num = num.strip(\"[\").strip(']').strip()\n        num = int(num)\n        indices.append(num)\n    return indices\ndef select_youtube_urls(url_list, indices):\n    fatal_error:Optional[str]= None\n    selected_urls = []\n    index_errors = 0\n    url_counts = len(url_list)\n    max_index = url_counts-1\n    for index in indices:\n        try:\n            url = url_list[index]\n            print(\"selected:\", url)\n            selected_urls.append(url)\n        except IndexError:\n            index_errors += 1\n            # TODO: handle error by recursively letting the LLM knows the error and querying answer.\n            # TODO: determine if error is fatal (not recoverable in 5 iterations)\n            # TODO: eliminate possibility of external cause of fatal error by inferance\n            print('index not found: %d (max index is %d)' % (index, max_index))\n    print(\"summary\".center(80, \"=\"))\n    print(\"given url counts: %d\" % url_counts)",
        "type": "code",
        "location": "/tests/chatgpt_multiagent_agent_product_line_multimodal_langchain_experiments/test_url_repair_extract_trace_media_source.py:43-69"
    },
    "2219": {
        "file_id": 226,
        "content": "This code defines two functions, `extract_indices` and `select_youtube_urls`. The first function extracts indices from the response string using regular expressions. The second function takes a list of URLs and a list of indices, selects the corresponding URLs based on the provided indices, handles index errors, and returns the selected URLs. It also includes TODO comments for potential error handling and improvement suggestions.",
        "type": "comment"
    },
    "2220": {
        "file_id": 226,
        "content": "    print(\"selected url counts: %d\", len(selected_urls))\n    print(\"index errors: %d\" % index_errors)\n    return selected_urls\nfrom urlextract import URLExtract\nextractor = URLExtract()\ndef extract_url(content):\n    \"\"\"\n    Just extract the url. Do not repair.\n    \"\"\"\n    urls = extractor.find_urls(content)\n    return urls\ndef repair_and_get_repaired_url_list(info):\n    content = repair_content_with_url(info)\n    url_list = extract_url(content)\n    return url_list\nif __name__ == '__main__':\n    info_direct = \"\"\"\nYoutube\n原标题A Thousand Miles-Neco are(FULL VERSION)\nhttps://youtu.be/Ddpx0JLOH6o?si=zZMjAEFj_TOXkQct\n音频下载：\nhttps://wwxa.lanzouj.com/idPN81a5u4ab\n密码:5292\n\"\"\"\n    info_indirect = \"\"\"\n转自Youtube\n/watch?v=mSqRH4WwnnY // By :Encrypted Lobster\n\"\"\"\n    info_list = [info_direct, info_indirect]\n    for i, info in enumerate(info_list):\n        print(f\"processing info #{i}\")\n        print(info)\n        repaired_urls = repair_and_get_repaired_url_list(info)\n        indices = get_youtube_selection_ids(repaired_urls)\n        selected_urls = select_youtube_urls(repaired_urls, indices)",
        "type": "code",
        "location": "/tests/chatgpt_multiagent_agent_product_line_multimodal_langchain_experiments/test_url_repair_extract_trace_media_source.py:70-114"
    },
    "2221": {
        "file_id": 226,
        "content": "This code is extracting and repairing URLs from provided information. It first extracts URLs without repairing them, then repairs the content with any broken URLs and extracts the repaired URLs. The code handles both direct and indirect Youtube links, processes each info in the list, repairs URLs, selects YouTube URLs based on specified selection IDs, and finally returns the selected URLs.",
        "type": "comment"
    },
    "2222": {
        "file_id": 226,
        "content": "        print(\"selected urls:\")\n        for url in selected_urls:\n            print(f'\\t{url}')\n        print()",
        "type": "code",
        "location": "/tests/chatgpt_multiagent_agent_product_line_multimodal_langchain_experiments/test_url_repair_extract_trace_media_source.py:115-118"
    },
    "2223": {
        "file_id": 226,
        "content": "This code segment prints the selected URLs from a list. The 'for' loop iterates through each URL in the list and displays it with a tab character for readability, followed by a newline to separate each URL for better visualization.",
        "type": "comment"
    },
    "2224": {
        "file_id": 227,
        "content": "/tests/chatgpt_multiagent_agent_product_line_multimodal_langchain_experiments/README.md",
        "type": "filepath"
    },
    "2225": {
        "file_id": 227,
        "content": "The code mentions the power of chatgpt-like bots and plans to run one using CPU instead of a powerful GPU. It also suggests utilizing moderation API and openAI API for testing purposes. The bot will start with a simple task involving constructing/extracting URLs from video descriptions.",
        "type": "summary"
    },
    "2226": {
        "file_id": 227,
        "content": "chatgpt-like bots are powerful.\nwe will run one using cpu, since we don't always have a powerful gpu\nwe can also use moderation api & openai api for testing\nfirst we will give the bot a simple task to construct/extract url from video description.",
        "type": "code",
        "location": "/tests/chatgpt_multiagent_agent_product_line_multimodal_langchain_experiments/README.md:1-7"
    },
    "2227": {
        "file_id": 227,
        "content": "The code mentions the power of chatgpt-like bots and plans to run one using CPU instead of a powerful GPU. It also suggests utilizing moderation API and openAI API for testing purposes. The bot will start with a simple task involving constructing/extracting URLs from video descriptions.",
        "type": "comment"
    },
    "2228": {
        "file_id": 228,
        "content": "/tests/bezier_paddlehub_dogcat_detector_serving/server.py",
        "type": "filepath"
    },
    "2229": {
        "file_id": 228,
        "content": "This code changes the directory, appends current path to sys.path, and imports specific configurations and classes for a Bezier PaddleHub ResNet50 Image DogCatDetectorServer in pyjom project.",
        "type": "summary"
    },
    "2230": {
        "file_id": 228,
        "content": "import sys\nimport os\ndef changeDirForImport():\n    os.chdir(\"/root/Desktop/works/pyjom\")\n    sys.path.append(\".\")\nif __name__ == '__main__':\n    changeDirForImport()\n    from pyjom.config.shared import pyjom_config\n    pyjom_config['BEZIER_PADDLE_RESNET50_IMAGE_DOG_CAT_DETECTOR_SERVER_INSTANCE']=True\n    from pyjom.imagetoolbox import bezierPaddleHubResnet50ImageDogCatDetectorServer\n    bezierPaddleHubResnet50ImageDogCatDetectorServer()",
        "type": "code",
        "location": "/tests/bezier_paddlehub_dogcat_detector_serving/server.py:1-13"
    },
    "2231": {
        "file_id": 228,
        "content": "This code changes the directory, appends current path to sys.path, and imports specific configurations and classes for a Bezier PaddleHub ResNet50 Image DogCatDetectorServer in pyjom project.",
        "type": "comment"
    },
    "2232": {
        "file_id": 229,
        "content": "/tests/bezier_paddlehub_dogcat_detector_serving/client.py",
        "type": "filepath"
    },
    "2233": {
        "file_id": 229,
        "content": "The code reads an image file from a specific location, changes the working directory for importing necessary libraries, initializes a client and server object for detecting dog/cat images using PaddleHub's ResNet50 model, reads the test image using OpenCV, performs detection on the image with the client object, and finally prints the result.",
        "type": "summary"
    },
    "2234": {
        "file_id": 229,
        "content": "test_image = \"/root/Desktop/works/pyjom/samples/image/dog_with_text.jpg\"\nfrom server import changeDirForImport\nchangeDirForImport()\nfrom pyjom.imagetoolbox import bezierPaddleHubResnet50ImageDogCatDetectorClient,bezierPaddleHubResnet50ImageDogCatDetectorServerChecker\nimport cv2\ntest_image = cv2.imread(test_image)\nbezierPaddleHubResnet50ImageDogCatDetectorServerChecker()\nresult = bezierPaddleHubResnet50ImageDogCatDetectorClient(test_image)\nprint(\"RESULT?\",result)",
        "type": "code",
        "location": "/tests/bezier_paddlehub_dogcat_detector_serving/client.py:1-10"
    },
    "2235": {
        "file_id": 229,
        "content": "The code reads an image file from a specific location, changes the working directory for importing necessary libraries, initializes a client and server object for detecting dog/cat images using PaddleHub's ResNet50 model, reads the test image using OpenCV, performs detection on the image with the client object, and finally prints the result.",
        "type": "comment"
    },
    "2236": {
        "file_id": 230,
        "content": "/tests/black_autopep8_ast_parser_formatter_skipexception/test.py",
        "type": "filepath"
    },
    "2237": {
        "file_id": 230,
        "content": "This code appears to be a mix of unrelated or incomplete fragments. It imports various modules but lacks cohesive structure, making it difficult to determine its purpose or functionality.",
        "type": "summary"
    },
    "2238": {
        "file_id": 230,
        "content": "# may be illegal.\n# use autopep8?\n# first, autopep8, next, black\n# both with 'unlimited' line of code.\n# finally, throw it to our dearly 'skipException'\n    from lib2to3.pgen2.pgen import DFAState\nfrom mimetypes import suffix_map\nfrom os import SCHED_FIFO\nfrom socket import _SendableFile\nfrom xml.dom.pulldom    import \\\n    SAX2DOM\n    print('aaa'\n        ) # there is no repairing on this bracket for autopep8\n# about the dog_or_cat recognition of our cover:\n# 1. throw away unqualified ones (using pop?)\n# 2. lower the threshold of yolo\n# 3. downscale picture before passing to yolo\n# we can go wild here.\n@redisLRUCache(dfsji,\nasdif[dfk,DFAState,\nsdfkg])\ndef shit(aaa, bbb,\nccc,ddd):\n    dd = 2314\n    ee = suffix_map[SAX2DOM,\n    df23, ddd][sdf,\n    sdf,sdf]\n    ss = efldife.dfief(_SendableFile,\n    saif,SCHED_FIFO,\n    asdif[fjisd,\n    sfdsif,sdf])",
        "type": "code",
        "location": "/tests/black_autopep8_ast_parser_formatter_skipexception/test.py:1-36"
    },
    "2239": {
        "file_id": 230,
        "content": "This code appears to be a mix of unrelated or incomplete fragments. It imports various modules but lacks cohesive structure, making it difficult to determine its purpose or functionality.",
        "type": "comment"
    },
    "2240": {
        "file_id": 231,
        "content": "/tests/black_autopep8_ast_parser_formatter_skipexception/format_test.sh",
        "type": "filepath"
    },
    "2241": {
        "file_id": 231,
        "content": "This code is formatting Python files, using autopep8 to enforce maximum line length and Black to ensure consistent formatting. It prevents changes on the original content by redirecting output to a view-only mode.",
        "type": "summary"
    },
    "2242": {
        "file_id": 231,
        "content": "# view only. no change on original content.\n# of course, for lines with long content, we will have trouble.\nMAXINT=1000000000\ncat test.py | autopep8 --max-line-length $MAXINT - | black -l $MAXINT -C - 2>/dev/null",
        "type": "code",
        "location": "/tests/black_autopep8_ast_parser_formatter_skipexception/format_test.sh:1-4"
    },
    "2243": {
        "file_id": 231,
        "content": "This code is formatting Python files, using autopep8 to enforce maximum line length and Black to ensure consistent formatting. It prevents changes on the original content by redirecting output to a view-only mode.",
        "type": "comment"
    },
    "2244": {
        "file_id": 232,
        "content": "/tests/black_autopep8_ast_parser_formatter_skipexception/format_functional.py",
        "type": "filepath"
    },
    "2245": {
        "file_id": 232,
        "content": "This code reads a Python file, encodes it, and then runs it through \"autopep8\" and \"Black\" formatting tools to ensure code follows PEP 8 style guide and is aesthetically pleasing. It also handles exceptions and prints the formatted code for further use.",
        "type": "summary"
    },
    "2246": {
        "file_id": 232,
        "content": "with open(\"test.py\", \"r\") as f:\n    code = f.read()\n# need binary data.\ncode_encoded = code.encode(\"utf-8\")\nimport subprocess\nMAXINT = 10000000000\ncommand = \"autopep8 --max-line-length {MAXINT} - | black -l {MAXINT} -C -\".format(\n    MAXINT=MAXINT\n)\ncommandLine = [\"bash\", \"-c\", command]\nresult = subprocess.run(commandLine, input=code_encoded, capture_output=True)\ntry:\n    assert result.returncode == 0\n    code_formatted = result.stdout.decode(\"utf-8\")\nexcept:\n    import traceback\n    traceback.print_exc()\n    print(\"STDOUT\", result.stdout)\n    print(\"STDERR\", result.stderr)\n    code_formatted = code\nprint(code_formatted)",
        "type": "code",
        "location": "/tests/black_autopep8_ast_parser_formatter_skipexception/format_functional.py:1-26"
    },
    "2247": {
        "file_id": 232,
        "content": "This code reads a Python file, encodes it, and then runs it through \"autopep8\" and \"Black\" formatting tools to ensure code follows PEP 8 style guide and is aesthetically pleasing. It also handles exceptions and prints the formatted code for further use.",
        "type": "comment"
    },
    "2248": {
        "file_id": 233,
        "content": "/tests/anime1_me_video_download/test_download.sh",
        "type": "filepath"
    },
    "2249": {
        "file_id": 233,
        "content": "This script downloads \"crossdressing.mp4\" from the URL using various cookies until successful, involving timestamp, header, and Google Analytics parameters.",
        "type": "summary"
    },
    "2250": {
        "file_id": 233,
        "content": "# curl -L -o crossdressing.mp4 --cookie \"e=1652443257\" https://shiro.v.anime1.me/1019/6b.mp4\n# curl -L -o crossdressing.mp4 --cookie \"e=1652443257; p=eyJpc3MiOiJhbmltZTEubWUiLCJleHAiOjE2NTI0NDMyNTcwMDAsImlhdCI6MTY1MjQyODk1NTAwMCwic3ViIjoiLzEwMTkvNmIubXA0In0\" https://shiro.v.anime1.me/1019/6b.mp4\n# curl -L -o crossdressing.mp4 --cookie \"h=i6CylEHO-BiMkCPCqFDk_A\" https://shiro.v.anime1.me/1019/6b.mp4\n# curl -L -o crossdressing.mp4 --cookie \"e=1652443257; p=eyJpc3MiOiJhbmltZTEubWUiLCJleHAiOjE2NTI0NDMyNTcwMDAsImlhdCI6MTY1MjQyODk1NTAwMCwic3ViIjoiLzEwMTkvNmIubXA0In0; h=i6CylEHO-BiMkCPCqFDk_A\" https://shiro.v.anime1.me/1019/6b.mp4\ncurl -L -o crossdressing.mp4 --cookie \"e=1652443257; p=eyJpc3MiOiJhbmltZTEubWUiLCJleHAiOjE2NTI0NDMyNTcwMDAsImlhdCI6MTY1MjQyODk1NTAwMCwic3ViIjoiLzEwMTkvNmIubXA0In0; h=i6CylEHO-BiMkCPCqFDk_A\" https://shiro.v.anime1.me/1019/6b.mp4 # the only way to be.\n# curl -L -o crossdressing.mp4 --cookie \"e=1652443257; p=eyJpc3MiOiJhbmltZTEubWUiLCJleHAiOjE2NTI0NDMyNTcwMDAsImlhdCI6MTY1Mj",
        "type": "code",
        "location": "/tests/anime1_me_video_download/test_download.sh:1-6"
    },
    "2251": {
        "file_id": 233,
        "content": "This script is downloading a video file named \"crossdressing.mp4\" from the URL \"https://shiro.v.anime1.me/1019/6b.mp4\", using different combinations of cookies to access and save the file, with each attempt providing additional cookie values until the final combination successfully downloads the video.",
        "type": "comment"
    },
    "2252": {
        "file_id": 233,
        "content": "QyODk1NTAwMCwic3ViIjoiLzEwMTkvNmIubXA0In0; h=i6CylEHO-BiMkCPCqFDk_A; _ga=GA1.2.1032429949.1652428850; _gid=GA1.2.244096696.1652428850\" https://shiro.v.anime1.me/1019/6b.mp4",
        "type": "code",
        "location": "/tests/anime1_me_video_download/test_download.sh:6-6"
    },
    "2253": {
        "file_id": 233,
        "content": "The code appears to be a string containing a series of parameters and URL for downloading an MP4 file. The specific parameters include a timestamp, header value, Google Analytics IDs, and the video URL.",
        "type": "comment"
    },
    "2254": {
        "file_id": 234,
        "content": "/tests/anime1_me_video_download/parse_static.py",
        "type": "filepath"
    },
    "2255": {
        "file_id": 234,
        "content": "Code is parsing HTML data from 'sample.html' using BeautifulSoup and finding video tags. It then extracts channel and episode information from JSON data, formats download link, and prints the episode ID and download link. The code likely automates the process of downloading videos from a specific website.",
        "type": "summary"
    },
    "2256": {
        "file_id": 234,
        "content": "source = \"sample.html\"\n# curl -L -o sample.html \"https://anime1.me/category/2022%e5%b9%b4%e6%98%a5%e5%ad%a3/%e5%8b%87%e8%80%85%e8%be%ad%e8%81%b7%e4%b8%8d%e5%b9%b9%e4%ba%86\"\nfrom bs4 import BeautifulSoup\ndata = open(source,\"r\",encoding=\"utf-8\").read()\ndom = BeautifulSoup(data)\n# dom = BeautifulSoup(data,features='lxml')\nimport urllib.parse as up\nimport json\nimport re\nvideos = dom.find_all(\"video\")\nformat_download_link = lambda c,e: \"https://shiro.v.anime1.me/{}/{}.mp4\".format(c,e)\nfor video in videos:\n    # print(dir(video))\n    data_src = \"data-apireq\"\n    json_obj = video[data_src]\n    json_obj = up.unquote(json_obj)\n    json_obj = json.loads(json_obj)\n    channel, episode = json_obj[\"c\"], json_obj[\"e\"]\n    link = format_download_link(channel, episode)\n    episode_id = re.findall(r\"\\d+\",episode)[0]\n    print(\"EPISODE:\",episode_id)\n    print(\"DOWNLOAD LINK:\",link)\n    # breakpoint()",
        "type": "code",
        "location": "/tests/anime1_me_video_download/parse_static.py:1-28"
    },
    "2257": {
        "file_id": 234,
        "content": "Code is parsing HTML data from 'sample.html' using BeautifulSoup and finding video tags. It then extracts channel and episode information from JSON data, formats download link, and prints the episode ID and download link. The code likely automates the process of downloading videos from a specific website.",
        "type": "comment"
    },
    "2258": {
        "file_id": 235,
        "content": "/tests/anime1_me_video_download/get_cookie_sample.py",
        "type": "filepath"
    },
    "2259": {
        "file_id": 235,
        "content": "This code downloads a file, displays progress in real-time, uses chunked data for memory efficiency, and sets cookies from response headers.",
        "type": "summary"
    },
    "2260": {
        "file_id": 235,
        "content": "import requests\nimport json\nimport urllib.parse as up\nimport sys\n# import multithread\nfrom fake_useragent import UserAgent\nua = UserAgent()\nuser_agent =ua.random\nurl = \"https://v.anime1.me/api\"\n# data = '{\"c\":\"1019\",\"e\":\"6b\",\"t\":1652428857,\"p\":0,\"s\":\"ec9042ac177510fd67dd508f4d974074\"}'\n# data = '%7B%22c%22%3A%221019%22%2C%22e%22%3A%222b%22%2C%22t%22%3A1652429744%2C%22p%22%3A0%2C%22s%22%3A%225a78c05bd07077f05278ed6b44897878%22%7D'\ndata = \"%7B%22c%22%3A%221019%22%2C%22e%22%3A%225b%22%2C%22t%22%3A1652429744%2C%22p%22%3A0%2C%22s%22%3A%222d424b87559a56d7f761c436bca72502%22%7D\"\ndata_unquote = up.unquote(data)\ndata_json = json.loads(data_unquote)\n# url0 = \"https://anime1.me/category/2022%e5%b9%b4%e6%98%a5%e5%ad%a3/%e5%8b%87%e8%80%85%e8%be%ad%e8%81%b7%e4%b8%8d%e5%b9%b9%e4%ba%86\"\ns = requests.Session()\ns.headers.update({\"User-Agent\":user_agent}) # no freaking drama.\n# s.get(url0)\n# r = requests.post(url,body=data)\nmdata = \"d={}\".format(data)\nmheaders = {'authority': 'v.anime1.me'\n  ,'accept': '*/*' \n  ,'accept-language': 'en-US,en;q=0.9' ",
        "type": "code",
        "location": "/tests/anime1_me_video_download/get_cookie_sample.py:1-28"
    },
    "2261": {
        "file_id": 235,
        "content": "Code imports necessary libraries, sets a random user agent, defines the URL and data for API request, creates a session with the user agent as header, and formats the data for the API call.",
        "type": "comment"
    },
    "2262": {
        "file_id": 235,
        "content": "  ,'content-type': 'application/x-www-form-urlencoded' \n  ,'origin': 'https://anime1.me' \n  ,'referer': 'https://anime1.me/'}\nrpost = s.post(url,data=mdata,headers=mheaders)\n# print(dir(rpost))\nmjson2 = rpost.json()\ndownload_url = mjson2['s']['src']\ndownload_url = \"https:\"+download_url\ndownload_name = \"sample\"\ndownload_name = \"{}.{}\".format(download_name,download_url.split(\".\")[-1])\n# '{\"success\":false,\"errors\":[\"Signature invalid.\"]}' <- shit.\n# breakpoint()\n# print(rpost.text) # good. then where is the cookie?\n# print(s.cookies)\nfilename = download_name\n# print(\"downloading target file:\",filename)\n# download_object = multithread.Downloader(download_url, filename,aiohttp_args= {\"headers\":mheaders_session}) # ther e is no 'Content-Length'\n# download_object.start()\nwith open(filename, 'wb') as f:\n    # response = requests.get(url, stream=True)\n    response = s.get(download_url,stream = True)\n    total = response.headers.get('content-length')\n    if total is None:\n        f.write(response.content)\n    else:\n        downloaded = 0",
        "type": "code",
        "location": "/tests/anime1_me_video_download/get_cookie_sample.py:29-60"
    },
    "2263": {
        "file_id": 235,
        "content": "This code downloads a video from anime1.me and saves it in the specified format. It uses requests library to handle HTTP requests, extracts download URL from JSON response, sets headers for post and get requests, opens file in write mode for downloading the video, checks content length of the video, and downloads it if content length is available.",
        "type": "comment"
    },
    "2264": {
        "file_id": 235,
        "content": "        total = int(total)\n        for data in response.iter_content(chunk_size=max(int(total/1000), 1024*1024)):\n            downloaded += len(data)\n            f.write(data)\n            done = int(50*downloaded/total)\n            sys.stdout.write('\\r[{}{}]'.format('█' * done, '.' * (50-done)))\n            sys.stdout.flush()\nsys.stdout.write('\\n')\n# print(download_content.headers)\n# now you have the freaking cookie.\n# <RequestsCookieJar[<Cookie e=1652444144 for .v.anime1.me/1019/2b.mp4>, <Cookie h=oRLPqsTE0KXMFmVWJD669g for .v.anime1.me/1019/2b.mp4>, <Cookie p=eyJpc3MiOiJhbmltZTEubWUiLCJleHAiOjE2NTI0NDQxNDQwMDAsImlhdCI6MTY1MjQzNDEzNzAwMCwic3ViIjoiLzEwMTkvMmIubXA0In0 for .v.anime1.me/1019/2b.mp4>]>\n# get set-cookie header.",
        "type": "code",
        "location": "/tests/anime1_me_video_download/get_cookie_sample.py:61-72"
    },
    "2265": {
        "file_id": 235,
        "content": "This code is downloading a file and displaying the progress in real-time. It uses chunked data to manage memory efficiently and sets cookies from the response headers.",
        "type": "comment"
    },
    "2266": {
        "file_id": 236,
        "content": "/tests/anime1_me_video_download/README.md",
        "type": "filepath"
    },
    "2267": {
        "file_id": 236,
        "content": "This code instructs to extract data from the video API, send it to a specific URL, use the resulting cookie for downloading the video from another URL to avoid errors.",
        "type": "summary"
    },
    "2268": {
        "file_id": 236,
        "content": "the data is hide in the video data-api. unquote it and we will get the info. \npost data to https://v.anime1.me/api, then use responded cookie p,h with the original e(timestamp) for download.\ndownload video from https://shiro.v.anime1.me/(or elsewhere) or somehow we will get it wrong.",
        "type": "code",
        "location": "/tests/anime1_me_video_download/README.md:1-5"
    },
    "2269": {
        "file_id": 236,
        "content": "This code instructs to extract data from the video API, send it to a specific URL, use the resulting cookie for downloading the video from another URL to avoid errors.",
        "type": "comment"
    },
    "2270": {
        "file_id": 237,
        "content": "/tests/anime1_me_video_download/get_best_edm.sh",
        "type": "filepath"
    },
    "2271": {
        "file_id": 237,
        "content": "The code uses FFmpeg to extract segments from the \"edm_super_summit.m4a\" audio file, saving them as \"best_edm_split.mp3\" and \"best_edm_split2.mp3\". The -ss option specifies the start time and -to the end time for each segment.",
        "type": "summary"
    },
    "2272": {
        "file_id": 237,
        "content": "# ffmpeg -y -i edm_super_summit.m4a -ss 00:00:50 -to 00:01:05 best_edm_split.mp3\nffmpeg -y -i edm_super_summit.m4a -ss 00:01:38 -to 00:01:49 best_edm_split2.mp3",
        "type": "code",
        "location": "/tests/anime1_me_video_download/get_best_edm.sh:1-2"
    },
    "2273": {
        "file_id": 237,
        "content": "The code uses FFmpeg to extract segments from the \"edm_super_summit.m4a\" audio file, saving them as \"best_edm_split.mp3\" and \"best_edm_split2.mp3\". The -ss option specifies the start time and -to the end time for each segment.",
        "type": "comment"
    },
    "2274": {
        "file_id": 238,
        "content": "/tests/anime1_me_video_download/api_curl.sh",
        "type": "filepath"
    },
    "2275": {
        "file_id": 238,
        "content": "The code sends a POST request to anime1.me API using cURL, containing video ID, episode number, timestamp, and secret key, likely for interacting with anime videos. It also includes the \"--compressed\" flag for file compression during download, saving storage space and time.",
        "type": "summary"
    },
    "2276": {
        "file_id": 238,
        "content": "curl 'https://v.anime1.me/api' \\\n  -H 'authority: v.anime1.me' \\\n  -H 'accept: */*' \\\n  -H 'accept-language: en-US,en;q=0.9' \\\n  -H 'content-type: application/x-www-form-urlencoded' \\\n  -H 'origin: https://anime1.me' \\\n  -H 'referer: https://anime1.me/' \\\n  --data-raw 'd=%7B%22c%22%3A%221019%22%2C%22e%22%3A%226b%22%2C%22t%22%3A1652431596%2C%22p%22%3A0%2C%22s%22%3A%221bc65800b44a935eb4e5287655c64cb2%22%7D' \\\n  --compressed\n# curl 'https://v.anime1.me/api' \\\n#   -H 'authority: v.anime1.me' \\\n#   -H 'accept: */*' \\\n#   -H 'accept-language: en-US,en;q=0.9' \\\n#   -H 'content-type: application/x-www-form-urlencoded' \\\n#   -H 'origin: https://anime1.me' \\\n#   -H 'referer: https://anime1.me/' \\\n#   -H 'sec-ch-ua: \" Not A;Brand\";v=\"99\", \"Chromium\";v=\"101\"' \\\n#   -H 'sec-ch-ua-mobile: ?1' \\\n#   -H 'sec-ch-ua-platform: \"Android\"' \\\n#   -H 'sec-fetch-dest: empty' \\\n#   -H 'sec-fetch-mode: cors' \\\n#   -H 'sec-fetch-site: same-site' \\\n#   -H 'user-agent: Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.4951.41 Mobile Safari/537.36' \\",
        "type": "code",
        "location": "/tests/anime1_me_video_download/api_curl.sh:1-24"
    },
    "2277": {
        "file_id": 238,
        "content": "This code sends a POST request to 'https://v.anime1.me/api' using cURL, with specified headers and data in the request body. The request includes an API key for authentication and retrieves data from the anime1.me website.",
        "type": "comment"
    },
    "2278": {
        "file_id": 238,
        "content": "#   --data-raw 'd=%7B%22c%22%3A%221019%22%2C%22e%22%3A%226b%22%2C%22t%22%3A1652431596%2C%22p%22%3A0%2C%22s%22%3A%221bc65800b44a935eb4e5287655c64cb2%22%7D' \\\n#   --compressed\n# curl 'https://v.anime1.me/api' \\\n#   -H 'authority: v.anime1.me' \\\n#   -H 'accept: */*' \\\n#   -H 'accept-language: en-US,en;q=0.9' \\\n#   -H 'content-type: application/x-www-form-urlencoded' \\\n#   -H 'cookie: _ga=GA1.2.354375679.1652431604; _gid=GA1.2.1847563412.1652431604; _gat=1' \\\n#   -H 'origin: https://anime1.me' \\\n#   -H 'referer: https://anime1.me/' \\\n#   -H 'sec-ch-ua: \" Not A;Brand\";v=\"99\", \"Chromium\";v=\"101\"' \\\n#   -H 'sec-ch-ua-mobile: ?1' \\\n#   -H 'sec-ch-ua-platform: \"Android\"' \\\n#   -H 'sec-fetch-dest: empty' \\\n#   -H 'sec-fetch-mode: cors' \\\n#   -H 'sec-fetch-site: same-site' \\\n#   -H 'user-agent: Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.4951.41 Mobile Safari/537.36' \\\n#   --data-raw 'd=%7B%22c%22%3A%221019%22%2C%22e%22%3A%226b%22%2C%22t%22%3A1652431596%2C%22p%22%3A0%2C%22s%22%3A%221bc65800b44a935eb4e5287655c64cb2%22%7D' \\",
        "type": "code",
        "location": "/tests/anime1_me_video_download/api_curl.sh:25-43"
    },
    "2279": {
        "file_id": 238,
        "content": "This code is making an API request to 'https://v.anime1.me/api' using curl command with various headers and a data payload in JSON format. The payload contains information such as video ID, episode number, timestamp, and secret key. It seems to be fetching information or performing an action related to an anime video from the anime1.me website.",
        "type": "comment"
    },
    "2280": {
        "file_id": 238,
        "content": "#   --compressed",
        "type": "code",
        "location": "/tests/anime1_me_video_download/api_curl.sh:44-44"
    },
    "2281": {
        "file_id": 238,
        "content": "The code snippet \"--compressed\" is used to compress the file during download, which can save storage space and reduce transfer time.",
        "type": "comment"
    },
    "2282": {
        "file_id": 239,
        "content": "/tests/bilibili_login_get_credential_view_data/dump_favourite_list.py",
        "type": "filepath"
    },
    "2283": {
        "file_id": 239,
        "content": "This code loads Bilibili API credentials, fetches user information and favorite lists, processes media data, utilizes TinyDB, and interacts with bilibili_api module. It iterates through elements, extracts bvid, title, updates desc as intro, searches for existing records, and upserts data if not present or loop breaks due to no more elements.",
        "type": "summary"
    },
    "2284": {
        "file_id": 239,
        "content": "from bilibili_api import favorite_list\n# that favourite list is public. i just want that.\n# dedeuserid = \"397424026\"\n# how to?\nfrom bilibili_api.user import get_self_info\nfrom bilibili_api import sync, Credential\n# how to load credential from our stored things?\n# from bilibili_api import user\nfrom lazero.search.api import getHomeDirectory\nimport os\nimport tinydb\nhome = getHomeDirectory()\ndbPath = os.path.join(home, \".bilibili_api.json\")\nimport tinydb\ndb = tinydb.TinyDB(dbPath)\ndbFavList = tinydb.TinyDB(\"bilibiliFavouriteList.json\")\nUser = tinydb.Query()\ndedeuserid = \"397424026\"  # pass it before you do shit!\ndataList = db.search(User.dedeuserid == dedeuserid)  # this will never change i suppose?\nif len(dataList) == 1:\n    data = dataList[0].copy()\n    print(\"try to login credential fetched from db:\", data)\n    oldName = data.pop(\"name\")\n    credential = Credential(**data)\n    from bilibili_api import sync\n    name = sync(get_self_info(credential))[\"name\"]\n    if oldName != name:\n        data[\"name\"] = name\n        db.upsert(data, User.dedeuserid == dedeuserid)",
        "type": "code",
        "location": "/tests/bilibili_login_get_credential_view_data/dump_favourite_list.py:1-34"
    },
    "2285": {
        "file_id": 239,
        "content": "This code aims to load the Bilibili API credentials from stored data and retrieve the user's information using a provided dedeuserid. It utilizes TinyDB for database operations, fetches the home directory, and interacts with the bilibili_api module. If a credential is found in the database, it will update the \"name\" field if necessary and print the retrieved credential information.",
        "type": "comment"
    },
    "2286": {
        "file_id": 239,
        "content": "    print(\"login successful:\", name)\n    # now you have it.\n    result = sync(\n        favorite_list.get_video_favorite_list(int(dedeuserid), None, credential)\n    )\n    print(result)  # None? wtf?\n    favLists = result[\"list\"]\n    for favList in favLists:\n        listId = favList[\"id\"]  # integer.\n        listName = favList[\"title\"]\n        print(\"processing favList:\", listName)\n        page = 0\n        while True:\n            import time\n            time.sleep(3)\n            page += 1\n            print(\"processing page:\", page)\n            result = sync(\n                favorite_list.get_video_favorite_list_content(\n                    listId, page=page, credential=credential\n                )\n            )\n            # import pprint\n            # pprint.pprint(result)\n            has_more = result[\"has_more\"]\n            # print(\"__________result__________\")\n            medias = result[\"medias\"]\n            if type(medias) != list or len(medias) == 0:\n                break\n            breakFlag = False\n            for elem in medias:",
        "type": "code",
        "location": "/tests/bilibili_login_get_credential_view_data/dump_favourite_list.py:35-66"
    },
    "2287": {
        "file_id": 239,
        "content": "Code is fetching user's favorite lists from Bilibili and processing each list's contents page by page. It checks for more content using \"has_more\" flag, fetches media data from the server, and breaks the loop when no more content is available or if the media data type is incorrect.",
        "type": "comment"
    },
    "2288": {
        "file_id": 239,
        "content": "                # print('ELEM:',elem)\n                # breakpoint()\n                # it has description.\n                videoData = {key: elem[key] for key in [\"bvid\", \"title\"]}\n                # here we call 'desc' as 'intro.\n                videoData.update({\"desc\": elem[\"intro\"]})\n                searchResult= dbFavList.search(User.bvid == videoData[\"bvid\"])\n                if len(searchResult) != 0:\n                    breakFlag=True\n                dbFavList.upsert(videoData, User.bvid == videoData[\"bvid\"])\n            if not has_more or breakFlag:\n                break",
        "type": "code",
        "location": "/tests/bilibili_login_get_credential_view_data/dump_favourite_list.py:67-78"
    },
    "2289": {
        "file_id": 239,
        "content": "This code iterates through elements, extracts bvid and title, updates with intro as desc, searches for existing records, upserts data if not already present or if the loop breaks due to no more elements.",
        "type": "comment"
    },
    "2290": {
        "file_id": 240,
        "content": "/tests/bilibili_login_get_credential_view_data/tinydb_test.py",
        "type": "filepath"
    },
    "2291": {
        "file_id": 240,
        "content": "This code imports the tinydb library and sets a database location. It creates a TinyDB object at that location, which serves as a lightweight NoSQL document database. The code defines a User query using the tinydb.Query() function and performs an upsert operation on the database, where it either updates or inserts a document based on the provided condition (User.ghi == 123). Note that there is also a commented-out section for updating the database without an insertion operation.",
        "type": "summary"
    },
    "2292": {
        "file_id": 240,
        "content": "import tinydb\ndbLocation = \"test_credential.json\"\ndb = tinydb.TinyDB(dbLocation)\n# table = db.table('mytable')\nUser = tinydb.Query()\ndb.upsert({\"abc\": \"def\", \"ghi\": 123}, User.ghi == 123)  # please specify a condition!\n# db.update({'abc': 'def', 'ghi': 123}) # no insert here!",
        "type": "code",
        "location": "/tests/bilibili_login_get_credential_view_data/tinydb_test.py:1-8"
    },
    "2293": {
        "file_id": 240,
        "content": "This code imports the tinydb library and sets a database location. It creates a TinyDB object at that location, which serves as a lightweight NoSQL document database. The code defines a User query using the tinydb.Query() function and performs an upsert operation on the database, where it either updates or inserts a document based on the provided condition (User.ghi == 123). Note that there is also a commented-out section for updating the database without an insertion operation.",
        "type": "comment"
    },
    "2294": {
        "file_id": 241,
        "content": "/tests/bilibili_login_get_credential_view_data/test_login_successful.py",
        "type": "filepath"
    },
    "2295": {
        "file_id": 241,
        "content": "The code retrieves user credentials from a local database and uses them to log in to Bilibili API. It checks if the user exists, updates the name if necessary, and prints the login status.",
        "type": "summary"
    },
    "2296": {
        "file_id": 241,
        "content": "from bilibili_api.user import get_self_info\nfrom bilibili_api import Credential\n# how to load credential from our stored things?\nfrom lazero.search.api import getHomeDirectory\nimport os\nimport tinydb\nhome = getHomeDirectory()\ndbPath = os.path.join(home, \".bilibili_api.json\")\nimport tinydb\ndb = tinydb.TinyDB(dbPath)\nUser = tinydb.Query()\ndedeuserid = \"397424026\"  # pass it before you do shit!\ndataList = db.search(User.dedeuserid == dedeuserid)  # this will never change i suppose?\nif len(dataList) == 1:\n    data = dataList[0].copy()\n    print(\"try to login credential fetched from db:\", data)\n    oldName = data.pop(\"name\")\n    credential = Credential(**data)\n    from bilibili_api import sync\n    name = sync(get_self_info(credential))[\"name\"]\n    if oldName != name:\n        data[\"name\"] = name\n        db.upsert(data, User.dedeuserid == dedeuserid)\n    print(\"login successful:\", name)",
        "type": "code",
        "location": "/tests/bilibili_login_get_credential_view_data/test_login_successful.py:1-29"
    },
    "2297": {
        "file_id": 241,
        "content": "The code retrieves user credentials from a local database and uses them to log in to Bilibili API. It checks if the user exists, updates the name if necessary, and prints the login status.",
        "type": "comment"
    },
    "2298": {
        "file_id": 242,
        "content": "/tests/bilibili_login_get_credential_view_data/test_login_fail.py",
        "type": "filepath"
    },
    "2299": {
        "file_id": 242,
        "content": "This code retrieves user credentials, logs in using session data, and updates the name if it changed; attempts logging in with expired data to check for failure.",
        "type": "summary"
    }
}