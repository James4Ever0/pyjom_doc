{
    "3600": {
        "file_id": 443,
        "content": "    # stream = ffmpeg.concat(stream_0, stream_1, stream_2)\n    stream = ffmpeg.output(video_stream, audio_stream,\"pipCrop.mp4\")\n    stream.run(overwrite_output=True)\n    # stream = ffmpeg.concat(stream_0.video, stream_0.audio, stream_1.video, stream_1.audio, stream_2.video, stream_2.audio, v=1, a=1)\n    # # there is no audio down here! fuck.\n    # stream = ffmpeg.output(stream,\"pipCrop.mp4\")\n    # stream.run(overwrite_output=True)\ndef concatVideoWithAudio():\n    stream_0 = ffmpeg.input(\"output.mp4\",ss=0, t=3)\n    stream_1 = ffmpeg.input(\"output.mp4\",ss=3, t=6)\n    stream = ffmpeg.concat(stream_0.video, stream_0.audio, stream_1.video, stream_1.audio, v=1, a=1)\n    # print(stream)\n    # breakpoint()\n    stream = ffmpeg.output(stream, \"concatVideo.mp4\")\n    # print(stream.get_args())\n    stream.run(overwrite_output=True)\ndef delogoTest():\n    from MediaInfo import MediaInfo\n    info = MediaInfo(filename = 'output.mp4')\n    infoData = info.getInfo()\n    # print(infoData)\n    # breakpoint()\n    defaultWidth = infoData[\"videoWidth\"]",
        "type": "code",
        "location": "/tests/ffmpeg_python_test/test.py:68-96"
    },
    "3601": {
        "file_id": 443,
        "content": "This code concatenates videos and audio streams using the FFmpeg library. It merges video and audio from separate inputs, then outputs the resulting stream to a file. The code also includes functions for MediaInfo to retrieve information about a media file.",
        "type": "comment"
    },
    "3602": {
        "file_id": 443,
        "content": "    defaultHeight = infoData[\"videoHeight\"]\n    import math\n    stream_0 = ffmpeg.input(\"output.mp4\", ss=0, to=3)\n    x,y,width, height = getRandomCrop(defaultWidth,defaultHeight) # get our delogo area.\n    stream_0_video = stream_0.video.filter(\"delogo\", x=x, y=y, w=width, h=height, show=1)\n    stream_0_audio = stream_0.audio\n    stream_1 = ffmpeg.input(\"output.mp4\", ss=3, to=6)\n    x,y,width, height = getRandomCrop(defaultWidth,defaultHeight) # get our delogo area.\n    stream_1_video = stream_1.video.filter(\"delogo\", x=x, y=y, w=width, h=height, show=1)\n    x,y,width, height = getRandomCrop(defaultWidth,defaultHeight) # get our delogo area.\n    stream_1_video = stream_1_video.filter(\"delogo\", x=x, y=y, w=width, h=height, show=1)\n    stream_1_audio = stream_1.audio\n    # we must specify the time first.\n    # it is like a compiler! ffmpeg commandline (also its library, mind-blowingly crazy and complex) really sucks. thanks, ffmpeg-python wrapper.\n    video_stream = ffmpeg.concat(stream_0_video, stream_1_video)",
        "type": "code",
        "location": "/tests/ffmpeg_python_test/test.py:97-114"
    },
    "3603": {
        "file_id": 443,
        "content": "Code snippet takes input video \"output.mp4\", crops and overlays delogo in different positions, concatenates the two resulting videos with a 3-second overlap, and assigns audio streams. The comment about ffmpeg commandline complexity reflects frustration with its API.",
        "type": "comment"
    },
    "3604": {
        "file_id": 443,
        "content": "    audio_stream = ffmpeg.concat(stream_0_audio, stream_1_audio, v=0,a=1)\n    stream = ffmpeg.output(video_stream, audio_stream,\"delogoTest.mp4\")\n    stream.run(overwrite_output=True)\nif __name__ == \"__main__\":\n    # cropVideoRegion()\n    # concatVideoWithAudio() # damn quiet out there.\n    delogoTest()",
        "type": "code",
        "location": "/tests/ffmpeg_python_test/test.py:115-122"
    },
    "3605": {
        "file_id": 443,
        "content": "The code is using the ffmpeg library to concatenate two audio streams (stream_0_audio and stream_1_audio) and then output the resulting video stream with the audio stream to a file named \"delogoTest.mp4\". The overwrite_output parameter ensures that if the file already exists, it will be overwritten. This code is part of the delogoTest() function, which is being executed if the script is run as the main program.",
        "type": "comment"
    },
    "3606": {
        "file_id": 444,
        "content": "/tests/neo4j_cypher_builder_template_why_you_suddenly_want_to_create_exceptions_and_find_solutions_to_hot_fix_reloading_and_edit_and_continue/thread_based_program.py",
        "type": "filepath"
    },
    "3607": {
        "file_id": 444,
        "content": "This Python code creates a multithreaded, event-driven program using threading. It starts two threads for main execution and event handling, checking if the event breaks the loop. This basic approach demonstrates multithreading with event-based communication in Python.\n\nSummary in 30 words: Python code utilizes multithreading and event-driven programming to create a program with two threads, one for main execution and another for event handling, checking if an event breaks the loop, showcasing basic approach for multithreading communication.",
        "type": "summary"
    },
    "3608": {
        "file_id": 444,
        "content": "import threading\nevent = threading.Event()\nevent.clear()\n# is it event driven? can we launch repl after this?\ndef program(*args): # in elixir/erlang this is simpler.\n    print('running program')\n    while True:\n        if event.wait(0.00000001):\n            break # this is blocking. fuck. not like elixir in any kind.\n        else:\n            event.set()\n    event.clear()\n    print('begin execution')\n    print(\"arguments:\", args)\n    raise Exception('shit man')\n    event.set()\n    result = 'myresult'\ndef mainThread():\n    threading.Thread(target=program, args=(1,2), daemon=True).start()\n    print('waiting output? probably never.')\n    while True:\n        if event.wait(0.00000001):\n            break # are you sure this is the event you want?\n        else:\n            event.set()\n    print('result:',result) # another thread? are you sharing things?\n    print('main thread execution succeed')\nprint('starting main thread')\nthreading.Thread(target=mainThread, daemon=True).run()\nprint('starting repl')\n# be ready to re-execute the program?",
        "type": "code",
        "location": "/tests/neo4j_cypher_builder_template_why_you_suddenly_want_to_create_exceptions_and_find_solutions_to_hot_fix_reloading_and_edit_and_continue/thread_based_program.py:1-35"
    },
    "3609": {
        "file_id": 444,
        "content": "This code creates an event-driven, multithreaded program using Python's threading module. It starts two threads: one for the main program execution and another for handling events. The main program runs indefinitely, checking if the event is set to break out of the loop. The main thread also sets the result variable. This code demonstrates a basic approach to multithreaded programming in Python with event-driven communication between threads.",
        "type": "comment"
    },
    "3610": {
        "file_id": 444,
        "content": "# do you want something like nodejs promises?\n# how to reload foreign files? fuck?",
        "type": "code",
        "location": "/tests/neo4j_cypher_builder_template_why_you_suddenly_want_to_create_exceptions_and_find_solutions_to_hot_fix_reloading_and_edit_and_continue/thread_based_program.py:36-37"
    },
    "3611": {
        "file_id": 444,
        "content": "This code snippet seems to express frustration about handling promises in Node.js and the difficulty of reloading foreign files.",
        "type": "comment"
    },
    "3612": {
        "file_id": 445,
        "content": "/tests/neo4j_cypher_builder_template_why_you_suddenly_want_to_create_exceptions_and_find_solutions_to_hot_fix_reloading_and_edit_and_continue/sql_inline.py",
        "type": "filepath"
    },
    "3613": {
        "file_id": 445,
        "content": "The code is attempting to utilize the chalk library for creating SQL queries. It first defines a string \"a\" as a SELECT query and \"b\" as a CREATE query in Cypher format. Then, it imports the required modules from JavaScript and initializes the chalk library using \"./cypher_inline.js\". The code then creates an instance of chalk's Query class (q) and calls its myfunc method with some arguments. Finally, it prints the VALUE and type of both 'a' and 'b'.",
        "type": "summary"
    },
    "3614": {
        "file_id": 445,
        "content": "a = \"select * from user\"\nb = \"create (n:person)\"  # cypher # not working!\nfrom javascript import require, globalThis\nchalk = require(\n    \"./cypher_inline.js\"chr\n)  # that might be some drop-in replacement for jinja? should they work together?\n# print(dir(chalk))\n# what the fuck?\nq = chalk.Query # use static method this time?\n# q = chalk.Query(1,2)\nval = q.myfunc(dict(somearg=1)) # this is similar to the original shit.\n# myfunc args: [ { somearg: 1 } ]\n# good?\n# val = chalk.myfunc()\nprint(\"VALUE\", list(val), type(val))  # it can be converted.\nval = q.otherfunc()\n# val = chalk.otherfunc()\nprint(\"VALUE\", val, type(val))  # it can be converted.",
        "type": "code",
        "location": "/tests/neo4j_cypher_builder_template_why_you_suddenly_want_to_create_exceptions_and_find_solutions_to_hot_fix_reloading_and_edit_and_continue/sql_inline.py:1-22"
    },
    "3615": {
        "file_id": 445,
        "content": "The code is attempting to utilize the chalk library for creating SQL queries. It first defines a string \"a\" as a SELECT query and \"b\" as a CREATE query in Cypher format. Then, it imports the required modules from JavaScript and initializes the chalk library using \"./cypher_inline.js\". The code then creates an instance of chalk's Query class (q) and calls its myfunc method with some arguments. Finally, it prints the VALUE and type of both 'a' and 'b'.",
        "type": "comment"
    },
    "3616": {
        "file_id": 446,
        "content": "/tests/neo4j_cypher_builder_template_why_you_suddenly_want_to_create_exceptions_and_find_solutions_to_hot_fix_reloading_and_edit_and_continue/mtest.py",
        "type": "filepath"
    },
    "3617": {
        "file_id": 446,
        "content": "The code defines an exception named \"my exception\" and lists the available attributes and methods for this exception class. It also defines a function named \"shit\" that raises an Exception with the message 'shit' and returns the string \"value\".",
        "type": "summary"
    },
    "3618": {
        "file_id": 446,
        "content": "e = Exception(\"my exception\")\n# print(dir(e))\n# ['__cause__', '__class__', '__context__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__suppress_context__', '__traceback__', 'args', 'with_traceback']\ndef shit():\n    raise Exception('shit')\n    return \"value\"",
        "type": "code",
        "location": "/tests/neo4j_cypher_builder_template_why_you_suddenly_want_to_create_exceptions_and_find_solutions_to_hot_fix_reloading_and_edit_and_continue/mtest.py:1-7"
    },
    "3619": {
        "file_id": 446,
        "content": "The code defines an exception named \"my exception\" and lists the available attributes and methods for this exception class. It also defines a function named \"shit\" that raises an Exception with the message 'shit' and returns the string \"value\".",
        "type": "comment"
    },
    "3620": {
        "file_id": 447,
        "content": "/tests/neo4j_cypher_builder_template_why_you_suddenly_want_to_create_exceptions_and_find_solutions_to_hot_fix_reloading_and_edit_and_continue/hy_repl_normal.py",
        "type": "filepath"
    },
    "3621": {
        "file_id": 447,
        "content": "The code imports the HyREPL module from hy.cmdline and initializes an instance of it called \"repl\". It prints a message before running the REPL, then runs the REPL using the repl.run() method. After that, it prints another message after the REPL. The code aims to demonstrate a normal usage of HyREPL.",
        "type": "summary"
    },
    "3622": {
        "file_id": 447,
        "content": "import hy.cmdline\n# this is different. no access to hidden member.\nprint('message before repl')\nrepl = hy.cmdline.HyREPL() # this is not reliable. exit will exit this shit for good.\nrepl.run()\nprint('message after repl')\n# no message after repl?",
        "type": "code",
        "location": "/tests/neo4j_cypher_builder_template_why_you_suddenly_want_to_create_exceptions_and_find_solutions_to_hot_fix_reloading_and_edit_and_continue/hy_repl_normal.py:1-7"
    },
    "3623": {
        "file_id": 447,
        "content": "The code imports the HyREPL module from hy.cmdline and initializes an instance of it called \"repl\". It prints a message before running the REPL, then runs the REPL using the repl.run() method. After that, it prints another message after the REPL. The code aims to demonstrate a normal usage of HyREPL.",
        "type": "comment"
    },
    "3624": {
        "file_id": 448,
        "content": "/tests/neo4j_cypher_builder_template_why_you_suddenly_want_to_create_exceptions_and_find_solutions_to_hot_fix_reloading_and_edit_and_continue/cypher_inline.js",
        "type": "filepath"
    },
    "3625": {
        "file_id": 448,
        "content": "This code defines two functions for creating SQL statements and a Query class with unclear inline function usage, potentially due to JavaScript's inconsistent behavior. It also logs undefined variables to the console.",
        "type": "summary"
    },
    "3626": {
        "file_id": 448,
        "content": "// https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Template_literals\n// official javascript driver\n// https://neo4j.com/developer/javascript/\nvar cypher = function(strArray, ...opts) { // this is bad.\n    console.log('input:', strArray, opts) // here we've got the thing.\n    // passed here. good.\n    // this will be good.\n    // suppose we put string, object into this thing.\n    // suppose we quote the thing.\n    return strArray;\n}\nvar sql = function(str) { return str; }\nvar myexpression = {obj:2}; // not supplied to cypher?\n// create (n)-[:married]->(r) [object Object]\n// wtf?\nvar myexpression2 = '3';\nvar b = `create (n)-[:married]->(r) ${myexpression}`\nconsole.log(b) // create (n)-[:married]->(r) 2\n    // this will format the thing.\nvar a = cypher `create (n:person{name:${myexpression}})-[:married]->(r) ${myexpression2}`; // well that's good.\nconsole.log(a);\nconst query = sql `SELECT * FROM users`;\nconsole.log(query);\n// function otherfunc(){\n//     console.log('calling otherfunc')\n//     return 'other func'",
        "type": "code",
        "location": "/tests/neo4j_cypher_builder_template_why_you_suddenly_want_to_create_exceptions_and_find_solutions_to_hot_fix_reloading_and_edit_and_continue/cypher_inline.js:2-28"
    },
    "3627": {
        "file_id": 448,
        "content": "The code defines two functions, `cypher` and `sql`, which create SQL statements using template literals. The `cypher` function takes a string array and optional parameters, while the `sql` function only takes a single string parameter. The code demonstrates how to use these functions by creating a Cypher query with placeholders for dynamic values and executing a SELECT query.",
        "type": "comment"
    },
    "3628": {
        "file_id": 448,
        "content": "// }\n// function myfunc() {\n//     otherfunc()\n//     return query;\n// }\n// // __export__\n// // console.log(module.loaded) // false\n// // export all functions?\n// module.exports = {otherfunc, myfunc} // also some bloated shit.\n// which one you want? damn...\n// you want some object?\n// what if they are interdependent?\n// this is some other strange shit.\n// exports = {\n//         otherfunc: () => {\n//             console.log('calling otherfunc');\n//             return 'otherfunc'\n//         },\n//         myfunc: () => {\n//             exports.otherfunc() // strange shit.\n//             return query;\n//         }\n//     }\n//     // console.log(module)\n// module.exports = exports // must use this to export things.\n// this is self-reference.\nclass Query {\n    constructor(a, b) {\n        this.a = a\n        this.b = b\n    }\n    static otherfunc() {\n        // otherfunc() {\n        console.log('calling otherfunc');\n        return 'otherfunc'\n    }\n    static myfunc(...args) {\n        // static myfunc() {\n            console.log('myfunc args:',args)",
        "type": "code",
        "location": "/tests/neo4j_cypher_builder_template_why_you_suddenly_want_to_create_exceptions_and_find_solutions_to_hot_fix_reloading_and_edit_and_continue/cypher_inline.js:29-71"
    },
    "3629": {
        "file_id": 448,
        "content": "The code defines a class \"Query\" with two static methods: \"otherfunc\" and \"myfunc\". \"otherfunc\" is called within \"myfunc\", creating an interdependence between the two functions. The code exports the entire class, using self-reference for the static methods.",
        "type": "comment"
    },
    "3630": {
        "file_id": 448,
        "content": "        Query.otherfunc() // strange shit.\n            // this.otherfunc() // still working for static functions.\n            // javascript is a beast.\n        return query;\n    }\n}\nmodule.exports = { Query }\n    // console.log(.cypher)\n    // console.log('QUERY?',globalThis.Query, this.sql) // all undefinded.",
        "type": "code",
        "location": "/tests/neo4j_cypher_builder_template_why_you_suddenly_want_to_create_exceptions_and_find_solutions_to_hot_fix_reloading_and_edit_and_continue/cypher_inline.js:72-80"
    },
    "3631": {
        "file_id": 448,
        "content": "This code defines a Query class with a strange inline function call and exports it. The otherfunc() is called inside the Query class, but its purpose is unclear. JavaScript's behavior for static functions versus instance methods seems inconsistent here. The code also logs variables to the console, but they are all undefined.",
        "type": "comment"
    },
    "3632": {
        "file_id": 449,
        "content": "/tests/neo4j_cypher_builder_template_why_you_suddenly_want_to_create_exceptions_and_find_solutions_to_hot_fix_reloading_and_edit_and_continue/babel_decorator.js",
        "type": "filepath"
    },
    "3633": {
        "file_id": 449,
        "content": "This code defines a decorator function using Babel's plugin for JavaScript syntax decorators. It wraps a function, logs the arguments passed to it, and then calls the original function. The decorated function, `myfunc`, is called with 'myval', and its return value is logged to the console.",
        "type": "summary"
    },
    "3634": {
        "file_id": 449,
        "content": "// require(\"@babel/core\").transformSync(\"code\", {\n//     plugins: [\"@babel/plugin-syntax-decorators\"]\n//   });\nfunction dec(func){\n    function innerfunc(...args){\n        console.log('calling func with args:', args)\n        func(...args)\n    }\n    return innerfunc\n}\n@dec\nfunction myfunc(val){\n    return val\n}\nval = myfunc('myval')\nconsole.log('val:', val)",
        "type": "code",
        "location": "/tests/neo4j_cypher_builder_template_why_you_suddenly_want_to_create_exceptions_and_find_solutions_to_hot_fix_reloading_and_edit_and_continue/babel_decorator.js:2-19"
    },
    "3635": {
        "file_id": 449,
        "content": "This code defines a decorator function using Babel's plugin for JavaScript syntax decorators. It wraps a function, logs the arguments passed to it, and then calls the original function. The decorated function, `myfunc`, is called with 'myval', and its return value is logged to the console.",
        "type": "comment"
    },
    "3636": {
        "file_id": 450,
        "content": "/tests/neo4j_cypher_builder_template_why_you_suddenly_want_to_create_exceptions_and_find_solutions_to_hot_fix_reloading_and_edit_and_continue/my_module/some_module.py",
        "type": "filepath"
    },
    "3637": {
        "file_id": 450,
        "content": "The code defines a function 'program' within the module 'some_module' that raises an exception. If the file is run directly, it enters a loop that attempts to execute the 'program' function repeatedly, reloading the module after each failure in order to hot fix issues and apply edits while continuing execution.",
        "type": "summary"
    },
    "3638": {
        "file_id": 450,
        "content": "# inside some_module.py\ndef program():\n    raise Exception(\"Exception in program\")\n    # return \"VALUE\"\nif __name__ == \"__main__\":\n    while True:\n        try:\n            import some_module # will it even succeed? doubt this.\n            val = some_module.program()\n            print(\"returned value:\", val)\n            break\n        except:\n            import traceback\n            traceback.print_exc()\n            input('are you done yet?')\n            import importlib\n            importlib.reload(some_module)",
        "type": "code",
        "location": "/tests/neo4j_cypher_builder_template_why_you_suddenly_want_to_create_exceptions_and_find_solutions_to_hot_fix_reloading_and_edit_and_continue/my_module/some_module.py:1-19"
    },
    "3639": {
        "file_id": 450,
        "content": "The code defines a function 'program' within the module 'some_module' that raises an exception. If the file is run directly, it enters a loop that attempts to execute the 'program' function repeatedly, reloading the module after each failure in order to hot fix issues and apply edits while continuing execution.",
        "type": "comment"
    },
    "3640": {
        "file_id": 451,
        "content": "/tests/mmd_human_dance_pose/test_detection_yolo.py",
        "type": "filepath"
    },
    "3641": {
        "file_id": 451,
        "content": "Importing necessary libraries and setting proxy environments to ensure proper model loading. Loading the YOLOv5 model from a local directory, and performing inference on an image file. Saving and displaying results. Some confusion regarding the 'bird' detection category.",
        "type": "summary"
    },
    "3642": {
        "file_id": 451,
        "content": "import torch\nimport os\nos.environ[\"http_proxy\"] = \"\"\nos.environ[\"https_proxy\"] = \"\"\n# Model\nlocalModelDir = '/root/Desktop/works/pyjom/pyjom/models/yolov5/ultralytics_yolov5_master/'\n# import os\nos.environ[\"YOLOV5_MODEL_DIR\"] = '/root/Desktop/works/pyjom/pyjom/models/yolov5/' # this is strange. must be a hack in the localModelDir\nmodel = torch.hub.load(localModelDir, 'yolov5s',source=\"local\")  # or yolov5m, yolov5l, yolov5x, custom\n# Images\nimg = '/media/root/help/pyjom/samples/image/miku_on_green.png'  # or file, Path, PIL, OpenCV, numpy, list\n# Inference\nresults = model(img)\n# Results\n# results.print() # or .show(),\nresults.save()\n# print(type(results),dir(results))\n# breakpoint()\nimport cv2\nimage = cv2.imread(\"runs/detect/exp3/miku_on_green.jpg\")\ncv2.imshow(\"NONE\",image)\n# results.print()  # or .show(),\n# hold it.\n# image 1/1: 720x1280 1 bird # what the fuck is a bird?\n# os.system(\"pause\")\n# input()\n# this shit has been detected but not in the right category.",
        "type": "code",
        "location": "/tests/mmd_human_dance_pose/test_detection_yolo.py:1-32"
    },
    "3643": {
        "file_id": 451,
        "content": "Importing necessary libraries and setting proxy environments to ensure proper model loading. Loading the YOLOv5 model from a local directory, and performing inference on an image file. Saving and displaying results. Some confusion regarding the 'bird' detection category.",
        "type": "comment"
    },
    "3644": {
        "file_id": 452,
        "content": "/tests/motion_vector_estimation/test.sh",
        "type": "filepath"
    },
    "3645": {
        "file_id": 452,
        "content": "This command runs a Python script (extract_mvs.py) using Python 3.10, processing the video file vid_h264.mp4. It includes options for previewing output and verbose logging.",
        "type": "summary"
    },
    "3646": {
        "file_id": 452,
        "content": "bash ./run.sh python3.10 extract_mvs.py vid_h264.mp4 --preview --verbose",
        "type": "code",
        "location": "/tests/motion_vector_estimation/test.sh:1-1"
    },
    "3647": {
        "file_id": 452,
        "content": "This command runs a Python script (extract_mvs.py) using Python 3.10, processing the video file vid_h264.mp4. It includes options for previewing output and verbose logging.",
        "type": "comment"
    },
    "3648": {
        "file_id": 453,
        "content": "/tests/motion_vector_estimation/optical_flow_colored_blocks_convolution.py",
        "type": "filepath"
    },
    "3649": {
        "file_id": 453,
        "content": "The code initializes motion vector estimation, filters horizontal movement, improves accuracy using various techniques, processes motion vectors from coordinates, visualizes motion with OpenCV, creates bounding boxes, and handles further options. The code plots multiple sets of data onto a graph, iterating through lists using nested for loops, creating separate plots if desired, and then displays the graph.",
        "type": "summary"
    },
    "3650": {
        "file_id": 453,
        "content": "###################################################\n# aim to create optical flow here, with directions and convolution\n###################################################\n# it contains subpixel motion vectors. fucking hell\n# source = \"/root/Desktop/works/pyjom/samples/video/dog_with_text.mp4\"\n# change source?\n# gif containers does not have motion vectors.\n# source = \"/root/Desktop/works/pyjom/samples/video/cat_invalid_eye_rolling.gif\"\n# source = \"/root/Desktop/works/pyjom/samples/video/kitty_flash_15fps.gif\"\n# without mestimate\n# source = \"/root/Desktop/works/pyjom/samples/video/cat_invalid_eye_rolling_without_mestimate.mp4\"\n# source = \"/root/Desktop/works/pyjom/samples/video/kitty_flash_15fps_without_mestimate.mp4\"\n# with mestimate\n# source = \"/root/Desktop/works/pyjom/samples/video/cat_invalid_eye_rolling_with_mestimate.mp4\"\n# source = \"/root/Desktop/works/pyjom/samples/video/kitty_flash_15fps_with_mestimate.mp4\"\n# source = \"/root/Desktop/works/pyjom/samples/video/nearly_duplicate_frames_detection_30fps.mp4\"",
        "type": "code",
        "location": "/tests/motion_vector_estimation/optical_flow_colored_blocks_convolution.py:1-22"
    },
    "3651": {
        "file_id": 453,
        "content": "This code aims to create optical flow with motion vectors and convolution, using video files as input. The source file changes depending on the specific test case (with or without mestimate, different videos).",
        "type": "comment"
    },
    "3652": {
        "file_id": 453,
        "content": "source = \"/root/Desktop/works/pyjom/samples/video/cute_cat_gif.mp4\"\nfrom lazero.utils.importers import cv2_custom_build_init\n# from sniffio import current_async_library\ncv2_custom_build_init()\nfrom mvextractor.videocap import VideoCap\nfrom caer.video.frames_and_fps import count_frames, get_res\nimport cv2\nframesCount = count_frames(source)\nres = get_res(source)  # (width, height)\nprint(\"RES: %s\" % str(res))\nres_x, res_y = res\nframe_common_divisor = min(res_x, res_y)\nimport math\ndef cartesianDistance(d2vector):\n    try:\n        x, y = d2vector\n        return math.sqrt(x**2 + y**2)\n    except:\n        print('item unpackable.', d2vector)\n        return 0\ndef XYWHToDiagonal(x, y, w, h):\n    return (x, y), (x + w, y + h)\n# 如果整除16那么就在这个范围里面 如果不整除范围就要扩大 扩大到相应的16的倍数\ndef get16Value(res_x):\n    rem_x = res_x % 16\n    val = res_x // 16\n    if rem_x != 0:\n        val += 1\n    return val\nx_16val = get16Value(res_x)\ny_16val = get16Value(res_y)\nmotion_render_frame = (x_16val * 16, y_16val * 16)\ntotal_block_weights = x_16val * y_16val * 2 * 2",
        "type": "code",
        "location": "/tests/motion_vector_estimation/optical_flow_colored_blocks_convolution.py:24-70"
    },
    "3653": {
        "file_id": 453,
        "content": "This code initializes necessary libraries and imports, gets the resolution of a video source, calculates the frame count, and sets up variables for motion vector estimation. The functions cartesianDistance and XYWHToDiagonal are defined for spatial calculations, and get16Value is used to ensure the resolution is a multiple of 16 for the motion vector estimation process. The total number of block weights is calculated based on the video's resolution.",
        "type": "comment"
    },
    "3654": {
        "file_id": 453,
        "content": "cap = VideoCap()\ncap.open(source)  # wtf is going on here?\n# if there is nothing we will breakup\n# visualize, show_picture = True, True\nvisualize, show_picture = False, False\n# so there can only be one such macroblock\ndef checkMacroBlock(value):\n    for mod in [16, 8]:\n        modValue = value % mod\n        if modValue == mod / 2:\n            return mod\n    # if not satisfied, we are shit.\nfrom functools import lru_cache\n@lru_cache(maxsize=4)\ndef getModXModYFromBlockCenterCoordinates(blockCenterCoordinates):\n    block_x, block_y = blockCenterCoordinates\n    mod_x, mod_y = checkMacroBlock(block_x), checkMacroBlock(block_y)\n    if mod_x is not None and mod_y is not None:\n        return mod_x, mod_y\n    else:\n        print(\"block center coordinates\", blockCenterCoordinates)\n        print(\"WTF IS GOING ON WITH THE BLOCK CENTER\")\n        breakpoint()\n        return 0, 0\ndef getRectangleXYWHFromBlockCenterCoordinates(blockCenterCoordinates):\n    block_x, block_y = blockCenterCoordinates\n    mod_x, mod_y = getModXModYFromBlockCenterCoordinates(blockCenterCoordinates)",
        "type": "code",
        "location": "/tests/motion_vector_estimation/optical_flow_colored_blocks_convolution.py:72-106"
    },
    "3655": {
        "file_id": 453,
        "content": "The code initializes a VideoCap object and opens a specified source. It then defines two functions: `checkMacroBlock` to determine the macroblock size based on given values, and `getModXModYFromBlockCenterCoordinates` to get the modX and modY from block center coordinates using `checkMacroBlock`. The code also includes error handling in case of unexpected block center coordinates.",
        "type": "comment"
    },
    "3656": {
        "file_id": 453,
        "content": "    mod_x_half, mod_y_half = mod_x / 2, mod_y / 2\n    x, y, w, h = block_x - mod_x_half, block_y - mod_y_half, mod_x, mod_y\n    return tuple([int(elem) for elem in [x, y, w, h]])\ndef getBlockWeightFromBlockCenterCoordinates(blockCenterCoordinates):\n    mod_x, mod_y = getModXModYFromBlockCenterCoordinates(blockCenterCoordinates)\n    weights = mod_x * mod_y / 8 / 8\n    return weights\nimport progressbar\nimport numpy as np\n# max_dst_x, max_dst_y = 0,0\ndef averageMotionVectors(motion_vector_list):\n    if len(motion_vector_list) == 0:\n        average_tuple = (0, 0)\n    if len(motion_vector_list) > 1:\n        marray = np.array(motion_vector_list)\n        # print(\"MAKING AVERAGE:\")\n        # print(marray)\n        average = np.average(marray, axis=0)\n        # breakpoint()\n        average_tuple = tuple(average)\n    else:\n        average_tuple = tuple(motion_vector_list[0])\n    return average_tuple\nmotion_area_ratio_array = []\n# average_weighted_motion_vector_array = []\n# average_global_weighted_motion_vector_array = []\naverage_weighted_motion_vector_cartesian_array = []",
        "type": "code",
        "location": "/tests/motion_vector_estimation/optical_flow_colored_blocks_convolution.py:107-142"
    },
    "3657": {
        "file_id": 453,
        "content": "Function `getBlockWeightFromBlockCenterCoordinates` calculates the weight of a block based on its center coordinates.\nThe `averageMotionVectors` function calculates the average motion vector from a list of motion vectors.\n`motion_area_ratio_array` is used to store area ratios for blocks, which will be used in calculations later.",
        "type": "comment"
    },
    "3658": {
        "file_id": 453,
        "content": "average_global_weighted_motion_vector_cartesian_array = []\naverage_weighted_motion_vectors_filtered_cartesian_distance_array = []\naverage_global_weighted_motion_vectors_filtered_cartesian_distance_array = []\nfor _ in progressbar.progressbar(range(framesCount)):\n    success, frame, motion_vectors, frame_type, timestamp = cap.read()\n    height, width, channels = frame.shape\n    # breakpoint()\n    if success:\n        # what is the content of this motion vector?\n        # print(motion_vectors)\n        # import pandas as pd\n        # df = pd.DataFrame(motion_vectors)\n        # df = pd.DataFrame(motion_vectors,index=['source_index','unk0','unk1','src_x','src_y','dst_x','dst_y','motion_x','motion_y','motion_scale'])\n        # breakpoint()\n        # print()\n        # print(\"_____________________________\")\n        condition = motion_vectors[:, 0] < 0\n        # print(condition)\n        # print(condition.shape)\n        # breakpoint()\n        motion_vectors_simplified = motion_vectors[condition, :][:, [0, 5, 6, 7, 8, 9]]",
        "type": "code",
        "location": "/tests/motion_vector_estimation/optical_flow_colored_blocks_convolution.py:143-164"
    },
    "3659": {
        "file_id": 453,
        "content": "This code calculates the motion vectors from video frames and filters them based on a condition. The condition checks if the x-component of the motion vector is less than 0, which may indicate horizontal movement. The code then selects specific columns (x, y coordinates, and scale) from the motion vectors that meet this condition for further processing.",
        "type": "comment"
    },
    "3660": {
        "file_id": 453,
        "content": "        motion_vectors_scale = motion_vectors_simplified[:, [5]]\n        motion_vectors_scale_inversed = 1 / motion_vectors_scale\n        motion_vectors_with_scale = motion_vectors_simplified[:, [3, 4]]\n        motion_vectors_scale_inversed_stacked = np.hstack(\n            [motion_vectors_scale_inversed] * 2\n        )\n        motion_vectors_restored = (\n            motion_vectors_scale_inversed_stacked * motion_vectors_with_scale\n        )  # just element wise?\n        # print('STACKED:', motion_vectors_scale_inversed_stacked.shape)\n        # print(\"WITH SCALE:\", motion_vectors_with_scale.shape)\n        # print(\"RESTORED:\",motion_vectors_restored.shape)\n        # print(motion_vectors_simplified.shape)\n        # print(motion_vectors_scale.shape)\n        # breakpoint()\n        motion_vectors_dest_coords_restored = np.hstack(\n            [motion_vectors_simplified[:, [1, 2]], motion_vectors_restored]\n        )\n        # motion_vectors_simplified = motion_vectors[:,[0,5,6,7,8]]\n        # motion_vectors_simplified_unique = np.unique(motion_vectors_simplified, axis=0)",
        "type": "code",
        "location": "/tests/motion_vector_estimation/optical_flow_colored_blocks_convolution.py:165-184"
    },
    "3661": {
        "file_id": 453,
        "content": "This code segment is involved in optical flow calculation. It performs scaling, inverse scaling, and stacking of motion vectors to restore the original motion vector array. The code then concatenates the destination coordinates with restored motion vectors. This process helps in improving the accuracy of motion vector estimation.",
        "type": "comment"
    },
    "3662": {
        "file_id": 453,
        "content": "        # print(motion_vectors_simplified_unique.shape, motion_vectors.shape)\n        # breakpoint()\n        motion_vectors_dict = {}\n        for mv in motion_vectors_dest_coords_restored:\n            # drop duplicates first!\n            (\n                dst_x,  # corresponding macro block.\n                dst_y,  # for destination only\n                motion_x,\n                motion_y,\n                # motion_scale,  # don't know what the fuck is wrong with the motion scale\n            ) = mv.tolist()\n            # say we just want source_index <0, aka mv compared to previous frame\n            # try:\n            #     assert motion_x / motion_scale == src_x - dst_x\n            #     assert motion_y / motion_scale == src_y - dst_y\n            # except:\n            #     print(src_x, dst_x, motion_x, motion_scale)\n            #     print(src_y, dst_y, motion_y, motion_scale)\n            #     print(\"*\" * 20)\n            # it will be inaccurate if we abandon this subpixel precision.\n            # if source_index >= 0:",
        "type": "code",
        "location": "/tests/motion_vector_estimation/optical_flow_colored_blocks_convolution.py:185-206"
    },
    "3663": {
        "file_id": 453,
        "content": "This code segment is extracting and processing motion vectors from a set of coordinates. It is checking for duplicates, performing calculations with source and destination coordinates, and possibly handling inaccuracies caused by subpixel precision. The code seems to be part of a larger process, as it includes debugging statements and references to variables that are not explicitly defined within the provided segment.",
        "type": "comment"
    },
    "3664": {
        "file_id": 453,
        "content": "            #     continue\n            # if dst_x>max_dst_x:\n            #     max_dst_x = dst_x\n            # if dst_y>max_dst_y:\n            #     max_dst_y = dst_y\n            destCoord = (dst_x, dst_y)\n            motion_vector = (motion_x, motion_y)\n            # print(destCoord)\n            # breakpoint()\n            if motion_vector == (0, 0):\n                # print(\"zero motion vector detected. skipping\")\n                # breakpoint()\n                continue\n            # print('destination coords:',destCoord)\n            # print('motion vector:',motion_vector)\n            motion_vectors_dict.update(\n                {destCoord: motion_vectors_dict.get(destCoord, []) + [motion_vector]}\n            )\n            # you know, different frame sources may lead to different results.\n            # these vectors could overlap. which one you want to keep? the smaller ones or the bigger ones?\n            # if destCoord in destCoords:\n            #     print(\"SKIPPING DUPLICATE DESTCOORD:\", destCoord)\n            #     print(\"PREVIOUS MV\",prevMV)",
        "type": "code",
        "location": "/tests/motion_vector_estimation/optical_flow_colored_blocks_convolution.py:207-230"
    },
    "3665": {
        "file_id": 453,
        "content": "This code iterates over motion vectors and updates a dictionary with the destination coordinates and corresponding motion vectors. It skips zero vectors and handles duplicate destinations, but doesn't specify which motion vector to keep in case of overlapping coordinates.",
        "type": "comment"
    },
    "3666": {
        "file_id": 453,
        "content": "            #     print(\"CURRENT MV\", mv)\n            #     continue\n            # else:\n            #     destCoords.add(destCoord)\n            # prevMV = mv\n            # try:\n            #     # src_x, src_y may not apply the same rule.\n            #     # assert src_x % 16 == 8\n            #     # assert src_y % 16 == 8\n            #     assert checkMacroBlock(dst_x) is not None\n            #     assert checkMacroBlock(dst_y) is not None\n            #     # assert dst_x<=res_x # dst_x can go beyond the res_x\n            #     # assert dst_y<=res_y\n            #     # so all rules applied.\n            # except:\n            #     # print('source',src_x, src_y)\n            #     print(\"res\", res_x, res_y)\n            #     print('destionation',dst_x, dst_y)\n            #     print('motion',motion_x, motion_y)\n            #     print(\"scale\",motion_scale)\n        motion_vectors_dict_averaged = {\n            key: averageMotionVectors(motion_vectors_dict[key])\n            for key in motion_vectors_dict.keys()\n        }",
        "type": "code",
        "location": "/tests/motion_vector_estimation/optical_flow_colored_blocks_convolution.py:231-254"
    },
    "3667": {
        "file_id": 453,
        "content": "This code is filtering and averaging motion vectors for macroblocks within a certain range. It checks if the source coordinates follow a specific rule, asserts that valid macroblock check functions are not None, and ensures the destination coordinates do not exceed the resolution limits. If any of these conditions fail, it prints debug information and continues execution. Finally, it calculates the averaged motion vectors for each macroblock in the dictionary.",
        "type": "comment"
    },
    "3668": {
        "file_id": 453,
        "content": "        # assuming no duplicates?\n        weighted_motion_vectors = []\n        weights = []\n        rectangles = []\n        motion_vectors_filtered = []  # for getting data later?\n        for (\n            blockCenterCoordinates,\n            average_motion_vector,\n        ) in motion_vectors_dict_averaged.items():\n            if average_motion_vector == (0, 0):\n                continue\n                # wtf is this? why fucking zero?\n                # print('skipping zero average motion vector')\n                # print(\"destination coords\", key)\n                # print('average motion vector', average_motion_vector)\n            else:\n                m_x, m_y = average_motion_vector\n                motion_vectors_filtered.append(average_motion_vector)\n                rectangle_XYWH = getRectangleXYWHFromBlockCenterCoordinates(\n                    blockCenterCoordinates\n                )\n                rectangles.append(rectangle_XYWH)\n                blockWeight = getBlockWeightFromBlockCenterCoordinates(\n                    blockCenterCoordinates",
        "type": "code",
        "location": "/tests/motion_vector_estimation/optical_flow_colored_blocks_convolution.py:255-278"
    },
    "3669": {
        "file_id": 453,
        "content": "This code filters out motion vectors with an average of (0, 0) and stores the remaining vectors in a list. It also extracts relevant information from the blockCenterCoordinates and calculates the weight for each block. Rectangles are created using the getRectangleXYWHFromBlockCenterCoordinates function, and weights are obtained through getBlockWeightFromBlockCenterCoordinates. The motion_vectors_filtered list keeps track of filtered motion vectors for later use.",
        "type": "comment"
    },
    "3670": {
        "file_id": 453,
        "content": "                )\n                weights.append(blockWeight)\n                weighted_motion_vectors.append(\n                    (\n                        m_x * blockWeight / frame_common_divisor,\n                        m_y * blockWeight / frame_common_divisor,\n                    )\n                )\n        weighted_motion_vectors = np.array(weighted_motion_vectors)\n        sum_weighted_motion_vector = np.sum(weighted_motion_vectors, axis=0)\n        average_global_weighted_motion_vector = (\n            sum_weighted_motion_vector / total_block_weights\n        )\n        sum_weights = sum(weights)\n        average_weighted_motion_vector = sum_weighted_motion_vector / sum_weights\n        motion_area_ratio = sum_weights / total_block_weights\n        # print(motion_vectors.shape)\n        motion_vectors_filtered_cartesian_distance = [\n            cartesianDistance(vector) for vector in motion_vectors_filtered\n        ] + [\n            0\n        ]  # to avoid errors.\n        motion_vectors_filtered_cartesian_distance = np.array(",
        "type": "code",
        "location": "/tests/motion_vector_estimation/optical_flow_colored_blocks_convolution.py:279-301"
    },
    "3671": {
        "file_id": 453,
        "content": "This code calculates the average global weighted motion vector and average weighted motion vector, as well as the motion area ratio. It also filters and stores cartesian distances for each motion vector.",
        "type": "comment"
    },
    "3672": {
        "file_id": 453,
        "content": "            motion_vectors_filtered_cartesian_distance\n        )\n        cartesianWeights = weights + [0]\n        cartesianWeights = np.array(cartesianWeights)\n        cartesianWeightsSum = np.sum(cartesianWeights)\n        weighted_motion_vectors_filtered_cartesian_distance = (\n            motion_vectors_filtered_cartesian_distance * cartesianWeights\n        )\n        sum_weighted_motion_vectors_filtered_cartesian_distance = np.sum(\n            weighted_motion_vectors_filtered_cartesian_distance\n        )\n        # print(\"SUM\", sum_weighted_motion_vectors_filtered_cartesian_distance)\n        # breakpoint()\n        average_weighted_motion_vectors_filtered_cartesian_distance = (\n            sum_weighted_motion_vectors_filtered_cartesian_distance / cartesianWeightsSum\n        )\n        average_global_weighted_motion_vectors_filtered_cartesian_distance = (\n            sum_weighted_motion_vectors_filtered_cartesian_distance\n            / total_block_weights # this is a number, not array!\n        )\n        min_cartesian = min(motion_vectors_filtered_cartesian_distance)",
        "type": "code",
        "location": "/tests/motion_vector_estimation/optical_flow_colored_blocks_convolution.py:302-328"
    },
    "3673": {
        "file_id": 453,
        "content": "This code calculates weighted average motion vectors by multiplying the distance of each vector with its corresponding weight, summing them, and dividing by the total weight. The minimum cartesian distance is also found.",
        "type": "comment"
    },
    "3674": {
        "file_id": 453,
        "content": "        max_cartesian = max(motion_vectors_filtered_cartesian_distance)\n        motion_area_ratio_array.append(motion_area_ratio)\n        # print()\n        # print(average_weighted_motion_vector)\n        # print(average_global_weighted_motion_vector)\n        # breakpoint()\n        average_weighted_motion_vector_cartesian=cartesianDistance(average_weighted_motion_vector)\n        average_weighted_motion_vector_cartesian_array.append(average_weighted_motion_vector_cartesian)\n        average_global_weighted_motion_vector_cartesian = cartesianDistance(average_global_weighted_motion_vector)\n        average_global_weighted_motion_vector_cartesian_array.append(\n        average_global_weighted_motion_vector_cartesian\n        )\n        average_weighted_motion_vectors_filtered_cartesian_distance_array.append(\n            average_weighted_motion_vectors_filtered_cartesian_distance\n        )\n        average_global_weighted_motion_vectors_filtered_cartesian_distance_array.append(\n            average_global_weighted_motion_vectors_filtered_cartesian_distance",
        "type": "code",
        "location": "/tests/motion_vector_estimation/optical_flow_colored_blocks_convolution.py:329-346"
    },
    "3675": {
        "file_id": 453,
        "content": "Calculates the average weighted motion vector and global weighted motion vector in Cartesian distance, then appends them to corresponding arrays. It also computes and appends filtered Cartesian distances of both types of vectors to their respective arrays. No print statements or breakpoints are executed.",
        "type": "comment"
    },
    "3676": {
        "file_id": 453,
        "content": "        )\n        if motion_vectors_dict_averaged != {}:\n            # breakpoint()\n            if visualize:\n                print(\"motion_area_ratio\", motion_area_ratio)\n                print(\"average_weighted_motion_vector_cartesian\", average_weighted_motion_vector_cartesian)\n                print(\n                    \"average_global_weighted_motion_vecto_cartesianr\",\n                    average_global_weighted_motion_vector_cartesian,\n                )\n                print(\n                    \"average_weighted_motion_vectors_filtered_cartesian_distance\",\n                    average_weighted_motion_vectors_filtered_cartesian_distance,\n                )\n                print(\n                    \"average_global_weighted_motion_vectors_filtered_cartesian_distance\",\n                    average_global_weighted_motion_vectors_filtered_cartesian_distance,\n                )\n                motion_mask = np.zeros(\n                    (motion_render_frame[1], motion_render_frame[0], 1)\n                )\n                for index, (x, y, w, h) in enumerate(rectangles):",
        "type": "code",
        "location": "/tests/motion_vector_estimation/optical_flow_colored_blocks_convolution.py:347-369"
    },
    "3677": {
        "file_id": 453,
        "content": "The code checks if there are any motion vectors in the dictionary and prints various motion-related information and creates a motion mask with zeros.",
        "type": "comment"
    },
    "3678": {
        "file_id": 453,
        "content": "                    pt1, pt2 = XYWHToDiagonal(x, y, w, h)\n                    # print(pt1, pt2)\n                    current_cartesian = motion_vectors_filtered_cartesian_distance[\n                        index\n                    ]\n                    # print(type(pt1), type(pt1[0]))\n                    relative_motion_cartesian = (current_cartesian - min_cartesian) / (\n                        max_cartesian - min_cartesian\n                    )  # must from 0 to 1 so we can plot this,\n                    # relative_motion_cartesian = 255*((current_cartesian-min_cartesian)/(max_cartesian-min_cartesian))\n                    # relative_motion_cartesian = int(relative_motion_cartesian)\n                    # relative_motion_cartesian = min(255,max(0, relative_motion_cartesian))\n                    # breakpoint()\n                    cv2.rectangle(\n                        motion_mask,\n                        pt1,\n                        pt2,\n                        color=(relative_motion_cartesian,),\n                        thickness=-1,",
        "type": "code",
        "location": "/tests/motion_vector_estimation/optical_flow_colored_blocks_convolution.py:370-388"
    },
    "3679": {
        "file_id": 453,
        "content": "This code calculates the relative motion vector cartesian distance and draws a rectangle on an image using OpenCV's `cv2.rectangle` function. The rectangle dimensions are based on the input x, y, w, and h parameters, and its color is determined by the relative motion vector cartesian distance, converted to a range of 0-255 for image intensity values.",
        "type": "comment"
    },
    "3680": {
        "file_id": 453,
        "content": "                    )\n                # should we gaussian blur, threshold this, do convolution and then apply bounding box on it?\n                # # visualize this.\n                if show_picture:\n                    cv2.imshow(\"motion_mask\", motion_mask)\n                    cv2.waitKey(100)\n            # may you create bounding box for this? for tracking motion? or not?\n        # breakpoint()\n    else:\n        break\n# print('max_dst_x', max_dst_x)\n# print('max_dst_y', max_dst_y)\nimport matplotlib.pyplot as plt\n# plt.style.use('dark_background')\na, b = 5, 1\nfigure, axis = plt.subplots(a, b)\ndata = [\n    motion_area_ratio_array,\n    # average_weighted_motion_vector_array,\n    # average_global_weighted_motion_vector_array,\n    average_weighted_motion_vector_cartesian_array,\n    average_global_weighted_motion_vector_cartesian_array,\n    average_weighted_motion_vectors_filtered_cartesian_distance_array,\n    average_global_weighted_motion_vectors_filtered_cartesian_distance_array,\n]\ntitles = [\n    \"motion_area_ratio\",",
        "type": "code",
        "location": "/tests/motion_vector_estimation/optical_flow_colored_blocks_convolution.py:389-419"
    },
    "3681": {
        "file_id": 453,
        "content": "The code is visualizing and analyzing motion data using image processing techniques. It displays a motion mask, creates a bounding box for motion tracking, and plots various motion-related data on subplots. The code also includes options to blur, threshold, apply convolution, and display the results.",
        "type": "comment"
    },
    "3682": {
        "file_id": 453,
        "content": "    # \"average_weighted_motion_vector\",\n    # \"average_global_weighted_motion_vector\",\n    \"average_weighted_motion_vector_cartesian\",\n    \"average_global_weighted_motion_vector_cartesian\",\n    \"average_weighted_motion_vectors_filtered_cartesian_distance\",\n    \"average_global_weighted_motion_vectors_filtered_cartesian_distance\",\n]\n# breakpoint()\nassert len(titles) == len(data)\nassert a*b >= len(titles)\nfor _a in range(a):\n    for _b in range(b):\n        index = _a * b + _b\n        if index > len(data) - 1:\n            break\n        if a == 1:\n            if b == 1:\n                axis[0].plot(data[index])\n                axis[0].set_title(titles[index])\n            else:\n                axis[_b].plot(data[index])\n                axis[_b].set_title(titles[index])\n        elif b == 1:\n            axis[_a].plot(data[index])\n            axis[_a].set_title(titles[index])\n        else:\n            axis[_a, _b].plot(data[index])\n            axis[_a, _b].set_title(titles[index])\nplt.show()",
        "type": "code",
        "location": "/tests/motion_vector_estimation/optical_flow_colored_blocks_convolution.py:420-449"
    },
    "3683": {
        "file_id": 453,
        "content": "This code is plotting multiple sets of data onto a graph, with each set corresponding to an item in two lists of titles and data. The code asserts that the lengths of both lists are equal, and then iterates through each element of the lists using nested for loops. If a single plot is desired, it plots and labels one line of data at a time. If multiple plots are desired, it creates and labels a separate plot for each line of data. Finally, it displays the graph.",
        "type": "comment"
    },
    "3684": {
        "file_id": 454,
        "content": "/tests/motion_vector_estimation/test.py",
        "type": "filepath"
    },
    "3685": {
        "file_id": 454,
        "content": "The code processes video frames and estimates motion vectors, then plots the results using matplotlib and handles potential errors. It uses pandas, numpy, and OpenCV for calculations.",
        "type": "summary"
    },
    "3686": {
        "file_id": 454,
        "content": "# it contains subpixel motion vectors. fucking hell\n# source = \"/root/Desktop/works/pyjom/samples/video/dog_with_text.mp4\"\n# change source?\n# gif containers does not have motion vectors.\n# source = \"/root/Desktop/works/pyjom/samples/video/cat_invalid_eye_rolling.gif\"\n# source = \"/root/Desktop/works/pyjom/samples/video/kitty_flash_15fps.gif\"\n# without mestimate\n# source = \"/root/Desktop/works/pyjom/samples/video/cat_invalid_eye_rolling_without_mestimate.mp4\"\n# source = \"/root/Desktop/works/pyjom/samples/video/kitty_flash_15fps_without_mestimate.mp4\"\n# with mestimate\n# source = \"/root/Desktop/works/pyjom/samples/video/cat_invalid_eye_rolling_with_mestimate.mp4\"\n# source = \"/root/Desktop/works/pyjom/samples/video/kitty_flash_15fps_with_mestimate.mp4\"\n# source = \"/root/Desktop/works/pyjom/samples/video/nearly_duplicate_frames_detection_30fps.mp4\"\nsource = \"/root/Desktop/works/pyjom/samples/video/cute_cat_gif.mp4\"\nfrom lazero.utils.importers import cv2_custom_build_init\ncv2_custom_build_init()\nfrom mvextractor.videocap import VideoCap",
        "type": "code",
        "location": "/tests/motion_vector_estimation/test.py:1-25"
    },
    "3687": {
        "file_id": 454,
        "content": "This code is setting the source video file path for various test cases involving motion vector estimation. The files include different types of videos such as MP4 and GIF, with and without mestimate data, and nearly duplicate frame detection tests. It also initializes custom CV2 build and imports necessary modules.",
        "type": "comment"
    },
    "3688": {
        "file_id": 454,
        "content": "from caer.video.frames_and_fps import count_frames, get_res\nimport cv2\nframesCount = count_frames(source)\nres = get_res(source)  # (width, height)\nprint(\"RES: %s\" % str(res))\nres_x, res_y = res\nframe_common_divisor = min(res_x, res_y)\nimport math\ndef cartesianDistance(d2vector):\n    try:\n        x, y = d2vector\n        return math.sqrt(x**2 + y**2)\n    except:\n        print('item unpackable.', d2vector)\n        return 0\ndef XYWHToDiagonal(x, y, w, h):\n    return (x, y), (x + w, y + h)\n# 如果整除16那么就在这个范围里面 如果不整除范围就要扩大 扩大到相应的16的倍数\ndef get16Value(res_x):\n    rem_x = res_x % 16\n    val = res_x // 16\n    if rem_x != 0:\n        val += 1\n    return val\nx_16val = get16Value(res_x)\ny_16val = get16Value(res_y)\nmotion_render_frame = (x_16val * 16, y_16val * 16)\ntotal_block_weights = x_16val * y_16val * 2 * 2\ncap = VideoCap()\ncap.open(source)  # wtf is going on here?\n# if there is nothing we will breakup\n# visualize, show_picture = True, True\nvisualize, show_picture = False, False\n# so there can only be one such macroblock\ndef checkMacroBlock(value):",
        "type": "code",
        "location": "/tests/motion_vector_estimation/test.py:26-75"
    },
    "3689": {
        "file_id": 454,
        "content": "This code is initializing variables and functions related to video processing. It calculates the resolution of a source, determines if it's divisible by 16, adjusts if necessary, and sets up variables for motion vector estimation and visualization. The VideoCap class is opened but its functionality remains unclear. The checkMacroBlock function checks for one macroblock value.",
        "type": "comment"
    },
    "3690": {
        "file_id": 454,
        "content": "    for mod in [16, 8]:\n        modValue = value % mod\n        if modValue == mod / 2:\n            return mod\n    # if not satisfied, we are shit.\nfrom functools import lru_cache\n@lru_cache(maxsize=4)\ndef getModXModYFromBlockCenterCoordinates(blockCenterCoordinates):\n    block_x, block_y = blockCenterCoordinates\n    mod_x, mod_y = checkMacroBlock(block_x), checkMacroBlock(block_y)\n    if mod_x is not None and mod_y is not None:\n        return mod_x, mod_y\n    else:\n        print(\"block center coordinates\", blockCenterCoordinates)\n        print(\"WTF IS GOING ON WITH THE BLOCK CENTER\")\n        breakpoint()\n        return 0, 0\ndef getRectangleXYWHFromBlockCenterCoordinates(blockCenterCoordinates):\n    block_x, block_y = blockCenterCoordinates\n    mod_x, mod_y = getModXModYFromBlockCenterCoordinates(blockCenterCoordinates)\n    mod_x_half, mod_y_half = mod_x / 2, mod_y / 2\n    x, y, w, h = block_x - mod_x_half, block_y - mod_y_half, mod_x, mod_y\n    return tuple([int(elem) for elem in [x, y, w, h]])\ndef getBlockWeightFromBlockCenterCoordinates(blockCenterCoordinates):",
        "type": "code",
        "location": "/tests/motion_vector_estimation/test.py:76-107"
    },
    "3691": {
        "file_id": 454,
        "content": "This code defines several functions for handling block center coordinates in a specific context. It checks the modulo value of the coordinates to determine the size and position of the blocks, and uses these values to calculate the rectangle's dimensions and the block weight. The `lru_cache` decorator is used to cache the results of the `getModXModYFromBlockCenterCoordinates` function to improve performance.",
        "type": "comment"
    },
    "3692": {
        "file_id": 454,
        "content": "    mod_x, mod_y = getModXModYFromBlockCenterCoordinates(blockCenterCoordinates)\n    weights = mod_x * mod_y / 8 / 8\n    return weights\nimport progressbar\nimport numpy as np\n# max_dst_x, max_dst_y = 0,0\ndef averageMotionVectors(motion_vector_list):\n    if len(motion_vector_list) == 0:\n        average_tuple = (0, 0)\n    if len(motion_vector_list) > 1:\n        marray = np.array(motion_vector_list)\n        # print(\"MAKING AVERAGE:\")\n        # print(marray)\n        average = np.average(marray, axis=0)\n        # breakpoint()\n        average_tuple = tuple(average)\n    else:\n        average_tuple = tuple(motion_vector_list[0])\n    return average_tuple\nmotion_area_ratio_array = []\n# average_weighted_motion_vector_array = []\n# average_global_weighted_motion_vector_array = []\naverage_weighted_motion_vector_cartesian_array = []\naverage_global_weighted_motion_vector_cartesian_array = []\naverage_weighted_motion_vectors_filtered_cartesian_distance_array = []\naverage_global_weighted_motion_vectors_filtered_cartesian_distance_array = []",
        "type": "code",
        "location": "/tests/motion_vector_estimation/test.py:108-140"
    },
    "3693": {
        "file_id": 454,
        "content": "Function to calculate the average motion vectors based on a list of motion vectors. If the list is empty, returns (0, 0). If the list has more than one vector, calculates the average and returns it as a tuple. Else, returns the first vector in the list.\n\nInitializes arrays for storing average_weighted_motion_vector_cartesian, average_global_weighted_motion_vector_cartesian, average_weighted_motion_vectors_filtered_cartesian_distance, and average_global_weighted_motion_vectors_filtered_cartesian_distance.",
        "type": "comment"
    },
    "3694": {
        "file_id": 454,
        "content": "for _ in progressbar.progressbar(range(framesCount)):\n    success, frame, motion_vectors, frame_type, timestamp = cap.read()\n    height, width, channels = frame.shape\n    # breakpoint()\n    if success:\n        # what is the content of this motion vector?\n        # print(motion_vectors)\n        # import pandas as pd\n        # df = pd.DataFrame(motion_vectors)\n        # df = pd.DataFrame(motion_vectors,index=['source_index','unk0','unk1','src_x','src_y','dst_x','dst_y','motion_x','motion_y','motion_scale'])\n        # breakpoint()\n        # print()\n        # print(\"_____________________________\")\n        condition = motion_vectors[:, 0] < 0\n        # print(condition)\n        # print(condition.shape)\n        # breakpoint()\n        motion_vectors_simplified = motion_vectors[condition, :][:, [0, 5, 6, 7, 8, 9]]\n        motion_vectors_scale = motion_vectors_simplified[:, [5]]\n        motion_vectors_scale_inversed = 1 / motion_vectors_scale\n        motion_vectors_with_scale = motion_vectors_simplified[:, [3, 4]]\n        motion_vectors_scale_inversed_stacked = np.hstack(",
        "type": "code",
        "location": "/tests/motion_vector_estimation/test.py:142-163"
    },
    "3695": {
        "file_id": 454,
        "content": "The code reads frames and motion vectors from a video stream, processes them, and stores selected information in separate arrays. It uses pandas for data processing and numpy for array manipulation.",
        "type": "comment"
    },
    "3696": {
        "file_id": 454,
        "content": "            [motion_vectors_scale_inversed] * 2\n        )\n        motion_vectors_restored = (\n            motion_vectors_scale_inversed_stacked * motion_vectors_with_scale\n        )  # just element wise?\n        # print('STACKED:', motion_vectors_scale_inversed_stacked.shape)\n        # print(\"WITH SCALE:\", motion_vectors_with_scale.shape)\n        # print(\"RESTORED:\",motion_vectors_restored.shape)\n        # print(motion_vectors_simplified.shape)\n        # print(motion_vectors_scale.shape)\n        # breakpoint()\n        motion_vectors_dest_coords_restored = np.hstack(\n            [motion_vectors_simplified[:, [1, 2]], motion_vectors_restored]\n        )\n        # motion_vectors_simplified = motion_vectors[:,[0,5,6,7,8]]\n        # motion_vectors_simplified_unique = np.unique(motion_vectors_simplified, axis=0)\n        # print(motion_vectors_simplified_unique.shape, motion_vectors.shape)\n        # breakpoint()\n        motion_vectors_dict = {}\n        for mv in motion_vectors_dest_coords_restored:\n            # drop duplicates first!",
        "type": "code",
        "location": "/tests/motion_vector_estimation/test.py:164-184"
    },
    "3697": {
        "file_id": 454,
        "content": "This code segment is responsible for restoring the motion vectors after scaling and stacking. It performs element-wise multiplication of two arrays, one containing scaled motion vectors and the other being the stacked motion vectors. The result is stored in \"motion_vectors_restored\". Then, it horizontally stacks the simplified motion vector coordinates and the restored ones using numpy's hstack function, resulting in \"motion_vectors_dest_coords_restored\". Finally, a dictionary named \"motion_vectors_dict\" is initialized but not fully populated in this snippet.",
        "type": "comment"
    },
    "3698": {
        "file_id": 454,
        "content": "            (\n                dst_x,  # corresponding macro block.\n                dst_y,  # for destination only\n                motion_x,\n                motion_y,\n                # motion_scale,  # don't know what the fuck is wrong with the motion scale\n            ) = mv.tolist()\n            # say we just want source_index <0, aka mv compared to previous frame\n            # try:\n            #     assert motion_x / motion_scale == src_x - dst_x\n            #     assert motion_y / motion_scale == src_y - dst_y\n            # except:\n            #     print(src_x, dst_x, motion_x, motion_scale)\n            #     print(src_y, dst_y, motion_y, motion_scale)\n            #     print(\"*\" * 20)\n            # it will be inaccurate if we abandon this subpixel precision.\n            # if source_index >= 0:\n            #     continue\n            # if dst_x>max_dst_x:\n            #     max_dst_x = dst_x\n            # if dst_y>max_dst_y:\n            #     max_dst_y = dst_y\n            destCoord = (dst_x, dst_y)\n            motion_vector = (motion_x, motion_y)",
        "type": "code",
        "location": "/tests/motion_vector_estimation/test.py:185-208"
    },
    "3699": {
        "file_id": 454,
        "content": "This code segment is related to motion vector estimation in video processing. It calculates the destination coordinates and motion vector for a macro block, but there seems to be an issue with the motion scale. The code tries to assert that the motion_x and motion_y are scaled correctly based on the motion_scale, but it is causing some problem.",
        "type": "comment"
    }
}