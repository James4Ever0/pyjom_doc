{
    "4600": {
        "file_id": 595,
        "content": "from test_commons import *\nfrom pyjom.platforms.bilibili.database import resolveSubTidsFromTid\ntid = 217\nresult = resolveSubTidsFromTid(tid)\nprint(\"RESULT?\", result)",
        "type": "code",
        "location": "/tests/test_bilibili_resolve_tid.py:1-6"
    },
    "4601": {
        "file_id": 595,
        "content": "This code imports necessary modules, sets tid to 217, and calls resolveSubTidsFromTid function with the tid. The returned result is then printed. It tests resolving subTids from a given tid in Bilibili platform's database.",
        "type": "comment"
    },
    "4602": {
        "file_id": 596,
        "content": "/tests/test_commons.py",
        "type": "filepath"
    },
    "4603": {
        "file_id": 596,
        "content": "The code changes the current working directory, adds the current directory to Python's module search path, and removes the proxy environment variables to ignore global proxies during testing.",
        "type": "summary"
    },
    "4604": {
        "file_id": 596,
        "content": "import sys\nimport os\nos.chdir(\"../\")\nsys.path.append(\".\")\n# ignore the global proxy now, we are not going to use that.\nos.environ[\"http_proxy\"] = \"\"\nos.environ[\"https_proxy\"] = \"\"",
        "type": "code",
        "location": "/tests/test_commons.py:1-8"
    },
    "4605": {
        "file_id": 596,
        "content": "The code changes the current working directory, adds the current directory to Python's module search path, and removes the proxy environment variables to ignore global proxies during testing.",
        "type": "comment"
    },
    "4606": {
        "file_id": 597,
        "content": "/tests/test_dummy.py",
        "type": "filepath"
    },
    "4607": {
        "file_id": 597,
        "content": "This code imports necessary modules, initializes a ContentProducer and ContentReviewer objects, runs their main methods, and prints their identifier data. It tests content production and reviewing functionality.",
        "type": "summary"
    },
    "4608": {
        "file_id": 597,
        "content": "from test_commons import *\nfrom pyjom.main import *\nproducer = ContentProducer()\nproducer.main()\nprint(producer.identifier.data)\nreviewer = ContentReviewer()\nreviewer.main()\nprint(reviewer.identifier.data)",
        "type": "code",
        "location": "/tests/test_dummy.py:1-10"
    },
    "4609": {
        "file_id": 597,
        "content": "This code imports necessary modules, initializes a ContentProducer and ContentReviewer objects, runs their main methods, and prints their identifier data. It tests content production and reviewing functionality.",
        "type": "comment"
    },
    "4610": {
        "file_id": 598,
        "content": "/tests/test_dummy.sh",
        "type": "filepath"
    },
    "4611": {
        "file_id": 598,
        "content": "This code is executing a Python script named \"test_dummy.py\" using the default installed Python3 interpreter. It's likely being run in a Unix-like environment as it uses \"python3\" instead of \"python\". The purpose of running this script might be for testing, debugging or execution of the code within \"test_dummy.py\".",
        "type": "summary"
    },
    "4612": {
        "file_id": 598,
        "content": "python3 test_dummy.py",
        "type": "code",
        "location": "/tests/test_dummy.sh:1-1"
    },
    "4613": {
        "file_id": 598,
        "content": "This code is executing a Python script named \"test_dummy.py\" using the default installed Python3 interpreter. It's likely being run in a Unix-like environment as it uses \"python3\" instead of \"python\". The purpose of running this script might be for testing, debugging or execution of the code within \"test_dummy.py\".",
        "type": "comment"
    },
    "4614": {
        "file_id": 599,
        "content": "/tests/test_iterator_generator_wrapper_lazero_utils.py",
        "type": "filepath"
    },
    "4615": {
        "file_id": 599,
        "content": "This code tests the functionality of lazero's iteratorWrapper with different parameters such as init_repeat, repeat, and max_iter. It compares the generated results to predefined objective lists for validation.",
        "type": "summary"
    },
    "4616": {
        "file_id": 599,
        "content": "from lazero.utils.tools import iteratorWrapper, flattenUnhashableList\nsequence = [i for i in range(10)]\nINIT_REPEAT = 3\nobjective_init_repeat = [sequence[0]] * INIT_REPEAT + sequence\nREPEAT = 2\nobjective_repeat = [sequence[0]] * INIT_REPEAT + flattenUnhashableList(\n    list(zip(*([sequence] * (1 + REPEAT))))\n)\nMAX_ITER = 4\nobjective_max_iter = [sequence[0]] * INIT_REPEAT + flattenUnhashableList(\n    list(zip(*([sequence[:MAX_ITER]] * (1 + REPEAT))))\n)\ndef test_init_repeat():\n    result = list(iteratorWrapper((s for s in sequence), init_repeat=INIT_REPEAT))\n    assert result == objective_init_repeat\ndef test_repeat():\n    result = list(\n        iteratorWrapper((s for s in sequence), init_repeat=INIT_REPEAT, repeat=REPEAT)\n    )\n    assert result == objective_repeat\ndef test_max_iter():\n    result = list(\n        iteratorWrapper(\n            (s for s in sequence),\n            init_repeat=INIT_REPEAT,\n            repeat=REPEAT,\n            max_iter=MAX_ITER,\n        )\n    )\n    assert result == objective_max_iter",
        "type": "code",
        "location": "/tests/test_iterator_generator_wrapper_lazero_utils.py:1-42"
    },
    "4617": {
        "file_id": 599,
        "content": "This code tests the functionality of lazero's iteratorWrapper with different parameters such as init_repeat, repeat, and max_iter. It compares the generated results to predefined objective lists for validation.",
        "type": "comment"
    },
    "4618": {
        "file_id": 600,
        "content": "/tests/test_local_reviewer.py",
        "type": "filepath"
    },
    "4619": {
        "file_id": 600,
        "content": "This code imports necessary modules, initializes a FilesystemContentReviewer object with a directory path, and calls its main() method to perform content review on the specified directory.",
        "type": "summary"
    },
    "4620": {
        "file_id": 600,
        "content": "from test_commons import *\nfrom pyjom.primitives import *  # this is capitalized.\nwbRev = FilesystemContentReviewer(dirpath=\"./samples/video/\")\nwbRev.main()",
        "type": "code",
        "location": "/tests/test_local_reviewer.py:1-5"
    },
    "4621": {
        "file_id": 600,
        "content": "This code imports necessary modules, initializes a FilesystemContentReviewer object with a directory path, and calls its main() method to perform content review on the specified directory.",
        "type": "comment"
    },
    "4622": {
        "file_id": 601,
        "content": "/tests/test_manual_censor.py",
        "type": "filepath"
    },
    "4623": {
        "file_id": 601,
        "content": "The code imports necessary functions from test_commons and contentCensoring, initiates a test sequence by calling censorInterface with input data for title, content and prints collected data.",
        "type": "summary"
    },
    "4624": {
        "file_id": 601,
        "content": "from test_commons import *\nfrom contentCensoring import *\nprint(\"initiating test sequence...\")\nmdata = censorInterface(\n    \"test_title\",\n    None,\n    \"test_content\",\n)\nprint(\"interface closed.\")\nprint(\"collected data:\", mdata)",
        "type": "code",
        "location": "/tests/test_manual_censor.py:1-11"
    },
    "4625": {
        "file_id": 601,
        "content": "The code imports necessary functions from test_commons and contentCensoring, initiates a test sequence by calling censorInterface with input data for title, content and prints collected data.",
        "type": "comment"
    },
    "4626": {
        "file_id": 602,
        "content": "/tests/test_manual_censorInterface.py",
        "type": "filepath"
    },
    "4627": {
        "file_id": 602,
        "content": "The code imports necessary modules, defines lists of tags, shuffles them, and uses the censorInterface function to perform content censorship on a title and content with specified tags. It then prints the result.",
        "type": "summary"
    },
    "4628": {
        "file_id": 602,
        "content": "from test_commons import *\nfrom pyjom.modules.contentCensoring.core import censorInterface\nmcounter = 20\nmtags0 = [\"superLongtag{}\".format(x) for x in range(mcounter)]  # must be differet.\nmtags1 = [\"tag{}\".format(x) for x in range(mcounter)]\nmtags = mtags0 + mtags1\nimport random\nrandom.shuffle(mtags)\nresult = censorInterface(\n    \"title\", [\"mytopic\", \"another topic\"], \"mycontent\", mtags=mtags\n)\nprint(result)",
        "type": "code",
        "location": "/tests/test_manual_censorInterface.py:1-18"
    },
    "4629": {
        "file_id": 602,
        "content": "The code imports necessary modules, defines lists of tags, shuffles them, and uses the censorInterface function to perform content censorship on a title and content with specified tags. It then prints the result.",
        "type": "comment"
    },
    "4630": {
        "file_id": 603,
        "content": "/tests/test_medialang.py",
        "type": "filepath"
    },
    "4631": {
        "file_id": 603,
        "content": "This code imports necessary modules, defines test paths, and iterates through each path. It creates a Medialang object with the specified script path and prettifies it in-place.",
        "type": "summary"
    },
    "4632": {
        "file_id": 603,
        "content": "from test_commons import *\nfrom pyjom.medialang.core import *\nimport os\ntestpaths = [\n    \"processor_demo.mdl\",\n    \"processor_multi.mdl\",\n    \"recipe.mdl\",\n    \"audiolang.mdl\",\n    \"videolang.mdl\",\n]\n# testcontent = open(testpath,\"r\").read()\nfor path in testpaths:\n    testpath = os.path.join(\"/root/Desktop/works/pyjom/test/\", path)\n    mdl = Medialang(script_path=testpath)  # will be parsed.\n    mdl.prettify(inplace=True)",
        "type": "code",
        "location": "/tests/test_medialang.py:1-18"
    },
    "4633": {
        "file_id": 603,
        "content": "This code imports necessary modules, defines test paths, and iterates through each path. It creates a Medialang object with the specified script path and prettifies it in-place.",
        "type": "comment"
    },
    "4634": {
        "file_id": 604,
        "content": "/tests/test_ocr_entity_detector.py",
        "type": "filepath"
    },
    "4635": {
        "file_id": 604,
        "content": "The code loads JSON data containing stationary and moving text, checks their locations over time using similarity metrics, and performs forced combination of OCR results, iterating through the combined results to print content and type.",
        "type": "summary"
    },
    "4636": {
        "file_id": 604,
        "content": "from test_commons import *\nfrom pyjom.medialang.functions.detectors.entityDetector import *\nimport json\n# check if text is movement or we have to mark its trajectory.\n# feeling like i am a game maker.\ndataPath = \"/root/Desktop/works/pyjom/logs/local/1649678716_663207.json\"\nmdata = open(dataPath, \"r\", encoding=\"utf8\").read()\nmdata = json.loads(mdata)\n# minMaxThresh = 14 # max difference is ten pixel. or it is considered as moving.\n# strDisThreshold = 1 # or considered as changing?\n# certThreshold = 0.7\n# changingMinMaxThresh = 25\n# changingstrDisThreshold = 2\n# timeThreshold = 0.3 # i intentially set it.\n# blockTimeThreshold = 0.3 # at least last this long?\n# strSimThreshold = 0.8\n# print(mtext, key) # this is stationary.\nfor elem in mdata:\n    # maybe something in a sequence? like location similarity?\n    # if location is similar, but text is different, do we really need to handle it?\n    # we need to collect similar frames, so we can deduct further.\n    try:\n        rev = elem[\"review\"][\"review\"][1]\n        ocrData = rev[\"subtitle_detector\"][\"subtitle_result\"][\"paddleocr\"]",
        "type": "code",
        "location": "/tests/test_ocr_entity_detector.py:1-30"
    },
    "4637": {
        "file_id": 604,
        "content": "This code is loading data from a JSON file and iterating through each element in the data. It appears to be checking if the text is stationary or moving by comparing its location and text over time, and possibly using similarity metrics like string distance and similarity threshold. The code seems to involve subtitle detection using PaddleOCR, as indicated by the \"paddleocr\" attribute in the JSON data.",
        "type": "comment"
    },
    "4638": {
        "file_id": 604,
        "content": "        # here is the core.\n        myresult = makeOCREntity(ocrData, blockTimeThreshold=0, timeThreshold=0.1)\n        myNewResult = staticOCRCombinator(myresult)  # this is forced combination.\n        # print(json.dumps(myNewResult,indent=4))\n        for key in myNewResult.keys():\n            myElem = myNewResult[key]\n            print(myElem[\"content\"], key)\n        breakpoint()\n    except:\n        import traceback\n        traceback.print_exc()\n        breakpoint()",
        "type": "code",
        "location": "/tests/test_ocr_entity_detector.py:31-43"
    },
    "4639": {
        "file_id": 604,
        "content": "Code snippet initializes and performs a forced combination of OCR results, then iterates over the combined results and prints their content and type.",
        "type": "comment"
    },
    "4640": {
        "file_id": 605,
        "content": "/tests/test_remerge_demanded_cut_spans.py",
        "type": "filepath"
    },
    "4641": {
        "file_id": 605,
        "content": "This code defines cut_spans as ranges for processing and creates a test function to check if the list of spans has consistent order, duration, and is remerged correctly.",
        "type": "summary"
    },
    "4642": {
        "file_id": 605,
        "content": "cut_spans = [(0, 1), (1, 2), (2, 9), (9, 100), (100, 101), (101, 102)]\n# cut_spans=[(0, 2.43475), (2.43475, 4.3458125), (4.3458125, 7.543145833333333), (7.543145833333333, 10.7313125), (10.7313125, 13.928645833333333), (13.928645833333333, 16.492041666666665), (16.492041666666665, 22.216020833333335), (22.216020833333335, 25.4225625), (25.4225625, 30.530958333333334), (30.530958333333334, 33.709916666666665), (33.709916666666665, 36.907270833333335), (36.907270833333335, 39.46145833333333), (39.46145833333333, 42.649625), (42.649625, 46.499291666666664), (46.499291666666664, 49.0443125), (49.0443125, 52.54485416666667), (52.54485416666667, 55.10825), (55.10825, 57.65325), (57.65325, 61.806125), (61.806125, 64.99429166666667), (64.99429166666667, 67.55766666666666), (67.55766666666666, 70.1026875), (70.1026875, 73.28164583333333), (73.28164583333333, 76.16660416666667), (76.16660416666667, 79.99791666666667), (79.99791666666667, 82.23054166666667), (82.23054166666667, 85.1063125), (85.10",
        "type": "code",
        "location": "/tests/test_remerge_demanded_cut_spans.py:1-2"
    },
    "4643": {
        "file_id": 605,
        "content": "Code defines a list of cut_spans, where each span represents a range of values for further processing or analysis.",
        "type": "comment"
    },
    "4644": {
        "file_id": 605,
        "content": "63125, 87.97289583333334), (87.97289583333334, 91.1610625), (91.1610625, 93.09047916666667), (93.09047916666667, 96.26945833333333), (96.26945833333333, 100.42233333333333), (100.42233333333333, 102.97652083333334), (102.97652083333334, 106.80783333333333), (106.80783333333333, 111.27308333333333), (111.27308333333333, 117.33702083333333), (117.33702083333333, 119.57883333333334), (119.57883333333334, 123.0701875), (123.0701875, 127.250625), (127.250625, 129.7864375), (129.7864375, 134.57327083333334), (134.57327083333334, 137.7614375), (137.7614375, 140.95877083333335), (140.95877083333335, 146.06716666666668), (146.06716666666668, 150.5324375), (150.5324375, 153.72058333333334), (153.72058333333334, 157.55189583333333), (157.55189583333333, 160.74922916666668), (160.74922916666668, 163.3034375), (163.3034375, 164.25895833333334), (164.25895833333334, 164.89291666666668), (164.89291666666668, 171.576)]\nfrom test_commons import *\nfrom pyjom.lyrictoolbox import remergeDemandedCutSpans\ndef test_cut_spans_valid(list_of_spans, min_span=1.5, max_span=10, no_range_test=False):",
        "type": "code",
        "location": "/tests/test_remerge_demanded_cut_spans.py:2-7"
    },
    "4645": {
        "file_id": 605,
        "content": "This code defines a function `test_cut_spans_valid` that takes a list of spans and optional arguments for minimum and maximum span duration. It calls the `remergeDemandedCutSpans` function from `pyjom.lyrictoolbox`. The code also imports functions from `test_commons` module and defines some variables.",
        "type": "comment"
    },
    "4646": {
        "file_id": 605,
        "content": "    start = list_of_spans[0][0]\n    init_end = list_of_spans[0][1]\n    minit_duration = list_of_spans[0][1] - start\n    if not no_range_test:\n        assert start < list_of_spans[0][1]\n        assert minit_duration >= min_span and minit_duration <= max_span\n    # end = list_of_spans[-1][1]\n    for i, span in enumerate(list_of_spans[1:]):\n        mstart, mend = span\n        try:\n            assert mstart == init_end\n        except:\n            print(mstart, mend, init_end, i + 1)\n            print(list_of_spans[max(0, i - 2) : min(len(list_of_spans), i + 2)])\n            breakpoint()\n        assert mstart < mend\n        duration = mend - mstart\n        if not no_range_test:\n            assert duration >= min_span and duration <= max_span\n        init_end = mend\ntest_cut_spans_valid(cut_spans, no_range_test=True)\nnew_spans = remergeDemandedCutSpans(cut_spans)\nprint(\"new spans?\", new_spans)\ntest_cut_spans_valid(new_spans)\nassert cut_spans[0][0] == new_spans[0][0]\nassert cut_spans[-1][1] == new_spans[-1][1]",
        "type": "code",
        "location": "/tests/test_remerge_demanded_cut_spans.py:8-35"
    },
    "4647": {
        "file_id": 605,
        "content": "This code checks if the list of spans has a consistent order and duration. It asserts that the start of each span is less than its end, and the duration (end - start) adheres to specified minimum and maximum span values. If any assertion fails, it prints the offending span and surrounding spans for debugging. The code then tests if the list of spans has been remerged correctly using the remergeDemandedCutSpans function, ensuring that the first and last spans remain unchanged.",
        "type": "comment"
    },
    "4648": {
        "file_id": 606,
        "content": "/tests/test_talib_stream_ema.py",
        "type": "filepath"
    },
    "4649": {
        "file_id": 606,
        "content": "This code is testing the speed of two different methods for calculating a Simple Moving Average (SMA) using Talib library. The first method uses Function API and the second method uses Streaming API. It measures the time taken to execute each method and prints the results along with original data.",
        "type": "summary"
    },
    "4650": {
        "file_id": 606,
        "content": "import talib\nfrom talib import stream\nimport numpy as np\n# check the difference\nimport timeit\nclose = np.random.random(100)\nprint(close.dtype)\nbreakpoint()\n# close = np.append(close,10)\nclose = np.append(close[1:], 10)\nmtime = timeit.timeit(lambda: np.append(close, 10), number=1)  # why so many times?\n# the Function API\n# really don't know which is faster.\noutput = timeit.timeit(\n    lambda: talib.SMA(close), number=1\n)  # why you take it so damn long?\n# the Streaming API\nlatest = timeit.timeit(lambda: stream.SMA(close[-20:]), number=1)\nprint(output)\nprint(latest)\nprint(close)\nprint(mtime)  # why taking so long?",
        "type": "code",
        "location": "/tests/test_talib_stream_ema.py:1-28"
    },
    "4651": {
        "file_id": 606,
        "content": "This code is testing the speed of two different methods for calculating a Simple Moving Average (SMA) using Talib library. The first method uses Function API and the second method uses Streaming API. It measures the time taken to execute each method and prints the results along with original data.",
        "type": "comment"
    },
    "4652": {
        "file_id": 607,
        "content": "/tests/test_weibo_pets.py",
        "type": "filepath"
    },
    "4653": {
        "file_id": 607,
        "content": "This code imports necessary modules, initializes a WeiboPetsReviewer object with specific parameters and then runs its main function to perform an automated review of Weibo Pets data.",
        "type": "summary"
    },
    "4654": {
        "file_id": 607,
        "content": "from test_commons import *\nfrom pyjom.primitives import *  # this is capitalized.\ntemplate_names = [\"subtitle_detector.mdl.j2\"]\nautoArgs = {\"subtitle_detector\": {\"timestep\": 0.2}}\nwbRev = WeiboPetsReviewer(\n    auto=True,\n    semiauto=False,\n    dummy_auto=False,\n    args=autoArgs,\n    template_names=template_names,\n)\n# wbRev.main(skip_review=True) # to test feedback.\nwbRev.main()",
        "type": "code",
        "location": "/tests/test_weibo_pets.py:1-15"
    },
    "4655": {
        "file_id": 607,
        "content": "This code imports necessary modules, initializes a WeiboPetsReviewer object with specific parameters and then runs its main function to perform an automated review of Weibo Pets data.",
        "type": "comment"
    },
    "4656": {
        "file_id": 608,
        "content": "/tests/test_weibo_pets.sh",
        "type": "filepath"
    },
    "4657": {
        "file_id": 608,
        "content": "This code is running a Python script called 'test_weibo_pets.py'. This could be part of an automated testing process, likely to test the functionality of a Weibo Pets module or application.",
        "type": "summary"
    },
    "4658": {
        "file_id": 608,
        "content": "python3 test_weibo_pets.py",
        "type": "code",
        "location": "/tests/test_weibo_pets.sh:1-1"
    },
    "4659": {
        "file_id": 608,
        "content": "This code is running a Python script called 'test_weibo_pets.py'. This could be part of an automated testing process, likely to test the functionality of a Weibo Pets module or application.",
        "type": "comment"
    },
    "4660": {
        "file_id": 609,
        "content": "/tests/title_cover_generator/commons.py",
        "type": "filepath"
    },
    "4661": {
        "file_id": 609,
        "content": "The code loads and preprocesses training data using load_train_data_core function, iterating through indexes of text chunks, transforming words, and creating Word class instances. It applies shuffle and progress bar for efficient data access.",
        "type": "summary"
    },
    "4662": {
        "file_id": 609,
        "content": "sample_data = [\"【翎伶】world.execute;(me);\", \"【封校日常】沙拉制作\", \"【Blender场景动画】新代 : 城市【VictoryLuode】\", \"历时733天! 圆了挖机梦，我独立造了一台可遥控小型挖机\", \"【难懂的数学】傅里叶、拉普拉斯、卷积、欧拉方程、梯度散度、拉格朗日方程、奈奎斯特采样、虚数等抽象难懂数学一网打尽\", \"这些up主是中学生和大学生的救星啊啊啊啊啊！！！学习方法｜免费课程｜兴趣技能｜生涯规划\", \"【不止游戏】游戏和电影中的M4，究竟有多经典？\", \"Steam++ 新版v2.7发布 新功能介绍\", \"手绘503张！还原数码宝贝OP\", \"好可爱鸭~ summertime\", \"男室友偷偷逛站酷网，毕设惊艳全校！\", \"对不起，我笑得真的很大声！【第一届立直麻将联赛】\", \"在南京每天画画一小时，在家接单养活自己！\", \"没有什么事情是一个纸团解决不了的，如果有那就用很多个\", \"到底是什么让我能在公园大爷面前如此的自信？\", \"欲拔山城寨，先过五虎将\", \"杨侃最下饭｜27 杨毅：经纪人不能太贪心\", \"【深渊的呼唤V】全球总决赛-决赛 Wolves vs SST\", \"【安特卫普MAJOR】亚洲区预选赛 TYLOO vs Renegades\", \"狼队第五人格分部成立两周年啦！\", \"【守望先锋联赛】英雄崛起!准备好迎接2022赛季!\"]\nimport progressbar\nimport random\ndef load_train_data_core(shuffle=True,batchsize=1,len_threshold = 2,no_unk=True):\n    filepath = \"/media/root/help/pyjom/tests/title_cover_generator/DianJing/data/basic_data_80k_v2.pkl\"\n    # warning...\n    import pickle\n    fobj = open(filepath, 'rb')\n    # print([fobj])\n    # breakpoint()\n    class Word:\n        def __init__(self,val,tf,df):\n            self.val = val",
        "type": "code",
        "location": "/tests/title_cover_generator/commons.py:1-17"
    },
    "4663": {
        "file_id": 609,
        "content": "The code imports the progressbar and random libraries, defines a function load_train_data_core which takes optional parameters shuffle, batchsize, len_threshold, and no_unk. The filepath variable stores the path to a pickle file containing data for training. The function opens the file using pickle's open function in read binary mode and does not perform any additional operations on its contents before returning.",
        "type": "comment"
    },
    "4664": {
        "file_id": 609,
        "content": "            self.tf = tf\n            self.df = df\n        def __repr__(self):\n            pass\n    _, word2idx, idx2word, targets, srcs= pickle.load(fobj) # freaking swap.\n    # titles, abstracts\n    # print(titles) # these are not freaking words. numbers.\n    # print(abstracts)\n    for key in idx2word:\n        elem = idx2word[key]\n        if elem.startswith('<') and elem.endswith('>'):\n            elem = elem[1:-1].upper()\n            elem = \"[{}]\".format(elem)\n            idx2word[key] =elem\n    # you can freaking get the data.\n    # title = titles[0]\n    len_indexs = len(targets)\n    # indexs = [x for x in range(indexs)]\n        # random.shuffle(indexs)\n    randomIdx = [x for x in range(len_indexs)]\n    if shuffle:\n        random.shuffle(randomIdx)\n    randomIdx2 = [randomIdx[x*batchsize:(x+1)*batchsize] for x in range(len(randomIdx)//batchsize+1)]\n    len_srcs = len(srcs)\n    len_targets = len(targets)\n    # mfilter = lambda x: x.replace(\" \",\"\").replace(\"\\n\",\"\")\n    for indexs in progressbar.progressbar(randomIdx2):",
        "type": "code",
        "location": "/tests/title_cover_generator/commons.py:18-44"
    },
    "4665": {
        "file_id": 609,
        "content": "The code is loading a pickle file, extracting relevant data including titles and abstracts. It then modifies some elements in the idx2word dictionary by replacing specific characters with formatted strings. The code provides random indexes for accessing the data and applies a shuffle if required. Lastly, it uses a progress bar for iterating over the shuffled indexes to access the data.",
        "type": "comment"
    },
    "4666": {
        "file_id": 609,
        "content": "        src_result=[]\n        target_result=[]\n        for index in indexs:\n            if index < len_srcs and index < len_targets:\n                src, target = srcs[index], targets[index]\n                src, target = [idx2word[x] for x in src], [idx2word[x] for x in target]\n                src, target = \"\".join(src),\"\".join(target)\n                if no_unk:\n                    src, target = src.replace(\"[UNK]\",\"\"), target.replace(\"[UNK]\",\"\")\n                # src, target = mfilter(src), mfilter(target)\n                if max(len(src),len(target)) > len_threshold:\n                    src_result.append(src)\n                    target_result.append(target)\n        if len(src_result) >0:\n            yield src_result,target_result\n    # for index in indexs:\n    #     title = titles[index]\n    #     mytitle = [idx2word[x] for x in title]\n    #     abstract = abstracts[index]\n    #     myabstract = [idx2word[x] for x in abstract]\n    #     if join:\n    #         yield \"\".join(mytitle), \"\".join(myabstract)\n    #     else: yield mytitle, myabstract",
        "type": "code",
        "location": "/tests/title_cover_generator/commons.py:45-67"
    },
    "4667": {
        "file_id": 609,
        "content": "This code is iterating through indexes in two lists of text chunks, transforming them to word form, joining the words into strings, and removing [UNK] tokens if specified. If any resulting string exceeds a certain length threshold, it appends them to two result lists. The code yields these two result lists if there are at least one entry.",
        "type": "comment"
    },
    "4668": {
        "file_id": 609,
        "content": "    # print(mytitle)\n    # breakpoint()\ndef import_word():\n    # if __name__ == \"__main__\":\n    class Word:\n        def __init__(self,val,tf,df):\n            self.val = val\n            self.tf = tf\n            self.df = df\n        def __repr__(self):\n            pass\n    return Word\nif __name__ == '__main__':\n    Word = import_word()\n    for title, abstract in load_train_data_core():\n        print(title)\n        print(abstract) # we have <unk> tokens. how do we freaking deal with it?\n        breakpoint()",
        "type": "code",
        "location": "/tests/title_cover_generator/commons.py:68-87"
    },
    "4669": {
        "file_id": 609,
        "content": "The code defines a function `import_word` that returns a class named Word. The class has attributes `val`, `tf`, and `df`. The code then checks if it is being run as the main program and creates instances of the Word class from loaded data, printing title and abstract. It encounters a breakpoint to debug or inspect the handling of tokens in the code.",
        "type": "comment"
    },
    "4670": {
        "file_id": 610,
        "content": "/tests/title_cover_generator/gpt2_title_data_prep.py",
        "type": "filepath"
    },
    "4671": {
        "file_id": 610,
        "content": "Code copies train data as test data, creates a new data directory if it doesn't exist, writes train data to a JSON file, and then copies the train file to the test file.",
        "type": "summary"
    },
    "4672": {
        "file_id": 610,
        "content": "# simply copy train shit as test shit.\nfrom commons import load_train_data_core, import_word\nWord = import_word()\nimport json\ndata = []\nimport os\ndata_dir = \"/media/root/help/pyjom/tests/title_cover_generator/GPT2-NewsTitle/data_dir\"\nif not os.path.exists(data_dir):\n    os.mkdir(data_dir)\ntrain_file = os.path.join(data_dir,\"train_data.json\")\ntest_file = os.path.join(data_dir,\"test_data.json\")\nfor content, title in load_train_data_core():\n    sample = {\"title\": title[0],\"content\":content[0]}\n    data.append(sample) # is that necessary?\nwith open(train_file,\"w+\",encoding=\"utf8\") as f:\n    f.write(json.dumps(data,ensure_ascii=False,indent=4))\nimport shutil\nshutil.copy(train_file, test_file)",
        "type": "code",
        "location": "/tests/title_cover_generator/gpt2_title_data_prep.py:1-26"
    },
    "4673": {
        "file_id": 610,
        "content": "Code copies train data as test data, creates a new data directory if it doesn't exist, writes train data to a JSON file, and then copies the train file to the test file.",
        "type": "comment"
    },
    "4674": {
        "file_id": 611,
        "content": "/tests/title_cover_generator/gpt2_train.sh",
        "type": "filepath"
    },
    "4675": {
        "file_id": 611,
        "content": "This code navigates into the \"GPT2-NewsTitle\" directory and creates a new directory named \"output_dir\". It then runs the Python script \"train.py\" to train a GPT-2 model for generating news titles or covers.",
        "type": "summary"
    },
    "4676": {
        "file_id": 611,
        "content": "cd GPT2-NewsTitle\nmkdir output_dir\npython3 train.py",
        "type": "code",
        "location": "/tests/title_cover_generator/gpt2_train.sh:1-3"
    },
    "4677": {
        "file_id": 611,
        "content": "This code navigates into the \"GPT2-NewsTitle\" directory and creates a new directory named \"output_dir\". It then runs the Python script \"train.py\" to train a GPT-2 model for generating news titles or covers.",
        "type": "comment"
    },
    "4678": {
        "file_id": 612,
        "content": "/tests/title_cover_generator/paddlenlp_word_label.py",
        "type": "filepath"
    },
    "4679": {
        "file_id": 612,
        "content": "This code imports Taskflow from paddlenlp and commons.sample_data. It iterates through sample data, applying various tasks (word segmentation, NER, POS tagging, etc.) to each element using Taskflow. If the flow is information extraction, it uses a schema for processing. After processing, it deletes the Taskflow object and prints the task name along with the processed data.",
        "type": "summary"
    },
    "4680": {
        "file_id": 612,
        "content": "from paddlenlp import  Taskflow\nfrom commons import sample_data\n# LAC 词语重要性\nfor elem in sample_data:\n    flows = [\"word_segmentation\",\"ner\",\"pos_tagging\",\"dependency_parsing\",\"information_extraction\",\"sentiment_analysis\",\"text_correction\",\"knowledge_mining\"]\n    for flow in flows:\n        if flow !=\"information_extraction\":\n            seg = Taskflow(flow) # need schema for information extraction.\n        else:\n            schema = [\"主语\",\"谓语\",\"宾语\"]\n            seg = Taskflow(flow, schema=schema) # need schema for information extraction\n        data = seg(elem)\n        del seg\n        print(flow,data)",
        "type": "code",
        "location": "/tests/title_cover_generator/paddlenlp_word_label.py:1-16"
    },
    "4681": {
        "file_id": 612,
        "content": "This code imports Taskflow from paddlenlp and commons.sample_data. It iterates through sample data, applying various tasks (word segmentation, NER, POS tagging, etc.) to each element using Taskflow. If the flow is information extraction, it uses a schema for processing. After processing, it deletes the Taskflow object and prints the task name along with the processed data.",
        "type": "comment"
    },
    "4682": {
        "file_id": 613,
        "content": "/tests/title_cover_generator/pegasus_trainer.py",
        "type": "filepath"
    },
    "4683": {
        "file_id": 613,
        "content": "This code trains a PEGASUS machine translation model using MT5ForConditionalGeneration, loads data, tokenizes text, and saves weights every 5000 updates for 1000 epochs. It includes optimization steps, error handling, and backup saving functionality. The trainer is dissatisfied with the current performance.",
        "type": "summary"
    },
    "4684": {
        "file_id": 613,
        "content": "from commons import load_train_data_core, import_word\nWord = import_word()\n# print(Word)\n# break()\n#importing the PEGASUS Transformer model\nimport torch\nfrom transformers import MT5ForConditionalGeneration\nfrom tokenizer import T5PegasusTokenizer\nmodel_path = \"./pegasus_title_generation/pegasus_1\" # trained on paraphrase tasks.\n# model_name = './t5_pegasus_training/t5_pegasus'\nmodel_name = model_path\nmodel_name_or_path = model_name\ntorch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntokenizer = T5PegasusTokenizer.from_pretrained(model_name)\nmodel = MT5ForConditionalGeneration.from_pretrained(model_name).to(torch_device)\n# import random\n# import progressbar\ndef mydataset(len_threshold = 2,batchsize=1): # train till you fucking die. this almost depleted my VRAM. better train this shit elsewhere.\n    for a,b in load_train_data_core(len_threshold = 2,batchsize=1): yield a,b # freaking shit.\ndef get_train_data(batchsize=2,max_length=1024):",
        "type": "code",
        "location": "/tests/title_cover_generator/pegasus_trainer.py:2-27"
    },
    "4685": {
        "file_id": 613,
        "content": "Code imports necessary libraries for loading training data, tokenizing text with T5PegasusTokenizer and initializing the PEGASUS Transformer model. The MT5ForConditionalGeneration model is loaded from a pre-trained checkpoint located at `model_path` or `model_name`. The code also specifies the device to use for training (either CPU or CUDA-enabled GPU). A function named `mydataset` is defined, which generates a dataset from load_train_data_core with specified batch size and length threshold. Another function `get_train_data` takes in batch size and maximum sequence length as inputs. This code seems to be used for training the PEGASUS model on specific tasks.",
        "type": "comment"
    },
    "4686": {
        "file_id": 613,
        "content": "    for source_sentences, target_sentences in mydataset(batchsize=batchsize):\n        # targetSentence = [\"今天天气不错\",\"你吃了没有\"]\n        batchsize = len(source_sentences)\n        if batchsize >0:\n        # print([source_sentence,target_sentence])\n            input_ids = tokenizer.batch_encode_plus(source_sentences,max_length=max_length,padding=True,truncation=True, return_tensors=\"pt\").input_ids.to(device)\n            labels = tokenizer.batch_encode_plus(target_sentences,return_tensors=\"pt\",padding=True,truncation=True,max_length=max_length,).input_ids.to(device) # what is the freaking max_length?\n            yield input_ids, labels\n# from torch.optim import SGD\n# from torch.optim import ASGD as SGD\nfrom torch.optim import RMSprop as SGD\nbatchsize = 2\n# optimizer = SGD(model.parameters(), momentum=0.9, lr=0.000001*batchsize, weight_decay=0.0001)\noptimizer = SGD(model.parameters(), lr=0.00001*batchsize, weight_decay=0.0001)\nloss_mean = []\nmean_loss_period = 100\nepochs = 1000\nmsaveperiod = 5000 # wtf is 30000\nupdate_period = 1 # hell man.",
        "type": "code",
        "location": "/tests/title_cover_generator/pegasus_trainer.py:28-49"
    },
    "4687": {
        "file_id": 613,
        "content": "This code is training a model with batch size 2. The maximum length of input sentences is not defined. The optimizer is using RMSprop algorithm, with a learning rate adjusted by the batch size. The mean loss over the last 100 batches is stored in 'loss_mean' list. Training continues for 1000 epochs and model weights are saved every 5000 updates. Updates to the model occur once per iteration.",
        "type": "comment"
    },
    "4688": {
        "file_id": 613,
        "content": "#setting up the model\n# def get_response(input_text):\n#   batch = tokenizer.encode(input_text, return_tensors=\"pt\").to(torch_device)\n#   translated = model.generate(batch,decoder_start_token_id=tokenizer.cls_token_id,eos_token_id=tokenizer.sep_token_id,max_length=30).cpu().numpy()[0]\n#   tgt_text = ''.join(tokenizer.decode(translated[1:])).replace(' ', '')\n#   return tgt_text\n# not so bad?\n# can you train this shit?\n# print(get_response(\"你吃了没有\"))\nfor epoch in range(epochs):\n    print(\"STARTING EPOCH {} TOTAL {}\".format(epoch,epochs))\n    for index, (input_ids, labels) in enumerate(get_train_data(batchsize=batchsize)):\n        try:\n            if index%update_period == 0:\n                optimizer.zero_grad()\n            # print([input_ids, labels])\n            outputs = model(input_ids=input_ids, labels=labels)\n            loss = outputs.loss\n            floss = loss.tolist()\n            loss_mean.append(floss)\n            if len(loss_mean) == mean_loss_period:\n                mloss = sum(loss_mean)/mean_loss_period",
        "type": "code",
        "location": "/tests/title_cover_generator/pegasus_trainer.py:52-77"
    },
    "4689": {
        "file_id": 613,
        "content": "This code sets up a model for generating translations, trains it using train data, and calculates the loss. It uses a tokenizer to encode input texts and generate translations, then calculates the mean loss over a specified period.",
        "type": "comment"
    },
    "4690": {
        "file_id": 613,
        "content": "                print(\"EPOCH {} TOTAL {}\".format(epoch,epochs))\n                print(\"MEAN LOSS OVER {} SAMPLES: {}\".format(mean_loss_period,str(mloss)[:5]))\n                loss_mean = []\n            loss.backward()\n            # logits = outputs.logits\n            if index % update_period == 0:\n                optimizer.step() # this is shit. i should run this shit in kaggle.\n        except:\n            import traceback\n            traceback.print_exc()\n            print(\"POSSIBLY OOM\")\n        if index > (msaveperiod - 1) and index%msaveperiod == 0:\n            print(\"SAVING MODEL AT {} SAMPLES\".format(index))\n            model.save_pretrained(model_name_or_path)\n            # shutil.copy(model_name_or_path,model_name_or_path+\"-backup\")\n            model.save_pretrained(model_name_or_path+\"-backup\")\n            ## it is working.",
        "type": "code",
        "location": "/tests/title_cover_generator/pegasus_trainer.py:78-94"
    },
    "4691": {
        "file_id": 613,
        "content": "This code appears to be part of a training loop for a machine learning model. It keeps track of the mean loss over a certain period and saves the model after a set number of samples. The code includes an optimization step, error handling, and backup saving functionality. The trainer seems frustrated with the line \"this is shit. i should run this shit in kaggle.\"",
        "type": "comment"
    },
    "4692": {
        "file_id": 614,
        "content": "/tests/title_cover_generator/pyltp_server.py",
        "type": "filepath"
    },
    "4693": {
        "file_id": 614,
        "content": "The code initializes LTP models for NLP tasks, offering functions for segmentation, part-of-speech tagging, named entity recognition, and dependency syntax parsing. It uses PyLTL to extract subject-predicate-object triples from sentences, identifies relationships, and appends them to Dynamic_relation if applicable.",
        "type": "summary"
    },
    "4694": {
        "file_id": 614,
        "content": "# !/usr/bin/env python3\n# -*- coding:utf-8 -*-\n# create on 5/26/20\n__author__ = \"sinsa\"\nimport os\nimport logging\nfrom logging import info, error, warn\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s - PID:%(process)d - %(levelname)s: %(message)s\",\n)\nfrom pyltp import Segmentor\nfrom pyltp import Postagger\nfrom pyltp import NamedEntityRecognizer\nfrom pyltp import Parser\nfrom pyltp import SentenceSplitter\nclass LTP_MODEL:\n    def __init__(self):\n        LTP_DATA_DIR = \"./pyltp_data/ltp_data_v3.4.0\"  # ltp模型目录的路径\n        info(\"loading models ...\")\n        self.cws_model_path = os.path.join(\n            LTP_DATA_DIR, \"cws.model\"\n        )  # 分词模型路径，模型名称为`cws.model`\n        self.segmentor = Segmentor(self.cws_model_path)  # 初始化实例\n        # self.segmentor.load(self.cws_model_path)  # 加载模型\n        info(\"has loaded 分词模型\")\n        self.pos_model_path = os.path.join(\n            LTP_DATA_DIR, \"pos.model\"\n        )  # 词性标注模型路径，模型名称为`pos.model`\n        self.postaggers = Postagger(self.pos_model_path)  # 初始化实例",
        "type": "code",
        "location": "/tests/title_cover_generator/pyltp_server.py:1-35"
    },
    "4695": {
        "file_id": 614,
        "content": "This code initializes the necessary LTP (Language Technology Platform) models for Natural Language Processing tasks. It sets the LTP data directory, loads and initializes models for word segmentation, part-of-speech tagging, named entity recognition, and parsing. The logger is configured to provide status updates during the loading process.",
        "type": "comment"
    },
    "4696": {
        "file_id": 614,
        "content": "        # self.postaggers.load(self.pos_model_path)  # 加载模型\n        info(\"has loaded 词性标注模型\")\n        self.ner_model_path = os.path.join(\n            LTP_DATA_DIR, \"ner.model\"\n        )  # 命名实体识别模型路径，模型名称为`pos.model`\n        self.recognizer = NamedEntityRecognizer(self.ner_model_path)  # 初始化实例\n        # self.recognizer.load(self.ner_model_path)  # 加载模型\n        info(\"has loaded 命名实体识别模型\")\n        self.par_model_path = os.path.join(\n            LTP_DATA_DIR, \"parser.model\"\n        )  # 依存句法分析模型路径，模型名称为`parser.model`\n        self.parser = Parser(self.par_model_path)  # 初始化实例\n        # self.parser.load(self.par_model_path)  # 加载模型\n        info(\"has loaded 依存句法分析模型\")\n    def __release__(self):\n        self.segmentor.release()  # 释放模型\n        self.postaggers.release()  # 释放模型\n        self.recognizer.release()  # 释放模型\n        self.parser.release()  # 释放模型\n    def SplitSentence(self, sentence):\n        sents_list = SentenceSplitter.split(sentence)  # 分句\n        return list(sents_list)\n    def segment(self, input_list):",
        "type": "code",
        "location": "/tests/title_cover_generator/pyltp_server.py:36-61"
    },
    "4697": {
        "file_id": 614,
        "content": "The code loads three models (POS tagging, Named Entity Recognition, and Dependency Parsing) and initializes corresponding recognizers or parsers for each model. It also provides methods to release the models when finished and split a sentence into individual sentences.",
        "type": "comment"
    },
    "4698": {
        "file_id": 614,
        "content": "        \"\"\"\n        功能：实现分词文本的分词\n        返回值：每个文本的形成一个列表[['word1','word2'],['word1','word3'],……]\n        \"\"\"\n        segmented_text_list = []\n        for text in input_list:\n            words = self.segmentor.segment(text)  # 分词\n            segmented_text_list.append(list(words))\n        return segmented_text_list\n    def postagger(self, input_list, return_words_list=False):\n        \"\"\"\n        功能：实现文本中每个词的词性标注\n        返回值：每个文本是一个列表，列表中的每个词也是个列表[[['word1',u'O'],['word2',u'O']],[['word2',u'O'],['word5',u'O']],……]\n        \"\"\"\n        postagger_text_list = []\n        words_list = self.segment(input_list)\n        postags_list = []\n        for words in words_list:\n            postags = self.postaggers.postag(words)  # 词性标注\n            postags_list.append(list(postags))\n            words_postags = list(zip(words, list(postags)))\n            postagger_text_list.append(words_postags)\n        if return_words_list:\n            return words_list, postags_list\n        else:\n            return postagger_text_list\n    def NamedEntityRecognizer(self, input_list, Entity_dist=False, repead=False):",
        "type": "code",
        "location": "/tests/title_cover_generator/pyltp_server.py:62-90"
    },
    "4699": {
        "file_id": 614,
        "content": "This code defines three functions: \"segment\", \"postagger\", and \"NamedEntityRecognizer\". The \"segment\" function takes a list of texts as input, performs segmentation on each text to obtain a list of words, and returns the segmented text as a list of lists. The \"postagger\" function takes a list of texts and performs part-of-speech tagging on each word in the list. It then returns the tagged text as a list of lists. If the \"return_words_list\" parameter is True, it also returns the original words list. The \"NamedEntityRecognizer\" function recognizes named entities in the input texts based on the provided parameters (\"Entity_dist\" and \"repead\").",
        "type": "comment"
    }
}