{
    "summary": "This code utilizes YOLOv5 for object detection and a video tracker to monitor dog movement in frames, identifying dogs and providing bounding box coordinates above threshold. Additionally, it closes OpenCV-created video windows.",
    "details": [
        {
            "comment": "This code imports necessary libraries, sets the local model directory, and loads a YOLOv5 model for object detection. It defines a function to get the bounding box coordinates of a dog in an image, with the option to set a minimum confidence threshold. The code then performs inference using the loaded model on the input frame image and returns the bounding box information if the detected class is \"dog\" and the confidence meets or exceeds the threshold.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/tests/video_detector_tests/singleTracker.py\":0-30",
            "content": "import cv2\n# import imutils #another dependency?\n# tracker = cv2.TrackerCSRT_create() # outdated tracker.\n# i really don't know what is a dog.\nimport torch\n# don't really know how paddleocr recognize chars.\nlocalModelDir = '/root/Desktop/works/pyjom/pyjom/models/yolov5/ultralytics_yolov5_master/'\nimport os\nos.environ[\"YOLOV5_MODEL_DIR\"] = '/root/Desktop/works/pyjom/pyjom/models/yolov5/'\nmodel = torch.hub.load(localModelDir, 'yolov5s',source=\"local\")\ndef getDogBB(frame,thresh=0.7):\n    img = frame[:,:,::-1].transpose((2,0,1))\n    # Inference\n    # reshape this shit.\n    # img = np.reshape()\n    results = model(img) # pass the image through our model\n    df = results.pandas().xyxy[0]\n    print(df)\n    data = []\n    for index,line in df.iterrows():\n        # print(line)\n        left = (line[\"xmin\"],line[\"ymin\"])\n        right = (line[\"xmax\"],line[\"ymax\"])\n        confidence = line[\"confidence\"]\n        class_ = line[\"class\"]\n        name = line[\"name\"]\n        if name == \"dog\" and confidence >= thresh: # better figure out all output names."
        },
        {
            "comment": "Code initializes a video tracker using different algorithms and prepares parameters for the 'DaSiamRPN' tracker, which is slower but uses deep learning. It then checks if a dog is present in each frame of a video and returns its bounding box coordinates.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/tests/video_detector_tests/singleTracker.py\":31-55",
            "content": "            data.append({\"location\":[left,right],\"confidence\":confidence,\"identity\":{\"class\":class_,\"name\":name}})\n    print(data)\n    data = list(sorted(data,key=lambda x: -x[\"confidence\"]))\n    if len(data)>0:\n        target= data[0]\n        xmin,ymin = target[\"location\"][0]\n        xmax,ymax = target[\"location\"][1]\n        return int(xmin),int(ymin),int(xmax-xmin),int(ymax-ymin)\ndef checkDog(frame,thresh=0.5):\n    return getDogBB(frame,thresh=thresh) == None # dog missing.\n# better use something else?\n# tracker = cv2.TrackerMIL_create()\ntracker_types = ['MIL', 'GOTURN', 'DaSiamRPN']\ntracker_type = tracker_types[2]\nbasepath = \"./OpenCV-Object-Tracker-Python-Sample\"\nif tracker_type == 'MIL':\n    tracker = cv2.TrackerMIL_create()\nelif tracker_type == 'DaSiamRPN': # deeplearning.\n    # this tracker is slow as hell. really.\n    params = cv2.TrackerDaSiamRPN_Params()\n    params.model = os.path.join(basepath,\"model/DaSiamRPN/dasiamrpn_model.onnx\")\n    params.kernel_r1 = os.path.join(basepath,\"model/DaSiamRPN/dasiamrpn_kernel_r1.onnx\")"
        },
        {
            "comment": "This code initializes a tracker using the cv2.TrackerDaSiamRPN or cv2.TrackerGOTURN methods based on the tracker_type variable. It then creates parameters for the GOTURN tracker and creates a video capture object to read a video file. The loop reads frames from the video, checks if a bounding box is None every yoloRate frames, and if it's None, it initializes the BB variable. This code appears to be part of a video tracking application.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/tests/video_detector_tests/singleTracker.py\":56-85",
            "content": "    params.kernel_cls1 = os.path.join(basepath,\"model/DaSiamRPN/dasiamrpn_kernel_cls1.onnx\")\n    tracker = cv2.TrackerDaSiamRPN_create(params)\n    # tracker = cv2.TrackerDaSiamRPN_create()\nelif tracker_type == 'GOTURN': #also need config file.\n    # this is bad though.\n    params = cv2.TrackerGOTURN_Params()\n    params.modelTxt = os.path.join(basepath,\"model/GOTURN/goturn.prototxt\") # save this shit without BOM.\n    params.modelBin = os.path.join(basepath,\"model/GOTURN/goturn.caffemodel\")\n    tracker = cv2.TrackerGOTURN_create(params)\n    # tracker = cv2.TrackerGOTURN_create()\n# we have to feed dog coordinates into the shit.\nvideo = cv2.VideoCapture(\"../../samples/video/dog_with_text.mp4\")\n_,frame = video.read()\n# frame = imutils.resize(frame,width=720) #why?\nindex = 0\nyoloRate = 10\ntrack_success = False\nupdate_track = 3\nBB = None\ninit=False\nwhile frame is not None:\n    index +=1\n    _, frame = video.read()\n    if frame is None:\n        print(\"VIDEO END.\")\n        break\n    if index%yoloRate == 0:\n        if BB is None:"
        },
        {
            "comment": "This code initializes a tracker and tracks a dog's movement in video frames. It checks if the dog is present in the bounding box and updates the position accordingly. If the dog goes missing, it reinitializes the tracker with new dog's bounding box. The tracked dog's position is displayed on the frame, which is then shown as output.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/tests/video_detector_tests/singleTracker.py\":86-113",
            "content": "            BB = getDogBB(frame)\n        else:\n            x, y, w, h = BB\n            if len(frame.shape) == 3:\n                dogFrame = frame[y:y+h,x:x+w,:]\n            else:\n                dogFrame = frame[y:y+h,x:x+w]\n            result = checkDog(dogFrame)\n            if result: # dog gone missing.\n                BB = getDogBB(frame)\n                init=False\n    if BB is not None:\n        if not init:\n            tracker.init(frame, BB) # how to init this shit?\n            init=True\n    # when lost, we know there is no dog inside the bounding box.\n    # frame = imutils.resize(frame,width=720)\n        if index % update_track == 0:\n            track_success,BB = tracker.update(frame)\n        if track_success and BB:\n            top_left = (int(BB[0]),int(BB[1]))\n            bottom_right = (int(BB[0]+BB[2]), int(BB[1]+BB[3]))\n            cv2.rectangle(frame,top_left,bottom_right,(0,255,0),5)\n    cv2.imshow('Output',frame)\n    key  =  cv2.waitKey(1) & 0xff\n    if key == ord('q'):\n        break\nvideo.release()"
        },
        {
            "comment": "This code is used to close all the video windows created by OpenCV (cv2).",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/tests/video_detector_tests/singleTracker.py\":114-114",
            "content": "cv2.destroyAllWindows()"
        }
    ]
}