{
    "summary": "This code initializes Detectron2 for instance segmentation on COCO dataset, sets up a YOLO object detector for video frames, and tracks their positions across frames. It processes detection information, updates tracked objects using a tracker, draws circles with names based on estimated positions. The code displays a video frame, waits for 'q' key press to break loop, destroys windows, and writes the frame to file if display is not enabled.",
    "details": [
        {
            "comment": "The code imports necessary libraries and sets up the Detectron2 object detector using a pre-trained model for instance segmentation on COCO dataset. The model weight file is located at \"/root/Desktop/works/pyjom/tests/video_detector_tests/detectron2\\_models/model\\_final\\_f10217.pkl\".",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/tests/video_detector_tests/yolo_norfair.py\":0-21",
            "content": "import cv2\nimport numpy as np\nfrom detectron2.config import get_cfg\nfrom detectron2.engine import DefaultPredictor\nfrom cocoNames import cocoName\nfrom norfair import Detection, Tracker, Video, draw_tracked_objects\ndef euclidean_distance(detection, tracked_object):\n    return np.linalg.norm(detection.points - tracked_object.estimate)\n# Set up Detectron2 object detector\ncfg = get_cfg()\ncfg.merge_from_file(\"norfair/demos/faster_rcnn_R_50_FPN_3x.yaml\")\ncfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5 # looks like it does not recognize dog.\n# cfg.MODEL.WEIGHTS = \"detectron2://COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl\"\ncfg.MODEL.WEIGHTS = \"/root/Desktop/works/pyjom/tests/video_detector_tests/detectron2_models/model_final_f10217.pkl\"\n# it is stored in s3\n# https://dl.fbaipublicfiles.com/detectron2/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl\n# download cache: /root/.torch/iopath_cache/detectron2/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl"
        },
        {
            "comment": "This code is initializing a YOLO object detector, reading a video file, setting up a tracker, and then iterating through each frame of the video. The detector predicts objects in each frame, and if any are detected, it saves the detections and continues to the next frame. If no objects are detected, it skips that frame. The tracker helps keep track of object positions across frames, but the implementation details are unclear.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/tests/video_detector_tests/yolo_norfair.py\":23-47",
            "content": "detector = DefaultPredictor(cfg)\n# what are the classes output by the model?\n# Norfair\n# video_path = \"/root/Desktop/works/pyjom/samples/video/dog_with_text.mp4\"\nvideo_path = \"/root/Desktop/works/pyjom/samples/video/LlfeL29BP.mp4\"\nvideo = Video(input_path=video_path)\ntracker = Tracker(distance_function=euclidean_distance, distance_threshold=200,hit_inertia_min=2,hit_inertia_max=20,initialization_delay=1) # what the heck?\n# you should implement standalone tracker function, optical.\n# maybe you can track the object via framedifference?\ntracked_objects = None\ndisplay=True\nfor index, frame in enumerate(video): # we need to speed up.\n    if index%10 == 0:\n        detections = detector(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n        print(\"original detections:\",detections)\n        instances = detections[\"instances\"]\n        print(\"instances:\",instances)\n        # breakpoint()\n        pred_classes = instances.pred_classes\n        if len(pred_classes) == 0:\n            continue\n        detections2=[]\n        for index,class_ in enumerate(pred_classes):"
        },
        {
            "comment": "The code is processing and printing detection information, appending detections to a list (detections2), and updating tracked objects using a tracker. The comment criticizes a previous line of code for potentially losing data. The final if statement checks if any tracked_objects were found.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/tests/video_detector_tests/yolo_norfair.py\":48-65",
            "content": "            print(\"index:\",index)\n            class_ = int(class_.cpu().numpy().tolist())\n            print(\"class:\",class_)\n            box = instances.pred_boxes.tensor[index].cpu().numpy().tolist()\n            box = [int(x) for x in box]\n            score = float(instances.scores[index].cpu().numpy().tolist())\n            print('box:',box)\n            print('score:',score)\n            det = Detection(instances.pred_boxes.get_centers()[index].cpu().numpy(),scores=np.array([score]),data={\"box\":box,\"class\":{\"id\":class_,\"name\":cocoRealName[class_]}})\n            detections2.append(det)\n            # breakpoint()\n        # detections = [Detection(p) for p in instances.pred_boxes.get_centers().cpu().numpy()] # what is this instance anyway?\n        # you would lost data you dick!\n        print(\"detections2\",detections2)\n        tracked_objects = tracker.update(detections=detections2)\n        # print(detections)\n        print(\"tracked objects:\",tracked_objects) # you don't track shit?\n    if tracked_objects is not None:"
        },
        {
            "comment": "The code checks if there are any tracked objects. If there are, it estimates the object's position and draws a circle at that point on the frame. The object's name is also displayed near the circle. Finally, the code calls a function to draw the objects differently using a specific color.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/tests/video_detector_tests/yolo_norfair.py\":66-93",
            "content": "        if tracked_objects!=[]:\n            # there is no bounding box avaliable?\n            for obj in tracked_objects:\n                point = obj.estimate[0]\n                position = tuple(point.astype(int))\n                color = (255,0,0)\n                # breakpoint()\n                name = obj.last_detection.data[\"class\"][\"name\"]\n                cv2.circle(\n                        frame,\n                        position,\n                        radius=100,\n                        color=color,\n                        thickness=2,\n                    )\n                cv2.putText(\n                    frame,\n                    \"[{}][{}]\".format(str(obj.id),name),\n                    (position[0]-100,position[1]),\n                    cv2.FONT_HERSHEY_SIMPLEX,\n                    2,\n                    (0,255,0),\n                    3,\n                    cv2.LINE_AA,\n                )\n            # breakpoint()\n        # i want to draw you in a different way.\n        # draw_tracked_objects(frame, tracked_objects,color=(255,0,0))"
        },
        {
            "comment": "This code snippet displays a video frame on the screen, waits for a key press (specifically 'q'), and breaks the loop if 'q' is pressed. It then destroys all windows created. The frame is written to the video file if display is not enabled.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/tests/video_detector_tests/yolo_norfair.py\":94-102",
            "content": "    if display:\n        cv2.imshow(\"window\",frame)\n        key  =  cv2.waitKey(1) & 0xff\n        if key == ord('q'):\n            break\n        # maybe we shall print this shit somehow.\n    # video.write(frame) # you write what?\nif display:\n    cv2.destroyAllWindows()"
        }
    ]
}