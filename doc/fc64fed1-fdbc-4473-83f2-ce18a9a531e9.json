{
    "summary": "This code creates a text-to-speech synthesis model, initializes the architecture, and processes English and Chinese models. It concatenates audio files and deletes objects upon deallocation.",
    "details": [
        {
            "comment": "The code is importing necessary libraries and modules, defining model aliases for acoustic models (fastspeech2) and vocoders (pwgan), and potentially updating pretrained_models dictionary.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/tests/english_chinese_mixing_spliter/paddlebobo_paddletools_tts.py\":0-34",
            "content": "import os\nimport numpy as np\nimport paddle\nimport soundfile as sf\nimport yaml\nfrom yacs.config import CfgNode\nfrom paddlespeech.cli.utils import download_and_decompress\nfrom paddlespeech.cli.utils import MODEL_HOME\nfrom paddlespeech.t2s.frontend import English\nfrom paddlespeech.s2t.utils.dynamic_import import dynamic_import\nfrom paddlespeech.t2s.frontend.zh_frontend import Frontend\nfrom paddlespeech.t2s.modules.normalizer import ZScore\nfrom paddlespeech.cli.tts.infer import model_alias, pretrained_models\nmodel_alias2 = {\n    # acoustic model\n    \"fastspeech2\": \"paddlespeech.t2s.models.fastspeech2:FastSpeech2\",\n    \"fastspeech2_inference\": \"paddlespeech.t2s.models.fastspeech2:StyleFastSpeech2Inference\",\n    # voc\n    \"pwgan\":\n    \"paddlespeech.t2s.models.parallel_wavegan:PWGGenerator\",\n    \"pwgan_inference\":\n    \"paddlespeech.t2s.models.parallel_wavegan:PWGInference\",\n}\nmodel_alias.update(model_alias2)\n# pretrained_models = {\n#     # fastspeech2\n#     \"fastspeech2_csmsc-zh\": {\n#         'url':\n#         'https://p"
        },
        {
            "comment": "This code is a dictionary containing two models: \"fastspeech2_csmsc-zh\" and \"fastspeech2_ljspeech-en\". Each model has its URL, MD5, config file, checkpoint file, and optional statistics files. These models seem to be used for speech synthesis, as they require pitch and energy stats.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/tests/english_chinese_mixing_spliter/paddlebobo_paddletools_tts.py\":34-68",
            "content": "addlespeech.bj.bcebos.com/Parakeet/released_models/fastspeech2/fastspeech2_nosil_baker_ckpt_0.4.zip',\n#         'md5':\n#         '637d28a5e53aa60275612ba4393d5f22',\n#         'config':\n#         'default.yaml',\n#         'ckpt':\n#         'snapshot_iter_76000.pdz',\n#         'speech_stats':\n#         'speech_stats.npy',\n#         'phones_dict':\n#         'phone_id_map.txt',\n#         'pitch_stats':\n#         'pitch_stats.npy',\n#         'energy_stats':\n#         'energy_stats.npy',\n#     },\n#     # pwgan\n#     \"pwgan_csmsc-zh\": {\n#         'url':\n#         'https://paddlespeech.bj.bcebos.com/Parakeet/released_models/pwgan/pwg_baker_ckpt_0.4.zip',\n#         'md5':\n#         '2e481633325b5bdf0a3823c714d2c117',\n#         'config':\n#         'pwg_default.yaml',\n#         'ckpt':\n#         'pwg_snapshot_iter_400000.pdz',\n#         'speech_stats':\n#         'pwg_stats.npy',\n#     },\n# }\nfor k in [\"fastspeech2_csmsc-zh\",\"fastspeech2_ljspeech-en\"]:\n    model_config = {'pitch_stats':\n        'pitch_stats.npy',\n        'energy_stats':"
        },
        {
            "comment": "The code is initializing a TTSExecutor object with config and model_tag parameters. It checks if the model_tag matches the language specified, and then retrieves the necessary paths for the acoustic model, phones dictionary, and pitch statistics using the pretrained_models dictionary.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/tests/english_chinese_mixing_spliter/paddlebobo_paddletools_tts.py\":69-90",
            "content": "        'energy_stats.npy',}\n    pretrained_models[k].update(model_config)\nclass TTSExecutor():\n    def __init__(self, config,model_tag = 'fastspeech2_csmsc-zh', voc_tag = \"pwgan_csmsc-zh\",lang=\"zh\"):\n        langId1 = model_tag.split(\"-\")[-1]\n        langId2 = voc_tag.split(\"-\")[-1]\n        assert langId1 == langId2\n        assert langId2 == lang\n        assert lang in [\"zh\",\"en\"]\n        self.lang = lang\n        # match the freaking dataset!\n        #FastSpeech2 or something else. we need freaking english!\n        am_res_path = self._get_pretrained_path(model_tag)\n        am_config = os.path.join(am_res_path,pretrained_models[model_tag]['config'])\n        am_ckpt = os.path.join(am_res_path,pretrained_models[model_tag]['ckpt'])\n        am_stat = os.path.join(am_res_path, pretrained_models[model_tag]['speech_stats'])\n        # must have phones_dict in acoustic\n        phones_dict = os.path.join(am_res_path, pretrained_models[model_tag]['phones_dict'])\n        # StyleFastSpeech\n        pitch_stats = os.path.join(am_res_path, pretrained_models[model_tag]['pitch_stats'])"
        },
        {
            "comment": "This code is loading pre-trained models and configuration files for an automatic speech recognition (ASR) system. It joins different file paths, opens the configuration files to parse them into CfgNodes, and determines the vocabulary size based on a phone ID list. The code seems to be part of a larger ASR system implementation, initializing variables before using the models for prediction or inference tasks.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/tests/english_chinese_mixing_spliter/paddlebobo_paddletools_tts.py\":91-117",
            "content": "        energy_stats = os.path.join(am_res_path, pretrained_models[model_tag]['energy_stats'])\n        #VOC\n        voc_res_path = self._get_pretrained_path(voc_tag)\n        voc_config = os.path.join(voc_res_path,pretrained_models[voc_tag]['config'])\n        voc_ckpt = os.path.join(voc_res_path,pretrained_models[voc_tag]['ckpt'])\n        voc_stat = os.path.join(voc_res_path, pretrained_models[voc_tag]['speech_stats'])\n        # Init body.\n        with open(am_config) as f:\n            self.am_config = CfgNode(yaml.safe_load(f))\n        with open(voc_config) as f:\n            voc_config = CfgNode(yaml.safe_load(f))\n        with open(config) as f:\n            self.style_config = CfgNode(yaml.safe_load(f))\n        with open(phones_dict, \"r\") as f:\n            phn_id = [line.strip().split() for line in f.readlines()]\n        vocab_size = len(phn_id)\n        #print(\"vocab_size:\", vocab_size)\n        # acoustic model\n        odim = self.am_config.n_mels\n        # wtf?\n        main_name0 = model_tag.split(\"_\")[0]"
        },
        {
            "comment": "The code dynamically imports classes based on model aliases and tags, instantiates models for speech synthesis and vocoder, loads model parameters and normalization stats, and sets up the inference environment for both English and Chinese languages.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/tests/english_chinese_mixing_spliter/paddlebobo_paddletools_tts.py\":118-140",
            "content": "        am_class = dynamic_import(main_name0, model_alias)\n        am_inference_class = dynamic_import('{}_inference'.format(main_name0), model_alias)\n        am = am_class(idim=vocab_size, odim=odim, spk_num=1, **self.am_config[\"model\"])\n        am.set_state_dict(paddle.load(am_ckpt)[\"main_params\"])\n        am.eval()\n        am_mu, am_std = np.load(am_stat)\n        am_mu = paddle.to_tensor(am_mu)\n        am_std = paddle.to_tensor(am_std)\n        am_normalizer = ZScore(am_mu, am_std)\n        if lang == \"en\":\n            self.am_inference = am_inference_class(am_normalizer, am) # you can also try tensorflowTTS, hifigan with high clarity.\n        else:\n            self.am_inference = am_inference_class(am_normalizer, am, pitch_stats, energy_stats)\n        self.am_inference.eval()\n        # vocoder\n        main_name1 = voc_tag.split(\"_\")[0]\n        voc_class = dynamic_import(main_name1, model_alias)\n        voc_inference_class = dynamic_import('{}_inference'.format(main_name1), model_alias)\n        voc = voc_class(**voc_config[\"generator_params\"])"
        },
        {
            "comment": "This code sets up a model for text-to-speech (TTS) synthesis. It loads pretrained models and parameters, initializes the model architecture, applies normalization to input features, selects the appropriate frontend for the language (English or Chinese), and provides a method to download pretrained resources.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/tests/english_chinese_mixing_spliter/paddlebobo_paddletools_tts.py\":141-163",
            "content": "        voc.set_state_dict(paddle.load(voc_ckpt)[\"generator_params\"])\n        voc.remove_weight_norm()\n        voc.eval()\n        voc_mu, voc_std = np.load(voc_stat)\n        voc_mu = paddle.to_tensor(voc_mu)\n        voc_std = paddle.to_tensor(voc_std)\n        voc_normalizer = ZScore(voc_mu, voc_std)\n        self.voc_inference = voc_inference_class(voc_normalizer, voc)\n        self.voc_inference.eval()\n        if lang == \"zh\":\n            self.frontend = Frontend(phone_vocab_path=phones_dict, tone_vocab_path=None)\n        elif lang == \"en\":\n            self.phones_dict = os.path.join(\n                am_res_path, pretrained_models[model_tag]['phones_dict'])\n            self.frontend = English(phone_vocab_path=self.phones_dict)\n        else: raise Exception(\"Unknown language ID: {}\".format(lang))\n    def _get_pretrained_path(self, tag):\n        \"\"\"\n        Download and returns pretrained resources path of current task.\n        \"\"\"\n        assert tag in pretrained_models, 'Can not find pretrained resources of {}.'.format(tag)"
        },
        {
            "comment": "This code is related to a text-to-speech (TTS) system. It first downloads and decompresses the necessary pretrained model files, then processes the input text into phone_ids, which are used to generate speech using the am_inference function. The code handles both English and Chinese languages but seems to be missing tone information for Chinese.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/tests/english_chinese_mixing_spliter/paddlebobo_paddletools_tts.py\":164-186",
            "content": "        res_path = os.path.join(MODEL_HOME, tag)\n        decompressed_path = download_and_decompress(pretrained_models[tag],\n                                                    res_path)\n        decompressed_path = os.path.abspath(decompressed_path)\n        return decompressed_path\n    def run(self, text, output):\n        #\u6587\u672c\u8f93\u5165\n        sentences = [str(text)]\n        # \u957f\u53e5\u5904\u7406\n        for sentence in sentences:\n            if self.lang == \"zh\":\n                input_ids = self.frontend.get_input_ids(sentence, merge_sentences=False, get_tone_ids=False) # what the heck? no freaking tone?\n            else:\n                input_ids = self.frontend.get_input_ids(sentence, merge_sentences=False) # what the heck? no freaking tone?\n            phone_ids = input_ids[\"phone_ids\"]\n            flags = 0\n            for part_phone_ids in phone_ids:\n                with paddle.no_grad():\n                    if self.lang == \"en\":\n                        mel = self.am_inference(\n                                        part_phone_ids)"
        },
        {
            "comment": "This code chunk performs text-to-speech (TTS) conversion for Chinese language by first obtaining the Mel Spectrogram using `am_inference` function. The Mel Spectrogram is then converted to a WAV audio file using the `voc_inference` function. If flags equals 0, it assigns the result directly to `wav_all`.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/tests/english_chinese_mixing_spliter/paddlebobo_paddletools_tts.py\":187-203",
            "content": "                                        # must get the scale using ffmpeg.\n                    elif self.lang == \"zh\":\n                        mel = self.am_inference(\n                                        part_phone_ids,\n                                        durations=None,\n                                        durations_scale = 1 / float(self.style_config['TTS']['SPEED']),\n                                        durations_bias = None,\n                                        pitch = None,\n                                        pitch_scale = float(self.style_config['TTS']['PITCH']),\n                                        pitch_bias = None,\n                                        energy = float(self.style_config['TTS']['ENERGY']),\n                                        energy_scale = None,\n                                        energy_bias = None,\n                                        )\n                    wav = self.voc_inference(mel)\n                if flags == 0:\n                    wav_all = wav"
        },
        {
            "comment": "This code is concatenating audio files and saving them as a single output file. If the flag is set to 1, it stops concatenation and writes the existing audio file. The __del__ method deletes various objects when the instance is deallocated.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/tests/english_chinese_mixing_spliter/paddlebobo_paddletools_tts.py\":204-218",
            "content": "                    flags = 1\n                else:\n                    wav_all = paddle.concat([wav_all, wav])\n            sf.write(\n                output,\n                wav_all.numpy(),\n                samplerate=self.am_config.fs)\n        return output\n    # def __del__(self):\n    #     del self.voc_inference\n    #     del self.am_inference\n    #     del self.am_config\n    #     del self.style_config\n    #     del self.frontend\n    #     del self"
        }
    ]
}