{
    "summary": "This code provides image processing functions such as cropping, text recognition, and K-means clustering using EasyOCR. It also includes a customizable FastAPI server for detecting animals using PaddlePaddle's ResNet50 classifier and applies preprocessing techniques on images to extract animal objects.",
    "details": [
        {
            "comment": "The code defines two functions: `imageCropWithDiagonalRectangle` and `draw_bounding_box_with_contour`. The first function crops an image based on a given diagonal rectangle, considering both opencv and normal orders. The second function draws bounding boxes around contours in an image, using the top-k approach. It takes contours, the image, area threshold, and debug flag as inputs.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/pyjom/imagetoolbox.py\":0-34",
            "content": "from pyjom.commons import *\nimport numpy as np\nimport cv2\nfrom functools import lru_cache\nfrom lazero.utils.tools import flattenUnhashableList\nfrom typing import Literal\ndef imageCropWithDiagonalRectangle(\n    image, diagonalRectangle, order: Literal[\"opencv\", \"normal\"] = \"opencv\"\n):\n    # order is opencv.\n    assert order in [\"opencv\", \"normal\"]\n    x0, y0, x1, y1 = flattenUnhashableList(diagonalRectangle)\n    imageShape = image.shape\n    if len(imageShape) == 3:\n        if order == \"opencv\":\n            return image[y0:y1, x0:x1, :]\n        elif order == \"normal\":\n            return image[x0:x1, y0:y1, :]\n    elif len(imageShape) == 2:\n        if order == \"opencv\":\n            return image[y0:y1, x0:x1]\n        elif order == \"normal\":\n            return image[x0:x1, y0:y1]\n    else:\n        raise Exception(\"unknown image shape:\", imageShape)\ndef draw_bounding_box_with_contour(\n    contours, image, area_threshold=20, debug=False\n):  # are you sure?\n    # this is the top-k approach.\n    # Call our function to get the list of contour areas"
        },
        {
            "comment": "Looping through contours, finding largest boxes above area threshold.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/pyjom/imagetoolbox.py\":35-66",
            "content": "    # cnt_area = contour_area(contours)\n    # Loop through each contour of our image\n    x0, y0, x1, y1 = [None] * 4\n    for i in range(0, len(contours), 1):\n        cnt = contours[i]\n        # Only draw the the largest number of boxes\n        if cv2.contourArea(cnt) > area_threshold:\n            # if (cv2.contourArea(cnt) > cnt_area[number_of_boxes]):\n            # Use OpenCV boundingRect function to get the details of the contour\n            x, y, w, h = cv2.boundingRect(cnt)\n            if x0 == None:\n                x0, y0, x1, y1 = x, y, x + w, y + h\n            if x < x0:\n                x0 = x\n            if y < y0:\n                y0 = y\n            if x + w > x1:\n                x1 = x + w\n            if y + h > y1:\n                y1 = y + h\n            # Draw the bounding box\n    if x0 is not None:\n        if debug:\n            image = cv2.rectangle(image, (x0, y0), (x1, y1), (0, 0, 255), 2)\n            cv2.imshow(\"with_bounding_box\", image)\n            cv2.waitKey(0)\n    if x0 is None:\n        height, width = image.shape[:2]"
        },
        {
            "comment": "The code defines a function `imageLoader` that loads an image based on its type (file path or URL). It checks if the image is a file path and reads it using OpenCV, if it's a URL, it uses requests to download the content and decodes it into an image using OpenCV. If the image is neither a file path nor a valid URL, it raises an exception. The function `getDeltaWidthHeight` calculates the aspect ratio adjustments for resizing images based on default width and height, ensuring they are within specific ranges.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/pyjom/imagetoolbox.py\":67-96",
            "content": "        x0, y0, x1, y1 = 0, 0, width, height\n    return (x0, y0), (x1, y1)\ndef imageLoader(image):\n    if type(image) == str:\n        if os.path.exists(image):\n            image = cv2.imread(image)\n        elif image.startswith(\"http\"):\n            import requests\n            r = requests.get(image)\n            content = r.content\n            content = np.asarray(bytearray(content), dtype=\"uint8\")\n            image = cv2.imdecode(content, cv2.IMREAD_COLOR)\n        else:\n            raise Exception(\"unknown image link: %s\" % image)\n    return image\ndef getDeltaWidthHeight(defaultWidth, defaultHeight):\n    deltaWidthRatio = 4 + (4 - 3) * (defaultWidth / defaultHeight - 16 / 9) / (\n        16 / 9 - 9 / 16\n    )\n    deltaWidthRatio = makeValueInRange(deltaWidthRatio, 3, 4)\n    deltaHeightRatio = 8 + (8 - 6) * (defaultHeight / defaultWidth - 16 / 9) / (\n        16 / 9 - 9 / 16\n    )\n    deltaHeightRatio = makeValueInRange(deltaHeightRatio, 6, 8)\n    deltaWidth, deltaHeight = int(defaultWidth / deltaWidthRatio), int("
        },
        {
            "comment": "Function 'getFourCorners' takes in x, y, defaultWidth and defaultHeight coordinates and returns the coordinates of four corners of an image by calculating deltaWidth and deltaHeight using 'getDeltaWidthHeight'. It scales the image while maintaining its aspect ratio.\n\nThe function 'getEasyOCRReader' imports the 'easyocr' library and creates a Reader object with specified language (langs), GPU usage (gpu) and recognizer type (recognizer). This function utilizes caching using the decorator '@lru_cache(maxsize=1)' to speed up future calls by storing the last N results.\n\nThe function 'getImageTextAreaRecognized' takes an image, optional language parameter (langs), GPU and recognizer settings, and returns the recognized text from the image using the EasyOCR Reader created in 'getEasyOCRReader'. The results can be optionally returned with their respective coordinates by setting 'return_res=True'.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/pyjom/imagetoolbox.py\":97-129",
            "content": "        defaultHeight / deltaHeightRatio\n    )\n    return deltaWidth, deltaHeight\ndef getFourCorners(x, y, defaultWidth, defaultHeight):\n    deltaWidth, deltaHeight = getDeltaWidthHeight(defaultWidth, defaultHeight)\n    # (x1, y1), (x2, y2)\n    fourCorners = [\n        [(0, 0), (deltaWidth, deltaHeight)],\n        [(defaultWidth - deltaWidth, 0), (defaultWidth, deltaHeight)],\n        [\n            (defaultWidth - deltaWidth, defaultHeight - deltaHeight),\n            (defaultWidth, defaultHeight),\n        ],\n        [(0, defaultHeight - deltaHeight), (deltaWidth, defaultHeight)],\n    ]\n    fourCorners = [[(a + x, b + y), (c + x, d + y)] for [(a, b), (c, d)] in fourCorners]\n    return fourCorners\n@lru_cache(maxsize=1)\ndef getEasyOCRReader(langs: tuple, gpu=True, recognizer=False):\n    import easyocr\n    # no metal? no dbnet18?\n    reader = easyocr.Reader(langs, gpu=gpu, recognizer=recognizer)\n    return reader\n# @lru_cache(maxsize=30)\ndef getImageTextAreaRecognized(\n    image, langs: tuple = (\"en\",), gpu=True, recognizer=False, return_res=False"
        },
        {
            "comment": "This code defines a function that detects and recognizes text in an image using EasyOCR reader. It also includes a partial_blur function for image processing with optional kernel size.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/pyjom/imagetoolbox.py\":130-164",
            "content": "):\n    reader = getEasyOCRReader(langs, gpu=gpu, recognizer=recognizer)\n    if type(image) == str:\n        image = cv2.imread(image)\n    frame = image\n    height, width = frame.shape[:2]\n    res = (width, height)\n    detection, recognition = reader.detect(frame)  # not very sure.\n    if return_res:\n        return res, (detection, recognition)\n    else:\n        return detection, recognition\nfrom typing import Literal\ndef partial_blur(image0, mask, kernel=None):\n    # need improvement. malnly the boundary.\n    if kernel is None:\n        height, width = image0.shape[:2]\n        kernel_w = max(int(width / 40), 1) * 4\n        kernel_h = max(int(height / 40), 1) * 4\n    else:\n        kernel_w, kernel_h = kernel\n        kernel_w = max(int(kernel_w / 4), 1) * 4\n        kernel_h = max(int(kernel_h / 4), 1) * 4\n    kernel = (kernel_w, kernel_h)\n    mask_total = mask\n    inv_mask_total = 255 - mask_total\n    # mask0 = mask\n    # mask0 = mask/255\n    # inv_mask0 = inv_mask/255\n    non_blur_image = cv2.bitwise_and(image0, image0, mask=inv_mask_total)"
        },
        {
            "comment": "The code provides three functions: 'partial_blur', 'imageInpainting', and 'imageFourCornersInpainting'. The first function, 'partial_blur', applies blur to an image while preserving certain areas specified by a mask. The second function, 'imageInpainting', takes an image and a mask, and using the OpenCV library's inpaint or blur methods, replaces the specified area in the image with the corresponding color from its surroundings. The last function, 'imageFourCornersInpainting', reads an image from a file or takes a string path of the image, creates a black image of the same size, and then uses the 'getFourCorners' function to draw rectangles around four corners of the image. It finally applies inpainting on the entire image using the 'imageInpainting' function.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/pyjom/imagetoolbox.py\":165-190",
            "content": "    blur_image0 = cv2.blur(image0, kernel)  # half quicklier.\n    blur_image0 = cv2.bitwise_and(blur_image0, blur_image0, mask=mask_total)\n    dst0 = blur_image0 + non_blur_image\n    return dst0\ndef imageInpainting(image, mask, method: Literal[\"inpaint\", \"blur\"] = \"inpaint\"):\n    if method == \"inpaint\":\n        return cv2.inpaint(image, mask, 3, cv2.INPAINT_TELEA)\n    elif method == \"blur\":\n        return partial_blur(image, mask)\n    else:\n        raise Exception(\"image inpainting method not supported:\", method)\ndef imageFourCornersInpainting(image, method=\"inpaint\"):\n    if type(image) == str:\n        image = cv2.imread(image)\n    defaultHeight, defaultWidth = image.shape[:2]\n    fourCorners = getFourCorners(0, 0, defaultWidth, defaultHeight)\n    img = np.zeros((defaultHeight, defaultWidth), dtype=np.uint8)\n    for (x1, y1), (x2, y2) in fourCorners:\n        w, h = x2 - x1, y2 - y1\n        x, y = x1, y1\n        cv2.rectangle(img, (x, y), (x + w, y + h), 255, -1)\n    return imageInpainting(image, img, method=method)"
        },
        {
            "comment": "This function calculates the text area ratio of an image by detecting and recognizing the text. If edge detection is enabled, it applies Canny edge detection to the image. It then passes the image through getImageTextAreaRecognized function to get the detection and recognition results. It creates a zero matrix with the same dimensions as the image and draws rectangles around detected text areas on this matrix. Finally, it calculates the text area ratio by summing the pixels in the matrix and dividing it by the total possible value for the matrix multiplied by the image's width and height. If debug is enabled, it prints the text area percentage and displays an image showing the text areas.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/pyjom/imagetoolbox.py\":193-224",
            "content": "def getImageTextAreaRatio(\n    image,\n    langs: tuple = (\"en\",),\n    gpu=True,\n    recognizer=False,\n    debug=False,\n    inpaint=False,\n    method=\"inpaint\",\n    edgeDetection=False,\n):\n    image_passed = image.copy()\n    if edgeDetection:\n        image_passed = cv2.Canny(image_passed, 20, 210, apertureSize=3)\n    res, (detection, recognition) = getImageTextAreaRecognized(\n        image_passed, langs=langs, gpu=gpu, recognizer=recognizer, return_res=True\n    )\n    width, height = res\n    img = np.zeros((height, width), dtype=np.uint8)\n    if detection == [[]]:\n        diagonalRects = []\n    else:\n        diagonalRects = [LRTBToDiagonal(x) for x in detection[0]]\n    for x1, y1, x2, y2 in diagonalRects:\n        w, h = x2 - x1, y2 - y1\n        x, y = x1, y1\n        cv2.rectangle(img, (x, y), (x + w, y + h), 255, -1)\n    # calculate the portion of the text area.\n    textArea = np.sum(img)\n    textAreaRatio = (textArea / 255) / (width * height)\n    if debug:\n        print(\"text area: {:.2f} %\".format(textAreaRatio))\n        cv2.imshow(\"TEXT AREA\", img)"
        },
        {
            "comment": "This code contains several image processing functions:\n1. cv2.waitKey(0) displays the window containing an image for a specific time, or until a key is pressed.\n2. imageDenoising uses fastNlMeansDenoisingColored or fastNlMeansDenoising to remove noise from images.\n3. LRTBToDiagonal takes a rectangle (left, right, top, bottom) and returns its diagonal as (x0, y0, x1, y1).\n4. getImageColorCentrality identifies central colors in an image using k-means clustering, considering the percentage of nearby pixels to determine color centrality.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/pyjom/imagetoolbox.py\":225-266",
            "content": "        cv2.waitKey(0)\n    if inpaint:\n        return imageInpainting(image, img, method=method)\n    return textAreaRatio\ndef LRTBToDiagonal(lrtb):\n    left, right, top, bottom = lrtb\n    x0, y0, x1, y1 = left, top, right, bottom\n    return (x0, y0, x1, y1)\ndef imageDenoise(image):\n    shape = image.shape\n    if len(shape) == 3:\n        if shape[2] == 3:\n            return cv2.fastNlMeansDenoisingColored(image, None, 10, 10, 7, 21)\n    return cv2.fastNlMeansDenoising(image, None, 4, 7, 35)\ndef getImageColorCentrality(\n    image,\n    sample_size_limit=5000,\n    epsilon=0.01,  # shit man.\n    shift=2,\n    n_clusters=5,\n    batch_size=45,\n    max_no_improvement=10,\n):\n    # image is of numpy.array\n    # multiple centers.\n    # CENTER: [246.76865924 226.40763256 216.41472476]\n    # POSITIVE COUNT: 95497\n    # SUM: 286491.0 MIN: 0 MAX: 3\n    # NEARBY CENTER PERCENTAGE: 6.74 %\n    # CENTRALITY: 7.32 %\n    import numpy as np\n    # image = cv2.imread(src)\n    shape = image.shape\n    if len(shape) > 3 or len(shape) < 2:\n        print(\"weird image shape for getImageColorCentrality:\", shape)"
        },
        {
            "comment": "This code seems to be performing image reshaping and sampling, potentially for a data processing or analysis task. The code initially reshapes the image into a 2D array of shape (-1, 3), then selects random samples from it based on a specified sample size limit. It also includes debugging print statements and potential breakpoints, indicating some experimentation or debugging process. Overall, the purpose of this section appears to be data processing for further analysis or usage.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/pyjom/imagetoolbox.py\":267-306",
            "content": "        breakpoint()\n    if len(shape) == 2:\n        image = image.reshape(-1, -1, 1)\n    # for i in range(3):\n    #     image[:,:,i] = i\n    # col_0, col_1 = shape[:2]\n    # coords = []\n    # for c0 in range(col_0):\n    #     for c1 in range(col_1):\n    #         coords.append((c0,c1))\n    # coords = np.array(coords)\n    # print(image.reshape(-1,3))\n    reshapedImage = image.reshape(-1, 3)  # are you sure about this?\n    length, color_channels = reshapedImage.shape\n    reshapedImageIndexs = np.arange(0, length)\n    # so now it is good.\n    sampleIndexs = np.random.choice(\n        reshapedImageIndexs, size=min(sample_size_limit, length)\n    )\n    # print(sampleIndexs)\n    # print(sampleIndexs.shape)\n    sample = reshapedImageIndexs[sampleIndexs]\n    sample = reshapedImage[sample, :]\n    # print(sample)\n    # print(sample.shape)\n    # breakpoint()\n    # sampleCoords = coords[sampleIndexs]\n    # sample = np.hstack([sample, sampleCoords])\n    # print(sample)\n    # print(sample.shape)\n    # breakpoint()\n    # warning: OOM?"
        },
        {
            "comment": "Code attempts to cluster the sample data using KMeans algorithm and handles potential error logs by ignoring all errors.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/pyjom/imagetoolbox.py\":307-339",
            "content": "    # now cluster shit shall we?\n    # from sklearn.neighbors import NearestNeighbors\n    # neigh = NearestNeighbors(n_neighbors=5)\n    # X = sample\n    # neigh.fit(X)\n    # A = neigh.kneighbors_graph(X)\n    # A.toarray()\n    # print(A)\n    # print(A.shape) # sparse matrix? wtf?\n    # from sklearn.cluster import MiniBatchKMeans  # better?\n    from sklearn.cluster import KMeans\n    X = sample\n    # kmeans = KMeans(n_clusters=5).fit(X) # not deterministic please?\n    # here we've got issue.\n    # import numpy as np\n    # np.seterr(all='ignore')\n    kmeans_model = KMeans(init=\"k-means++\", n_clusters=n_clusters)\n    kmeans = kmeans_model.fit(X)  # fix this shit\n    # keep popping up error logs.\n    # kmeans = MiniBatchKMeans(\n    #     init=\"k-means++\",\n    #     n_clusters=n_clusters,\n    #     batch_size=batch_size,\n    #     # n_init=10,\n    #     max_no_improvement=max_no_improvement,\n    #     verbose=0,\n    # ).fit(X)\n    # from lazero.utils import inspectObject\n    # inspectObject(kmeans)\n    # breakpoint()\n    # labels = kmeans.labels_"
        },
        {
            "comment": "Code performs K-means clustering and calculates the percentage of labeled points in each cluster. It then creates a mask based on the cluster centers and applies it to an image, counting the positive pixels and checking if they are close to 3.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/pyjom/imagetoolbox.py\":340-368",
            "content": "    cluster_centers = kmeans.cluster_centers_\n    # print(labels)\n    # print(cluster_centers)\n    # sample_size = len(sampleIndexs) # this is the real sample size.\n    # label_percentage = {\n    #     x: np.count_nonzero(labels == x) / sample_size for x in range(n_clusters)\n    # }\n    flagged_image = image.copy()\n    flagged_image[:, :, :] = 1  # every element is 1 now.\n    percents = []\n    for center in cluster_centers:\n        # fetch area nearby given center\n        # center = center5[:3]\n        # center_int = center.astype(np.uint8)\n        # i just don't know what the fuck is going on here.\n        upper = center + shift\n        lower = center - shift\n        mask = cv2.inRange(image, lower, upper)\n        # not image.\n        output = cv2.bitwise_and(flagged_image, flagged_image, mask=mask)\n        # print(output)\n        # print(output.shape)\n        mOutput = output.reshape(-1, 3)\n        mOutput = np.sum(mOutput, axis=1)\n        # breakpoint()\n        positive_count = np.count_nonzero(abs(mOutput - 3) < epsilon)"
        },
        {
            "comment": "The code calculates the centrality and maximum nearby center percentage of image features using K-means clustering. It scans an image with a resizable window, returns centrality and max nearby center percentages, and optionally the direction if specified.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/pyjom/imagetoolbox.py\":369-402",
            "content": "        percent = positive_count / len(mOutput)\n        # print(mOutput)\n        # print(mOutput.shape)\n        # breakpoint()\n        # print(\"CENTER:\",center)\n        # print('POSITIVE COUNT:', positive_count)\n        # mSum = sum(mOutput)\n        # print(\"SUM:\", mSum, \"MIN:\", min(mOutput), 'MAX:', max(mOutput))\n        # print(\"NEARBY CENTER PERCENTAGE: {:.2f} %\".format(percent*100))\n        percents.append(percent)\n    max_nearby_center_percentage = max(percents)\n    print(\n        \"NEARBY CENTER PERCENTAGE: {:.2f} %\".format(max_nearby_center_percentage * 100)\n    )\n    centrality = sum(percents)\n    print(\"CENTRALITY: {:.2f} %\".format(centrality * 100))\n    del kmeans\n    del kmeans_model\n    return centrality, max_nearby_center_percentage\nimport math\ndef scanImageWithWindowSizeAutoResize(\n    image,\n    width,\n    height,\n    return_direction=False,\n    threshold=0.1,  # minimum 'fresh' area left for scanning\n):  # shall you use torch? no?\n    shape = image.shape\n    assert len(shape) == 3\n    ih, iw, channels = shape"
        },
        {
            "comment": "This code resizes an image to a target size while considering the aspect ratio, and then determines the scan direction based on the width-to-height ratio. If the ratios are equal, it adds the resized image as the only element in the image series. Otherwise, if the width-to-height ratio is less than the target ratio, it scans vertically along the height axis, defining a start and end point for each segment based on threshold values.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/pyjom/imagetoolbox.py\":403-427",
            "content": "    targetWidth = max(width, math.floor(iw * height / ih))\n    targetHeight = max(height, math.floor(ih * width / iw))\n    resized = cv2.resize(\n        image, (targetWidth, targetHeight), interpolation=cv2.INTER_CUBIC\n    )\n    # determine scan direction here.\n    imageSeries = []\n    if targetWidth / targetHeight == width / height:\n        imageSeries = [resized]  # as image series.\n        direction = None\n    elif targetWidth / targetHeight < width / height:\n        direction = \"vertical\"\n        # the scanning is along the vertical axis, which is the height.\n        index = 0\n        while True:\n            start, end = height * index, height * (index + 1)\n            if start < targetHeight:\n                if end > targetHeight:\n                    if 1 - (end - targetHeight) / targetHeight >= threshold:\n                        end = targetHeight\n                        start = targetHeight - height\n                    else:\n                        break\n                # other conditions, just fine\n            else:"
        },
        {
            "comment": "This code snippet is used for cropping an image based on a target width, either vertically or horizontally. If the target width cannot be reached, it stops cropping. It appends each cropped section to a list of images and returns them along with the direction (horizontal or vertical) if specified.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/pyjom/imagetoolbox.py\":428-459",
            "content": "                break  # must exit since nothing to scan.\n            cropped = resized[start:end, :, :]  # height, width, channels\n            imageSeries.append(cropped)\n            index += 1\n    else:\n        direction = \"horizontal\"\n        index = 0\n        while True:\n            start, end = width * index, width * (index + 1)\n            if start < targetWidth:\n                if end > targetWidth:\n                    if 1 - (end - targetWidth) / targetWidth >= threshold:\n                        end = targetWidth\n                        start = targetWidth - width\n                    else:\n                        break\n                # other conditions, just fine\n            else:\n                break  # must exit since nothing to scan.\n            cropped = resized[:, start:end, :]  # height, width, channels\n            imageSeries.append(cropped)\n            index += 1\n    if return_direction:\n        return imageSeries, direction\n    else:\n        return imageSeries\nfrom typing import Literal\ndef resizeImageWithPadding("
        },
        {
            "comment": "This code resizes an image while maintaining its aspect ratio, allowing for optional border padding. It first checks if width and height are of type int. If not, it assigns default values based on the provided parameters. Then, calculates targetWidth and targetHeight to ensure proper scaling without distortion. Finally, uses OpenCV's cv2.copyMakeBorder function with a constant black value for border_type to create the final padded image.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/pyjom/imagetoolbox.py\":460-486",
            "content": "    image,\n    width: Union[int, None],\n    height: Union[int, None],\n    border_type: Literal[\"constant_black\", \"replicate\"] = \"constant_black\",\n):\n    assert any([type(param) == int for param in [width, height]])\n    shape = image.shape\n    assert len(shape) == 3\n    ih, iw, channels = shape\n    if width is None:\n        width = max(1, math.floor((height / ih) * iw))\n    if height is None:\n        height = max(1, math.floor((width / iw) * ih))\n    targetWidth = min(width, math.floor(iw * height / ih))\n    targetHeight = min(height, math.floor(ih * width / iw))\n    resized = cv2.resize(\n        image, (targetWidth, targetHeight), interpolation=cv2.INTER_CUBIC\n    )\n    BLACK = [0] * channels\n    top = max(0, math.floor((height - targetHeight) / 2))\n    bottom = max(0, height - targetHeight - top)\n    left = max(0, math.floor((width - targetWidth) / 2))\n    right = max(0, width - targetWidth - left)\n    if border_type == \"constant_black\":\n        padded = cv2.copyMakeBorder(\n            resized, top, bottom, left, right, cv2.BORDER_CONSTANT, value=BLACK"
        },
        {
            "comment": "This code defines functions to get a PaddlePaddle ResNet50 animals classifier, read label files, and possibly connects to a BEZIER server for a PaddlePaddle ResNet50 dog/cat detector.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/pyjom/imagetoolbox.py\":487-521",
            "content": "        )\n    elif border_type == \"replicate\":\n        padded = cv2.copyMakeBorder(\n            resized, top, bottom, left, right, cv2.BORDER_REPLICATE, value=BLACK\n        )\n    else:\n        raise Exception(\"unknown border_type: %s\" % border_type)\n    return padded\nimport paddlehub as hub\nfrom functools import lru_cache\n@lru_cache(maxsize=1)\ndef getPaddleResnet50AnimalsClassifier():\n    classifier = hub.Module(name=\"resnet50_vd_animals\")\n    return classifier\n@lru_cache(maxsize=3)\ndef labelFileReader(filename):\n    with open(filename, \"r\") as f:\n        content = f.read()\n        content = content.split(\"\\n\")\n        content = [elem.replace(\"\\n\", \"\").strip() for elem in content]\n        content = [elem for elem in content if len(elem) > 0]\n    return content\nfrom pyjom.mathlib import multiParameterExponentialNetwork\nBEZIER_PADDLE_RESNET50_IMAGE_DOG_CAT_DETECTOR_SERVER_ENDPOINT = \"analyzeImage\"\nBEZIER_PADDLE_RESNET50_IMAGE_DOG_CAT_DETECTOR_SERVER_PORT = 4675\nBEZIER_PADDLE_RESNET50_IMAGE_DOG_CAT_DETECTOR_SERVER_HELLO = ("
        },
        {
            "comment": "The code defines a function for the Bezier PaddleHub Resnet50 Image DogCat Detector Server checker, which waits for the server to be up and running. If the server instance is not configured in the pyjom_config, it calls the checker function. The function also includes parameters for image, port, endpoint, input_bias, skew, dog_label_file_path, cat, and other possible parameters.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/pyjom/imagetoolbox.py\":522-551",
            "content": "    \"Bezier PaddleHub Resnet50 Image DogCat Detector Server\"\n)\nfrom pyjom.config.shared import pyjom_config\n# TODO: support serving and with redis lock\nfrom lazero.network.checker import waitForServerUp\ndef bezierPaddleHubResnet50ImageDogCatDetectorServerChecker(\n    port=BEZIER_PADDLE_RESNET50_IMAGE_DOG_CAT_DETECTOR_SERVER_PORT,\n    message=BEZIER_PADDLE_RESNET50_IMAGE_DOG_CAT_DETECTOR_SERVER_HELLO,\n):\n    waitForServerUp(port=port, message=message)\n# fuck?\nif not pyjom_config[\"BEZIER_PADDLE_RESNET50_IMAGE_DOG_CAT_DETECTOR_SERVER_INSTANCE\"]:\n    bezierPaddleHubResnet50ImageDogCatDetectorServerChecker()\ndef bezierPaddleHubResnet50ImageDogCatDetectorClient(\n    image,\n    port=BEZIER_PADDLE_RESNET50_IMAGE_DOG_CAT_DETECTOR_SERVER_PORT,\n    endpoint=BEZIER_PADDLE_RESNET50_IMAGE_DOG_CAT_DETECTOR_SERVER_ENDPOINT,\n    input_bias=0.0830047243746045,\n    skew=-0.4986098769473948,\n    # threshold=0.5,\n    dog_label_file_path=\"/root/Desktop/works/pyjom/tests/animals_paddlehub_classification_resnet/dogs.txt\",\n    cat"
        },
        {
            "comment": "This function uses a PaddleHub ResNet50 model to classify images as either cat or dog. It takes an image input, and optionally adjusts the input_bias and skew parameters for fine-tuning. The labels for cats and dogs are specified in separate text files. If the image is not in byte format, it converts it using numpy_serializer before making a POST request to a local server endpoint. It returns the response in JSON format.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/pyjom/imagetoolbox.py\":551-582",
            "content": "_label_file_path=\"/root/Desktop/works/pyjom/tests/animals_paddlehub_classification_resnet/cats.txt\",\n    debug=False,\n):\n    isString = type(image) == str\n    import requests\n    url = \"http://localhost:{}/{}\".format(port, endpoint)\n    import numpy_serializer\n    if type(image) == np.ndarray:\n        image = numpy_serializer.to_bytes(image)\n    data = dict(image=image)\n    params = dict(\n        input_bias=input_bias,\n        isBytes=not isString,\n        skew=skew,\n        dog_label_file_path=dog_label_file_path,\n        cat_label_file_path=cat_label_file_path,\n        debug=debug,\n    )\n    r = requests.post(url, data=data, params=params)\n    # what is the result? fuck?\n    return r.json()\ndef bezierPaddleHubResnet50ImageDogCatDetectorCore(\n    image,\n    input_bias=0.0830047243746045,\n    skew=-0.4986098769473948,\n    # threshold=0.5,\n    dog_label_file_path=\"/root/Desktop/works/pyjom/tests/animals_paddlehub_classification_resnet/dogs.txt\",\n    cat_label_file_path=\"/root/Desktop/works/pyjom/tests/animals_paddlehub_classification_resnet/cats.txt\","
        },
        {
            "comment": "The code reads image labels from two label files and checks if the given name matches any of them or ends with a specific dog/cat suffix. If it does not contain forbidden words, it categorizes the input as a \"dog\" or \"cat\". The function is used to detect if an image contains a dog or cat based on the provided name.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/pyjom/imagetoolbox.py\":583-624",
            "content": "    debug=False,\n    use_gpu=False,\n    dog_suffixs=[\"\u72d7\", \"\u72ac\", \"\u6897\"],\n    cat_suffixs=[\"\u732b\"],\n    forbidden_words=[\n        \"\u7075\u732b\",\n        \"\u718a\u732b\",\n        \"\u732b\u72ee\",\n        \"\u732b\u5934\u9e70\",\n        \"\u4e01\u4e01\u732b\u513f\",\n        \"\u7eff\u732b\u9e1f\",\n        \"\u732b\u9f2c\",\n        \"\u732b\u9c7c\",\n        \"\u73bb\u7483\u732b\",\n        \"\u732b\u773c\",\n        \"\u732b\u86f1\u8776\",\n    ],\n):\n    curve_function_kwargs = {\n        \"start\": (0, 0),\n        \"end\": (1, 1),\n        \"skew\": skew,\n    }  # maximize the output.\n    if type(image) == str:\n        image = cv2.imread(image)\n    frame = image\n    # ends with this, and not containing forbidden words.\n    dog_labels = labelFileReader(dog_label_file_path)\n    cat_labels = labelFileReader(cat_label_file_path)\n    def dog_cat_name_recognizer(name):\n        if name in dog_labels:\n            return \"dog\"\n        elif name in cat_labels:\n            return \"cat\"\n        elif name not in forbidden_words:\n            for dog_suffix in dog_suffixs:\n                if name.endswith(dog_suffix):\n                    return \"dog\"\n            for cat_suffix in cat_suffixs:\n                if name.endswith(cat_suffix):"
        },
        {
            "comment": "This code segment is responsible for performing animal detection and classifying the detected animals as either \"cat\" or None. It first defines two functions: `paddleAnimalDetectionResultToList()` for converting the detection result into a sorted list of names and confidence values, and `translateResultListToDogCatList()` for translating the list to distinguish between dogs and cats using the `dog_cat_name_recognizer`. The code then initializes a PaddleResnet50AnimalsClassifier and performs animal detection on the provided frame, getting the top 3 classification results. These results are converted into a sorted list of dog/cat names with confidence values, which can be used to determine if there is a cat present in the frame.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/pyjom/imagetoolbox.py\":625-651",
            "content": "                    return \"cat\"\n        return None\n    classifier = getPaddleResnet50AnimalsClassifier()\n    def paddleAnimalDetectionResultToList(result):\n        resultDict = result[0]\n        resultList = [(key, value) for key, value in resultDict.items()]\n        resultList.sort(key=lambda item: -item[1])\n        return resultList\n    def translateResultListToDogCatList(resultList):\n        final_result_list = []\n        for name, confidence in resultList:\n            new_name = dog_cat_name_recognizer(name)\n            final_result_list.append((new_name, confidence))\n        return final_result_list\n    # dataList = []\n    # for frame in getVideoFrameIteratorWithFPS(videoPath, -1, -1, fps=1):\n    padded_resized_frame = resizeImageWithPadding(\n        frame, 224, 224, border_type=\"replicate\"\n    )  # pass the test only if three of these containing 'cats'\n    result = classifier.classification(\n        images=[padded_resized_frame], top_k=3, use_gpu=use_gpu  # cuda oom?\n    )  # check it?\n    resultList = paddleAnimalDetectionResultToList(result)"
        },
        {
            "comment": "The code translates a result list to dog/cat detection list and applies a multi-parameter exponential network function to each detection's confidence score. The detections are appended to a list with respective identities and confidences, then returned as the final output. It uses PaddleHub ResNet model for dog/cat detection on input images.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/pyjom/imagetoolbox.py\":652-675",
            "content": "    final_result_list = translateResultListToDogCatList(resultList)\n    if debug:\n        sprint(\"RESULT LIST:\", final_result_list)\n    detections = []\n    for index, (label, confidence) in enumerate(final_result_list):\n        scope = final_result_list[index:]\n        scope_confidences = [elem[1] for elem in scope if elem[0] == label]\n        output = multiParameterExponentialNetwork(\n            *scope_confidences,\n            input_bias=input_bias,\n            curve_function_kwargs=curve_function_kwargs,\n        )\n        # treat each as a separate observation in this frame.\n        detections.append({\"identity\": label, \"confidence\": output})\n    return detections\ndef bezierPaddleHubResnet50ImageDogCatDetector(\n    image,\n    input_bias=0.0830047243746045,\n    skew=-0.4986098769473948,\n    # threshold=0.5,\n    dog_label_file_path=\"/root/Desktop/works/pyjom/tests/animals_paddlehub_classification_resnet/dogs.txt\",\n    cat_label_file_path=\"/root/Desktop/works/pyjom/tests/animals_paddlehub_classification_resnet/cats.txt\","
        },
        {
            "comment": "This code defines a function `bezierPaddleHubResnet50ImageDogCatDetector` that takes an image and some optional parameters, and returns dog and cat detection results. If the `as_client` parameter is True, it creates a client instance of the detector; otherwise, it calls the core detection function. The code also includes a comment about adding a Redis-based lock for image checks in the future. The function also imports necessary modules and sets up a Redis connection for potential future use.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/pyjom/imagetoolbox.py\":676-711",
            "content": "    debug=False,\n    use_gpu=False,\n    as_client=True,  # by default!\n):\n    if as_client:\n        return bezierPaddleHubResnet50ImageDogCatDetectorClient(\n            image,\n            input_bias=input_bias,\n            skew=skew,\n            dog_label_file_path=dog_label_file_path,\n            cat_label_file_path=cat_label_file_path,\n            debug=debug,\n        )\n    # from pyjom.imagetoolbox import resizeImageWithPadding\n    detections = bezierPaddleHubResnet50ImageDogCatDetectorCore(\n        image,\n        input_bias=input_bias,\n        skew=skew,\n        dog_label_file_path=dog_label_file_path,\n        cat_label_file_path=cat_label_file_path,\n        debug=debug,\n        use_gpu=use_gpu,\n    )\n    return detections\n# TODO: create/get a redis based lock when doing image checks.\nimport redis_lock\nimport redis\nfrom pyjom.commons import commonRedisPort\nredis_connection = redis.StrictRedis(port=commonRedisPort)\ndef bezierPaddleHubResnet50ImageDogCatDetectorServer(\n    server_port=BEZIER_PADDLE_RESNET50_IMAGE_DOG_CAT_DETECTOR_SERVER_PORT,"
        },
        {
            "comment": "Endpoint definition for a FastAPI app with two routes: a GET endpoint to return a server hello message and a POST endpoint to receive an image. The app uses numpy_serializer, allows specifying additional parameters like debug mode, input bias, skew, and label file paths for dog and cat classification models.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/pyjom/imagetoolbox.py\":712-739",
            "content": "    endpoint=BEZIER_PADDLE_RESNET50_IMAGE_DOG_CAT_DETECTOR_SERVER_ENDPOINT,\n    serverHelloMessage: str = BEZIER_PADDLE_RESNET50_IMAGE_DOG_CAT_DETECTOR_SERVER_HELLO,\n    connection: redis.Redis = redis_connection,\n    lockName: str = \"bezier_paddlehub_resnet50_image_dog_cat_detector_server\",\n    timeout: float = 10,\n    expire: float = 60,\n):\n    from fastapi import FastAPI, Body\n    import numpy_serializer\n    app = FastAPI()\n    @app.get(\"/\")\n    def serverHello():\n        return serverHelloMessage\n    @app.post(\"/\" + endpoint)\n    def receiveImage(\n        image: bytes = Body(default=None),\n        isBytes: bool = False,\n        encoding: str = \"utf-8\",\n        debug: bool = False,\n        input_bias: float = 0.0830047243746045,\n        skew: float = -0.4986098769473948,\n        dog_label_file_path: str = \"/root/Desktop/works/pyjom/tests/animals_paddlehub_classification_resnet/dogs.txt\",\n        cat_label_file_path: str = \"/root/Desktop/works/pyjom/tests/animals_paddlehub_classification_resnet/cats.txt\",\n        download_timeout: int = 2,"
        },
        {
            "comment": "Code handles image processing with Redis locking mechanism and urllib parsing. It attempts to acquire a lock, then checks if the image data is in bytes or not, decoding it accordingly. If the image URL starts with \"http\", it uses requests module to download the image content before proceeding. The code also mentions a warning about handling GIFs but the specifics are not explained.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/pyjom/imagetoolbox.py\":740-765",
            "content": "    ):\n        detections = []  # nothing good.\n        try:\n            lock = redis_lock.Lock(connection, name=lockName, expire=expire)\n            if lock.acquire(blocking=True, timeout=timeout):\n                # return book\n                # print('image type:',type(image))\n                # print(image)\n                import urllib.parse\n                image = image.removeprefix(b\"image=\")  # fuck man.\n                image = urllib.parse.unquote_to_bytes(image)\n                if debug:\n                    print(\"isBytes:\", isBytes)\n                if not isBytes:\n                    image = image.decode(encoding)  # fuck?\n                    # read image from path, url\n                    if image.startswith(\"http\"):\n                        import requests\n                        img_bytes = requests.get(\n                            image, proxies=None, timeout=download_timeout\n                        ).content\n                        # warning! you deal with gif somehow!\n                        import tempfile"
        },
        {
            "comment": "This code reads an image or video from a given file path. It creates a temporary file, writes the image bytes to it, and tries to read it as an image using OpenCV. If that fails, it attempts to open it with VideoCapture and read the first frame. If all else fails, it prints an error message.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/pyjom/imagetoolbox.py\":767-785",
            "content": "                        with tempfile.NamedTemporaryFile(\"wb\", suffix=\".media\") as f:\n                            filepath = f.name\n                            f.write(img_bytes)\n                            try:\n                                image = cv2.imread(filepath)\n                                if image is None:\n                                    cap = cv2.VideoCapture(filepath)\n                                    success, image = cap.read()\n                                    if not success:\n                                        image = None\n                            except:\n                                import traceback\n                                traceback.print_exc()\n                                print(\"error while reading visual media file from web.\")\n                                print(\"source url:\", image)\n                        # nparr = np.fromstring(img_bytes, np.uint8)\n                        # image = cv2.imdecode(nparr, flags=1)\n                    elif os.path.exists(image):"
        },
        {
            "comment": "This code attempts to load an image from a specified URL or filepath. If the image cannot be found, it raises an exception. It then applies image processing and detection using the bezierPaddleHubResnet50ImageDogCatDetectorCore function, and potentially returns the detections. The code also includes optional debugging functionality to print image shape and detections for further inspection.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/pyjom/imagetoolbox.py\":786-813",
            "content": "                        image = cv2.imread(image)\n                    else:\n                        raise Exception(\n                            \"image cannot be found as url or filepath:\", image\n                        )\n                else:\n                    image = numpy_serializer.from_bytes(image)\n                if debug:\n                    print(\"shape?\", image.shape)\n                    print(\"image?\", image)\n                detections = bezierPaddleHubResnet50ImageDogCatDetectorCore(\n                    image,\n                    input_bias=input_bias,\n                    skew=skew,\n                    # threshold=0.5,\n                    dog_label_file_path=dog_label_file_path,\n                    cat_label_file_path=cat_label_file_path,\n                    debug=debug,\n                )\n                lock.release()\n                # return detections\n        except:\n            import traceback\n            traceback.print_exc()\n        if debug:\n            print(\"DETECTIONS?\")\n            print(detections)"
        },
        {
            "comment": "This code is using the FastAPI framework along with uvicorn to run a server. It includes a function for cropping images and removing black areas, which utilizes ffmpeg and OpenCV. The function takes an image, adjusts the crop threshold, debug mode flag, and crop flag as parameters. It then loads the image using imageLoader, calculates the total area of the image, generates a temporary file path, creates a media path for the image, writes the image to the media path, runs ffmpeg to detect black areas and crops the image if necessary, and saves the result in the temporary file path. The function returns the detections made on the image.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/pyjom/imagetoolbox.py\":814-849",
            "content": "        return detections\n    import uvicorn\n    # checking: https://9to5answer.com/python-how-to-use-fastapi-and-uvicorn-run-without-blocking-the-thread\n    def run(host=\"0.0.0.0\", port=server_port):\n        \"\"\"\n        This function to run configured uvicorn server.\n        \"\"\"\n        uvicorn.run(app=app, host=host, port=port)\n    run()\ndef imageCropoutBlackArea(image, cropped_area_threshold=0.1, debug=False, crop=True):\n    image = imageLoader(image)\n    height, width = image.shape[:2]\n    total_area = height * width\n    import ffmpeg\n    # it must be a existing image.\n    from lazero.filesystem.temp import tmpfile\n    import uuid\n    path = \"/dev/shm/cropdetect_ffmpeg_black_border/{}.png\".format(str(uuid.uuid4()))\n    x, y, x1, y1 = 0, 0, width, height\n    with tmpfile(path=path) as TF:\n        mediaPath = path\n        cv2.imwrite(mediaPath, image)\n        stdout, stderr = (\n            ffmpeg.input(mediaPath, loop=1, t=1)\n            .filter(\"cropdetect\")\n            .output(\"null\", f=\"null\")\n            .run(capture_stdout=True, capture_stderr=True)"
        },
        {
            "comment": "The code parses stderr output, extracts crop information, and stores unique crop strings in common_crops list.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/pyjom/imagetoolbox.py\":850-877",
            "content": "        )\n        stdout_decoded = stdout.decode(\"utf-8\")\n        stderr_decoded = stderr.decode(\"utf-8\")\n        # nothing here.\n        # for line in stdout_decoded.split(\"\\n\"):\n        #     print(line)\n        # breakpoint()\n        import parse\n        common_crops = []\n        for line in stderr_decoded.split(\"\\n\"):\n            line = line.replace(\"\\n\", \"\").strip()\n            formatString = \"[{}] x1:{x1:d} x2:{x2:d} y1:{y1:d} y2:{y2:d} w:{w:d} h:{h:d} x:{x:d} y:{y:d} pts:{pts:g} t:{t:g} crop={}:{}:{}:{}\"\n            # print(line)\n            result = parse.parse(formatString, line)\n            if result is not None:\n                # print(result)\n                cropString = \"{}_{}_{}_{}\".format(\n                    *[result[key] for key in [\"w\", \"h\", \"x\", \"y\"]]\n                )\n                # print(cropString)\n                # breakpoint()\n                common_crops.append(cropString)\n            # [Parsed_cropdetect_0 @ 0x56246a16cbc0] x1:360 x2:823 y1:0 y2:657 w:464 h:656 x:360 y:2 pts:3 t:0.120000 crop=464:656:360:2"
        },
        {
            "comment": "This code calculates the area of a common crop and determines if it should be cropped. It first counts the occurrences of each crop string in the common_crops list, then selects the most frequent one and parses its dimensions. The code checks if these dimensions overlap with the image's dimensions, calculates the area of the common crop, and computes the cropped area ratio by dividing this area with the total image area. If the cropped area ratio is greater than a threshold value, it prints the result.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/pyjom/imagetoolbox.py\":878-899",
            "content": "            # this crop usually will never change. but let's count?\n        area = 0\n        # x,x1,y,y1= 0,width, 0, height\n        if len(common_crops) > 0:\n            common_crops_count_tuple_list = [\n                (cropString, common_crops.count(cropString))\n                for cropString in set(common_crops)\n            ]\n            common_crops_count_tuple_list.sort(key=lambda x: -x[1])\n            selected_crop_string = common_crops_count_tuple_list[0][0]\n            result = parse.parse(\"{w:d}_{h:d}_{x:d}_{y:d}\", selected_crop_string)\n            w, h, x, y = [result[key] for key in [\"w\", \"h\", \"x\", \"y\"]]\n            x1, y1 = min(x + w, width), min(y + h, height)\n            if x < x1 and y < y1:\n                # allow to calculate the area.\n                area = (x1 - x) * (y1 - y)\n        cropped_area_ratio = 1 - (area / total_area)  # 0.5652352766414517\n        # use 0.1 as threshold?\n        print(\"CROPPED AREA RATIO:\", cropped_area_ratio)\n        if cropped_area_ratio > cropped_area_threshold:"
        },
        {
            "comment": "This function takes an image and checks if there are black borders that need to be cropped. If so, it crops the image using the diagonal rectangle method. It also detects blur areas in the image and performs other operations such as four corners inpainting and getting image text area ratio (if debug is set). The function returns either the cropped image or the diagonal rectangle if crop is not set.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/pyjom/imagetoolbox.py\":900-931",
            "content": "            print(\"we need to crop this. no further processing needed\")\n            if debug:\n                image_black_cropped = image[y:y1, x:x1]\n                cv2.imshow(\"CROPPED IMAGE\", image_black_cropped)\n                cv2.waitKey(0)\n        else:\n            print(\"image no need to crop black borders. further processing needed\")\n    diagonalRect = [(x, y), (x1, y1)]\n    if crop:\n        return imageCropWithDiagonalRectangle(image, diagonalRect)\n    return diagonalRect\ndef imageCropoutBlurArea(\n    image, thresh=10, max_thresh=120, min_thresh=50, debug=False, crop=True, value=False\n):\n    import numpy\n    import BlurDetection\n    img = imageLoader(image)\n    # import sys\n    # sys.path.append(\"/root/Desktop/works/pyjom/\")\n    # from pyjom.imagetoolbox import imageFourCornersInpainting, getImageTextAreaRatio\n    # img = imageFourCornersInpainting(img)\n    # img = getImageTextAreaRatio(img, inpaint=True, edgeDetection=True)\n    img_fft, val, blurry = BlurDetection.blur_detector(img, thresh=thresh)\n    if debug:"
        },
        {
            "comment": "The code performs blur detection and displays the original image, mask, and inverse mask if debug mode is enabled. It uses OpenCV to find contours in the inverse mask and then draws a bounding box around the largest contour.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/pyjom/imagetoolbox.py\":932-957",
            "content": "        print(\"this image {0} blurry\".format([\"isn't\", \"is\"][blurry]))\n    msk, result, blurry = BlurDetection.blur_mask(\n        img, min_thresh=min_thresh, max_thresh=max_thresh\n    )\n    if value:\n        return result\n    inv_msk = 255 - msk\n    def display(title, img, max_size=200000):\n        assert isinstance(img, numpy.ndarray), \"img must be a numpy array\"\n        assert isinstance(title, str), \"title must be a string\"\n        scale = numpy.sqrt(min(1.0, float(max_size) / (img.shape[0] * img.shape[1])))\n        print(\"image is being scaled by a factor of {0}\".format(scale))\n        shape = (int(scale * img.shape[1]), int(scale * img.shape[0]))\n        img = cv2.resize(img, shape)\n        cv2.imshow(title, img)\n    # BlurDetection.scripts.display('img', img)\n    if debug:\n        display(\"img\", img)\n        # display(\"msk\", msk)\n        display(\"inv_msk\", inv_msk)\n    # BlurDetection.scripts.display('msk', msk)\n    contours, hierarchy = cv2.findContours(inv_msk, 1, 2)\n    rectangle_boundingbox = draw_bounding_box_with_contour(contours, img, debug=debug)"
        },
        {
            "comment": "The code defines a function for dog and cat detection and cropping for cover extraction. It uses the YOLOv5 model to detect areas with significant dog or cat presence, applies color transfer for better matching, and allows crop parameter for image processing. The function takes in an image and optional parameters such as desired area threshold, confidence threshold, expansion rate, default crop dimensions, debug mode, and crop option. It returns the detected most significant dog area.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/pyjom/imagetoolbox.py\":958-995",
            "content": "    if crop:\n        return imageCropWithDiagonalRectangle(img, rectangle_boundingbox)\n    return rectangle_boundingbox\ndef imageHistogramMatch(image, reference, delta=0.2):\n    from color_transfer import color_transfer\n    target = imageLoader(image)\n    source = imageLoader(reference)\n    transfer = color_transfer(source, target)\n    import numpy as np\n    transfer_02 = (target * (1 - delta) + transfer * delta).astype(np.uint8)\n    return transfer_02\ndef imageDogCatDetectionForCoverExtraction(\n    image,\n    dog_or_cat: Literal[\"dog\", \"cat\"] = \"dog\",\n    area_threshold=0.08,  # min area?\n    confidence_threshold=0.85,  # this is image quality maybe.\n    y_expansion_rate=0.03,  # to make the starting point on y axis less \"headless\"\n    defaultCropWidth=1920,\n    defaultCropHeight=1080,\n    debug=False,\n    debug_show=False,\n    crop=False,\n    mod=0.8,\n):\n    # return detected most significant dog area?\n    model = configYolov5()\n    # dog_or_cat = \"dog\"\n    # Images\n    # img = '/media/root/help/pyjom/samples/image/miku_on_green.png'  # or file, Path, PIL, OpenCV, numpy, list"
        },
        {
            "comment": "This code reads an image and applies object detection using a pre-trained model. It extracts the detected objects' bounding boxes and calculates their areas relative to the image size. The resulting DataFrame is filtered based on area ratio, confidence threshold, and the detected object name (either dog or cat). If debugging is enabled, it prints the detection DataFrame.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/pyjom/imagetoolbox.py\":996-1030",
            "content": "    # img = \"/root/Desktop/works/pyjom/samples/image/dog_with_text.jpg\"\n    # imgPath = \"/root/Desktop/works/pyjom/samples/image/dog_blue_sky.png\"\n    imgPath = image\n    # img = cv2.imread(imgPath)\n    img = imageLoader(imgPath)\n    defaultHeight, defaultWidth = img.shape[:2]\n    total_area = defaultHeight * defaultWidth\n    # Inference\n    results = model(img)\n    # print(results)\n    # # Results\n    # breakpoint()\n    animal_detection_dataframe = results.pandas().xyxy[0]\n    # results.show()\n    # # results.print() # or .show(),\n    area = (animal_detection_dataframe[\"xmax\"] - animal_detection_dataframe[\"xmin\"]) * (\n        animal_detection_dataframe[\"ymax\"] - animal_detection_dataframe[\"ymin\"]\n    )\n    animal_detection_dataframe[\"area_ratio\"] = area / total_area\n    df = animal_detection_dataframe\n    if debug:\n        print(\"DETECTION DATAFRAME\")\n        print(df)\n    new_df = df.loc[\n        (df[\"area_ratio\"] >= area_threshold)\n        & (df[\"confidence\"] >= confidence_threshold)\n        & (df[\"name\"] == dog_or_cat)"
        },
        {
            "comment": "Code snippet is performing image cropping and resizing based on a confidence threshold. It reads data from a DataFrame (new_df) which contains the coordinates of the image, confidence level, and other information. The code ensures that at least one row in new_df exists before proceeding. If no row exists, it returns False. Else, it selects the first row of new_df, extracts the relevant values, performs typecasting to integers for coordinates, and stores them in x0, y0, x1, and y1 respectively. The code also includes a debug print statement which prints the selected_col dictionary if the debug flag is set.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/pyjom/imagetoolbox.py\":1031-1061",
            "content": "    ].sort_values(\n        by=[\"confidence\"]\n    )  # this one is for 0.13\n    # count = new_df.count(axis=0)\n    count = len(new_df)\n    # print(\"COUNT: %d\" % count)\n    # this is just to maintain the ratio.\n    # you shall find the code elsewhere?\n    allowedHeight = min(\n        int(defaultWidth / defaultCropWidth * defaultHeight), defaultHeight\n    )\n    croppedImageCoverResized = None\n    flag = count >= 1\n    # if not crop:\n    #     return flag\n    if flag:\n        selected_col = new_df.iloc[0]  # it is a dict-like object.\n        # print(new_df)\n        if debug:\n            print(\"selected_col\")\n            print(selected_col)\n        # breakpoint()\n        # selected_col_dict = dict(selected_col)\n        # these are floating point shits.\n        # {'xmin': 1149.520263671875, 'ymin': 331.6445007324219, 'xmax': 1752.586181640625, 'ymax': 1082.3826904296875, 'confidence': 0.9185908436775208, 'class': 16, 'name': 'dog', 'area_ratio': 0.13691652620239364}\n        x0, y0, x1, y1 = [\n            int(selected_col[key]) for key in [\"xmin\", \"ymin\", \"xmax\", \"ymax\"]"
        },
        {
            "comment": "This code determines the size and position of a cropped area based on specified dimensions and a random modifier. It uses variables such as `y0_altered`, `height_current`, `width_current`, `randStart`, `randEnd`, `randRange`, `randModRange`, `randModStart`, `randModEnd`, and `x0_framework` to calculate the final crop position and size. The result is stored in `framework_XYWH`.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/pyjom/imagetoolbox.py\":1062-1082",
            "content": "        ]\n        y0_altered = max(int(y0 - (y1 - y0) * y_expansion_rate), 0)\n        height_current = min((y1 - y0_altered), allowedHeight)  # reasonable?\n        width_current = min(\n            int((height_current / defaultCropHeight) * defaultCropWidth), defaultWidth\n        )  # just for safety. not for mathematical accuracy.\n        # height_current = min(allowedHeight, int((width_current/defaultCropWidth)*defaultCropHeight))\n        # (x1+x0)/2-width_current/2\n        import random\n        randStart, randEnd = max((x1 - width_current), 0), min(\n            x0, defaultWidth - width_current\n        )\n        randRange = randEnd - randStart\n        randModRange = int(randRange * (1 - mod) / 2)\n        randModStart = min(randStart + randModRange, randEnd)\n        randModEnd = max(randModStart, randEnd - randModRange)\n        x0_framework = random.randint(randModStart, randModEnd)\n        framework_XYWH = (x0_framework, y0_altered, width_current, height_current)\n        x_f, y_f, w_f, h_f = framework_XYWH"
        },
        {
            "comment": "Function to detect and crop best cover with dog or cat detection. If no cover found, returns default image. If debug is enabled, displays original and resized cropped image.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/pyjom/imagetoolbox.py\":1083-1111",
            "content": "        diagonalRect = [(x_f, y_f), (x_f + w_f, y_f + h_f)]\n        if not crop:\n            return diagonalRect\n        # croppedImageCover = img[y_f : y_f + h_f, x_f : x_f + w_f, :]\n        croppedImageCover = imageCropWithDiagonalRectangle(img, diagonalRect)\n        # breakpoint()\n        # resize image\n        croppedImageCoverResized = cv2.resize(\n            croppedImageCover, (defaultCropWidth, defaultCropHeight)\n        )\n        if debug_show:\n            cv2.imshow(\"CROPPED IMAGE COVER\", croppedImageCover)\n            cv2.imshow(\"CROPPED IMAGE COVER RESIZED\", croppedImageCoverResized)\n            # print(selected_col_dict)\n            # print(count)\n            # breakpoint()\n            cv2.waitKey(0)\n    else:\n        if debug:\n            print(\"NO COVER FOUND.\")\n    # if not crop:\n    # return [(0, 0), (defaultWidth, defaultHeight)]\n    return croppedImageCoverResized\ndef getImageBestConfidenceWithBezierDogCatDetector(\n    frame, dog_or_cat: Literal[\"dog\", \"cat\"] = \"dog\", debug=False\n):\n    best_confidence = 0"
        },
        {
            "comment": "The code snippet demonstrates the use of a detector for identifying dogs and cats in an image. The function `getImageBestConfidenceWithBezierDogCatDetector` detects objects in the frame, selects the best detection based on confidence level, and returns it. The function `filterImageBestConfidenceWithBezierDogCatDetector` filters the detected confidence value with a specified threshold. The `imageDogCatCoverCropAdvanced` function uses this to crop the image focusing on either dog or cat object.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/pyjom/imagetoolbox.py\":1112-1144",
            "content": "    detections = bezierPaddleHubResnet50ImageDogCatDetector(\n        frame, use_gpu=False\n    )  # no gpu avaliable\n    mDetections = [x for x in detections if x[\"identity\"] == dog_or_cat]\n    mDetections.sort(key=lambda x: -x[\"confidence\"])  # select the best one.\n    if len(mDetections) > 0:\n        best_confidence = mDetections[0][\"confidence\"]\n        if debug:\n            print(\"BEST CONFIDENCE:\", best_confidence)\n    return best_confidence\ndef filterImageBestConfidenceWithBezierDogCatDetector(\n    frame,\n    dog_or_cat: Literal[\"dog\", \"cat\"] = \"dog\",\n    debug=False,\n    confidence_threshold={\"min\": 0.7},\n):\n    best_confidence = getImageBestConfidenceWithBezierDogCatDetector(\n        frame, dog_or_cat=dog_or_cat, debug=debug\n    )\n    return checkMinMaxDict(best_confidence, confidence_threshold)\ndef imageDogCatCoverCropAdvanced(\n    frame,\n    dog_or_cat=\"dog\",\n    confidence_threshold={\"min\": 0.7},\n    yolov5_confidence_threshold=0.4,\n    text_area_threshold={\"max\": 0.2},\n    gpu=True,\n    corner=True,\n    area_threshold=0.2,"
        },
        {
            "comment": "The code defines a function that processes an image frame, detects whether it contains a dog or cat using a BezierPaddleHubResnet50ImageDogCatDetector, and if successful, applies inpainting to the target area. The best confidence score is checked against a threshold before proceeding with inpainting. If no GPU is available, detection is not performed.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/pyjom/imagetoolbox.py\":1145-1172",
            "content": "    debug=False,\n):\n    processed_frame = None\n    frame = imageLoader(frame)\n    height, width = frame.shape[:2]\n    area = height * width\n    # detections = bezierPaddleHubResnet50ImageDogCatDetector(\n    #     frame, use_gpu=False\n    # )  # no gpu avaliable\n    # mDetections = [x for x in detections if x[\"identity\"] == dog_or_cat]\n    # mDetections.sort(key=lambda x: -x[\"confidence\"])  # select the best one.\n    # if len(mDetections) > 0:\n    # best_confidence =\n    # if best_confidence >0: # just stub.\n    #     best_confidence = mDetections[0][\"confidence\"]\n    #     print(\"BEST CONFIDENCE:\", best_confidence)\n    # if checkMinMaxDict(best_confidence, confidence_threshold):\n    if filterImageBestConfidenceWithBezierDogCatDetector(\n        frame,\n        dog_or_cat=dog_or_cat,\n        debug=debug,\n        confidence_threshold=confidence_threshold,\n    ):\n        # target = getImageTextAreaRatio(frame, inpaint=True, gpu=gpu)\n        # target = imageFourCornersInpainting(target)\n        # processed_frame = target"
        },
        {
            "comment": "The code checks if the text area ratio is within the threshold and applies image processing techniques to remove unwanted areas. If a valid frame is obtained, it performs dog or cat detection for covering extraction.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/pyjom/imagetoolbox.py\":1173-1197",
            "content": "        # break\n        text_area_ratio = getImageTextAreaRatio(\n            frame,\n            gpu=gpu,\n        )\n        # text_area_ratio = getImageTextAreaRatio(frame, gpu=gpu)\n        print(\"TEXT AREA RATIO\", text_area_ratio)\n        # if animalCropDiagonalRect is not None:\n        if checkMinMaxDict(text_area_ratio, text_area_threshold):\n            mFrame = getImageTextAreaRatio(frame, gpu=gpu, inpaint=True)\n            if corner:\n                mFrame = imageFourCornersInpainting(mFrame)\n            mFrame = imageCropoutBlackArea(mFrame)\n            mFrame = imageCropoutBlurArea(mFrame)\n            # cv2.imshow(\"PRE_FINAL_IMAGE\", mFrame)\n            # cv2.waitKey(0)\n            processed_frame = imageDogCatDetectionForCoverExtraction(\n                mFrame,\n                dog_or_cat=dog_or_cat,\n                confidence_threshold=yolov5_confidence_threshold,\n                # area_threshold=0.15,\n                crop=True,\n                debug=True,\n            )\n    if processed_frame is not None:"
        },
        {
            "comment": "The code calculates the area of the processed frame and checks if it's smaller than a certain threshold. If so, sets processed_frame to None. Then, applies dog or cat detection using Bezier Dog/Cat Detector with specified parameters, setting processed_frame to None if no suitable detection is found.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/pyjom/imagetoolbox.py\":1198-1211",
            "content": "        p_height, p_width = processed_frame.shape[:2]\n        p_area = p_height * p_width\n        if p_area / area < area_threshold:\n            processed_frame = None\n        elif not filterImageBestConfidenceWithBezierDogCatDetector(\n            frame,\n            dog_or_cat=dog_or_cat,\n            debug=debug,\n            confidence_threshold=confidence_threshold,\n        ):\n            processed_frame = None\n    return processed_frame"
        }
    ]
}