{
    "summary": "This code generates typography for videos using TTS, external tools, and FFMPEG, performing directory operations, merging audio/video, exporting, calculating tempo, and applying it to the audio track.",
    "details": [
        {
            "comment": "The code imports necessary libraries and defines functions for handling sentences, obtaining speech output, and merging audio segments. It also sets up a function that generates a video using the functional_gen_typo_video_seq module. The code uses bash scripts and external tools like PaddleSpeech and MediaInfo to manipulate text-to-speech and audio/video files.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/tests/bilibili_practices/bilibili_tarot/functional_voice_with_pictures.py\":0-34",
            "content": "import os\nfrom test_common import *\nimport shutil\ndef split_sentences(sent):\n    spliters = \"\\n\uff0c\u3002\u3001\uff1f\uff1a \"\n    cursent = \"\"\n    results = []\n    for elem in sent:\n        cursent += elem\n        if elem in spliters:\n            results.append(cursent)\n            cursent = \"\"\n    if len(cursent) > 0:\n        results.append(cursent)\n    return results\ndef get_speech(sent,output):\n    assert output.endswith(\".wav\")\n    os.system(\"bash kill_pdspc.sh\")\n    with open(\"temp.txt\", \"w+\",encoding=\"utf-8\") as f:\n        f.write(sent.replace(\"\\n\",\"\")) # important.\n    os.system(\"cat temp.txt | paddlespeech tts --output {}\".format(output))\nfrom pydub import AudioSegment\nfrom functional_gen_typo_video_seq import gen_video\n# import matplotlib # doing this before importing moviepy editor. or we will fail.\n# matplotlib.use(\"TkAgg\")\n# from moviepy.editor import VideoFileClip\n# cannot mix moviepy with vidpy or we get fucked.\nfrom MediaInfo import MediaInfo\ndef merge_audio(asegs):\n    audio_3 = AudioSegment.empty() #shit\n    for seg in asegs:"
        },
        {
            "comment": "This code is attempting to generate typography for a video using voice and pictures. It first clears the existing voice and video directories, then creates new ones. It splits the input text into sentences, and for each sentence, it attempts to get speech audio from that sentence and create a corresponding picture. If no audio is found, it skips that sentence. Finally, it returns the generated audio.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/tests/bilibili_practices/bilibili_tarot/functional_voice_with_pictures.py\":35-65",
            "content": "        try:\n            audio_3 = audio_3.append(seg,crossfade=100) # also shit.\n        except:\n            audio_3 = audio_3.append(seg,crossfade=0) # also shit.\n    return audio_3\n    # audio_3.export(\"audio_3.wav\", format=\"wav\")\ndef gen_typography_part2(intro_text, bgm_path,target_video):\n    # intro_text = \"\"\"\u5854\u7f57\u724c\uff0c\u7531\u201cTAROT\u201d\u4e00\u8bcd\u97f3\u8bd1\u800c\u6765\uff0c\u88ab\u79f0\u4e3a\u201c\u5927\u81ea\u7136\u7684\u5965\u79d8\u5e93\u201d\u3002\u62bd\u53d6\u4e00\u5f20\u5854\u7f57\u724c\uff0c\u4eca\u5929\u7684\u4f60\u4f1a\u662f\u600e\u6837\u7684\u5462\uff1f\"\"\"\n    os.system(\"bash kill_pdspc.sh\")\n    sents = split_sentences(intro_text)\n    # breakpoint()\n    voice_dir = \"voice\"\n    video_dir = \"video\"\n    os.system(\"rm -rf {}\".format(voice_dir))\n    os.system(\"rm -rf {}\".format(video_dir))\n    os.mkdir(\"{}\".format(voice_dir))\n    os.mkdir(\"{}\".format(video_dir))\n    index = 0\n    voice_clips = []\n    video_names = []\n    for i,sent in enumerate(sents):\n        print(\"READING:\",sent)\n        aname = \"{}/{}.wav\".format(voice_dir,i)\n        get_speech(sent,aname)\n        lsent = len(sent)\n        # if no audio then just skip.\n        if not os.path.exists(aname):\n            index += lsent\n            continue"
        },
        {
            "comment": "This code generates videos for each segment of audio and appends the video names to a list. It then combines all audio clips into one merged audio file, overlays background music, and saves the final audio and video files. The code also includes debugging tools like breakpoint() to help with troubleshooting.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/tests/bilibili_practices/bilibili_tarot/functional_voice_with_pictures.py\":66-90",
            "content": "        seg = AudioSegment.from_wav(aname)\n        duration = seg.duration_seconds\n        voice_clips.append(seg)\n        # get the duration you fuck.\n        # breakpoint()\n        current_indexs = list(range(index,index+lsent))\n        # you can generate video for it.\n        index += lsent\n        vname = \"{}/{}.mp4\".format(video_dir,i)\n        gen_video(vname,current_indexs,duration) # where from?\n        video_names.append(vname)\n    # and finally?\n    final_video = \"{}/final_video.mp4\".format(video_dir)\n    final_audio = \"{}/final_audio.wav\".format(voice_dir)\n    audio_merged = merge_audio(voice_clips)\n    # bgm_path = \"/root/Desktop/works/bilibili_tarot/some_bgm.mp3\"\n    bgm = AudioSegment.from_mp3(bgm_path)\n    # duration2 = audio_merged.duration_seconds\n    # bgm = bgm[:duration2*1000] # really?\n    # breakpoint()\n    # audio_merged = audio_merged.overlay(audio_merged,bgm,loop=True)  #wtf?\n    audio_merged = audio_merged.overlay(bgm,loop=True)\n    # audio_merged = audio_merged.normalize()\n    # is it needed?"
        },
        {
            "comment": "This code performs video and audio processing, using ffmpeg commands to merge and manipulate the files. It exports an audio file in wav format, creates a mylist.txt file with video names, concatenates videos using ffmpeg, calculates the tempo between audio and video duration, applies the tempo to the final audio track, and finally moves the final video to the target location. This function also includes a shell command to kill pdspc process when finished.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/tests/bilibili_practices/bilibili_tarot/functional_voice_with_pictures.py\":91-113",
            "content": "    # shit.\n    audio_merged.export(final_audio, format=\"wav\")\n    final_video2 = \"{}/final_video2.mp4\".format(video_dir)\n    with open(\"mylist.txt\",\"w+\") as f:\n        for n in video_names:\n            f.write(\"file \"+n+\"\\n\")\n    os.system(\"ffmpeg -f concat -safe 0 -i mylist.txt -c copy {}\".format(final_video))\n    # output_length = VideoFileClip(final_video).duration\n    output_length = MediaInfo(filename=final_video).getInfo()[\"videoDuration\"]\n    output_length = float(output_length)\n    input_length = AudioSegment.from_wav(final_audio).duration_seconds\n    tempo = input_length/output_length\n    t_a,t_b = tempo.as_integer_ratio()\n    os.system('ffmpeg -i {} -i {} -c:v copy -c:a aac -filter:a \"atempo={}/{}\" -map 0:v:0 -map 1:a:0 {}'.format(final_video,final_audio,t_a,t_b,final_video2))\n    shutil.move(final_video2,target_video)\ndef gen_typography_part3(intro_text, target_video): #slient\n    # intro_text = \"\"\"\u5854\u7f57\u724c\uff0c\u7531\u201cTAROT\u201d\u4e00\u8bcd\u97f3\u8bd1\u800c\u6765\uff0c\u88ab\u79f0\u4e3a\u201c\u5927\u81ea\u7136\u7684\u5965\u79d8\u5e93\u201d\u3002\u62bd\u53d6\u4e00\u5f20\u5854\u7f57\u724c\uff0c\u4eca\u5929\u7684\u4f60\u4f1a\u662f\u600e\u6837\u7684\u5462\uff1f\"\"\"\n    os.system(\"bash kill_pdspc.sh\")\n    sents = split_sentences(intro_text)"
        },
        {
            "comment": "This code removes existing voice and video directories, creates new ones, reads sentences, saves corresponding audio files for each sentence, checks if audio files are generated correctly, generates videos based on the sentences and their respective positions in the text, and stores the names of generated videos.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/tests/bilibili_practices/bilibili_tarot/functional_voice_with_pictures.py\":114-146",
            "content": "    # breakpoint()\n    voice_dir = \"voice\"\n    video_dir = \"video\"\n    os.system(\"rm -rf {}\".format(voice_dir))\n    os.system(\"rm -rf {}\".format(video_dir))\n    os.mkdir(\"{}\".format(voice_dir))\n    os.mkdir(\"{}\".format(video_dir))\n    index = 0\n    voice_clips = []\n    video_names = []\n    for i,sent in enumerate(sents):\n        print(\"READING:\",sent)\n        aname = \"{}/{}.wav\".format(voice_dir,i)\n        get_speech(sent,aname)\n        lsent = len(sent)\n        # if no audio then just skip.\n        if not os.path.exists(aname):\n            index += lsent\n            continue\n        seg = AudioSegment.from_wav(aname)\n        duration = seg.duration_seconds\n        voice_clips.append(seg)\n        # get the duration you fuck.\n        # breakpoint()\n        current_indexs = list(range(index,index+lsent))\n        # you can generate video for it.\n        index += lsent\n        vname = \"{}/{}.mp4\".format(video_dir,i)\n        gen_video(vname,current_indexs,duration) # where from?\n        video_names.append(vname)\n    # and finally?"
        },
        {
            "comment": "This code is performing audio and video merging, exporting the final audio file, creating a mylist.txt file for ffmpeg concatenation, and determining the output length of the final video. The code seems to have undergone revisions as there are comments stating \"wtf?\", \"shit.\", and \"is it needed?\" suggesting possible confusion or uncertainty about certain parts of the code.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/tests/bilibili_practices/bilibili_tarot/functional_voice_with_pictures.py\":147-169",
            "content": "    final_video = \"{}/final_video.mp4\".format(video_dir)\n    final_audio = \"{}/final_audio.wav\".format(voice_dir)\n    audio_merged = merge_audio(voice_clips)\n    # bgm_path = \"/root/Desktop/works/bilibili_tarot/some_bgm.mp3\"\n    # bgm = AudioSegment.from_mp3(bgm_path)\n    # duration2 = audio_merged.duration_seconds\n    # bgm = bgm[:duration2*1000] # really?\n    # breakpoint()\n    # audio_merged = audio_merged.overlay(audio_merged,bgm,loop=True)  #wtf?\n    # audio_merged = audio_merged.overlay(bgm,loop=True)\n    # audio_merged = audio_merged.normalize()\n    # is it needed?\n    # shit.\n    audio_merged.export(final_audio, format=\"wav\")\n    final_video2 = \"{}/final_video2.mp4\".format(video_dir)\n    with open(\"mylist.txt\",\"w+\") as f:\n        for n in video_names:\n            f.write(\"file \"+n+\"\\n\")\n    os.system(\"ffmpeg -f concat -safe 0 -i mylist.txt -c copy {}\".format(final_video))\n    # output_length = VideoFileClip(final_video).duration\n    output_length = MediaInfo(filename=final_video).getInfo()[\"videoDuration\"]"
        },
        {
            "comment": "This code calculates the tempo of an audio file and then applies it to another audio-video file using FFMPEG. It then moves the resulting file to a specified target location.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/tests/bilibili_practices/bilibili_tarot/functional_voice_with_pictures.py\":170-175",
            "content": "    output_length = float(output_length)\n    input_length = AudioSegment.from_wav(final_audio).duration_seconds\n    tempo = input_length/output_length\n    t_a,t_b = tempo.as_integer_ratio()\n    os.system('ffmpeg -i {} -i {} -c:v copy -c:a aac -filter:a \"atempo={}/{}\" -map 0:v:0 -map 1:a:0 {}'.format(final_video,final_audio,t_a,t_b,final_video2))\n    shutil.move(final_video2,target_video)"
        }
    ]
}