{
    "summary": "This code imports libraries, defines image and video processing functions, reads a JSON file, applies these functions to create the final image, processes bounding boxes, creates rectangles, blurs, visualizes, and displays images while waiting for key presses.",
    "details": [
        {
            "comment": "The code is importing necessary libraries and defining a function `getVideoPixels` to retrieve the default video width and height from a given video file. It then sets the base path, target file, and original file paths. The code reads the target JSON file, loads it into a variable `mJson`, and retrieves the video dimensions using the `getVideoPixels` function. Finally, it defines a function `getBlackPicture` to create a black grayscale image with the specified width and height.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/tests/unittest_convolution_bilibili_translate_text_detect.py\":0-40",
            "content": "import json\nfrom test_commons import *\nfrom pyjom.commons import *\nimport cv2\ndef getVideoPixels(videoPath):\n    from MediaInfo import MediaInfo\n    info = MediaInfo(filename=videoPath)\n    infoData = info.getInfo()\n    # print(infoData)\n    # breakpoint()\n    defaultWidth = infoData[\"videoWidth\"]\n    defaultHeight = infoData[\"videoHeight\"]\n    return defaultWidth, defaultHeight\n# easy gig, you said.\n# basePath = \"/Users/jamesbrown/desktop/works/pyjom_remote\"\nbasePath = \"/root/Desktop/works/pyjom\"\ntargetFile = (\n    basePath + \"/tests/bilibili_practices/bilibili_video_translate/japan_day.json\"\n)\noriginalFile = (\n    basePath + \"/tests/bilibili_practices/bilibili_video_translate/japan_day.webm\"\n)\n# visualization can only be done here?\n# where is the original file?\nmJson = json.loads(open(targetFile, \"r\", encoding=\"utf-8\").read())\nimport numpy as np\nwidth, height = getVideoPixels(originalFile)\ndef getBlackPicture(width, height):\n    blackPicture = np.zeros((height, width, 1), dtype=\"uint8\")  # this is grayscale.\n    return blackPicture"
        },
        {
            "comment": "This code defines a function `getConvBlurredCurrentShot` that averages multiple blurred images to create a final image. It also initializes variables for convolution bounding boxes and blurred spans based on a range of keys in `mJson`. The resulting image is then thresholded and converted to 8-bit format before being returned.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/tests/unittest_convolution_bilibili_translate_text_detect.py\":43-85",
            "content": "mKeys = list(mJson.keys())\nmIntKeys = [int(x) for x in mKeys]\nminKey, maxKey = min(mIntKeys), max(mIntKeys)\n# imutils is created by pyimagesearch.\nfrom imutils.object_detection import non_max_suppression\ndef getConvBlurredCurrentShot(blurredSpan, span=5):\n    # honor the most the latest one.\n    mImage = None\n    for index, blurredImage in enumerate(blurredSpan):\n        ratio = index / span\n        if mImage is None:\n            mImage = blurredImage * ratio\n        else:\n            mImage += blurredImage * ratio\n    # print(mImage.shape)\n    # breakpoint()\n    # change this mImage.\n    mImage = mImage > 128\n    mImage = mImage.astype(np.uint8)\n    mImage = mImage * 255\n    return mImage\n    # return 256*((mImage>128).astype(np.uint8))\nconvolutionSpan = 20\nconvolutionBoundingBoxSpan = []\nconvolutionBlurredSpan = []\nfor intKey in range(minKey, maxKey + 1):\n    strKey = str(intKey)\n    target = mJson[strKey]\n    boundingBoxes = []\n    for item in target:\n        location = item[0]\n        text, confidence = item[1]\n        # print(\"location\",location) # four points. do not know if there is any rotation here."
        },
        {
            "comment": "The code processes bounding boxes from a convolution operation, filters them based on confidence score, and performs non-maximum suppression to eliminate overlapping boxes. It then creates an array of non-overlapping bounding boxes and generates a black picture with the same width and height as the original image.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/tests/unittest_convolution_bilibili_translate_text_detect.py\":86-110",
            "content": "        if confidence > 0.7:\n            npLocation = np.array(location)\n            xlocs = npLocation[:, 0]\n            ylocs = npLocation[:, 1]\n            # print(xlocs)\n            # print(ylocs)\n            # breakpoint()\n            minX, maxX = min(xlocs), max(xlocs)\n            minY, maxY = min(ylocs), max(ylocs)\n            boundingBox = [minX, minY, maxX, maxY]\n            boundingBoxes.append(boundingBox.copy())\n            # breakpoint()\n        # print(\"text\", text)\n        # print(\"confidence\", confidence)\n    convolutionBoundingBoxSpan.append(boundingBoxes.copy())\n    if len(convolutionBoundingBoxSpan) > convolutionSpan:\n        convolutionBoundingBoxSpan.pop(0)\n    # do your calculation!\n    flatSpan = [y for x in convolutionBoundingBoxSpan for y in x]\n    flatSpan = np.array(flatSpan)\n    currentNonOverlappingBoxes = non_max_suppression(flatSpan)\n    # print(intKey,target)\n    # this time we do not care about the text inside.\n    blackPicture = getBlackPicture(width, height)\n    for rectangle in flatSpan:"
        },
        {
            "comment": "The code creates a rectangle from input, fills it in the black picture, blurs the filled image, appends it to a list if length is less than convolutionSpan, pops oldest if length exceeds convolutionSpan, gets the current blurred image from the list, prints the bounding boxes count, and if no elements in flatSpan, continues. It then finds contours in the current blurred image and creates a new image for visualization of bounding rectangles.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/tests/unittest_convolution_bilibili_translate_text_detect.py\":111-141",
            "content": "        # make it all int.\n        x0, y0, x1, y1 = [int(num) for num in rectangle]\n        loc0 = (x0, y0)\n        loc1 = (x1, y1)\n        cv2.rectangle(\n            blackPicture, loc0, loc1, 255, cv2.FILLED\n        )  # we fill so we can merge shits.\n    blackPictureBlurred = cv2.GaussianBlur(blackPicture, (33, 33), 0)\n    convolutionBlurredSpan.append(blackPictureBlurred.copy())\n    if len(convolutionBlurredSpan) > convolutionSpan:\n        convolutionBlurredSpan.pop(0)\n    currentBlackPictureBlurred = getConvBlurredCurrentShot(\n        convolutionBlurredSpan, span=convolutionSpan\n    )\n    # print(currentBlackPictureBlurred.shape)\n    print(\"boundingBoxes:\", len(flatSpan))\n    if len(flatSpan) == 0:\n        continue\n    contours = cv2.findContours(\n        currentBlackPictureBlurred, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n    )\n    contours = contours[0] if len(contours) == 2 else contours[1]\n    currentBoundingBoxesVisualize = getBlackPicture(width, height)\n    for i in contours:\n        x, y, w, h = cv2.boundingRect(i)"
        },
        {
            "comment": "This code snippet is responsible for visualizing bounding boxes, displaying an image, and waiting for a key press. It prints the non-overlapping boxes but may require visualization. The code will close all windows at the end with a final message \"THE END\".",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/tests/unittest_convolution_bilibili_translate_text_detect.py\":142-155",
            "content": "        cv2.rectangle(currentBoundingBoxesVisualize, (x, y), (x + w, y + h), 255, 4)\n    cv2.imshow(\"IMAGE\", currentBoundingBoxesVisualize)\n    cv2.waitKey(10)\n    print(\"showing image:\", intKey)\n    # print\n    # cv2.waitKey(1000)\n    # print(\"NON OVERLAPPING BOXES:\")\n    # print(currentNonOverlappingBoxes)\n    # we need to visualize this shit.\n    # breakpoint()\ncv2.destroyAllWindows()\nprint(\"THE END\")"
        }
    ]
}