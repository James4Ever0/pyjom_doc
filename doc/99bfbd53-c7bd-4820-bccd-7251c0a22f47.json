{
    "summary": "This code imports libraries, applies language models and topic modeling techniques using CountVectorizer and LatentDirichletAllocation to analyze document content.",
    "details": [
        {
            "comment": "The code is importing necessary libraries, such as stopwords and tokenize from nltk, PorterStemmer for stemming, and spacy's en_core_web_sm model. It then loads the model and applies it to a text. The code also defines stop words which will be used to filter out common words like \"the\" or \"and\" from the text. Additionally, it initializes an empty list for lemma word 1 and mentions the inclusion of language tags in some elements.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/tests/topic_modeling/poc_english_topic_modeling.py\":0-26",
            "content": "# https://huggingface.co/spacy/en_core_web_sm\n# https://medium.com/analytics-vidhya/nlp-essentials-removing-stopwords-and-performing-text-normalization-using-nltk-and-spacy-in-python-2c4024d2e343\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import PorterStemmer\n# from lazero.utils import inspectObject\nfrom lazero.utils import sprint  # print with spliter\n# metalazero belongs to lazero package.\nimport en_core_web_sm\nnlp = en_core_web_sm.load()\ndoc = nlp(\n    \"\"\"He determined to drop his litigation with the monastry, and relinguish his claims to the wood-cuting and fishery rihgts at once. He was the more ready to do this becuase the rights had become much less valuable, and he had indeed the vaguest idea where the wood and river in question were.\"\"\"\n)\n# the sentence spliter includes unwanted \"\\n\" char\nset(stopwords.words(\"english\"))\nstop_words = set([elem.lower() for elem in stopwords.words(\"english\")])\nlemma_word1 = []\n# this shit has the lang tag. it might be useful for language detection. really?"
        },
        {
            "comment": "The code preprocesses text data by removing certain tokens, stemming words, and padding the resulting list. It then converts the list of words into a string, creates a TF-IDF vectorizer with unigrams and bigrams, and reads the string as input using StringIO.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/tests/topic_modeling/poc_english_topic_modeling.py\":27-70",
            "content": "for token in doc:\n    if token.pos_ in [\"PRON\", \"CCONJ\", \"ADP\", \"PART\", \"PUNCT\", \"AUX\"]:\n        continue\n    if token.text.lower() in stop_words:\n        continue\n    lemma_word1.append(token.text)\nsprint(lemma_word1)  # there is no such -PRON- thing.\n# 1st step.\nStem_words = []\nps = PorterStemmer()\nfor w in lemma_word1:\n    rootWord = ps.stem(w)\n    Stem_words.append(rootWord)\nsprint(Stem_words)  # 3rd step\nStem_words += ((len(Stem_words) - 1) % 5) * [\"\"]  # padding\nimport numpy as np\nStem_words = np.array(Stem_words)\nStem_words = Stem_words.reshape(5, -1)\n# sprint(Stem_words)\n# row, col = Stem_words.shape\n# exit()\n# for reasons that shit can understand.\n# np.nditer is for iteration over every elem\ndataList = []\nfor row in Stem_words:\n    # print(row)\n    elem = \" \".join(row)\n    dataList.append(elem)\ndata = \"\\n\".join(dataList)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n# In[8]:\n# \u521b\u5efa\u4e00\u4e2aCountVectoerizer\u5b9e\u4f8b\ntfidf = TfidfVectorizer(ngram_range=(1, 2))\n# \u6253\u5f00\u521a\u521a\u4fdd\u5b58\u7684txt\u6587\u6863\nfrom io import StringIO\nf = StringIO(data)"
        },
        {
            "comment": "This code uses CountVectorizer to transform data and LatentDirichletAllocation to fit the transformed data, then defines a function print_topics that iterates over the topics in the model, prints their index, and displays the highest frequency words for each topic. It also calculates and prints the set of word groups, word count dictionary, and filtered list of meaningful words.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/tests/topic_modeling/poc_english_topic_modeling.py\":71-98",
            "content": "# \u4f7f\u7528CountVectorizer\u62df\u5408\u6570\u636e\nx_train = tfidf.fit_transform(f)\nfrom sklearn.decomposition import LatentDirichletAllocation\nlda = LatentDirichletAllocation(n_components=5)\nlda.fit(x_train)\ndef print_topics(model, feature_names, n_top_words):\n    # \u9996\u5148\u662f\u904d\u5386\u6a21\u578b\u4e2d\u5b58\u50a8\u7684\u8bdd\u9898\u5e8f\u53f7\u548c\u8bdd\u9898\u5185\u5bb9\n    for topic_idx, topic in enumerate(model.components_):\n        # \u7136\u540e\u6253\u5370\u8bdd\u9898\u7684\u5e8f\u53f7\u4ee5\u53ca\u6307\u5b9a\u6570\u91cf\u7684\u6700\u9ad8\u9891\u7684\u5173\u952e\u8bcd\n        message = \"topic #%d:\" % topic_idx\n        mList = [feature_names[i] for i in topic.argsort()[: -n_top_words - 1 : -1]]\n        mListStr = \" \".join(\n            mList\n        )\n        message += mListStr\n        mSet  = set(mList) # the set contains word groups like 'river question'\n        cDict = {k:mList.count(k) for k in mSet}\n        mRealList = mListStr.split(\" \")\n        mRealList = [x.strip() for x in mRealList if len(x.strip()) > 1] # usually things shorter than 2 letters are no good.\n        mRealSet = set(mRealList)\n        cRealDict = {k:mRealList.count(k) for k in mRealSet}\n        print(\"MESSAGE\",message)\n        print(\"SET\", mSet)\n        print(\"COUNT DICT\", cDict) # pointless to count here?"
        },
        {
            "comment": "This code section prints the real set (mRealSet) and count dictionary (cRealDict), then it displays the top words from LDA topic modeling using print_topics function. This helps in analyzing the distribution of topics across documents.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/tests/topic_modeling/poc_english_topic_modeling.py\":99-105",
            "content": "        print(\"RealSET\", mRealSet)\n        print(\"RealCOUNT DICT\", cRealDict)\n    print()\nn_top_words = 10\nprint_topics(lda, tfidf.get_feature_names(), n_top_words)"
        }
    ]
}