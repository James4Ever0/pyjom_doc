{
    "summary": "This code searches AniDB for anime using a specified query and fake user agent, extracting title and link from the resulting HTML table. It then uses pandas to convert the table data into a DataFrame, retrieves video data as dictionaries, and prints keys of each dictionary.",
    "details": [
        {
            "comment": "This code is searching for anime on AniDB using the specified query. It makes a GET request with the query and fake user agent to ensure an accurate search result. The code checks if a table containing the search results is found, and if not, it may suggest changing the user agent or the page could be a page jump directly to the anime. The code uses BeautifulSoup to parse the HTML content of the response.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/tests/anime_highlight_cuts/theme_collector/anidb_search_parse.py\":0-37",
            "content": "url = \"https://anidb.net/anime/\"\n# query = \"Yahari Ore no Seishun Lovecome wa Machigatte Iru.\"\nquery = \"Yahari Ore no Seishun Love Come wa Machigatteiru.\"  # this will guide you to something different.\nparams = {\"adb.search\": query, \"do.update\": \"Search\", \"noalias\": 1}\nimport pandas\nimport requests\nimport fake_useragent\nua = fake_useragent.UserAgent()\nr = requests.get(\n    url, params=params, headers={\"User-Agent\": ua.random}\n)  # beautiful. really?\nstatus_code = r.status_code\nprint(\"STATUS CODE?\", status_code)\nassert status_code == 200\ntext = r.text\nfrom bs4 import BeautifulSoup\nsoup = BeautifulSoup(text, \"html.parser\")\n# print(soup) # forbidden? wtf?\n# breakpoint()\nimport pandas\n# table = soup.find('table')\ntable = soup.find(\"table\", attrs={\"class\": \"animelist\"})\nif not table:\n    print(\"table not found.\")\n    # you may want to change user agent.\n    breakpoint()\n    # or it is just a page jump. directly to your anime.\nelse:\n    table_str = str(table)\n    # ['No', 'Image', 'Title', 'Award', 'Type', 'Eps', 'Rating', 'Average', 'Reviews', 'User', 'Aired', 'Ended']"
        },
        {
            "comment": "The code is searching for a specific table on an anime website, extracting the title and link from each row. It then reads the HTML table into a pandas DataFrame, iterates over the rows to obtain the video data as a dictionary, and finally prints the keys of each video's data dictionary.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/tests/anime_highlight_cuts/theme_collector/anidb_search_parse.py\":38-56",
            "content": "    # where is the damn link?\n    for title in table.find_all(\"td\", attrs={\"data-label\": \"Title\"}):\n        title_ref = table.find(\"a\")\n        title_text = title_ref.text\n        title_link = title_ref[\"href\"]\n        print(f\"[{title_link}] - {title_text}\")\n    data = pandas.read_html(table_str)[0]  # must be the first table.\n    # now you have it. sorted?\n    # print(data)\n    # breakpoint()\n    for index, videoDataFrame in data.iterrows():\n        videoData = videoDataFrame.to_dict()\n        print(videoData.keys())\n        # Main Title?\n        breakpoint()\n        # title = videoData['Title']\n        # # where's the damn link? we don't need such thing.\n        # aired, ended = videoData['Aired'], videoData['Ended']\n        # print(f'[{index}] - {title}')"
        }
    ]
}