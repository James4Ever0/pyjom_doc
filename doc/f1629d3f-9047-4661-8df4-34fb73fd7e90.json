{
    "summary": "The code uses Spacy and Jieba tokenizer to check if a string contains English, removes non-English elements, and prints the tokens along with their POS and dependency tags. Proper nouns list may be updated and improvements are potential.",
    "details": [
        {
            "comment": "The code is importing necessary libraries and defining a function for recursive text search. It uses the Spacy library for Chinese language processing, but it seems to be in progress as it mentions potential improvements and updates. The proper nouns list may be updated or changed later.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/tests/title_cover_generator/spacy_word_swapper.py\":0-29",
            "content": "# just use some simple analysis to extract the template. may not be cost effective like DianJing also you can try the freaking gpt2 model, or pegasus.\nfrom commons import sample_data\n# first assume all to be freaking chinese.\n# import nltk\nimport spacy\nimport jieba\nfrom spacy.lang.zh.examples import sentences \nimport re\ndef recursiveCompiledSearch(compiledRegex, pattern,initPos=0,resultTotal = []):\n    result = compiledRegex.search(pattern)\n    if result !=None:\n        match = result[0]\n        span = result.span()\n        realSpan = (span[0]+initPos, span[1]+initPos)\n        # initialSpan = span[0]\n        endSpan = span[1]\n        initPos += endSpan\n        mresult = {\"match\":match, \"span\":realSpan}\n        resultTotal.append(mresult)\n        newPattern = pattern[endSpan:]\n        return recursiveCompiledSearch(compiledRegex,newPattern,initPos,resultTotal)\n    else: return resultTotal\nnlp = spacy.load(\"zh_core_web_sm\")\n# proper_nouns = ['\u5b88\u671b\u5148\u950b','\u7b2c\u4e94\u4eba\u683c']\n# whatever. we can always change shit.\n# nlp.tokenizer.pkuseg_update_user_dict(proper_nouns)"
        },
        {
            "comment": "This code checks if a given string contains English language, removes spaces and non-English elements using Jieba tokenizer and Spacy, and then prints the tokens along with their part of speech (POS) and dependency tags for further analysis.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/tests/title_cover_generator/spacy_word_swapper.py\":30-55",
            "content": "# this is imoortant.\nenglish = re.compile(r\"([a-zA-Z]+([ \\-,\\.:;?!]+)?)+\")\ndef check_has_language(string,language_re): result = recursiveCompiledSearch(language_re,string,resultTotal=[]); return len(result) >0\nfor elem in sample_data:\n    hasSpace = False\n    # we need to eliminate some english things.\n    # we also have some spaces. remove them before proceed.\n    if \" \" in elem:\n        hasSpace = True\n        elem = elem.replace(\" \", \"\")\n    # some flashy text will never be accepted. if outside of english, chinese we accept nothing.\n    # english is not included in spacy.\n    data = [x for x in jieba.cut(elem)] # contradictory.\n    english_check = check_has_language(elem,english)\n    if english_check:\n        print(\"HAS ENGLISH\")\n        print(elem)\n        continue\n    # check if words contains english. remove these titles.\n    # print(data)\n    nlp.tokenizer.pkuseg_update_user_dict(data)\n    doc = nlp(elem)\n    print(doc.text)\n    for token in doc:\n        print(token.text, token.pos_, token.dep_)"
        }
    ]
}