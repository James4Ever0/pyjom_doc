{
    "summary": "The code utilizes a trained model to detect NSFW content in videos and images, ensuring compliance by posting non-sexual content through an API. It stores classification probabilities and handles exceptions for unknown test_flags. However, only GIFs can be posted currently with caution about picture stretching.",
    "details": [
        {
            "comment": "The code imports necessary libraries, initializes certain functions and variables, and defines the processNSFWServerImageReply function which processes image classification reply from the server. It is for testing NSFW detection in videos or images, with options to test different aspects such as scanning, padding, etc. Note that it may not be recommended to use some parts of the code.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/tests/unittest_nsfw_video_score.py\":0-43",
            "content": "# we take max for the concerned ones, and take mean for the unconcerned ones.\nfrom test_commons import *\nimport requests\nfrom lazero.network.checker import waitForServerUp\nfrom pyjom.videotoolbox import getVideoFrameIteratorWithFPS\nfrom typing import Literal\ngateway = \"http://localhost:8511/\"\nfrom pyjom.mathlib import superMean, superMax\nfrom lazero.utils.importers import cv2_custom_build_init\ncv2_custom_build_init()\nimport cv2\n# suggest you not to use this shit.\n# import math\nfrom pyjom.imagetoolbox import resizeImageWithPadding, scanImageWithWindowSizeAutoResize\nfrom lazero.filesystem import tmpdir, tmpfile\ntmpdirPath = \"/dev/shm/medialang/nsfw\"\nimport uuid\nwaitForServerUp(8511, \"nsfw nodejs server\")\nimport os\ntest_flag = \"nsfw_video\"\n# test_flag = \"nsfw_image\"\n# test_flag = \"scanning\"\n# test_flag = \"paddinging\"\nsource = \"/root/Desktop/works/pyjom/samples/video/cute_cat_gif.gif\"\nimport numpy as np\ndef processNSFWServerImageReply(reply):\n    mDict = {}\n    for elem in reply:\n        className, probability = elem[\"className\"], elem[\"probability\"]"
        },
        {
            "comment": "This code processes an NSFW report array and returns a filtered dictionary. It updates the dictionary with class names as keys and their corresponding probabilities. Then, it calculates the average and maximum scores for certain classes. Lastly, it applies filters to the resulting dictionary based on specified minimum or maximum threshold values for each class.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/tests/unittest_nsfw_video_score.py\":44-79",
            "content": "        mDict.update({className: probability})\n    return mDict\ndef processNSFWReportArray(\n    NSFWReportArray,\n    average_classes=[\"Neutral\"],\n    get_max_classes=[\"Drawing\", \"Porn\", \"Sexy\", \"Hentai\"],\n):\n    assert set(average_classes).intersection(set(get_max_classes)) == set()\n    NSFWReport = {}\n    for element in NSFWReportArray:\n        for key in element.keys():\n            NSFWReport[key] = NSFWReport.get(key, []) + [element[key]]\n    for average_class in average_classes:\n        NSFWReport[average_class] = superMean(NSFWReport.get(average_class, [0]))\n    for get_max_class in get_max_classes:\n        NSFWReport[get_max_class] = superMax(NSFWReport.get(get_max_class, [0]))\n    return NSFWReport\nfrom pyjom.commons import checkMinMaxDict\n# you can reuse this, really.\ndef NSFWFilter(\n    NSFWReport,\n    filter_dict={\n        \"Neutral\": {\"min\": 0.5},\n        \"Sexy\": {\"max\": 0.5},\n        \"Porn\": {\"max\": 0.5},\n        \"Hentai\": {\"max\": 0.5},\n        \"Drawing\": {\"max\": 0.5},\n    },\n    debug=False,\n):\n    for key in filter_dict:"
        },
        {
            "comment": "The code snippet checks if a video passes the NSFW filter based on certain key values, and then displays the video frames in different scenarios: when testing for padding, it shows each frame with padding; when testing for scanning, it displays each frame after scanning with a specified threshold; and if test_flag is set to \"nsfw_video\", it processes another source.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/tests/unittest_nsfw_video_score.py\":80-107",
            "content": "        value = NSFWReport.get(key, 0)\n        key_filter = filter_dict[key]\n        result = checkMinMaxDict(value, key_filter)\n        if not result:\n            if debug:\n                print(\"not passing NSFW filter: %s\" % key)\n                print(\"value: %s\" % value)\n                print(\"filter: %s\" % str(key_filter))\n            return False\n    return True\nif test_flag == \"padding\":\n    for frame in getVideoFrameIteratorWithFPS(source, -1, -1, fps=1):\n        image = resizeImageWithPadding(frame, 1280, 720, border_type=\"replicate\")\n        # i'd like to view this.\n        cv2.imshow(\"PADDED\", image)\n        cv2.waitKey(0)\nelif test_flag == \"scanning\":\n    for frame in getVideoFrameIteratorWithFPS(source, -1, -1, fps=1):\n        scanned_array = scanImageWithWindowSizeAutoResize(\n            frame, 1280, 720, threshold=0.3\n        )\n        for index, image in enumerate(scanned_array):\n            cv2.imshow(\"SCANNED %d\" % index, image)\n            cv2.waitKey(0)\nelif test_flag == \"nsfw_video\":\n    # use another source?"
        },
        {
            "comment": "This code is looping through video frames, resizing and saving them as JPEGs in a temporary directory. It then posts each image to an API endpoint for NSFW content classification and appends the response JSON to a list of responses. The breakpoint and print statement are optional for debugging purposes.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/tests/unittest_nsfw_video_score.py\":108-129",
            "content": "    with tmpdir(path=tmpdirPath) as T:\n        responses = []\n        for frame in getVideoFrameIteratorWithFPS(source, -1, -1, fps=1):\n            padded_resized_frame = resizeImageWithPadding(\n                frame, 224, 224, border_type=\"replicate\"\n            )\n            # i'd like to view this.\n            basename = \"{}.jpg\".format(uuid.uuid4())\n            jpg_path = os.path.join(tmpdirPath, basename)\n            with tmpfile(path=jpg_path) as TF:\n                cv2.imwrite(jpg_path, padded_resized_frame)\n                files = {\"image\": (basename, open(jpg_path, \"rb\"), \"image/jpeg\")}\n                r = requests.post(\n                    gateway + \"nsfw\", files=files\n                )  # post gif? or just jpg?\n                try:\n                    response_json = r.json()\n                    response_json = processNSFWServerImageReply(response_json)\n                    # breakpoint()\n                    # print(\"RESPONSE:\", response_json)\n                    responses.append(\n                        response_json  # it contain 'messages'"
        },
        {
            "comment": "The code processes a server response for NSFW content classification and checks if the test passed. It uses the processNSFWReportArray function to analyze the responses and stores the result in the variable NSFWReport. If there's at least one response, it proceeds with the NSFWFilter function to evaluate the report. If the result is true, it prints \"NSFW test passed\" and source information. The code includes a case for the NSFW_IMAGE test flag and specifies a source file path.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/tests/unittest_nsfw_video_score.py\":130-148",
            "content": "                    )  # there must be at least one response, i suppose?\n                except:\n                    import traceback\n                    traceback.print_exc()\n                    print(\"error when processing NSFW server response\")\n        NSFWReport = processNSFWReportArray(responses)\n        # print(NSFWReport)\n        # breakpoint()\n        result = NSFWFilter(NSFWReport)\n        if result:\n            print(\"NSFW test passed.\")\n            print(\"source %s\" % source)\n# we don't want drawing dogs.\n# [{'className': 'Neutral', 'probability': 0.9995943903923035}, {'className': 'Drawing', 'probability': 0.00019544694805517793}, {'className': 'Porn', 'probability': 0.00013213469355832785}, {'className': 'Sexy', 'probability': 6.839347042841837e-05}, {'className': 'Hentai', 'probability': 9.632151886762585e-06}]\nelif test_flag == \"nsfw_image\":\n    source = \"/root/Desktop/works/pyjom/samples/image/kitty_flash.bmp\"\n    # RESPONSE: [{'className': 'Neutral', 'probability': 0.9997681975364685}"
        },
        {
            "comment": "This code demonstrates the results of a classification model for detecting different content categories in images. The provided examples show how the model predicts various probabilities for classes like 'Porn', 'Drawing', 'Hentai', and others, given specific image sources.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/tests/unittest_nsfw_video_score.py\":148-153",
            "content": ", {'className': 'Drawing', 'probability': 0.0002115015813615173}, {'className': 'Porn', 'probability': 1.3146535820851568e-05}, {'className': 'Hentai', 'probability': 4.075543984072283e-06}, {'className': 'Sexy', 'probability': 3.15313491228153e-06}]\n    # source = '/root/Desktop/works/pyjom/samples/image/pig_really.bmp'\n    # RESPONSE: [{'className': 'Neutral', 'probability': 0.9634107351303101}, {'className': 'Porn', 'probability': 0.0244674663990736}, {'className': 'Drawing', 'probability': 0.006115634460002184}, {'className': 'Hentai', 'probability': 0.003590137232095003}, {'className': 'Sexy', 'probability': 0.002416097791865468}]\n    # source = \"/root/Desktop/works/pyjom/samples/image/dog_with_text.bmp\"\n    # source = '/root/Desktop/works/pyjom/samples/image/dick2.jpeg'\n    # [{'className': 'Porn', 'probability': 0.7400921583175659}, {'className': 'Hentai', 'probability': 0.2109236866235733}, {'className': 'Sexy', 'probability': 0.04403943940997124}, {'className': 'Neutral', 'probability': 0.0034419416915625334}, {'className': 'Drawing', 'probability': 0.0015027812914922833}]"
        },
        {
            "comment": "This code is testing the classification accuracy of an image classification model for NSFW content. The comments describe three test cases with different images and the corresponding classifications provided by the model, highlighting the need for improving the model's ability to accurately identify NSFW content.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/tests/unittest_nsfw_video_score.py\":154-160",
            "content": "    # source = '/root/Desktop/works/pyjom/samples/image/dick4.jpeg'\n    # RESPONSE: [{'className': 'Porn', 'probability': 0.8319052457809448}, {'className': 'Hentai', 'probability': 0.16578854620456696}, {'className': 'Sexy', 'probability': 0.002254955470561981}, {'className': 'Neutral', 'probability': 3.2827374525368214e-05}, {'className': 'Drawing', 'probability': 1.8473130694474094e-05}]\n    # source = '/root/Desktop/works/pyjom/samples/image/porn_shemale.jpeg'\n    # no good for this one. this is definitely some unacceptable shit, with just cloth wearing.\n    # RESPONSE: [{'className': 'Neutral', 'probability': 0.6256022453308105}, {'className': 'Hentai', 'probability': 0.1276213526725769}, {'className': 'Porn', 'probability': 0.09777139872312546}, {'className': 'Sexy', 'probability': 0.09318379312753677}, {'className': 'Drawing', 'probability': 0.05582122132182121}]\n    # source ='/root/Desktop/works/pyjom/samples/image/dick3.jpeg'\n    # [{'className': 'Porn', 'probability': 0.9784200787"
        },
        {
            "comment": "This code reads an image from a known source, generates a unique filename, saves it temporarily, pads and resizes the image for classification, and then passes the processed image to the model for probability prediction. The goal is to lower the probability of being classified as porn by adding black padding around the image before processing.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/tests/unittest_nsfw_video_score.py\":160-169",
            "content": "54425}, {'className': 'Hentai', 'probability': 0.01346961222589016}, {'className': 'Sexy', 'probability': 0.006554164923727512}, {'className': 'Neutral', 'probability': 0.0015426197787746787}, {'className': 'Drawing', 'probability': 1.354961841570912e-05}]\n    # a known source causing unwanted shits.\n    image = cv2.imread(source)\n    basename = \"{}.jpg\".format(uuid.uuid4())\n    jpg_path = os.path.join(tmpdirPath, basename)\n    with tmpfile(path=jpg_path) as TF:\n        # black padding will lower the probability of being porn.\n        padded_resized_frame = resizeImageWithPadding(image, 224, 224)\n        # RESPONSE: [{'className': 'Neutral', 'probability': 0.6441782116889954}, {'className': 'Porn', 'probability': 0.3301379978656769}, {'className': 'Sexy', 'probability': 0.010329035110771656}, {'className': 'Hentai', 'probability': 0.010134727694094181}, {'className': 'Drawing', 'probability': 0.005219993181526661}]\n        # padded_resized_frame = resizeImageWithPadding(image, 224, 224,border_type='replicate')"
        },
        {
            "comment": "Code snippet is performing the following actions: \n1. Storing response from API containing classification probabilities for video.\n2. Writing frame to JPG format and posting it to gateway as non-sexual content using requests.\n3. If unknown test_flag, raising exception.\n4. Note mentions that only GIF can be posted now and caution about stretching pictures.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/tests/unittest_nsfw_video_score.py\":170-179",
            "content": "        # RESPONSE: [{'className': 'Neutral', 'probability': 0.6340386867523193}, {'className': 'Porn', 'probability': 0.3443007171154022}, {'className': 'Sexy', 'probability': 0.011606302112340927}, {'className': 'Hentai', 'probability': 0.006618513725697994}, {'className': 'Drawing', 'probability': 0.0034359097480773926}]\n        # neutral again? try porn!\n        cv2.imwrite(jpg_path, padded_resized_frame)\n        files = {\"image\": (basename, open(jpg_path, \"rb\"), \"image/jpeg\")}\n        r = requests.post(gateway + \"nsfw\", files=files)  # post gif? or just jpg?\n        print(\"RESPONSE:\", r.json())\nelse:\n    raise Exception(\"unknown test_flag: %s\" % test_flag)\n# you can only post gif now, or you want to post some other formats?\n# if you post shit, you know it will strentch your picture and produce unwanted shits."
        }
    ]
}