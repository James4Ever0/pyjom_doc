{
    "summary": "The code initializes an App object, tracks key points using PyrLK algorithm, calculates optical flow between frames, maintains maximum length of tracks and displays results. It uses OpenCV, numpy and Flownet2-pytorch model for processing and detecting key points.",
    "details": [
        {
            "comment": "App class initialization and video reading\n\nCode for creating and initializing the App object, capturing video frames from a specified source.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/tests/optical_flow/sparse_cpu.py\":0-36",
            "content": "#coding=utf-8\nimport numpy as np\nimport cv2\n# from common import anorm2, draw_str\n# from time import clock\nimport cmath\nlk_params = dict(winSize=(15, 15),\n                 maxLevel=2,\n                 criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))\n# maxCorners : \u8bbe\u7f6e\u6700\u591a\u8fd4\u56de\u7684\u5173\u952e\u70b9\u6570\u91cf\u3002\n# qualityLevel : \u53cd\u5e94\u4e00\u4e2a\u50cf\u7d20\u70b9\u5f3a\u5ea6\u6709\u591a\u5f3a\u624d\u80fd\u6210\u4e3a\u5173\u952e\u70b9\u3002\n# minDistance : \u5173\u952e\u70b9\u4e4b\u95f4\u7684\u6700\u5c11\u50cf\u7d20\u70b9\u3002\n# blockSize : \u8ba1\u7b97\u4e00\u4e2a\u50cf\u7d20\u70b9\u662f\u5426\u4e3a\u5173\u952e\u70b9\u65f6\u6240\u53d6\u7684\u533a\u57df\u5927\u5c0f\u3002\n# useHarrisDetector :\u4f7f\u7528\u539f\u58f0\u7684 Harris \u89d2\u4fa6\u6d4b\u5668\u6216\u6700\u5c0f\u7279\u5f81\u503c\u6807\u51c6\u3002\n# k : \u4e00\u4e2a\u7528\u5728Harris\u4fa6\u6d4b\u5668\u4e2d\u7684\u81ea\u7531\u53d8\u91cf\u3002\nfeature_params = dict(maxCorners=5000000,\n                      qualityLevel=0.1,\n                      minDistance=7,\n                      blockSize=7)\nclass App:\n    def __init__(self, video_src):  # \u6784\u9020\u65b9\u6cd5\uff0c\u521d\u59cb\u5316\u4e00\u4e9b\u53c2\u6570\u548c\u89c6\u9891\u8def\u5f84\n        self.track_len = 10\n        self.detect_interval = 1\n        self.tracks = []\n        self.cam = cv2.VideoCapture(video_src)\n        self.frame_idx = 0\n        self.num = 0\n        self.i = 0\n        self.all_distance = 0\n        self.count = 0\n    def run(self):  # \u5149\u6d41\u8fd0\u884c\u65b9\u6cd5\n        while True:\n            ret, frame = self.cam.read()  # \u8bfb\u53d6\u89c6\u9891\u5e27"
        },
        {
            "comment": "This code is performing optical flow tracking using the Pyramid Lucas-Kanade algorithm (PyrLK) on a video frame. It reads the previous and current frames, detects key points in the previous frame, calculates the new positions of these key points in the current frame, and updates the tracks list if any key point is found. The status array indicates whether each tracked point was found or not, and err presumably contains error information related to tracking. The code writes the x and y coordinates of each tracked point to a text file.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/tests/optical_flow/sparse_cpu.py\":37-57",
            "content": "            if ret == True:\n                frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)  # \u8f6c\u5316\u4e3a\u7070\u5ea6\u865a\u56fe\u50cf\n                # vis = frame.copy()\n                h, w = frame.shape[:2]\n                vis = np.ones((h, w), )\n                f = open('./shuibo_8_LK(x1,y1,x2,y2).txt','w+')\n                if len(self.tracks) > 0:  # \u68c0\u6d4b\u5230\u89d2\u70b9\u540e\u8fdb\u884c\u5149\u6d41\u8ddf\u8e2a\n                    img0, img1 = self.prev_gray, frame_gray\n                    p0 = np.float32([tr[-1] for tr in self.tracks]).reshape(-1, 1, 2)\n                    \"\"\"\n                    nextPts, status, err = calcOpticalFlowPyrLK(prevImg, nextImg, prevPts[, nextPts[, status[, \n                    err[, winSize[, maxLevel[, criteria[, flags[, minEigThreshold]]]]]]]])\n                    \u53c2\u6570\u8bf4\u660e\uff1a\n                      prevImage \u524d\u4e00\u5e278-bit\u56fe\u50cf\n                      nextImage \u5f53\u524d\u5e278-bit\u56fe\u50cf\n                      prevPts \u5f85\u8ddf\u8e2a\u7684\u7279\u5f81\u70b9\u5411\u91cf\n                      nextPts \u8f93\u51fa\u8ddf\u8e2a\u7279\u5f81\u70b9\u5411\u91cf\n                      status \u7279\u5f81\u70b9\u662f\u5426\u627e\u5230\uff0c\u627e\u5230\u7684\u72b6\u6001\u4e3a1\uff0c\u672a\u627e\u5230\u7684\u72b6\u6001\u4e3a0\n                      err \u8f93\u51fa\u9519\u8bef\u5411\u91cf\uff0c\uff08\u4e0d\u592a\u7406\u89e3\u7528\u9014...\uff09\n                      winSize \u641c\u7d22\u7a97\u53e3\u7684\u5927\u5c0f"
        },
        {
            "comment": "This code calculates optical flow between two images using cv2.calcOpticalFlowPyrLK, tracking points from one image to another. It then compares the tracked points with the actual points and measures the displacement. Points with displacement greater than 1 are considered as incorrect and removed. The remaining points form new_tracks, which is a list of successful tracks.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/tests/optical_flow/sparse_cpu.py\":58-73",
            "content": "                      maxLevel \u6700\u5927\u7684\u91d1\u5b57\u5854\u5c42\u6570\n                      flags \u53ef\u9009\u6807\u8bc6\uff1aOPTFLOW_USE_INITIAL_FLOW   OPTFLOW_LK_GET_MIN_EIGENVALS\n                    \"\"\"\n                    p1, st, err = cv2.calcOpticalFlowPyrLK(img0, img1, p0, None,\n                                                           **lk_params)  # \u524d\u4e00\u5e27\u7684\u89d2\u70b9\u548c\u5f53\u524d\u5e27\u7684\u56fe\u50cf\u4f5c\u4e3a\u8f93\u5165\u6765\u5f97\u5230\u89d2\u70b9\u5728\u5f53\u524d\u5e27\u7684\u4f4d\u7f6e\n                    p0r, st, err = cv2.calcOpticalFlowPyrLK(img1, img0, p1, None,\n                                                            **lk_params)  # \u5f53\u524d\u5e27\u8ddf\u8e2a\u5230\u7684\u89d2\u70b9\u53ca\u56fe\u50cf\u548c\u524d\u4e00\u5e27\u7684\u56fe\u50cf\u4f5c\u4e3a\u8f93\u5165\u6765\u627e\u5230\u524d\u4e00\u5e27\u7684\u89d2\u70b9\u4f4d\u7f6e\n                    d = abs(p0 - p0r).reshape(-1, 2).max(-1)  # \u5f97\u5230\u89d2\u70b9\u56de\u6eaf\u4e0e\u524d\u4e00\u5e27\u5b9e\u9645\u89d2\u70b9\u7684\u4f4d\u7f6e\u53d8\u5316\u5173\u7cfb\n                    # good = d < 1  # \u5224\u65add\u5185\u7684\u503c\u662f\u5426\u5c0f\u4e8e1\uff0c\u5927\u4e8e1\u8ddf\u8e2a\u88ab\u8ba4\u4e3a\u662f\u9519\u8bef\u7684\u8ddf\u8e2a\u70b9\n                    good=d\n                    new_tracks = []\n                    for tr, (x, y), good_flag in zip(self.tracks, p1.reshape(-1, 2), good):  # \u5c06\u8ddf\u8e2a\u6b63\u786e\u7684\u70b9\u5217\u5165\u6210\u529f\u8ddf\u8e2a\u70b9\n                        if not good_flag:\n                            continue\n                        tr.append((x, y))#tr\u662f\u524d\u4e00\u5e27\u7684\u89d2\u70b9\uff0c\u4e0e\u5f53\u524d\u5e27\u7684\u89d2\u70b9(x,y)\u5408\u5e76\u3002\u6807\u5fd7\u4e3agood_flag"
        },
        {
            "comment": "This code tracks optical flow points across multiple frames, storing the tracked points in 'tracks'. It appends new tracks and deletes old ones to maintain a maximum length. The x and y coordinates of current points are plotted on a visualization ('vis'). Finally, it calculates the Euclidean distance between consecutive points and writes them into file 'f', while printing the total number of correctly tracked points.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/tests/optical_flow/sparse_cpu.py\":74-97",
            "content": "                        if len(tr) > self.track_len:\n                            del tr[0]\n                        new_tracks.append(tr)\n                        # print(x,y)\n                        # breakpoint()\n                        cv2.circle(vis, (int(x), int(y)), 2, (0, 255, 0), -1)#\u5f53\u524d\u5e27\u89d2\u70b9\u753b\u5706\n                    self.tracks = new_tracks #self.tracks\u4e2d\u7684\u503c\u7684\u683c\u5f0f\u662f\uff1a(\u524d\u4e00\u5e27\u89d2\u70b9)(\u5f53\u524d\u5e27\u89d2\u70b9)\n                    # print(self.tracks[0])\n                    # print(self.tracks[1])\n                    distance = 0\n                    for tr in self.tracks:\n                        # tr[0]=list(tr[0])\n                        # tr[1]=list(tr[1])\n                        x1=tr[0][0]\n                        y1=tr[0][1]\n                        x2 = tr[1][0]\n                        y2 = tr[1][1]\n                        f.writelines([ str(x1), ' ', str(y1), ' ', str(x2), ' ', str(y2),'\\n'])\n                        dis=cmath.sqrt((x2-x1)*(x2-x1)+(y2-y1)*(y2-y1))\n                        #\u6b63\u786e\u8ffd\u8e2a\u7684\u70b9\u7684\u4e2a\u6570\n                        print(len(self.tracks))"
        },
        {
            "comment": "Code calculates average pixel point displacement between frames and prints the results. It keeps track of all pixel point movements in a frame and counts the number of frames. The code checks for features every 1 frame, initializes a mask image, detects corners using goodFeaturesToTrack function, and stores the result if it is not None.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/tests/optical_flow/sparse_cpu.py\":98-116",
            "content": "                        #\u6bcf\u4e00\u4e2a\u6b63\u786e\u8ffd\u8e2a\u7684\u70b9\u7684\u50cf\u7d20\u70b9\u7684\u4f4d\u79fb\n                        print(dis.real)\n                        distance=distance+dis\n                    len_tracks = len(self.tracks)\n                    if len_tracks == 0:continue\n                    distance=distance/len_tracks\n                    self.all_distance=self.all_distance+distance\n                    self.count=self.count+1\n                    print(\"\u6bcf\u4e00\u5e27\u50cf\u7d20\u70b9\u5e73\u5747\u4f4d\u79fb\uff1a\",distance,\"\u7b2c\u51e0\u5e27\uff1a\",self.count)\n                    print(\"\u6240\u6709\u5e27\u5e73\u5747\u4f4d\u79fb\uff1a\",(self.all_distance/self.count).real)\n                f.close()\n                if self.frame_idx % self.detect_interval == 0:  #\u6bcf1\u5e27\u68c0\u6d4b\u4e00\u6b21\u7279\u5f81\u70b9\n                    mask = np.zeros_like(frame_gray)  # \u521d\u59cb\u5316\u548c\u89c6\u9891\u5927\u5c0f\u76f8\u540c\u7684\u56fe\u50cf\n                    mask[:] = 255  # \u5c06mask\u8d4b\u503c255\u4e5f\u5c31\u662f\u7b97\u5168\u90e8\u56fe\u50cf\u7684\u89d2\u70b9\n                    for x, y in [np.int32(tr[-1]) for tr in self.tracks]:  #\u8ddf\u8e2a\u7684\u89d2\u70b9\u753b\u5706\n                        cv2.circle(mask, (x, y), 5, 0, -1)\n                    p = cv2.goodFeaturesToTrack(frame_gray, mask=mask, **feature_params)  # \u50cf\u7d20\u7ea7\u522b\u89d2\u70b9\u68c0\u6d4b\n                    if p is not None:"
        },
        {
            "comment": "This code is a part of a video processing program. It reads frames from a video source, detects key points in each frame using the LK tracker, tracks these key points across successive frames to estimate optical flow, and displays the results. The code uses OpenCV library for image processing, numpy for numerical computations, and cv2.waitKey() function for window handling. It also imports a Flownet2-pytorch model from a git repository and installs custom layers.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/tests/optical_flow/sparse_cpu.py\":117-150",
            "content": "                        for x, y in np.float32(p).reshape(-1, 2):\n                            self.tracks.append([(x, y)])  # \u5c06\u68c0\u6d4b\u5230\u7684\u89d2\u70b9\u653e\u5728\u5f85\u8ddf\u8e2a\u5e8f\u5217\u4e2d\n                self.frame_idx += 1\n                self.prev_gray = frame_gray\n                cv2.imshow('lk_track', vis)\n            # ch = 0xFF & \n            if cv2.waitKey(20) == \"q\":\n                # cv2.imwrite(\"./mashiti-result4.png\", vis)\n                break\n# # get flownet2-pytorch source\n# git clone https://github.com/NVIDIA/flownet2-pytorch.git\n# cd flownet2-pytorch\n# # install custom layers\n# bash install.sh\ndef main():\n    import sys\n    try:\n        video_src = sys.argv[1]\n    except:\n        # video_src = \"./F/8/shuibo_8.avi\"\n        video_src = \"/media/root/help/pyjom/samples/video/dog_with_text.mp4\"\n    # print\n    # __doc__\n    App(video_src).run()\n    cv2.destroyAllWindows()\nif __name__ == '__main__':\n    main()"
        }
    ]
}