{
    "summary": "The code uses Detectron2 for object detection, tracks \"person\" or \"dog\", updates tracked_objects, and displays bounding boxes. It utilizes OpenCV for video display and waits for 'q' to terminate, closing windows upon exiting.",
    "details": [
        {
            "comment": "The code imports necessary libraries and sets up a Detectron2 object detector using pre-trained weights for instance segmentation. It also defines a function to calculate Euclidean distance between detection and tracked objects. The configuration file specifies the model architecture, which is R_50_FPN with three stages and the specific weights (model_final_f10217.pkl) to be used for detection. The weights can either be downloaded from a public S3 storage or retrieved from the local cache if already downloaded.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/tests/video_detector_tests/detectron2_norfair.py\":0-20",
            "content": "import cv2\nimport numpy as np\nfrom detectron2.config import get_cfg\nfrom detectron2.engine import DefaultPredictor\nfrom cocoNames import cocoRealName\nfrom norfair import Detection, Tracker, Video, draw_tracked_objects\ndef euclidean_distance(detection, tracked_object):\n    return np.linalg.norm(detection.points - tracked_object.estimate)\n# Set up Detectron2 object detector\ncfg = get_cfg()\ncfg.merge_from_file(\"norfair/demos/faster_rcnn_R_50_FPN_3x.yaml\")\ncfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5 # looks like it does not recognize dog.\n# cfg.MODEL.WEIGHTS = \"detectron2://COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl\"\ncfg.MODEL.WEIGHTS = \"/root/Desktop/works/pyjom/tests/video_detector_tests/detectron2_models/model_final_f10217.pkl\"\n# it is stored in s3\n# https://dl.fbaipublicfiles.com/detectron2/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl\n# download cache: /root/.torch/iopath_cache/detectron2/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl"
        },
        {
            "comment": "The code initializes a video detector using Detector class and then processes each frame of the video. It predicts instances in each frame, prints detected classes and instances, and continues only if there are predictions. The tracker is used to track objects over frames, but its parameters might need clarification. Speedup is mentioned as needed, which implies potential optimizations.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/tests/video_detector_tests/detectron2_norfair.py\":22-47",
            "content": "detector = DefaultPredictor(cfg)\n# what are the classes output by the model?\n# Norfair\nvideo_path = \"/root/Desktop/works/pyjom/samples/video/dog_with_text.mp4\"\n# video_path = \"/root/Desktop/works/pyjom/samples/video/LlfeL29BP.mp4\"\nvideo = Video(input_path=video_path)\ntracker = Tracker(distance_function=euclidean_distance, distance_threshold=400,hit_inertia_min=2,hit_inertia_max=20,initialization_delay=1) # what the heck?\ntracked_objects = None\ndisplay=True\nfor index, frame in enumerate(video): # we need to speed up.\n    if index%10 == 0:\n        detections = detector(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n        print(\"original detections:\",detections)\n        instances = detections[\"instances\"]\n        print(\"instances:\",instances)\n        # breakpoint()\n        pred_classes = instances.pred_classes\n        if len(pred_classes) == 0:\n            continue\n        detections2=[]\n        for index,class_ in enumerate(pred_classes):\n            print(\"index:\",index)\n            class_ = int(class_.cpu().numpy().tolist())"
        },
        {
            "comment": "This code is filtering and creating detection objects for \"person\" or \"dog\" instances from a given dataset. It prints the box coordinates, score, and class name before adding it to the detections2 list. The code then updates tracked_objects using the tracker function with the detections2 list.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/tests/video_detector_tests/detectron2_norfair.py\":48-67",
            "content": "            print(\"class:\",class_)\n            box = instances.pred_boxes.tensor[index].cpu().numpy().tolist()\n            box = [int(x) for x in box]\n            score = float(instances.scores[index].cpu().numpy().tolist())\n            print('box:',box)\n            print('score:',score)\n            className = cocoRealName[class_]\n            # we filter our targets.\n            if className not in [\"person\",\"dog\"]:\n                continue\n            mdata = {\"box\":box,\"class\":{\"id\":class_,\"name\":className}}\n            det = Detection(instances.pred_boxes.get_centers()[index].cpu().numpy(),scores=np.array([score]),data=mdata)\n            detections2.append(det)\n            # breakpoint()\n        # detections = [Detection(p) for p in instances.pred_boxes.get_centers().cpu().numpy()] # what is this instance anyway?\n        # you would lost data you dick!\n        print(\"detections2\",detections2)\n        tracked_objects = tracker.update(detections=detections2)\n        # print(detections)\n        print(\"tracked objects:\",tracked_objects) # you don't track shit?"
        },
        {
            "comment": "The code checks if there are any tracked objects and then proceeds to draw bounding boxes around them, add labels for the objects' class names, and display their IDs on the frame using OpenCV functions. Additionally, it offers an alternative way to draw the objects in a different color.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/tests/video_detector_tests/detectron2_norfair.py\":68-96",
            "content": "    if tracked_objects is not None:\n        if tracked_objects!=[]:\n            # there is no bounding box avaliable?\n            for obj in tracked_objects:\n                point = obj.estimate[0]\n                position = tuple(point.astype(int))\n                color = (255,0,0)\n                # breakpoint()\n                name = obj.last_detection.data[\"class\"][\"name\"]\n                cv2.circle(\n                        frame,\n                        position,\n                        radius=100,\n                        color=color,\n                        thickness=2,\n                    )\n                cv2.putText(\n                    frame,\n                    \"[{}][{}]\".format(str(obj.id),name),\n                    (position[0]-100,position[1]),\n                    cv2.FONT_HERSHEY_SIMPLEX,\n                    2,\n                    (0,255,0),\n                    3,\n                    cv2.LINE_AA,\n                )\n            # breakpoint()\n        # i want to draw you in a different way.\n        # draw_tracked_objects(frame, tracked_objects,color=(255,0,0))"
        },
        {
            "comment": "The code displays a frame from a video using OpenCV's imshow function and waits for a user input (key) to terminate. The key input is checked if it matches the character 'q', which signals a break in the loop. OpenCV's destroyAllWindows() is called to close the window when display is enabled.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/tests/video_detector_tests/detectron2_norfair.py\":97-105",
            "content": "    if display:\n        cv2.imshow(\"window\",frame)\n        key  =  cv2.waitKey(1) & 0xff\n        if key == ord('q'):\n            break\n        # maybe we shall print this shit somehow.\n    # video.write(frame) # you write what?\nif display:\n    cv2.destroyAllWindows()"
        }
    ]
}