{
    "summary": "This code imports libraries, initializes a motion detector algorithm and sets up video capture. It continuously reads frames from the source, applies an algorithm to create output images, displays them in separate windows, and runs until a frame is not ready or Esc key pressed.",
    "details": [
        {
            "comment": "This code imports necessary libraries and initializes a motion detector algorithm (FrameDifference) to track objects in a video. It also defines the video file path and sets up a VideoCapture object. The code waits for the video header, retrieves the current frame position, and is ready to process frames using the motion detection algorithm.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/tests/video_detector_tests/motion_github.py\":0-28",
            "content": "# motion detectors are used to track objects. though you may want to separate objects with it.\nimport numpy as np\nimport cv2\nimport pybgs as bgs\n# suspect by static image analysis, and then create bounding box over the thing.\n# check image quality.\nalgorithm = bgs.FrameDifference() # track object we need that.\n# algorithm = bgs.SuBSENSE()\n# video_file = \"../../samples/video/highway_car.avi\"\n# video_file = \"../../samples/video/dog_with_text.mp4\"\nvideo_file = \"../../samples/video/LiEIfnsvn.mp4\" # this one with cropped boundaries. \n# video_file = \"../../samples/video/LlfeL29BP.mp4\"\n# maybe we should consider something else to crop the thing? or not?\n# accumulate the delta over time to see the result?\n# use static detection method.\ncapture = cv2.VideoCapture(video_file)\nwhile not capture.isOpened():\n  capture = cv2.VideoCapture(video_file)\n  cv2.waitKey(1000)\n  print(\"Wait for the header\")\n#pos_frame = capture.get(cv2.cv.CV_CAP_PROP_POS_FRAMES)\n#pos_frame = capture.get(cv2.CV_CAP_PROP_POS_FRAMES)\npos_frame = capture.get(1)"
        },
        {
            "comment": "The code continuously reads frames from a video source and displays them. It captures the current frame position, applies an algorithm to create output images, and shows the output and background model images in separate windows. It keeps running until a frame is not ready or the user presses Esc key, closing all windows at the end.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/tests/video_detector_tests/motion_github.py\":29-61",
            "content": "while True:\n  flag, frame = capture.read()\n  if flag:\n    cv2.imshow('video', frame)\n    #pos_frame = capture.get(cv2.cv.CV_CAP_PROP_POS_FRAMES)\n    #pos_frame = capture.get(cv2.CV_CAP_PROP_POS_FRAMES)\n    pos_frame = capture.get(1)\n    #print str(pos_frame)+\" frames\"\n    img_output = algorithm.apply(frame)\n    img_bgmodel = algorithm.getBackgroundModel()\n    cv2.imshow('img_output', img_output)\n    cv2.imshow('img_bgmodel', img_bgmodel)\n  else:\n    #capture.set(cv2.cv.CV_CAP_PROP_POS_FRAMES, pos_frame-1)\n    #capture.set(cv2.CV_CAP_PROP_POS_FRAMES, pos_frame-1)\n    #capture.set(1, pos_frame-1)\n    #print \"Frame is not ready\"\n    cv2.waitKey(1000)\n    break\n  if 0xFF & cv2.waitKey(10) == 27:\n    break\n  #if capture.get(cv2.cv.CV_CAP_PROP_POS_FRAMES) == capture.get(cv2.cv.CV_CAP_PROP_FRAME_COUNT):\n  #if capture.get(cv2.CV_CAP_PROP_POS_FRAMES) == capture.get(cv2.CV_CAP_PROP_FRAME_COUNT):\n  #if capture.get(1) == capture.get(cv2.CV_CAP_PROP_FRAME_COUNT):\n    #break\ncv2.destroyAllWindows()"
        }
    ]
}