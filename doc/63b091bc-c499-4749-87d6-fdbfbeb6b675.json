{
    "summary": "This code defines a custom tokenizer class, T5PegasusTokenizer, which extends the BertTokenizer. It takes an optional pre_tokenizer function as input and initializes the base tokenizer. The _tokenize method splits the text into tokens using the pre_tokenizer, and if the token is in the vocabulary, it adds it to split_tokens. If not, it extends split_tokens with tokens generated by the base tokenizer's _tokenize method.",
    "details": [
        {
            "comment": "This code defines a custom tokenizer class, T5PegasusTokenizer, which extends the BertTokenizer. It takes an optional pre_tokenizer function as input and initializes the base tokenizer. The _tokenize method splits the text into tokens using the pre_tokenizer, and if the token is in the vocabulary, it adds it to split_tokens. If not, it extends split_tokens with tokens generated by the base tokenizer's _tokenize method.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/tests/title_cover_generator/tokenizer.py\":0-16",
            "content": "import jieba\nfrom transformers import BertTokenizer\n# alike structure as DianJing. but is it for gpt2?\nclass T5PegasusTokenizer(BertTokenizer):\n    def __init__(self, pre_tokenizer=lambda x: jieba.cut(x, HMM=False), *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.pre_tokenizer = pre_tokenizer\n    def _tokenize(self, text, *arg, **kwargs):\n        split_tokens = []\n        for text in self.pre_tokenizer(text):\n            if text in self.vocab:\n                split_tokens.append(text)\n            else:\n                split_tokens.extend(super()._tokenize(text))\n        return split_tokens"
        }
    ]
}