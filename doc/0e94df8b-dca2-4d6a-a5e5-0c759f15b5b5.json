{
    "summary": "This code uses Hyperopt library for parameter optimization, chooses hyperparameters from different cases via choice function, and samples 10 times for each search space.",
    "details": [
        {
            "comment": "This code uses the hyperopt library for parameter optimization. The hyperparameters are chosen from different cases using a choice function, including lambda functions. The space is sampled 10 times using stochastic sampling, and each sample is printed to the console.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/tests/hyper_param_optimization/test.py\":0-23",
            "content": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\nfrom hyperopt import hp\n# usually this hyper parameter optimization is done regularlly, and the optimized parameters will be used for a while till next update.\n# but can we optimize these parameters offline?\n# if not offline then we can only use traditional machine learning instead...\n# or this trial and error process is actually a kind of offline machine learning, like random search and graph inference...\n# better use hyperopt with a discriminator ML algorithm.\n# space = hp.choice(\n#     \"a\",\n#     [(\"case 1\", 1 + hp.lognormal(\"c1\", 0, 1)), (\"case 2\", hp.uniform(\"c2\", -10, 10))],\n# )\nimport hyperopt.pyll.stochastic as stochastic\nspace = hp.choice(\"lambda\",[lambda :1, lambda:2]) # if it is lambda, function will not resolve. however, after passing this thing into the main criterion function, it will utilize the lambda function.\nfor _ in range(10):\n    sample = stochastic.sample(space)\n    print(\"SAMPLE:\", sample) # this will return the tuple. can we put some custom functions here?"
        },
        {
            "comment": "This code defines and samples two hyperparameter search spaces using the Hyperopt library's Pyll module. The \"my_func\" function is defined within a scope, allowing for easy integration with custom functions like Scikit-Learn. It then prints and samples from these search spaces 10 times.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/tests/hyper_param_optimization/test.py\":24-39",
            "content": "    # there must be some integrations with custom functions. for example: scikit-learn\nprint(\"_______________________________\") # splited.\nfrom hyperopt.pyll import scope\n@scope.define # this is how we sample the \"LAMBDA\".\ndef my_func(a,b=1):\n    print(\"running function my_func\", a,b)\n    return a*b\nspace_0 = scope.my_func(hp.choice(\"myChoice\",[1,2]))\nspace_1 = scope.my_func(hp.choice(\"myChoice\",[1,2]), hp.choice(\"myChoice2\",[2,3,4]))\nfor _ in range(10):\n    print(stochastic.sample(space_0), stochastic.sample(space_1))"
        }
    ]
}