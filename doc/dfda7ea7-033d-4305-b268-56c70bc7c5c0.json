{
    "summary": "This code uses Tesseract OCR, topic modeling, and text preprocessing to extract text from font-specific images, removes unwanted characters, generates topics, applies TF-IDF vectors, paraphrases using ClueAI API, and offers multi-threaded input/output handling.",
    "details": [
        {
            "comment": "Code imports necessary libraries and defines a function `renderSingleLineTextUsingFont` that takes input text, output image name, font path, font size, and margin as parameters. It checks if the specified font exists, initializes pygame in headless mode, sets up colors, and renders the text onto an image with specified font, size, and margin.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/pyjom/languagetoolbox.py\":0-40",
            "content": "########################[FILTERING]#########################\n# DONE: use notofu for rendering then use tesseract for recognition\nimport pygame\nimport functools\n@functools.lru_cache(maxsize=1)\ndef initPygame():\n    os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"\n    # headless pygame\n    pygame.init()\nimport os\nimport pytesseract\ndef renderSingleLineTextUsingFont(\n    textContent: str,\n    output_name: str,\n    fontPath: str = os.path.join(\n        os.dirname(__file__),\n        \"../tests/render_and_recognize_long_text_to_filter_unwanted_characters/get_and_merge_fonts/GoNotoCurrent.ttf\",\n    ),\n    fontSize: int = 40,\n    margin: int = 20,\n):\n    assert os.path.exists(fontPath)\n    initPygame()\n    black, white = pygame.Color(\"black\"), pygame.Color(\"white\")\n    # pillow can also do that\n    # https://plainenglish.io/blog/generating-text-on-image-with-python-eefe4430fe77\n    # pygame.font.get_fonts()\n    # install your font to system please? but why all lower case font names?\n    # fontName = \"notosans\"\n    # this font is bad."
        },
        {
            "comment": "The code initializes a Pygame font, renders text on the surface, gets its size, sets up a display image, fills it with white, blits the rendered text onto the image, updates the display, and saves the final image.\nThe recognizeCharactersFromImageWithTesseract function uses Tesseract OCR to recognize characters in an image file for specified languages and returns the output as a string.\nThe convertToChineseOrEnglishOrJapaneseCharactersUsingTesseract function temporarily stores input text in a PNG file, uses Tesseract to extract Chinese, English, or Japanese characters from the image, and potentially returns the extracted text.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/pyjom/languagetoolbox.py\":42-73",
            "content": "    # font = pygame.font.SysFont(fontName,fontSize)\n    # fontPath = \"/usr/share/fonts/truetype/noto/NotoSans-Regular.ttf\" # shit this fails.\n    # use some kind of super large merged notofont.\n    font = pygame.font.Font(fontPath, fontSize)\n    word_surface = font.render(textContent, False, black)\n    word_width, word_height = word_surface.get_size()\n    SIZE = (word_width + margin * 2, word_height + margin * 2)\n    image = pygame.display.set_mode(SIZE, pygame.RESIZABLE)\n    image.fill(white)\n    image.blit(word_surface, (margin, margin))\n    pygame.display.update()\n    pygame.image.save(image, output_name)\ndef recognizeCharactersFromImageWithTesseract(\n    imagePath: str, langs: list = [\"eng\", \"chi_sim\", \"chi_tra\", \"jpn\"]\n):\n    # pytesseract.get_languages(config=\"\")\n    langCode = \"+\".join(langs)\n    output = pytesseract.image_to_string(imagePath, lang=langCode)\n    return output\nimport tempfile\ndef convertToChineseOrEnglishOrJapaneseCharactersUsingTesseract(char_list: str):\n    with tempfile.NamedTemporaryFile(\"wb\", suffix=\".png\") as f:"
        },
        {
            "comment": "This function filters out non-Chinese, English, or Japanese characters from a given list of characters. It iterates through each character and checks if it belongs to any of these three categories using the provided checkers dictionary. If a character does not belong to any of these categories, it is excluded from the output list.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/pyjom/languagetoolbox.py\":74-106",
            "content": "        imagePath = f.name\n        renderSingleLineTextUsingFont(char_list,imagePath)\n        output = recognizeCharactersFromImageWithTesseract(imagePath)\n        return output\n# bilibili title requirements may also applied to tags, descriptions\nimport re\nimport string as string_builtin\nfrom zhon.hanzi import punctuation as chinese_punctuation\ndef filterNonChineseOrEnglishOrJapaneseCharacters(char_list: str):\n    output = []\n    checkers = {\n        \"chinese\": lambda c: (\n            (c in chinese_punctuation) or (re.match(r\"[\\u4e00-\\u9fa5]\", c) is not None)\n        ),\n        \"english\": lambda c: (\n            (c in \" \" + string_builtin.punctuation)\n            or (re.match(r\"[a-zA-Z0-9]\", c) is not None)\n        ),\n        \"japanese\": lambda c: re.match(r\"[\u4e00-\u9fa0\u3041-\u3094\u30a1-\u30f4\u30fc\uff41-\uff5a\uff21-\uff3a\uff10-\uff19\u3005\u3006\u3024\u30f6]\", c) is not None,\n    }\n    for char in char_list:\n        signal = True\n        for key, checker in checkers.items():\n            signal = checker(char)\n            if signal in [False, 0, None]:\n                break\n        if signal:"
        },
        {
            "comment": "This code is implementing a function `get_topics` for retrieving topics and their corresponding words from a topic model. It iterates through the model's components, sorts them by frequency, selects top n_top_words, removes short/redundant keywords, and stores this information in the 'topics' list.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/pyjom/languagetoolbox.py\":107-135",
            "content": "            output.append(char)\n    return \"\".join(output)\n########################[FILTERING]#########################\n########################[PREPROCESSING & TOPIC MODELING]#########################\nenglishNLP = None\nenglishStopWords = None\nporterStemmer = None\ndef get_topics(model, feature_names, n_top_words):\n    # \u9996\u5148\u662f\u904d\u5386\u6a21\u578b\u4e2d\u5b58\u50a8\u7684\u8bdd\u9898\u5e8f\u53f7\u548c\u8bdd\u9898\u5185\u5bb9\n    topics = []\n    for topic_idx, topic in enumerate(model.components_):\n        # \u7136\u540e\u6253\u5370\u8bdd\u9898\u7684\u5e8f\u53f7\u4ee5\u53ca\u6307\u5b9a\u6570\u91cf\u7684\u6700\u9ad8\u9891\u7684\u5173\u952e\u8bcd\n        message = \"topic #%d:\" % topic_idx\n        mList = [feature_names[i] for i in topic.argsort()[: -n_top_words - 1 : -1]]\n        mListStr = \" \".join(mList)\n        message += mListStr\n        mSet = set(mList)  # the set contains word groups like 'river question'\n        cDict = {k: mList.count(k) for k in mSet}\n        mRealList = mListStr.split(\" \")\n        mRealList = [\n            x.strip() for x in mRealList if len(x.strip()) > 1\n        ]  # usually things shorter than 2 letters are no good.\n        mRealSet = set(mRealList)\n        cRealDict = {k: mRealList.count(k) for k in mRealSet}"
        },
        {
            "comment": "This code generates and prints the topics from a given model, along with their indexes. It then displays the top words for each topic and provides two count dictionaries: one for the original list and another for the filtered real list. The original list includes all words sorted by frequency, while the real list only contains words longer than 2 characters to exclude noise or irrelevant terms. The code also prints the sets of these lists and the count dictionaries.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/pyjom/languagetoolbox.py\":136-161",
            "content": "        topics.append({\"combined\": mList, \"separate\": mRealList})\n    return topics\ndef print_topics(model, feature_names, n_top_words):\n    # \u9996\u5148\u662f\u904d\u5386\u6a21\u578b\u4e2d\u5b58\u50a8\u7684\u8bdd\u9898\u5e8f\u53f7\u548c\u8bdd\u9898\u5185\u5bb9\n    for topic_idx, topic in enumerate(model.components_):\n        # \u7136\u540e\u6253\u5370\u8bdd\u9898\u7684\u5e8f\u53f7\u4ee5\u53ca\u6307\u5b9a\u6570\u91cf\u7684\u6700\u9ad8\u9891\u7684\u5173\u952e\u8bcd\n        message = \"topic #%d:\" % topic_idx\n        mList = [feature_names[i] for i in topic.argsort()[: -n_top_words - 1 : -1]]\n        mListStr = \" \".join(mList)\n        message += mListStr\n        mSet = set(mList)  # the set contains word groups like 'river question'\n        cDict = {k: mList.count(k) for k in mSet}\n        mRealList = mListStr.split(\" \")\n        mRealList = [\n            x.strip() for x in mRealList if len(x.strip()) > 1\n        ]  # usually things shorter than 2 letters are no good.\n        mRealSet = set(mRealList)\n        cRealDict = {k: mRealList.count(k) for k in mRealSet}\n        print(\"MESSAGE\", message)\n        print(\"SET\", mSet)\n        print(\"COUNT DICT\", cDict)  # pointless to count here?\n        print(\"RealSET\", mRealSet)\n        print(\"RealCOUNT DICT\", cRealDict)"
        },
        {
            "comment": "This code defines a function called \"englishSentencePreprocessing\" that performs English sentence preprocessing. It utilizes the NLTK and spaCy libraries for natural language processing. The function loads an English language model, tokenizes text into words, filters out unwanted parts of speech (POS) and stop words, and applies stemming to the remaining words. The filtered and stemmed words are stored in \"lemma_word1\" list which is then used further in the code.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/pyjom/languagetoolbox.py\":162-193",
            "content": "    print()\ndef englishSentencePreprocessing(\n    text, unwantedPOS=[\"PRON\", \"CCONJ\", \"ADP\", \"PART\", \"PUNCT\", \"AUX\"]\n):\n    global englishNLP, englishStopWords, porterStemmer\n    from nltk.corpus import stopwords\n    from nltk.tokenize import word_tokenize\n    import en_core_web_sm\n    from nltk.stem import PorterStemmer\n    if englishNLP is None:\n        englishNLP = en_core_web_sm.load()\n    doc = englishNLP(text)\n    if englishStopWords is None:\n        set(stopwords.words(\"english\"))\n        englishStopWords = set([elem.lower() for elem in stopwords.words(\"english\")])\n    if porterStemmer is None:\n        porterStemmer = PorterStemmer()\n    lemma_word1 = []\n    # this shit has the lang tag. it might be useful for language detection. really?\n    for token in doc:\n        if token.pos_ in unwantedPOS:\n            continue\n        if token.text.lower() in englishStopWords:\n            continue\n        lemma_word1.append(token.text)\n    Stem_words = []\n    for w in lemma_word1:\n        rootWord = porterStemmer.stem(w)"
        },
        {
            "comment": "The code defines several functions for natural language processing tasks. \"englishSentencePreprocessing\" stems words, \"sentenceFlatten\" removes newlines and extra spaces in a sentence, and \"englishTopicModeling\" tokenizes sentences using TfidfVectorizer from sklearn library to perform English topic modeling. It takes a list of sentences, sets parameters for n-gram range and the number of topics, and converts the data into TF-IDF vectors.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/pyjom/languagetoolbox.py\":194-229",
            "content": "        Stem_words.append(rootWord)\n    return Stem_words\ndef sentenceFlatten(sentence, padding=\" \"):\n    assert len(padding) == 1\n    assert type(padding) == str\n    for x in \"\\n\\r\\t\":\n        sentence = sentence.replace(x, padding)\n    while True:\n        if padding * 2 in sentence:\n            sentence = sentence.replace(padding * 2, padding)\n        else:\n            break\n    sentence = sentence.strip()\n    return sentence\ndef englishTopicModeling(sentences, n_top_words=10, ngram_range=(1, 2), n_components=5):\n    try:\n        dataList = []\n        for sentence in sentences:\n            sentence = sentenceFlatten(sentence)\n            row = englishSentencePreprocessing(sentence)\n            if len(row) > 0:\n                elem = \" \".join(row)\n                dataList.append(elem)\n        data = \"\\n\".join(dataList)\n        from sklearn.feature_extraction.text import TfidfVectorizer\n        # \u521b\u5efa\u4e00\u4e2aCountVectoerizer\u5b9e\u4f8b\n        tfidf = TfidfVectorizer(ngram_range=ngram_range)\n        # \u6253\u5f00\u521a\u521a\u4fdd\u5b58\u7684txt\u6587\u6863\n        from io import StringIO"
        },
        {
            "comment": "This code snippet is importing necessary libraries and defining a function to process text data. It uses CountVectorizer to convert text into numerical features, then applies LatentDirichletAllocation for topic modeling on the transformed data. If there's an exception during processing, it prints the error traceback and returns an empty list. Additionally, it defines another function getChineseStopWords which reads Chinese stopwords from different files and returns them as a list. The function getChineseStopWords is decorated with @lru_cache for performance optimization.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/pyjom/languagetoolbox.py\":231-268",
            "content": "        f = StringIO(data)\n        # \u4f7f\u7528CountVectorizer\u62df\u5408\u6570\u636e\n        x_train = tfidf.fit_transform(f)\n        from sklearn.decomposition import LatentDirichletAllocation\n        lda = LatentDirichletAllocation(n_components=n_components)\n        lda.fit(x_train)\n        topics = get_topics(lda, tfidf.get_feature_names(), n_top_words)\n    except:\n        import traceback\n        traceback.print_exc()\n        topics = []\n    return topics\nfrom functools import lru_cache\nfrom lazero.utils.logger import traceError\n# import os\n@lru_cache(maxsize=1)\ndef getChineseStopWords(\n    stopwordFileList=[\n        \"/root/Desktop/works/pyjom/tests/stopwords/chinese_stopwords.txt\",\n        \"/root/Desktop/works/pyjom/tests/stopwords/stopwords-zh/stopwords-zh.json\",\n    ]\n):\n    import json\n    stopwords = []\n    for filename in stopwordFileList:\n        # if os.path.exists(filename) and os.path.isfile(filename):\n        try:\n            with open(filename, \"r\") as f:\n                content = f.read()\n            if filename.endswith(\".json\"):"
        },
        {
            "comment": "The code loads stop words from a JSON file or provides them as a list, handles exceptions for reading and parsing errors, and returns the set of unique stop words. The chineseSentencePreprocessing function uses Jieba to tokenize Chinese sentences, filtering out punctuation and stop words, and returns a list of remaining words. The chineseTopicModeling function performs topic modeling on given sentences with specified parameters.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/pyjom/languagetoolbox.py\":269-305",
            "content": "                try:\n                    mList = json.loads(content)\n                    assert type(mList) == list\n                    stopwords += mList\n                except:\n                    traceError(_breakpoint=True)\n            else:\n                mList = content.split(\"\\n\")\n                mList = [x.replace(\"\\n\", \"\").strip() for x in mList]\n                mList = [x for x in mList if len(x) > 0]\n                stopwords += mList\n        except:\n            traceError(_breakpoint=True)\n    return list(set(stopwords))\ndef chineseSentencePreprocessing(sentence):\n    import jieba\n    import string\n    from zhon.hanzi import punctuation\n    chinese_stopwords = getChineseStopWords()\n    words = jieba.lcut(sentence)\n    rows = []\n    for word in words:\n        word = word.strip()\n        if word in punctuation:\n            continue\n        elif word in string.punctuation:\n            continue\n        elif word in chinese_stopwords:\n            continue\n        rows.append(word)\n    return rows\ndef chineseTopicModeling(sentences, n_top_words=10, ngram_range=(1, 2), n_components=5):"
        },
        {
            "comment": "This code performs text preprocessing and topic modeling. It flattens sentences, applies Chinese sentence processing, creates a TfidfVectorizer to transform the data, fits it with CountVectorizer, then uses LatentDirichletAllocation for topic modeling. It handles potential exceptions by printing traceback and returns an empty list if error occurs.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/pyjom/languagetoolbox.py\":306-342",
            "content": "    try:\n        dataList = []\n        for sentence in sentences:\n            sentence = sentenceFlatten(sentence)\n            row = chineseSentencePreprocessing(sentence)\n            if len(row) > 0:\n                elem = \" \".join(row)\n                dataList.append(elem)\n        data = \"\\n\".join(dataList)\n        from sklearn.feature_extraction.text import TfidfVectorizer\n        # \u521b\u5efa\u4e00\u4e2aCountVectoerizer\u5b9e\u4f8b\n        tfidf = TfidfVectorizer(ngram_range=ngram_range)\n        # \u6253\u5f00\u521a\u521a\u4fdd\u5b58\u7684txt\u6587\u6863\n        from io import StringIO\n        f = StringIO(data)\n        # \u4f7f\u7528CountVectorizer\u62df\u5408\u6570\u636e\n        x_train = tfidf.fit_transform(f)\n        from sklearn.decomposition import LatentDirichletAllocation\n        lda = LatentDirichletAllocation(n_components=n_components)\n        lda.fit(x_train)\n        topics = get_topics(lda, tfidf.get_feature_names(), n_top_words)\n    except:\n        import traceback\n        traceback.print_exc()\n        topics = []\n    return topics\n########################[PREPROCESSING & TOPIC MODELING]#########################"
        },
        {
            "comment": "This function, chineseParaphraserAPI, takes a string input and uses a list of API providers to obtain a paraphrased version of the input. It utilizes the requests library for making HTTP requests. The getClueAIClient function is a cached wrapper for initializing a clueai Client with an optional apiKey parameter.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/pyjom/languagetoolbox.py\":345-384",
            "content": "from typing import Literal\n########################[PARAPHRASING]########################\ndef chineseParaphraserAPI(\n    content: str,\n    debug: bool = False,\n    target_id: int = 0,\n    timeout: int = 10,\n    providers: list[str] = [\n        \"http://www.wzwyc.com/api.php?key=\",\n        \"http://ai.guiyigs.com/api.php?key=\",\n    ],  # it is about to close! fuck. \"\u672c\u7ad9\u4e8e2023\u5e742\u670819\u65e5\u5173\u7ad9\" buy code from \"1900373358\"\n):\n    import requests\n    target = providers[target_id]  # all the same?\n    data = {\"info\": content}\n    # target = \"http://www.xiaofamaoai.com/result.php\"\n    # xfm_uid = \"342206661e655450c1c37836d23dc3eb\"\n    # data = {\"contents\":content, \"xfm_uid\":xfm_uid, \"agreement\":\"on\"}\n    # nothing? fuck?\n    r = requests.post(target, data=data, timeout=timeout)\n    output = r.text\n    success = output.strip() != content.strip()\n    if debug:\n        print(output)\n    return output, success\nimport clueai\n@lru_cache(maxsize=1)\ndef getClueAIClient(apiKey: str):\n    if apiKey == \"\":\n        return clueai.Client(\"\", check_api_key=False)"
        },
        {
            "comment": "This code defines a function named `clueAIParaphraser` that returns a paraphrased text using the ClueAI API. It requires an API key, a title to paraphrase, optional generate_config parameters for the AI model, and a debug flag. The function generates a prediction from the input prompt, extracts the generated text, checks if the original title and predicted text are different (success), and optionally prints the prediction and success status if debugging is enabled.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/pyjom/languagetoolbox.py\":385-422",
            "content": "    else:\n        return clueai.Client(apiKey)\ndef clueAIParaphraser(\n    title: str,\n    apiKey: str = \"\",\n    generate_config: dict = {\n        \"do_sample\": True,\n        \"top_p\": 0.8,\n        \"max_length\": 128,  # notice! not too long.\n        \"min_length\": 5,\n        \"length_penalty\": 1.0,\n        \"num_beams\": 1,\n    },\n    prompt_template: str = \"\"\"\n\u751f\u6210\u4e0e\u4e0b\u5217\u6587\u5b57\u76f8\u540c\u610f\u601d\u7684\u53e5\u5b50\uff1a\n{}\n\u7b54\u6848\uff1a\n\"\"\",\n    debug: bool = False,\n):\n    cl = getClueAIClient(apiKey)  # good without API key\n    prompt = prompt_template.format(title)  # shit.\n    # generate a prediction for a prompt\n    # \u5982\u679c\u9700\u8981\u81ea\u7531\u8c03\u6574\u53c2\u6570\u81ea\u7531\u91c7\u6837\u751f\u6210\uff0c\u6dfb\u52a0\u989d\u5916\u53c2\u6570\u4fe1\u606f\u8bbe\u7f6e\u65b9\u5f0f\uff1agenerate_config=generate_config\n    prediction = cl.generate(\n        model_name=\"clueai-base\", prompt=prompt, generate_config=generate_config\n    )\n    # \u9700\u8981\u8fd4\u56de\u5f97\u5206\u7684\u8bdd\uff0c\u6307\u5b9areturn_likelihoods=\"GENERATION\"\n    output = prediction.generations[0].text\n    success = title.strip() != output.strip()\n    if debug:\n        # print the predicted text\n        print(\"prediction: {}\".format(output))\n        print(\"paraphrase success?\", success)\n    return output, success"
        },
        {
            "comment": "This code imports PaddleHub module, defines functions for getting Baidu translation and recognition models using LRU cache, sets API sleep time and lock file path, and includes functions for language detection and translation using Baidu API.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/pyjom/languagetoolbox.py\":425-465",
            "content": "import paddlehub as hub\n@lru_cache(maxsize=1)\ndef getBaiduLanguageTranslationModel():\n    language_translation_model = hub.Module(name=\"baidu_translate\")\n    return language_translation_model\n@lru_cache(maxsize=1)\ndef getBaiduLanguageRecognitionModel():\n    language_recognition_model = hub.Module(name=\"baidu_language_recognition\")\n    return language_recognition_model\nBAIDU_API_SLEEP_TIME = 1\nBAIDU_TRANSLATOR_LOCK_FILE = (\n    \"/root/Desktop/works/pyjom/tests/karaoke_effects/baidu_translator.lock\"\n)\ndef baidu_lang_detect(\n    content: str, sleep=BAIDU_API_SLEEP_TIME, lock_file=BAIDU_TRANSLATOR_LOCK_FILE\n):  # target language must be chinese.\n    import filelock\n    lock = filelock.FileLock(lock_file)\n    with lock:\n        import time\n        time.sleep(sleep)\n        language_recognition_model = getBaiduLanguageRecognitionModel()\n        langid = language_recognition_model.recognize(content)\n        return langid\ndef baidu_translate(\n    content: str,\n    source: str,\n    target: str,\n    sleep: int = BAIDU_API_SLEEP_TIME,"
        },
        {
            "comment": "This function utilizes Baidu's language translation model to paraphrase input content by translating it through a randomly chosen intermediate language from the provided list. The intermediate languages include Chinese, English, and Japanese. It uses a FileLock for synchronization when accessing the Baidu translation model, allowing safe parallel use in multi-threaded environments. The target language is detected using baidu_lang_detect function.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/pyjom/languagetoolbox.py\":466-504",
            "content": "    lock_file: str = BAIDU_TRANSLATOR_LOCK_FILE,\n):  # target language must be chinese.\n    import filelock\n    lock = filelock.FileLock(lock_file)\n    with lock:\n        import time\n        time.sleep(sleep)\n        language_translation_model = getBaiduLanguageTranslationModel()\n        translated_content = language_translation_model.translate(\n            content, source, target\n        )\n        return translated_content\nfrom typing import Iterable, Union\nimport random\ndef baiduParaphraserByTranslation(\n    content: str,\n    debug: bool = False,\n    paraphrase_depth: Union[\n        int, Iterable\n    ] = 1,  # only 1 intermediate language, default.\n    suggested_middle_languages: list[str] = [\n        \"zh\",\n        \"en\",\n        \"jp\",\n    ],  # english, japanese, chinese\n):\n    if issubclass(type(paraphrase_depth), Iterable):\n        paraphrase_depth = random.choice(paraphrase_depth)\n    target_language_id = baidu_lang_detect(content)\n    all_middle_languages = list(set(suggested_middle_languages + [target_language_id]))"
        },
        {
            "comment": "This code is generating a multi-level paraphrase by randomly selecting languages from a list of middle languages, excluding the target and current language. It ensures that the paraphrase_depth is greater than 1 before proceeding to select intermediate languages. The chosen middle language's content is translated using baidu_translate function. The selected language ID and content are added to the respective lists for each loop iteration, resulting in a multi-level paraphrase.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/pyjom/languagetoolbox.py\":506-532",
            "content": "    assert paraphrase_depth > 0\n    if paraphrase_depth > 1:\n        assert len(all_middle_languages) >= 3\n    current_language_id = target_language_id\n    middle_content = content\n    head_tail_indexs = set([0, paraphrase_depth - 1])\n    intermediate_languages = []\n    for loop_id in range(paraphrase_depth):\n        forbid_langs = set([current_language_id])\n        if loop_id in head_tail_indexs:\n            forbid_langs.add(target_language_id)\n        non_target_middle_languages = [\n            langid for langid in all_middle_languages if langid not in forbid_langs\n        ]\n        if debug:\n            print(f\"INDEX: {loop_id} INTERMEDIATE LANGS: {non_target_middle_languages}\")\n        middle_language_id = random.choice(non_target_middle_languages)\n        middle_content = baidu_translate(\n            middle_content, source=current_language_id, target=middle_language_id\n        )\n        current_language_id = middle_language_id\n        intermediate_languages.append(middle_language_id)\n    output_content = baidu_translate("
        },
        {
            "comment": "The code defines a function `paraphraser` which takes in a content string, specifies the method of paraphrasing (clueai_free, cn_nlp_online, baidu_translator) and an optional debug mode. The implemented methods are checked before proceeding to ensure that only valid inputs are accepted. Empty strings are handled with a return statement. Within a try block, the function calls the corresponding paraphrasing method based on the specified input and returns the output along with a boolean value indicating if paraphrase was successful.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/pyjom/languagetoolbox.py\":533-558",
            "content": "        middle_content, source=current_language_id, target=target_language_id\n    )\n    success = output_content.strip() != content.strip()\n    if debug:\n        print(\"SOURCE LANGUAGE:\", target_language_id)\n        print(\"USING INTERMEDIATE LANGUAGES:\", intermediate_languages)\n        print(\"PARAPHRASED:\", output_content)\n        print(\"paraphrase success?\", success)\n    return output_content, success\ndef paraphraser(\n    content: str,\n    method: Literal[\"clueai_free\", \"cn_nlp_online\", \"baidu_translator\"] = \"clueai_free\",\n    debug: bool = False,\n    configs: dict = {},\n):  # you could add some translation based methods.\n    implementedMethods = [\"clueai_free\", \"cn_nlp_online\", \"baidu_translator\"]\n    if method not in implementedMethods:\n        raise NotImplementedError(\"method '%s' not implemented\")\n    if content.strip() == \"\":\n        return content, True  # to protect paraphrasers.\n    try:\n        if method == \"clueai_free\":\n            output, success = clueAIParaphraser(content, debug=debug, **configs)"
        },
        {
            "comment": "This code checks the method for paraphrasing and executes the corresponding function. If the method is not implemented, it raises a NotImplementedError. If there's any other exception, it prints the traceback and returns the original content with failure signal.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/pyjom/languagetoolbox.py\":559-580",
            "content": "        elif method == \"cn_nlp_online\":\n            output, success = chineseParaphraserAPI(content, debug=debug, **configs)\n        elif method == \"baidu_translator\":\n            output, success = baiduParaphraserByTranslation(\n                content, debug=debug, **configs\n            )\n        # you should not be here.\n        else:\n            raise NotImplementedError(\"method '%s' not implemented\")\n        return output, success\n    except NotImplementedError as e:\n        raise e\n    except:\n        import traceback\n        traceback.print_exc()\n        print(\"Failed to paraphrase content using method '%s'\" % method)\n        print(\"Returning original content and failed signal.\")\n        return content, False\n########################[PARAPHRASING]########################"
        }
    ]
}