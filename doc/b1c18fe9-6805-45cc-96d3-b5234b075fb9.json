{
    "summary": "The code initializes LTP models for NLP tasks, offering functions for segmentation, part-of-speech tagging, named entity recognition, and dependency syntax parsing. It uses PyLTL to extract subject-predicate-object triples from sentences, identifies relationships, and appends them to Dynamic_relation if applicable.",
    "details": [
        {
            "comment": "This code initializes the necessary LTP (Language Technology Platform) models for Natural Language Processing tasks. It sets the LTP data directory, loads and initializes models for word segmentation, part-of-speech tagging, named entity recognition, and parsing. The logger is configured to provide status updates during the loading process.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/tests/title_cover_generator/pyltp_server.py\":0-34",
            "content": "# !/usr/bin/env python3\n# -*- coding:utf-8 -*-\n# create on 5/26/20\n__author__ = \"sinsa\"\nimport os\nimport logging\nfrom logging import info, error, warn\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s - PID:%(process)d - %(levelname)s: %(message)s\",\n)\nfrom pyltp import Segmentor\nfrom pyltp import Postagger\nfrom pyltp import NamedEntityRecognizer\nfrom pyltp import Parser\nfrom pyltp import SentenceSplitter\nclass LTP_MODEL:\n    def __init__(self):\n        LTP_DATA_DIR = \"./pyltp_data/ltp_data_v3.4.0\"  # ltp\u6a21\u578b\u76ee\u5f55\u7684\u8def\u5f84\n        info(\"loading models ...\")\n        self.cws_model_path = os.path.join(\n            LTP_DATA_DIR, \"cws.model\"\n        )  # \u5206\u8bcd\u6a21\u578b\u8def\u5f84\uff0c\u6a21\u578b\u540d\u79f0\u4e3a`cws.model`\n        self.segmentor = Segmentor(self.cws_model_path)  # \u521d\u59cb\u5316\u5b9e\u4f8b\n        # self.segmentor.load(self.cws_model_path)  # \u52a0\u8f7d\u6a21\u578b\n        info(\"has loaded \u5206\u8bcd\u6a21\u578b\")\n        self.pos_model_path = os.path.join(\n            LTP_DATA_DIR, \"pos.model\"\n        )  # \u8bcd\u6027\u6807\u6ce8\u6a21\u578b\u8def\u5f84\uff0c\u6a21\u578b\u540d\u79f0\u4e3a`pos.model`\n        self.postaggers = Postagger(self.pos_model_path)  # \u521d\u59cb\u5316\u5b9e\u4f8b"
        },
        {
            "comment": "The code loads three models (POS tagging, Named Entity Recognition, and Dependency Parsing) and initializes corresponding recognizers or parsers for each model. It also provides methods to release the models when finished and split a sentence into individual sentences.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/tests/title_cover_generator/pyltp_server.py\":35-60",
            "content": "        # self.postaggers.load(self.pos_model_path)  # \u52a0\u8f7d\u6a21\u578b\n        info(\"has loaded \u8bcd\u6027\u6807\u6ce8\u6a21\u578b\")\n        self.ner_model_path = os.path.join(\n            LTP_DATA_DIR, \"ner.model\"\n        )  # \u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u6a21\u578b\u8def\u5f84\uff0c\u6a21\u578b\u540d\u79f0\u4e3a`pos.model`\n        self.recognizer = NamedEntityRecognizer(self.ner_model_path)  # \u521d\u59cb\u5316\u5b9e\u4f8b\n        # self.recognizer.load(self.ner_model_path)  # \u52a0\u8f7d\u6a21\u578b\n        info(\"has loaded \u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u6a21\u578b\")\n        self.par_model_path = os.path.join(\n            LTP_DATA_DIR, \"parser.model\"\n        )  # \u4f9d\u5b58\u53e5\u6cd5\u5206\u6790\u6a21\u578b\u8def\u5f84\uff0c\u6a21\u578b\u540d\u79f0\u4e3a`parser.model`\n        self.parser = Parser(self.par_model_path)  # \u521d\u59cb\u5316\u5b9e\u4f8b\n        # self.parser.load(self.par_model_path)  # \u52a0\u8f7d\u6a21\u578b\n        info(\"has loaded \u4f9d\u5b58\u53e5\u6cd5\u5206\u6790\u6a21\u578b\")\n    def __release__(self):\n        self.segmentor.release()  # \u91ca\u653e\u6a21\u578b\n        self.postaggers.release()  # \u91ca\u653e\u6a21\u578b\n        self.recognizer.release()  # \u91ca\u653e\u6a21\u578b\n        self.parser.release()  # \u91ca\u653e\u6a21\u578b\n    def SplitSentence(self, sentence):\n        sents_list = SentenceSplitter.split(sentence)  # \u5206\u53e5\n        return list(sents_list)\n    def segment(self, input_list):"
        },
        {
            "comment": "This code defines three functions: \"segment\", \"postagger\", and \"NamedEntityRecognizer\". The \"segment\" function takes a list of texts as input, performs segmentation on each text to obtain a list of words, and returns the segmented text as a list of lists. The \"postagger\" function takes a list of texts and performs part-of-speech tagging on each word in the list. It then returns the tagged text as a list of lists. If the \"return_words_list\" parameter is True, it also returns the original words list. The \"NamedEntityRecognizer\" function recognizes named entities in the input texts based on the provided parameters (\"Entity_dist\" and \"repead\").",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/tests/title_cover_generator/pyltp_server.py\":61-89",
            "content": "        \"\"\"\n        \u529f\u80fd\uff1a\u5b9e\u73b0\u5206\u8bcd\u6587\u672c\u7684\u5206\u8bcd\n        \u8fd4\u56de\u503c\uff1a\u6bcf\u4e2a\u6587\u672c\u7684\u5f62\u6210\u4e00\u4e2a\u5217\u8868[['word1','word2'],['word1','word3'],\u2026\u2026]\n        \"\"\"\n        segmented_text_list = []\n        for text in input_list:\n            words = self.segmentor.segment(text)  # \u5206\u8bcd\n            segmented_text_list.append(list(words))\n        return segmented_text_list\n    def postagger(self, input_list, return_words_list=False):\n        \"\"\"\n        \u529f\u80fd\uff1a\u5b9e\u73b0\u6587\u672c\u4e2d\u6bcf\u4e2a\u8bcd\u7684\u8bcd\u6027\u6807\u6ce8\n        \u8fd4\u56de\u503c\uff1a\u6bcf\u4e2a\u6587\u672c\u662f\u4e00\u4e2a\u5217\u8868\uff0c\u5217\u8868\u4e2d\u7684\u6bcf\u4e2a\u8bcd\u4e5f\u662f\u4e2a\u5217\u8868[[['word1',u'O'],['word2',u'O']],[['word2',u'O'],['word5',u'O']],\u2026\u2026]\n        \"\"\"\n        postagger_text_list = []\n        words_list = self.segment(input_list)\n        postags_list = []\n        for words in words_list:\n            postags = self.postaggers.postag(words)  # \u8bcd\u6027\u6807\u6ce8\n            postags_list.append(list(postags))\n            words_postags = list(zip(words, list(postags)))\n            postagger_text_list.append(words_postags)\n        if return_words_list:\n            return words_list, postags_list\n        else:\n            return postagger_text_list\n    def NamedEntityRecognizer(self, input_list, Entity_dist=False, repead=False):"
        },
        {
            "comment": "This code snippet is responsible for identifying named entities in a given text, such as person names, place names, and organization names. It uses the postagger to identify words and their parts of speech (POS) and then applies the recognizer to recognize named entities based on these POS tags. If Entity_dist is set to True, it extracts entities into a dictionary format.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/tests/title_cover_generator/pyltp_server.py\":90-112",
            "content": "        \"\"\"\n        \u529f\u80fd\uff1a\u8bc6\u522b\u6587\u672c\u4e2d\u7684\u547d\u540d\u5b9e\u4f53\uff1a\u5730\u540d\uff0c\u7ec4\u7ec7\u540d\u548c\u673a\u6784\u540d\n        \u53c2\u6570repead\uff1a\u8868\u793a\u662f\u5426\u8fdb\u884c\u53bb\u91cd\u5904\u7406 \uff0c\u9ed8\u8ba4\u662f\u4e0d\u53bb\u91cd\n        \u53c2\u6570Entity_dist\uff1a\u8868\u793a\u6bcf\u4e2a\u6587\u672c\uff0c\u8fd4\u56de\u7684\u8bc6\u522b\u540e\u7684\u5217\u8868\uff0c\u8fd8\u662f\u62bd\u53d6\u540e\u7684\u5b9e\u4f53\u5b57\u5178\uff0c\u9ed8\u8ba4\u8fd4\u56de\u7684\u662f\u5217\u8868\n        \u8fd4\u56de\u503c\u7684\u5f62\u5f0f\uff1a1.[[['word1',u'O'],['word2',u'O'],['word3',u'O']],[['word2',u'O'],['word3',u'O'],['word4',u'O']],\u2026\u2026]\n                        2.[{'person':[],'place':[],'organization':[]},{'person':[],'place':[],'organization':[]},{'person':[],'place':[],'organization':[]},\u2026\u2026]\n        \"\"\"\n        words_list, postags_list = self.postagger(input_list, return_words_list=True)\n        entity_text_list = []\n        for words, postags in zip(words_list, postags_list):\n            netags = self.recognizer.recognize(\n                words, postags\n            )  # \u547d\u540d\u5b9e\u4f53\u8bc6\u522b \u4eba\u540d\uff08Nh\uff09\u3001\u5730\u540d\uff08Ns\uff09\u3001\u673a\u6784\u540d\uff08Ni\uff09\n            text = list(zip(words, netags))\n            entity_text_list.append(text)\n        if Entity_dist:\n            extract_entity_list = []\n            for words_entity_note_list in entity_text_list:\n                extract_entity_list.append(\n                    self.get_entity_dict(words_entity_note_list, repead)"
        },
        {
            "comment": "This code segment is part of a class method that identifies and categorizes named entities such as persons, places, and organizations from a given list. The code iterates through the list of words along with their corresponding entity tags (O, S, B, I, E) and adds them to separate lists based on the type of entity they represent. If the repead parameter is True, it performs deduplication on the final result.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/tests/title_cover_generator/pyltp_server.py\":113-150",
            "content": "                )\n            return extract_entity_list\n        else:\n            return entity_text_list\n    def get_entity_dict(self, words_entity_note_list, repead):\n        \"\"\"\n        \u529f\u80fd\uff1a\u6839\u636e\u5b9e\u4f53\u8bc6\u522b\u7684\u6807\u5fd7\uff0c\u7edf\u8ba1\u6587\u672c\u4e2d\u7684\u547d\u540d\u5b9e\u4f53\n        \u53c2\u6570repead\uff1a\u8868\u793a\u662f\u5426\u8fdb\u884c\u53bb\u91cd\u5904\u7406 \uff0c\u9ed8\u8ba4\u662f\u4e0d\u53bb\u91cd\n        \u8fd4\u56de\u503c\uff1a{'person':[],'place':[],'organization':[]}\n        \"\"\"\n        \"\"\"\n        O\uff1a\u8fd9\u4e2a\u8bcd\u4e0d\u662fNE\n        S\uff1a\u8fd9\u4e2a\u8bcd\u5355\u72ec\u6784\u6210\u4e00\u4e2aNE\n        B\uff1a\u8fd9\u4e2a\u8bcd\u4e3a\u4e00\u4e2aNE\u7684\u5f00\u59cb\n        I\uff1a\u8fd9\u4e2a\u8bcd\u4e3a\u4e00\u4e2aNE\u7684\u4e2d\u95f4\n        E\uff1a\u8fd9\u4e2a\u8bcd\u4f4d\u4e00\u4e2aNE\u7684\u7ed3\u5c3e\n        Nh\uff1a\u4eba\u540d\n        Ni\uff1a\u673a\u6784\u540d\n        Ns\uff1a\u5730\u540d\n        \"\"\"\n        name_entity_dist = {}\n        # \u5b58\u50a8\u4e0d\u540c\u5b9e\u4f53\u7684\u5217\u8868\n        name_entity_list = []\n        place_entity_list = []\n        organization_entity_list = []\n        ntag_E_Nh = \"\"\n        ntag_E_Ni = \"\"\n        ntag_E_Ns = \"\"\n        for word, ntag in words_entity_note_list:\n            # print word+\"/\"+ntag,\n            if ntag[0] != \"O\":\n                if ntag[0] == \"S\":\n                    if ntag[-2:] == \"Nh\":\n                        name_entity_list.append(word)\n                    elif ntag[-2:] == \"Ni\":\n                        organization_entity_list.append(word)"
        },
        {
            "comment": "The code is segmenting named entities (name and organization) using the NER (Named Entity Recognition) model. It appends words to separate variables based on their tags, forming name and organization lists when encountering \"Nh\" or \"Ni\". If no entity is detected, it simply adds the word to the place entity list.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/tests/title_cover_generator/pyltp_server.py\":151-174",
            "content": "                    else:\n                        place_entity_list.append(word)\n                elif ntag[0] == \"B\":\n                    if ntag[-2:] == \"Nh\":\n                        ntag_E_Nh = ntag_E_Nh + word\n                    elif ntag[-2:] == \"Ni\":\n                        ntag_E_Ni = ntag_E_Ni + word\n                    else:\n                        ntag_E_Ns = ntag_E_Ns + word\n                elif ntag[0] == \"I\":\n                    if ntag[-2:] == \"Nh\":\n                        ntag_E_Nh = ntag_E_Nh + word\n                    elif ntag[-2:] == \"Ni\":\n                        ntag_E_Ni = ntag_E_Ni + word\n                    else:\n                        ntag_E_Ns = ntag_E_Ns + word\n                else:\n                    if ntag[-2:] == \"Nh\":\n                        ntag_E_Nh = ntag_E_Nh + word\n                        name_entity_list.append(ntag_E_Nh)\n                        ntag_E_Nh = \"\"\n                    elif ntag[-2:] == \"Ni\":\n                        ntag_E_Ni = ntag_E_Ni + word\n                        organization_entity_list.append(ntag_E_Ni)"
        },
        {
            "comment": "This code defines a function `name_entity_dist` that handles named entity recognition and extraction. It identifies named entities (person, organization, place) and stores them in separate lists. The function then adds these lists to a dictionary called `name_entity_dist`, which is returned at the end. Additionally, there's another function `SyntaxParser` that performs dependency syntax parsing on the input list and returns a list of parsed relations between words.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/tests/title_cover_generator/pyltp_server.py\":175-197",
            "content": "                        ntag_E_Ni = \"\"\n                    else:\n                        ntag_E_Ns = ntag_E_Ns + word\n                        place_entity_list.append(ntag_E_Ns)\n                        ntag_E_Ns = \"\"\n        if repead:\n            name_entity_dist[\"person\"] = list(set(name_entity_list))\n            name_entity_dist[\"organization\"] = list(set(organization_entity_list))\n            name_entity_dist[\"place\"] = list(set(place_entity_list))\n        else:\n            name_entity_dist[\"person\"] = name_entity_list\n            name_entity_dist[\"organization\"] = organization_entity_list\n            name_entity_dist[\"place\"] = place_entity_list\n        return name_entity_dist\n    def SyntaxParser(self, input_list, return_words_pos=False):\n        \"\"\"\n        # head = parent+1\n        # relation = relate  \u53ef\u4ee5\u4ece\u4e2d\u95f4\u62bd\u53d6head \u548c relation \u6784\u6210LTP \u7684\u6807\u51c6\u8f93\u51fa\uff0c\u4f46\u662f\u4e3a\u4e86\u6839\u636e\u81ea\u5df1\u7684\u60c5\u51b5\uff0c\u76f4\u63a5\u8f93\u51fa\u8fd4\u56de\u7684\u5168\u90e8\u7684\u4fe1\u606f\n        \u529f\u80fd\uff1a\u5b9e\u73b0\u4f9d\u5b58\u53e5\u6cd5\u5206\u6790\n        \u8fd4\u56de\u503c\uff1a\u6bcf\u4e2a\u6587\u672c\u7684\u5f62\u6210\u4e00\u4e2a\u5217\u8868\n        [[{u'relate': u'WP', u'cont': u'\\uff0c', u'id': 4, u'parent': 3, u'pos': u'wp'},{u'relate': u'RAD', u'cont': u'\\u7684', u'id': 1, u'parent': 0, u'pos': u'u'}],\u2026\u2026]"
        },
        {
            "comment": "The code is performing syntax parsing using a parser. It takes an input list of words and their corresponding part-of-speech tags, then applies the parser to generate a syntactic parse tree for each word in the input list. The resulting parse trees are stored as a list of dictionaries with information about each word's id, content, part-of-speech tag, parent, and relation. If 'return_words_pos' is True, it returns the words list, postags list, and syntaxparser_text_list. Otherwise, it only returns the syntaxparser_text_list.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/tests/title_cover_generator/pyltp_server.py\":198-228",
            "content": "        \"\"\"\n        words_list, postags_list = self.postagger(input_list, return_words_list=True)\n        syntaxparser_text_list = []\n        for words, postags in zip(words_list, postags_list):\n            arcs = self.parser.parse(words, postags)  # \u53e5\u6cd5\u5206\u6790\n            # res = [(arc.head, arc.relation) for arc in arcs]\n            res = [arc for arc in arcs] # arguable.\n            # for arc in arcs:\n            #     print(arc)\n            # breakpoint()\n            text = []\n            for i in range(len(words)):\n                tt = {\n                    \"id\": i,\n                    \"cont\": words[i],\n                    \"pos\": postags[i],\n                    \"parent\": res[i][0],\n                    \"relate\": res[i][1],\n                }\n                text.append(tt)\n            syntaxparser_text_list.append(text)\n        if return_words_pos:\n            return words_list, postags_list, syntaxparser_text_list\n        else:\n            return syntaxparser_text_list\n    def triple_extract(self, intput_list):\n        \"\"\""
        },
        {
            "comment": "This function performs triplet extraction for a given sentence. It initializes various lists for different relationships and then extracts words, postags, arcs (syntax), and netags (named entities) using the input list. Finally, it builds a dictionary of child relationships from the extracted data.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/tests/title_cover_generator/pyltp_server.py\":229-257",
            "content": "        \u529f\u80fd: \u5bf9\u4e8e\u7ed9\u5b9a\u7684\u53e5\u5b50\u8fdb\u884c\u4e8b\u5b9e\u4e09\u5143\u7ec4\u62bd\u53d6\n        Args:\n            sentence: \u8981\u5904\u7406\u7684\u8bed\u53e5\n                        \u5f62\u5f0f\u662f\uff1a'\u771f\u5b9e\u7684\u53e5\u5b50'\n        \"\"\"\n        Subjective_guest = []  # \u4e3b\u8c13\u5bbe\u5173\u7cfb(e1,r,e2)\n        Dynamic_relation = []  # \u52a8\u5bbe\u5173\u7cfb\n        Guest = []  # \u4ecb\u5bbe\u5173\u7cfb\n        Name_entity_relation = []  # \u547d\u540d\u5b9e\u4f53\u4e4b\u95f4\u7684\u5173\u7cfb\n        # \u5206\u8bcd\u540e\u8bcd\u7684\u5217\u8868 words\uff0c\u8bcd\u6027\u5217\u8868 postags\uff0c\u5b9e\u4f53\u6807\u5fd7\u5217\u8868 netags\uff0c\u8bed\u6cd5\u5206\u6790\u5217\u8868 arcs\n        words = []\n        postags = []\n        netags = []\n        arcs = []\n        syntaxparser_text_list = self.SyntaxParser(intput_list)\n        entity_list = self.NamedEntityRecognizer(intput_list)\n        for words_property_list in syntaxparser_text_list[0]:\n            words.append(words_property_list[\"cont\"])\n            postags.append(words_property_list[\"pos\"])\n            arcs.append(\n                {\n                    \"head\": words_property_list[\"parent\"],\n                    \"relation\": words_property_list[\"relate\"],\n                }\n            )\n        for words_entity_list in entity_list[0]:\n            netags.append(words_entity_list[1])\n        child_dict_list = self.build_parse_child_dict(words, postags, arcs)"
        },
        {
            "comment": "This code is extracting subject-predicate-object (SPO) triples from a natural language sentence using PyLTP library. It identifies the verb as the center of the triple and checks for two possible structures: \"SBV\" followed by \"VOB\" or \"ATT\" relation after the verb. The code fills in the subject, predicate, and object entities based on the identified positions in the sentence.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/tests/title_cover_generator/pyltp_server.py\":259-283",
            "content": "        for index in range(len(postags)):\n            # \u62bd\u53d6\u4ee5\u8c13\u8bcd\u4e3a\u4e2d\u5fc3\u7684\u4e8b\u5b9e\u4e09\u5143\u7ec4\n            if postags[index] == \"v\":\n                child_dict = child_dict_list[index]\n                # \u4e3b\u8c13\u5bbe\n                if \"SBV\" in child_dict and \"VOB\" in child_dict:\n                    e1 = self.complete_e(\n                        words, postags, child_dict_list, child_dict[\"SBV\"][0]\n                    )\n                    r = words[index]\n                    e2 = self.complete_e(\n                        words, postags, child_dict_list, child_dict[\"VOB\"][0]\n                    )\n                    Subjective_guest.append((e1, r, e2))\n                # \u5b9a\u8bed\u540e\u7f6e\uff0c\u52a8\u5bbe\u5173\u7cfb\n                if arcs[index][\"relation\"] == \"ATT\":\n                    if \"VOB\" in child_dict:\n                        e1 = self.complete_e(\n                            words, postags, child_dict_list, arcs[index][\"head\"] - 1\n                        )\n                        r = words[index]\n                        e2 = self.complete_e(\n                            words, postags, child_dict_list, child_dict[\"VOB\"][0]"
        },
        {
            "comment": "This code checks for a specific relationship between the subject, verb, object, and complement in a sentence. It appends the relationship (e1, r, e2) to Dynamic_relation if it meets certain conditions such as not containing an existing temporary string or being part of the original text.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/tests/title_cover_generator/pyltp_server.py\":284-305",
            "content": "                        )\n                        temp_string = r + e2\n                        if temp_string == e1[: len(temp_string)]:\n                            e1 = e1[len(temp_string) :]\n                        if temp_string not in e1:\n                            Dynamic_relation.append((e1, r, e2))\n                # \u542b\u6709\u4ecb\u5bbe\u5173\u7cfb\u7684\u4e3b\u8c13\u52a8\u8865\u5173\u7cfb\n                if \"SBV\" in child_dict and \"CMP\" in child_dict:\n                    # e1 = words[child_dict['SBV'][0]]\n                    e1 = self.complete_e(\n                        words, postags, child_dict_list, child_dict[\"SBV\"][0]\n                    )\n                    cmp_index = child_dict[\"CMP\"][0]\n                    r = words[index] + words[cmp_index]\n                    if \"POB\" in child_dict_list[cmp_index]:\n                        e2 = self.complete_e(\n                            words,\n                            postags,\n                            child_dict_list,\n                            child_dict_list[cmp_index][\"POB\"][0],\n                        )"
        },
        {
            "comment": "This code attempts to extract named entity triples. It checks if the current tag is a start or begin tag, and then extracts the named entity based on that. If it meets specific conditions involving \"ATT\" relation and certain postags, it completes the entity and checks if the extracted entity is in the result.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/tests/title_cover_generator/pyltp_server.py\":306-330",
            "content": "                        Guest.append((e1, r, e2))\n            # \u5c1d\u8bd5\u62bd\u53d6\u547d\u540d\u5b9e\u4f53\u6709\u5173\u7684\u4e09\u5143\u7ec4\n            if netags[index][0] == \"S\" or netags[index][0] == \"B\":\n                ni = index\n                if netags[ni][0] == \"B\":\n                    while netags[ni][0] != \"E\":\n                        ni += 1\n                    e1 = \"\".join(words[index : ni + 1])\n                else:\n                    e1 = words[ni]\n                # \u4e0a\u9762\u662f\u62bd\u53d6\u5b9e\u4f53\uff0c\u6ca1\u6709\u5224\u65ad\u662f\u4ec0\u4e48\u7c7b\u578b\u7684\u5b9e\u4f53\u3002\u3002\n                if (\n                    arcs[ni][\"relation\"] == \"ATT\"\n                    and postags[arcs[ni][\"head\"] - 1] == \"n\"\n                    and netags[arcs[ni][\"head\"] - 1] == \"O\"\n                ):\n                    r = self.complete_e(\n                        words, postags, child_dict_list, arcs[ni][\"head\"] - 1\n                    )\n                    if e1 in r:\n                        r = r[(r.index(e1) + len(e1)) :]\n                    if (\n                        arcs[arcs[ni][\"head\"] - 1][\"relation\"] == \"ATT\"\n                        and netags[arcs[arcs[ni][\"head\"] - 1][\"head\"] - 1] != \"O\""
        },
        {
            "comment": "The code defines a function called `build_parse_child_dict` which takes in the words, postags, and arcs of a sentence. It creates a dictionary for each word in the sentence that stores its syntactic dependency children. If a relation word exists between two named entities, it is added to the Name_entity_relation list. The function returns four variables: Subjective_guest, Dynamic_relation, Guest, and Name_entity_relation",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/tests/title_cover_generator/pyltp_server.py\":331-353",
            "content": "                    ):\n                        e2 = self.complete_e(\n                            words,\n                            postags,\n                            child_dict_list,\n                            arcs[arcs[ni][\"head\"] - 1][\"head\"] - 1,\n                        )\n                        mi = arcs[arcs[ni][\"head\"] - 1][\"head\"] - 1\n                        li = mi\n                        if netags[mi][0] == \"B\":\n                            while netags[mi][0] != \"E\":\n                                mi += 1\n                            e = \"\".join(words[li + 1 : mi + 1])\n                            e2 += e\n                        if r in e2:\n                            e2 = e2[(e2.index(r) + len(r)) :]\n                        if r + e2 in sentence:\n                            Name_entity_relation.append((e1, r, e2))\n        return Subjective_guest, Dynamic_relation, Guest, Name_entity_relation\n    def build_parse_child_dict(self, words, postags, arcs):\n        \"\"\"\n        \u529f\u80fd\uff1a\u4e3a\u53e5\u5b50\u4e2d\u7684\u6bcf\u4e2a\u8bcd\u8bed\u7ef4\u62a4\u4e00\u4e2a\u4fdd\u5b58\u53e5\u6cd5\u4f9d\u5b58\u513f\u5b50\u8282\u70b9\u7684\u5b57\u5178"
        },
        {
            "comment": "This function takes in a list of words, their respective parts of speech (postags), and syntactic dependency relations (arcs) as input. It organizes the arcs into a dictionary structure for each word in the list, and returns this dictionary list. The next function aims to further refine or \"complete\" part of the identified entities by recursively calling itself with the appropriate parameters.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/tests/title_cover_generator/pyltp_server.py\":354-382",
            "content": "        Args:\n            words: \u5206\u8bcd\u5217\u8868\n            postags: \u8bcd\u6027\u5217\u8868\n            arcs: \u53e5\u6cd5\u4f9d\u5b58\u5217\u8868\n        \"\"\"\n        child_dict_list = []\n        for index in range(len(words)):\n            child_dict = dict()\n            for arc_index in range(len(arcs)):\n                if arcs[arc_index][\"head\"] == index + 1:\n                    if arcs[arc_index][\"relation\"] in child_dict:\n                        child_dict[arcs[arc_index][\"relation\"]].append(arc_index)\n                    else:\n                        child_dict[arcs[arc_index][\"relation\"]] = []\n                        child_dict[arcs[arc_index][\"relation\"]].append(arc_index)\n            child_dict_list.append(child_dict)\n        return child_dict_list\n    def complete_e(self, words, postags, child_dict_list, word_index):\n        \"\"\"\n        \u529f\u80fd\uff1a\u5b8c\u5584\u8bc6\u522b\u7684\u90e8\u5206\u5b9e\u4f53\n        \"\"\"\n        child_dict = child_dict_list[word_index]\n        prefix = \"\"\n        if \"ATT\" in child_dict:\n            for i in range(len(child_dict[\"ATT\"])):\n                prefix += self.complete_e(\n                    words, postags, child_dict_list, child_dict[\"ATT\"][i]"
        },
        {
            "comment": "This code segment is a part of the Named Entity Recognizer function in a Chinese language processing model. It takes an input list containing sentences, and based on postags (part-of-speech tags), it identifies named entities within the text and returns them. The code snippet handles verbs with \"VOB\" or \"SBV\" child nodes differently by appending prefixes accordingly, and then combines prefix, word, and postfix to generate the final output.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/tests/title_cover_generator/pyltp_server.py\":383-413",
            "content": "                )\n        postfix = \"\"\n        if postags[word_index] == \"v\":\n            if \"VOB\" in child_dict:\n                postfix += self.complete_e(\n                    words, postags, child_dict_list, child_dict[\"VOB\"][0]\n                )\n            if \"SBV\" in child_dict:\n                prefix = (\n                    self.complete_e(\n                        words, postags, child_dict_list, child_dict[\"SBV\"][0]\n                    )\n                    + prefix\n                )\n        return prefix + words[word_index] + postfix\nif __name__ == \"__main__\":\n    # intput_list = [\"\u4e2d\u56fd\u81ea\u79f0\u4e3a\u708e\u9ec4\u5b50\u5b59\u3001\u9f99\u7684\u4f20\u4eba\"]\n    # incorrect name spliters.\n    from commons import sample_data\n    intput_list = sample_data\n    model = LTP_MODEL()\n    input_sentence = \"\u96c5\u751f\u6d3b\u670d\u52a1\u7684\u7269\u4e1a\u7ba1\u7406\u670d\u52a1\u3002\"\n    # print(model.SplitSentence(input_sentence))\n    # print(model.segment(intput_list))\n    # print(model.postagger(intput_list))\n    # print(model.NamedEntityRecognizer(intput_list, Entity_dist=True))\n    print(model.NamedEntityRecognizer(intput_list))"
        },
        {
            "comment": "Extracting triples from input list using the model's triple_extract method, then printing them and releasing resources.",
            "location": "\"/media/root/Toshiba XG3/works/pyjom_doc/src/tests/title_cover_generator/pyltp_server.py\":414-425",
            "content": "    # print(model.SyntaxParser(intput_list))\n    (\n        Subjective_guest,\n        Dynamic_relation,\n        Guest,\n        Name_entity_relation,\n    ) = model.triple_extract(intput_list)\n    print(\"=\" * 30)\n    print(Subjective_guest, Dynamic_relation, Guest, Name_entity_relation)\n    model.__release__()"
        }
    ]
}